---
title: "[Deep Learning Basic] 02_딥러닝 입문(딥러닝과 선형회귀 개념), 신경망(Neural Network) 훈련원리"
categories: [AI, Deep Learning]
tags: [Deep Learning]
---

## 1. 딥러닝이란?

- **전통 머신러닝과 딥러닝 비교**

|          | **전통 머신러닝**                                               | **딥러닝**                                                                                                                                       |
| -------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **특징** | **- 깊은 전문 지식 필요<br />- 특성(feature) 설정이 매우 중요** | **- 도메인 지식과 수학적 지식이 상대적으로 덜 필요<br />- 중요한 특성을 스스로 학습하여 가중치 부여**                                            |
| **적용** | **- 수동으로 선택된 특성을 기반으로 모델을 학습**               | **- 여러 층(layer)을 통해 내부 파라미터를 자동으로 학습<br />- 원시 데이터(raw data)를 거의 그대로 사용 가능 (예: 컴퓨터 비전, 자연어 처리 등)** |

- **딥러닝과 통계학의 비교**

|                 | **통계학**                                           | **딥러닝**                                               |
| --------------- | ---------------------------------------------------- | -------------------------------------------------------- |
| **데이터의 양** | **제한된 데이터로부터 최상의 결과 도출**             | **충분한 양의 데이터가 필요**                            |
| **이상치**      | **이상치는 제거 필요** **누락된 값은 평균으로 대체** | **이상치(outlier) 포함하여 훈련해도 유연하게 처리 가능** |

## 2. 선형 회귀(Linear Regression)

#### 단변수 선형 회귀

> **하나의 변수(원인)이 어떠한 가설의 근거가 됨**

- **가설**: `y = a + bx`
- **목적**: 하나의 독립 변수(x)와 종속 변수(y) 사이의 선형 관계 모델링
- **비용 함수(Cost Function)**: MSE (Mean Squared Error), 모델의 예측 오류 측정
- **성능 지표**: 결정계수 (R²), 값이 1에 가까울수록 모델의 성능이 좋음

#### 다변수 선형 회귀

- **모델**: 두 개 이상의 독립 변수를 포함, 복잡한 실제 상황 모델링 가능

#### 신경망을 이용한 선형 회귀

- **신경망은 비선형성을 모델링할 수 있어, 단변수와 다변수 모두에서 유용**

## 3. 신경망(Neural Network) 훈련원리

#### Gradient Descent (경사하강법)

- **목적: 실제값과 예측값의 차이(손실)를 최소화하는 parameter 발견**
- **방법: 손실 함수의 기울기(gradient)를 계산하고, 그 기울기가 0으로 수렴하도록 parameter 조절**

#### Backpropagation(오차역전파)

- **손실함수를 최소화하는 방향으로 신경망 전체의 파라미터가 업데이트되도록 하는 기법**
  - **각각의 input data에 대하여 각각의 레이어별로 forward pass output 값을 계산**
  - **Output 레이어의 손실함수 값을 계산**
  - **backpropagation을 손실함수의 기울기를 전달계의 레이어로 전달**
  - **error term의 값에 따라 각 레이어의 가중치를 업데이트**

#### 손실함수

- **비용함수, 목적함수 등으로 불림**
- **경사하강법이 가능하도록 미분 가능한 함수를 정의**

##### 대표적 손실 함수

- **선형회귀: MSE**
- **이진분류: Binary Cross-entropy**
  - **y는 0 혹은 1인 경우만 존재**
- **다중분류: Categorical Crossentropy (Softmax Loss)**

## epoch

> **전체 데이터셋이 신경망을 통해 한 번 처리된 것**

- **epoch은 모델 훈련 시 하이퍼파라미터로 횟수 지정**
- **하나의 epoch은 한번에 처리하기 어려운 사이즈이기 때문에 여러 개의 batch로 나누어 처리한다.**
- **파라미터 훈련을 위해서는 여러 번의 epoch 반복이 필요**
- **one epoch 내에서 반복 횟수 = 전체 샘플 사이즈 / 배치 사이즈**
  - **1 epoch = 4 iterations = 2000/ 50**

### Hpyer-parameter

**종류**

- **러닝 레이트**
- **모멘텀 기간**
- **dropout rate - overfitting 방지**
- **배치 사이즈**

**결정 기준**

- **정해진 룰이 없음**
- **유사한 모델 참조**
- **경험에 의한 게싱**
- **Grid search - 종류별로 다 해보는 것**
