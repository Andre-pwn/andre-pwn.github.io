---
title: GCP  - GCP AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [01GCP]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

- [GCP AI](#gcp-ai)
  - [overall](#overall)
  - [Big Data and Machine Learning on GCP](#big-data-and-machine-learning-on-gcp)
    - [user example](#user-example)
  - [Data Engineering for Streaming Data](#data-engineering-for-streaming-data)
    - [Processing](#processing)
      - [user example](#user-example-1)
    - [pipelines design](#pipelines-design)
    - [Visualization with Looker](#visualization-with-looker)
      - [Looker](#looker)
      - [Looker studio](#looker-studio)
  - [Big Data with BigQuery](#big-data-with-bigquery)
    - [Storage and analytics](#storage-and-analytics)
      - [manages the storage and metadata for datasets](#manages-the-storage-and-metadata-for-datasets)
      - [analyzing data](#analyzing-data)
    - [BigQuery ML](#bigquery-ml)
      - [Phases of ML project](#phases-of-ml-project)

ref:
- [coursera - gcp-big-data-ml-fundamentals](https://www.coursera.org/learn/gcp-big-data-ml-fundamentals)

---

# GCP AI

## overall

![Screenshot 2023-09-24 at 23.11.24](/assets/img/post/Screenshot%202023-09-24%20at%2023.11.24.png)

![Screenshot 2023-09-24 at 23.12.05](/assets/img/post/Screenshot%202023-09-24%20at%2023.12.05_bmztwve7b.png)

![Screenshot 2023-09-24 at 23.11.52](/assets/img/post/Screenshot%202023-09-24%20at%2023.11.52_kovi61a36.png)

![Screenshot 2023-09-24 at 23.12.44](/assets/img/post/Screenshot%202023-09-24%20at%2023.12.44_rcs77spq0.png)

![Screenshot 2023-09-24 at 23.12.53](/assets/img/post/Screenshot%202023-09-24%20at%2023.12.53_80yhc94g1.png)

![Screenshot 2023-09-24 at 23.13.06](/assets/img/post/Screenshot%202023-09-24%20at%2023.13.06_7ck3v3dnb.png)

![Screenshot 2023-09-24 at 23.13.43](/assets/img/post/Screenshot%202023-09-24%20at%2023.13.43_7f9vdz0fk.png)

![Screenshot 2023-09-24 at 23.14.05](/assets/img/post/Screenshot%202023-09-24%20at%2023.14.05_2myval2rv.png)

![Screenshot 2023-09-24 at 23.15.02](/assets/img/post/Screenshot%202023-09-24%20at%2023.15.02.png)


- Data and AI have a powerful partnership;
  - data is the foundation of every application integrated with artificial intelligence.
  - Without `data`, there is nothing for AI to learn from, no pattern to recognize, and no insight to glean.
  - without `artificial intelligence`, large amounts of data can be unmanageable or underutilized.


![Screenshot 2023-09-24 at 23.29.13](/assets/img/post/Screenshot%202023-09-24%20at%2023.29.13.png)

![Screenshot 2023-09-24 at 23.29.20](/assets/img/post/Screenshot%202023-09-24%20at%2023.29.20.png)

- Google has nine products with over one billion users: Android, Chrome, Gmail, Google Drive, Google Maps, Google Search, the Google Play Store, YouTube, and Photos.
  - That’s a lot of data being processed every day! To meet the needs of a growing user base, Google has developed the infrastructure to ingest, manage, and serve high quantities of data from these applications.
  - And artificial intelligence and machine learning have been integrated into these products to make the user experience of each even more productive.
  - This includes features like search in Photos, recommendations in YouTube, or Smart Compose in Gmail.

---


## Big Data and Machine Learning on GCP

![Screenshot 2023-09-24 at 23.37.53](/assets/img/post/Screenshot%202023-09-24%20at%2023.37.53.png)


![Screenshot 2023-09-24 at 23.57.30](/assets/img/post/Screenshot%202023-09-24%20at%2023.57.30.png)

![Screenshot 2023-09-24 at 23.58.34](/assets/img/post/Screenshot%202023-09-24%20at%2023.58.34.png)

![Screenshot 2023-09-25 at 00.00.23](/assets/img/post/Screenshot%202023-09-25%20at%2000.00.23.png)

![Screenshot 2023-09-25 at 00.00.02](/assets/img/post/Screenshot%202023-09-25%20at%2000.00.02.png)


### user example

- system that scale up to handle ties of high throughput and then back down again

![Screenshot 2023-09-25 at 00.03.42](/assets/img/post/Screenshot%202023-09-25%20at%2000.03.42.png)


---

## Data Engineering for Streaming Data


### Processing

![Screenshot 2023-09-25 at 00.24.48](/assets/img/post/Screenshot%202023-09-25%20at%2000.24.48.png)

![Screenshot 2023-09-25 at 00.25.09](/assets/img/post/Screenshot%202023-09-25%20at%2000.25.09.png)


- **Batch processing** is when the processing and analysis happens on a set of stored data.
  - An example is Payroll and billing systems that have to be processed on either a weekly or monthly basis.

- **Streaming data** is a flow of data records generated by various data sources. The processing of streaming data happens as the data flows through a system. This results in the analysis and reporting of events as they happen.
  - An example would be fraud detection or intrusion detection.
  - Streaming data processing means that the data is analyzed in near real-time and that actions will be taken on the data as quickly as possible.

- **Modern data processing** has progressed from legacy batch processing of data toward working with real-time data streams.
  - An example of this is streaming music and movies. No longer is it necessary to download an entire movie or album to a local device. Data streams are a key part in the world of big data.

![Screenshot 2023-09-25 at 00.27.03](/assets/img/post/Screenshot%202023-09-25%20at%2000.27.03.png)

- In modern organizations, data engineers and data scientists are facing four major challenges. These are collectively known as the 4Vs. They are variety, volume, velocity, and veracity.

1. **variety**: data could come in from a variety of different sources and in various formats.
   1. Imagine hundreds of thousands of sensors for self-driving cars on roads around the world.
   2. The data is `returned in various formats, such as number, image, or even audio`.
   4. How do we alert our downstream systems of new transactions in an organized way with no duplicates.

2. **variety**: handle not only an arbitrary variety of input sources, but a `volume of data that varies from gigabytes to petabytes`.
   1. need to know whether the pipeline code and infrastructure can scale with those changes or whether it will grind to a halt or even crash.

![Screenshot 2023-09-25 at 00.29.35](/assets/img/post/Screenshot%202023-09-25%20at%2000.29.35.png)


1. **velocity 速度**: Data often needs to be processed in near real time as soon as it reaches the system.
   1. need a way to handle data that's arrives late, has bad data in the message, or needs to be transformed mid fight because it's streamed into a data warehouse.

![Screenshot 2023-09-25 at 00.30.00](/assets/img/post/Screenshot%202023-09-25%20at%2000.30.00.png)


4. **veracity 真实**: which refers to the `data quality`. Because big data involves a multitude of data dimensions resulting from different data types and sources, there's a possibility that gathered data will come with some inconsistencies and uncertainties.
   1. Challenges like these are common considerations for pipeline developers.

![Screenshot 2023-09-25 at 00.28.37](/assets/img/post/Screenshot%202023-09-25%20at%2000.28.37.png)


#### user example

- Message-oriented architecture

![Screenshot 2023-09-25 at 00.31.55](/assets/img/post/Screenshot%202023-09-25%20at%2000.31.55.png)

![Screenshot 2023-09-25 at 00.32.49](/assets/img/post/Screenshot%202023-09-25%20at%2000.32.49.png)

![Screenshot 2023-09-25 at 00.33.03](/assets/img/post/Screenshot%202023-09-25%20at%2000.33.03.png)

![Screenshot 2023-09-25 at 00.33.49](/assets/img/post/Screenshot%202023-09-25%20at%2000.33.49.png)

![Screenshot 2023-09-25 at 00.35.20](/assets/img/post/Screenshot%202023-09-25%20at%2000.35.20.png)

![Screenshot 2023-09-25 at 00.35.58](/assets/img/post/Screenshot%202023-09-25%20at%2000.35.58.png)


---


### pipelines design

![Screenshot 2023-09-25 at 00.36.22](/assets/img/post/Screenshot%202023-09-25%20at%2000.36.22.png)


![Screenshot 2023-09-25 at 00.36.39](/assets/img/post/Screenshot%202023-09-25%20at%2000.36.39.png)

- Apache Beam

![Screenshot 2023-09-25 at 00.38.03](/assets/img/post/Screenshot%202023-09-25%20at%2000.38.03.png)

![Screenshot 2023-09-25 at 00.38.33](/assets/img/post/Screenshot%202023-09-25%20at%2000.38.33.png)


![Screenshot 2023-09-25 at 00.39.11](/assets/img/post/Screenshot%202023-09-25%20at%2000.39.11.png)

![Screenshot 2023-09-25 at 00.39.42](/assets/img/post/Screenshot%202023-09-25%20at%2000.39.42.png)

![Screenshot 2023-09-25 at 00.40.38](/assets/img/post/Screenshot%202023-09-25%20at%2000.40.38.png)

![Screenshot 2023-09-25 at 00.41.16](/assets/img/post/Screenshot%202023-09-25%20at%2000.41.16.png)

- `Streaming templates`: for processing continuous, or real-time, data. For example:
  - Pub/Sub to BigQuery
  - Pub/Sub to Cloud Storage
  - Datastream to BigQuery
  - Pub/Sub to MongoDB
- `Batch templates`: for processing bulk data, or batch load data. For example:
  - BigQuery to Cloud Storage
  - Bigtable to Cloud Storage
  - Cloud Storage to BigQuery
  - Cloud Spanner to Cloud Storage
- `utility templates`: address activities related to bulk compression, deletion, and conversion.

---

### Visualization with Looker

#### Looker

![Screenshot 2023-09-25 at 00.43.11](/assets/img/post/Screenshot%202023-09-25%20at%2000.43.11.png)

![Screenshot 2023-09-25 at 00.43.31](/assets/img/post/Screenshot%202023-09-25%20at%2000.43.31.png)

![Screenshot 2023-09-25 at 00.43.49](/assets/img/post/Screenshot%202023-09-25%20at%2000.43.49.png)


Looker output

![Screenshot 2023-09-25 at 00.45.15](/assets/img/post/Screenshot%202023-09-25%20at%2000.45.15.png)


#### Looker studio

![Screenshot 2023-09-25 at 00.46.52](/assets/img/post/Screenshot%202023-09-25%20at%2000.46.52.png)

- Looker Studio dashboards are widely used across many Google products and applications.

- Looker Studio integration
  - Looker Studio is integrated into Google Analytics to help visualize, in this case, a summary of a marketing website.
    - This dashboard visualizes the total number of visitors through a map, compares month-over-month trends, and even displays visitor distribution by age.

  - Another Looker Studio integration is the GCP billing dashboard.

- 3 steps needed to create a Looker Studio dashboard.
  - choose a template. You can start with either a pre-built template or a blank report.
  - link the dashboard to a data source. This might come from BigQuery, a local file, or a Google application like Google Sheets or Google Analytics–or a combination of any of these sources.
  - explore dashboard


```bash
bq --location=us-east1 mk taxirides

bq --location=us-east1 mk \
    --time_partitioning_field timestamp \
    --schema ride_id:string,point_idx:integer,latitude:float,longitude:float, timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string, passenger_count:integer \
    -t taxirides.realtime

```


---

## Big Data with BigQuery


- BigQuery is a fully managed data warehouse.
  - A `data warehouse`: a large store, containing terabytes and petabytes of data gathered from a wide range of sources within an organization, that's used to guide management decisions.
  - A `data lake`: a pool of raw, unorganized, and unclassified data, which has no specified purpose.
  - A data warehouse on the other hand, contains structured and organized data, which can be used for advanced querying.


key features of BigQuery.

- `storage plus analytics`. It’s a place to store petabytes of data. For reference, 1 petabyte is equivalent to 11,000 movies at 4k quality. BigQuery is also a place to analyze data, with built-in features like machine learning, geospatial analysis, and business intelligence, which we will look at a bit later on.

- `fully managed serverless solution`, don’t need to worry about provisioning any resources or managing servers in the backend but only focus on using SQL queries to answer the organization's questions in the frontend.

- `flexible pay-as-you-go pricing model` where you pay for the number of bytes of data the query processes and for any permanent table storage. If you prefer to have a fixed bill every month, you can also subscribe to flat-rate pricing where you have a reserved amount of resources for use.

- `Data in BigQuery is encrypted at rest by default` without any action required from a customer. By encryption at rest, we mean encryption used to protect data that is stored on a disk, including solid-state drives, or backup media.

- `built-in machine learning features` to write ML models directly in BigQuery using SQL. Also, if you decide to use other professional tools—such as Vertex AI from GCP—to train the ML models, you can export datasets from BigQuery directly into Vertex AI for a seamless integration across the data-to-AI lifecycle.


**Data warehouse** solution architecture
- 4 challenges of big data, in modern organizations the data can be in `any format (variety), any size (volume), any speed (velocity), and possibly inaccurate (veracity)`.
- The input data can be either real-time or batch data.
  - If it's `streaming data`, which can be either structured or unstructured, high speed, and large volume, Pub/Sub is needed to digest the data.
  - If it’s `batch data`, it can be directly uploaded to Cloud Storage.

- both pipelines lead to Dataflow to process the data. That’s the place we `ETL – extract, transform, and load` – the data if needed.

- BigQuery sits in the middle to link data processes using Dataflow and data access through analytics, AI, and ML tools.
  - The job of the analytics engine of BigQuery at the end of a data pipeline is to ingest all the processed data after ETL, store and analyze it, and possibly output it for further use such as data visualization and machine learning.

- BigQuery outputs usually feed into two buckets: `business intelligence tools and AI/ML tools`.
  - business analyst or data analyst, connect to visualization tools like `Looker, Looker Studio, Tableau, or other BI tools`. If you prefer to work in spreadsheets, you can query both small or large BigQuery datasets directly from Google Sheets and even perform common operations like pivot tables.
  - data scientist or machine learning engineer, directly call the data from BigQuery through `AutoML or Workbench`. These AI/ML tools are part of Vertex AI, Google's unified ML platform.

![Screenshot 2023-09-25 at 01.21.28](/assets/img/post/Screenshot%202023-09-25%20at%2001.21.28.png)

- BigQuery is like a common staging area for data analytics workloads. When the data is there, business analysts, BI developers, data scientists, and machine learning engineers can be granted access to the data for their own insights.

---

### Storage and analytics

BigQuery provides two services in one.
- It's both a `fully-managed storage facility to load and store datasets` and also a `fast SQL-based analytical engine`.

- The two services are connected by Google's high-speed internal network. It's the super-fast network that allows BigQuery to scale both storage and compute independently based on demand.

![Screenshot 2023-09-27 at 00.07.58](/assets/img/post/Screenshot%202023-09-27%20at%2000.07.58.png)

![Screenshot 2023-09-27 at 00.04.37](/assets/img/post/Screenshot%202023-09-27%20at%2000.04.37.png)

#### manages the storage and metadata for datasets


- BigQuery can `ingest` datasets from various sources including internal data (data saved directly in BigQuery), external data, multi-Cloud data, and public data-sets.

![Screenshot 2023-09-27 at 00.05.08](/assets/img/post/Screenshot%202023-09-27%20at%2000.05.08.png)

- After the data is stored in BigQuery, it's `fully managed and is automatically replicated, backed up, and set to auto-scale`.

- BigQuery offers the option to `query` external data sources, like data stored in other GCP storage services (Cloud storage) or GCP database services (Spanner or Cloud SQL), and bypass BigQuery managed Storage.
  - a raw CSV file in Cloud storage or Google sheet can be used to write a query without being ingested by BigQuery first.
  - **inconsistency** might result from saving and processing data separately, consider using **DataFlow** to build a streaming data pipeline into BigQuery.

![Screenshot 2023-09-27 at 00.05.29](/assets/img/post/Screenshot%202023-09-27%20at%2000.05.29_s93ryq1cz.png)


- In addition to internal or native and external data sources, BigQuery can also ingest data from multi-Cloud data, which is data stored in multiple Cloud services, such as AWS or Azure, or a public data set.
- If you don't have any data of the own, you can analyze any of the datasets available in the public data set marketplace.

![Screenshot 2023-09-27 at 00.06.43](/assets/img/post/Screenshot%202023-09-27%20at%2000.06.43.png)


- There are **3 basic patterns to load data into BigQuery**.

  - `batch load`: source data is loaded into a BigQuery table in a single batch operation.
    - one-time operation or automated to occur on a schedule.
    - A batch load operation can create a new table or open data into an existing table.

  - `streaming`: smaller batches of data are streamed continuously so that the data is available for querying in near real-time.

  - `generated data`: where SQL statements are used to insert rows into an existing table or to write the results of a query to a table.

![Screenshot 2023-09-27 at 00.07.26](/assets/img/post/Screenshot%202023-09-27%20at%2000.07.26.png)

#### analyzing data

- optimized for running analytical queries over large datasets. It can perform queries on terabytes of data in seconds and petabytes in minutes.
- analyze large datasets efficiently and get insights in near real-time.

![Screenshot 2023-09-27 at 00.08.17](/assets/img/post/Screenshot%202023-09-27%20at%2000.08.17.png)

- analytics features

  - supports **ad hoc analysis** using `standard SQL, the BigQuery SQL dialect`, **geospatial analytics** using geography data types in `standard SQL geography functions`.

  - supports building **machine learning models** using `BigQuery ML` and building rich **interactive business intelligence dashboards** using `BigQuery BI Engine`.

- queries

  - By default, it runs `interactive queries`, which means that the queries are executed as needed.

  - offers `batch queries` where each query is queued on the behalf and the query starts when idle resources are available.

![Screenshot 2023-09-27 at 00.08.58](/assets/img/post/Screenshot%202023-09-27%20at%2000.08.58.png)

---

### BigQuery ML

- BigQuery started out solely as a data warehouse, over time it has evolved to provide features that support the data to AI lifecycle.

- building and training them can be very time intensive.
  - first `export` data from the data store into an IDE, Integrated Development Environment, such as Jupyter Notebook or Google Colab.
  - And then `transform` the data and perform the feature engineering steps before feed it into a training model.
  - Then `build` the model in Tensorflow or similar library and train it locally on a computer or on a virtual machine.
  - To improve the model performance, you also need to go back and forth to get more data and create new features. This process will need to be repeated, but it's so time intensive that you'll probably stop after a few iterations.

![Screenshot 2023-09-27 at 00.31.55](/assets/img/post/Screenshot%202023-09-27%20at%2000.31.55.png)

- Now you can create and execute machine learning models on the structured data sets in BigQuery in just a few minutes using SQL queries.
- 2 steps
  - create a model with a SQL statement. Here we can use the numbikes.model data set as an example.
  - write a SQL prediction query and invoke ml.PREDICT
  - you now have a model and can view the results.

  - Additional steps might include activities like evaluating the model, but if you know basic SQL you can now implement ml, that's pretty cool.

![Screenshot 2023-09-27 at 00.32.24](/assets/img/post/Screenshot%202023-09-27%20at%2000.32.24.png)

![Screenshot 2023-09-27 at 00.32.31](/assets/img/post/Screenshot%202023-09-27%20at%2000.32.31.png)

- BigQuery ML was designed to be simple, like building a model in two steps. That simplicity extends to defining the machine learning hyperparameters, which let you tune the model to achieve the best training result.
  - `Hyperparameters` are the settings apply to a model before the training starts, like a learning rate. With BigQuery ML, you can either manually control the hyperparameters. Or add it to BigQuery starting with a default hyperparameter setting and then automatic tuning.

- When using a structured dataset in BigQuery ML, you need to choose the appropriate **model type**.
  - Choosing which type of ML model depends on the business goal and the datasets.

  - BigQuery support `supervised and unsupervised models`.

  - **Supervised models** are task driven and identify a goal.
    - if the goal is to classify data like whether an email is spam, use logistic regression.
    - If the goal is to predict a number like shoe sales for the next three months, use linear regression

  - **unsupervised models** are data driven and identify a pattern.

    - if the goal is to identify patterns or clusters and then determine the best way to group them. Like grouping random photos of flowers into categories, you should use cluster analysis.

![Screenshot 2023-09-27 at 00.34.18](/assets/img/post/Screenshot%202023-09-27%20at%2000.34.18.png)

- decide on the best **model**.
  - Categories include classification and regression models. There are also other model options to choose from along with ML Ops.
  - Logistic regression is an example of a classification model,
  - linear regression is an example of a regression model.
  - We recommend that you start with these options and use the results to benchmark.
  - To compare against more complex models such as DNN, Deep Neural Networks, which may take more time in computing resources to train and deploy.

![Screenshot 2023-09-27 at 00.34.47](/assets/img/post/Screenshot%202023-09-27%20at%2000.34.47.png)
- BigQuery ML supports features to deploy, monitor and manage the ML production called ML Ops (machine learning operations).
  - Ops include importing Tensorflow models for batch prediction, exporting models from BigQuery ML for online prediction. And hyperparameter tuning using Cloud AI Vizier.

![Screenshot 2023-09-27 at 00.35.12](/assets/img/post/Screenshot%202023-09-27%20at%2000.35.12.png)

![Screenshot 2023-09-27 at 00.41.26](/assets/img/post/Screenshot%202023-09-27%20at%2000.41.26.png)

---

#### Phases of ML project


key phases of a machine learning project.


1. extract, transform and load data into BigQuery if it isn't there already.
   1. using Google products, look out for easy connectors to get the data into BigQuery before you build the own pipeline.
   2. You can enrich the existing data warehouse with other data sources by using ·.

![Screenshot 2023-09-27 at 11.44.09](/assets/img/post/Screenshot%202023-09-27%20at%2011.44.09.png)

2. select and preprocessed features.
   1. use SQL to `create the training dataset for the model` to learn from.
   2. BigQuery ML does some of the preprocessing, like one-hot encoding of the categorical variables.
      1. One-hot encoding converts the categorical data into numeric data that is required by a training model.

![Screenshot 2023-09-27 at 11.44.30](/assets/img/post/Screenshot%202023-09-27%20at%2011.44.30.png)

3. create and train the model inside BigQuery.
   1. using the create model command, give it a name, specify the model type and pass it in a sequel query with the training dataset, from there you can run the query.

![Screenshot 2023-09-27 at 11.44.45](/assets/img/post/Screenshot%202023-09-27%20at%2011.44.45.png)

4. evaluate the performance of the trained model
   1. execute an `ML.evaluate` query to evaluate the performance of the trained model on the evaluation dataset.
   2. analyze lost metrics like a root mean squared error for forecasting models and area under the curve accuracy, precision and recall for classification models.

![Screenshot 2023-09-27 at 11.45.00](/assets/img/post/Screenshot%202023-09-27%20at%2011.45.00.png)

5. use it to make predictions.
   1. invoke that `ML.predict` command on the trained model to return with predictions and the model's confidence in those predictions.
   2. With the results the label field will have predicted added to the field name. This is the model's prediction for that label.

![Screenshot 2023-09-27 at 11.45.16](/assets/img/post/Screenshot%202023-09-27%20at%2011.45.16.png)







。
