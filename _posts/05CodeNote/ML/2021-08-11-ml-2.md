---
title: ML - 2nd - Intermediate Machine Learning
date: 2021-08-11 11:11:11 -0400
categories: [1CodeNote, MLNote]
tags: [ML]
toc: true
---

- [ML - Intermediate Machine Learning](#ml---intermediate-machine-learning)
  - [Missing Values](#missing-values)
    - [Three Approaches](#three-approaches)
    - [Approach 1 (Drop Columns with Missing Values) 删除无数值列](#approach-1-drop-columns-with-missing-values-删除无数值列)
    - [Approach 2 (Imputation) 用其他数值代替](#approach-2-imputation-用其他数值代替)
    - [Approach 3 (An Extension to Imputation)](#approach-3-an-extension-to-imputation)
    - [summary](#summary)
  - [categorical variable](#categorical-variable)
    - [Three Approaches](#three-approaches-1)
    - [Drop Categorical Variables 删除空值](#drop-categorical-variables-删除空值)
    - [Ordinal Encoding 转换成数值](#ordinal-encoding-转换成数值)
    - [One-Hot Encoding](#one-hot-encoding)
    - [conclusion](#conclusion)

- ref:
  - https://www.kaggle.com/learn/intermediate-machine-learning
- data set:
  - [Melbourne Housing dataset](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/home)

---

# ML - Intermediate Machine Learning

> Welcome to Kaggle Learn's Intermediate Machine Learning micro-course!

> If you have some background in machine learning and you'd like to learn how to quickly improve the quality of the models, you're in the right place! In this micro-course, you will accelerate the machine learning expertise by learning how to:

- tackle data types often found in real-world datasets (**missing values, categorical variables**),
- design **pipelines** to improve the quality of the machine learning code,
- use advanced techniques for model validation (**cross-validation**),
- build state-of-the-art models that are widely used to win Kaggle competitions (**XGBoost**), and
- avoid common and important data science mistakes (**leakage**).

---

## Missing Values

Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.



example:

```py
import pandas as pd
from sklearn.model_selection import train_test_split

# Load the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Select target
y = data.Price

# To keep things simple, we'll use only numerical predictors
melb_predictors = data.drop(['Price'], axis=1)
X = melb_predictors.select_dtypes(exclude=['object'])

# Divide data into training and validation subsets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)


from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# Function for comparing different approaches
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=10, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)
```


### Three Approaches

### Approach 1 (Drop Columns with Missing Values) 删除无数值列
- simplest option is to drop columns with missing values.
- Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach.

> extreme example:
> consider a dataset with 10,000 rows, where one important column is missing a single entry.
> This approach would drop the column entirely!

![Sax80za-1](https://i.imgur.com/EjJsTGh.png)

- Since we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames.

```py
# Get names of columns with missing values
cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]

# Drop columns in training and validation data
reduced_X_train = X_train.drop(cols_with_missing, axis=1)
reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)

print("MAE from Approach 1 (Drop columns with missing values):")
print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))
# MAE from Approach 1 (Drop columns with missing values):
# 183550.22137772635
```



### Approach 2 (Imputation) 用其他数值代替
- Imputation **fills in the missing values with some number**
  - For instance, we can fill in the `mean value` along each column.
- The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.


![4BpnlPA](https://i.imgur.com/Ukc1i7n.png)

- use `SimpleImputer` to replace missing values with the `mean value` along each column.
- filling in the `mean value` generally performs quite well
  - but this varies by dataset
- While statisticians have experimented with more complex ways to determine imputed values (such as regression imputation, for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated machine learning models.

```py
from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print("MAE from Approach 2 (Imputation):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))
# MAE from Approach 2 (Imputation):
# 178166.46269899711

```


### Approach 3 (An Extension to Imputation)

- Imputation is the standard approach, and it usually works well.
  - However, imputed values may be systematically above or below their actual values
  - (which weren't collected in the dataset).
  - Or rows with missing values may be unique in some other way.
- In that case, make better predictions by **considering which values were originally missing**.
  - impute the missing values, as before.
  - And for each column with missing entries in the original dataset, add a new column that shows the location of the imputed entries.
- In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.

![UWOyg4a](https://i.imgur.com/Os0pAeH.png)

```py
# Make copy to avoid changing original data (when imputing)
X_train_plus = X_train.copy()
X_valid_plus = X_valid.copy()

# Make new columns indicating what will be imputed
for col in cols_with_missing:
    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()
    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()

# Imputation
my_imputer = SimpleImputer()
imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))
imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))

# Imputation removed column names; put them back
imputed_X_train_plus.columns = X_train_plus.columns
imputed_X_valid_plus.columns = X_valid_plus.columns

print("MAE from Approach 3 (An Extension to Imputation):")
print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))
# MAE from Approach 3 (An Extension to Imputation):
# 178927.503183954
```

---


### summary

```py
# Shape of training data (num_rows, num_columns)
print(X_train.shape)
# (10864, 12)

# Number of missing values in each column of training data
missing_val_count_by_column = (X_train.isnull().sum())
print(missing_val_count_by_column[missing_val_count_by_column > 0])
# Car               49
# BuildingArea    5156
# YearBuilt       4307
# dtype: int64

```


---

## categorical variable

A **categorical variable** takes only a limited number of values.

> Consider a survey that asks how often you eat breakfast and provides four options: "Never", "Rarely", "Most days", or "Every day".
> In this case, the data is categorical
> because responses fall into a fixed set of categories.

> If people responded to a survey about which what brand of car they owned, the responses would fall into categories like "Honda", "Toyota", and "Ford".
> In this case, the data is also categorical.

- You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first.

There are three approaches to prepare the categorical data.



---

### Three Approaches


```py
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Separate target from predictors
y = data.Price
X = data.drop(['Price'], axis=1)

# Divide data into training and validation subsets
X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)

# Drop columns with missing values (simplest approach)
cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]
X_train_full.drop(cols_with_missing, axis=1, inplace=True)
X_valid_full.drop(cols_with_missing, axis=1, inplace=True)

# "Cardinality" means the number of unique values in a column
# Select categorical columns with relatively low cardinality (convenient but arbitrary)
low_cardinality_cols = [cname for cname in X_train_full.columns
                        if X_train_full[cname].nunique() < 10
                        and X_train_full[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in X_train_full.columns
                  if X_train_full[cname].dtype in ['int64', 'float64']]

# Keep selected columns only
my_cols = low_cardinality_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()

X_train.head()
# Type	Method	Regionname	Rooms	Distance	Postcode	Bedroom2	Bathroom	Landsize	Lattitude	Longtitude	Propertycount
# 12167	u	S	Southern Metropolitan	1	5.0	3182.0	1.0	1.0	0.0	-37.85984	144.9867	13240.0
# 6524	h	SA	Western Metropolitan	2	8.0	3016.0	2.0	2.0	193.0	-37.85800	144.9005	6380.0
# 8413	h	S	Western Metropolitan	3	12.6	3020.0	3.0	1.0	555.0	-37.79880	144.8220	3755.0
# 2919	u	SP	Northern Metropolitan	3	13.0	3046.0	3.0	1.0	265.0	-37.70830	144.9158	8870.0
# 6043	h	S	Western Metropolitan	3	13.3	3020.0	3.0	1.0	673.0	-37.76230	144.8272	4217.0


# get list of all categorical variables in the training data.
# checking the data type (or dtype) of each column.
# The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text -> indicate categorical variables.
s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables:")
print(object_cols)
# ['Type', 'Method', 'Regionname']



# Define Function function score_dataset()
# to Measure Quality of Each Approach
# to compare the three different approaches to dealing with categorical variables.
# This function reports the mean absolute error (MAE) from a random forest model.
# In general, we want the MAE to be as low as possible!
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# Function for comparing different approaches
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=100, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)
```

---

### Drop Categorical Variables 删除空值

- The easiest approach to dealing with categorical variables is to simply remove them from the dataset.
- This approach will only work well if the columns did not contain useful information.

drop the object columns with the `select_dtypes()` method.

```py
drop_X_train = X_train.select_dtypes(exclude=['object'])
drop_X_valid = X_valid.select_dtypes(exclude=['object'])
print("MAE from Approach 1 (Drop categorical variables):")
print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))
# MAE from Approach 1 (Drop categorical variables):
# 175703.48185157913
```

---

### Ordinal Encoding 转换成数值

- assigns each unique value to a different integer.

![tEogUAr](https://i.imgur.com/ILMPr32.png)

> assumes an ordering of the categories:
> "Never" (0) < "Rarely" (1) < "Most days" (2) < "Every day" (3).

- This assumption makes sense in this example, because there is an indisputable ranking to the categories.
- Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables.
- For tree-based models (like decision trees and random forests), you can expect **ordinal encoding** to work well with **ordinal variables**.

- Scikit-learn has a `OrdinalEncoder` class that can be used to get ordinal encodings.
- loop over the **categorical variables** and apply the `ordinal encoder` separately to each column.

- for each column, we randomly assign each unique value to a different integer.
- This is a common approach that is simpler than providing custom labels;
- however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables.


```py
from sklearn.preprocessing import OrdinalEncoder

# Make copy to avoid changing original data
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# Apply ordinal encoder to each column with categorical data
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

print("MAE from Approach 2 (Ordinal Encoding):")
print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))
# MAE from Approach 2 (Ordinal Encoding):
# 165936.40548390493
```

---


### One-Hot Encoding

- One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.

![TW5m0aJ](https://i.imgur.com/z88ShxY.png)

> In the original dataset, "Color" is a categorical variable with three categories: "Red", "Yellow", and "Green".
> The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset.
> Wherever the original value was "Red", we put a 1 in the "Red" column;
> if the original value was "Yellow", we put a 1 in the "Yellow" column, and so on.

- In contrast to ordinal encoding, one-hot encoding **does not assume an ordering of the categories**.

- can expect this approach to work particularly well
  - if there is no clear ordering in the categorical data
  - (e.g., "Red" is neither more nor less than "Yellow").
  - refer to **categorical variables** without an `intrinsic ranking` as **nominal variables**.

- it does not perform well if the categorical variable takes on a large number of values
- (i.e., you generally won't use it for variables taking more than 15 different values).


- We use the `OneHotEncoder` class from scikit-learn to get one-hot encodings.
- There are a number of parameters that can be used to customize its behavior.
  - `handle_unknown='ignore'` to avoid errors when the validation data contains classes that aren't represented in the training data
  - `sparse=False` ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).
- To use the encoder, we supply only the **categorical columns** that we want to be one-hot encoded.
- For instance, to encode the training data, we supply `X_train[object_cols]`.
- (object_cols in the code cell below is a list of the column names with **categorical data**, and so `X_train[object_cols]` contains all of the categorical data in the training set.)

```py
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_obj_cols_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_obj_cols_X_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
OH_obj_cols_X_train.index = X_train.index
OH_obj_cols_X_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
OH_num_cols_X_train = X_train.drop(object_cols, axis=1)
OH_num_cols_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([OH_num_cols_X_train, OH_obj_cols_X_train], axis=1)
OH_X_valid = pd.concat([OH_num_cols_X_valid, OH_obj_cols_X_valid], axis=1)

print("MAE from Approach 3 (One-Hot Encoding):")
print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))
# MAE from Approach 3 (One-Hot Encoding):
# 166089.4893009678
```

### conclusion

- In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score.
- As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.
- In general, one-hot encoding (Approach 3) will typically perform best,
- and dropping the categorical columns (Approach 1) typically performs worst,
- but it varies on a case-by-case basis.


```py
# =============== Set up code checking
# import os
# if not os.path.exists("../input/train.csv"):
#     os.symlink("../input/home-data-for-ml-course/train.csv", "../input/train.csv")  
#     os.symlink("../input/home-data-for-ml-course/test.csv", "../input/test.csv")
# from learntools.core import binder
# binder.bind(globals())
# from learntools.ml_intermediate.ex3 import *
# print("Setup Complete")


# =============== function reports the mean absolute error (MAE) from a random forest model.
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
​
# function for comparing different approaches
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=100, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)



# =============== load the training and validation sets in X_train, X_valid, y_train, and y_valid.
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
X = pd.read_csv('../input/train.csv', index_col='Id')
X_test = pd.read_csv('../input/test.csv', index_col='Id')

# Remove rows with missing target, separate target from predictors
X.dropna(axis=0, subset=['SalePrice'], inplace=True)
X.drop(['SalePrice'], axis=1, inplace=True)
y = X.SalePrice

# To keep things simple, we'll drop columns with missing values
cols_with_missing = [col for col in X.columns if X[col].isnull().any()]
X.drop(cols_with_missing, axis=1, inplace=True)
X_test.drop(cols_with_missing, axis=1, inplace=True)


# =============== Break off validation set from training data
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)

X_train.head()
# 	MSSubClass	MSZoning	LotArea	Street	LotShape	LandContour	Utilities	LotConfig	LandSlope	Neighborhood	...	OpenPorchSF	EnclosedPorch	3SsnPorch	ScreenPorch	PoolArea	MiscVal	MoSold	YrSold	SaleType	SaleCondition
# Id																					
# 619	20	RL	11694	Pave	Reg	Lvl	AllPub	Inside	Gtl	NridgHt	...	108	0	0	260	0	0	7	2007	New	Partial
# 871	20	RL	6600	Pave	Reg	Lvl	AllPub	Inside	Gtl	NAmes	...	0	0	0	0	0	0	8	2009	WD	Normal
# 93	30	RL	13360	Pave	IR1	HLS	AllPub	Inside	Gtl	Crawfor	...	0	44	0	0	0	0	8	2009	WD	Normal
# 818	20	RL	13265	Pave	IR1	Lvl	AllPub	CulDSac	Gtl	Mitchel	...	59	0	0	0	0	0	7	2008	WD	Normal
# 303	20	RL	13704	Pave	IR1	Lvl	AllPub	Corner	Gtl	CollgCr	...	81	0	0	0	0	0	1	2006	WD	Normal
# 5 rows × 60 columns


# =============== the dataset contains both numerical and categorical variables.
# need to encode the categorical data before training a model.
num_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]

s = (X_train.dtypes == 'object')
obj_cols = list(s[s].index)




# =============== Step 1: Solution: Drop columns in training and validation data
drop_X_train = X_train.select_dtypes(exclude=['object'])
drop_X_valid = X_valid.select_dtypes(exclude=['object'])

print("MAE from Approach 1 (Drop categorical variables):")
print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))
# MAE from Approach 1 (Drop categorical variables):
# 17837.82570776256
print("Unique values in 'Condition2' column in training data:", X_train['Condition2'].unique())
print("\nUnique values in 'Condition2' column in validation data:", X_valid['Condition2'].unique())
# Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']
# Unique values in 'Condition2' column in validation data: ['Norm' 'Feedr''PosN' 'Artery' 'RRAn' 'RRNn' ]



# =============== Step 2: Ordinal encoding
# Part A
# fit an ordinal encoder to the training data, and then use it to transform both the training and validation data,
from sklearn.preprocessing import OrdinalEncoder
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

ordinal_encoder = OrdinalEncoder()
label_X_train[obj_cols] = ordinal_encoder.fit_transform(X_train[obj_cols])
label_X_valid[obj_cols] = ordinal_encoder.transform(X_valid[obj_cols])


# ===============


# ===============


# ===============


# ===============


# ===============


# ===============




```





---

















.
