---
title: AIML - AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AIML]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

# AIML - AI

- [AIML - AI](#aiml---ai)
  - [Overall](#overall)
  - [AI](#ai)
    - [Divisions of AI](#divisions-of-ai)
      - [Artificial Narrow Intelligence (ANI)](#artificial-narrow-intelligence-ani)
      - [Artificial General Intelligence (AGI)](#artificial-general-intelligence-agi)
  - [GenAI](#genai)
    - [Traditional AIML vs GenAI](#traditional-aiml-vs-genai)
    - [RNN - Recurrent neural networks](#rnn---recurrent-neural-networks)
    - [Attention is all you need](#attention-is-all-you-need)
      - [High alignment](#high-alignment)
      - [Multi-Headed Attention](#multi-headed-attention)
    - [subject thoery](#subject-thoery)
  - [LLM - Large Language Model](#llm---large-language-model)
    - [Features of LLMs](#features-of-llms)
      - [Translation](#translation)
      - [Automating Mundane Tasks 自动化日常任务](#automating-mundane-tasks-自动化日常任务)
      - [Emergent Abilities 新兴能力](#emergent-abilities-新兴能力)
    - [Drawbacks of LLMs](#drawbacks-of-llms)
      - [Hallucination](#hallucination)
      - [Bias](#bias)
      - [Glitch tokens](#glitch-tokens)
      - [LLM Generation Inefficient](#llm-generation-inefficient)
        - [Speculative 推测的 Decoding](#speculative-推测的-decoding)
    - [In-context learning](#in-context-learning)
      - [Data preprocessing / embedding](#data-preprocessing--embedding)
      - [Prompt construction / retrieval](#prompt-construction--retrieval)
      - [Prompt execution / inference](#prompt-execution--inference)
      - [AI agents frameworks](#ai-agents-frameworks)
    - [Prompt](#prompt)
    - [LLM Tools](#llm-tools)
      - [Medusa](#medusa)
        - [Medusa heads](#medusa-heads)
        - [Tree attention](#tree-attention)
        - [Typical acceptance](#typical-acceptance)
        - [accelerate models](#accelerate-models)
        - [Ablation Study 消融研究](#ablation-study-消融研究)
  - [Hugging Face](#hugging-face)
    - [Generative AI Time Series Forecasting](#generative-ai-time-series-forecasting)
    - [Multivariate Time Series Forecasting](#multivariate-time-series-forecasting)
    - [Generative AI Transformers for Time Series Forecasting](#generative-ai-transformers-for-time-series-forecasting)
    - [Falcon 40b](#falcon-40b)
    - [CodeParrot](#codeparrot)
    - [TAPEX](#tapex)
  - [Juptyper](#juptyper)


ref:
- [OWAPS Top10 for LLM v1](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0.pdf)
- https://www.freecodecamp.org/news/large-language-models-and-cybersecurity/
- https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html
- https://docs.whylabs.ai/docs/integrations-llm-whylogs-container
- https://hackernoon.com/security-threats-to-high-impact-open-source-large-language-models
- https://a16z.com/emerging-architectures-for-llm-applications/
- [Examining Zero-Shot Vulnerability Repair with Large Language Models](https://www.connectedpapers.com/main/a5731122200fbb8b37f048010a1e1ca4474aa606/Examining-Zero%20Shot-Vulnerability-Repair-with-Large-Language-Models/graph)
- [medusa](https://together.ai/blog/medusa)
- [awesome-generative-ai](https://github.com/steven2358/awesome-generative-ai)
- [Google’s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)


---

## Overall

> Research in artificial intelligence is increasing at an exponential rate. It’s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.

“AI Canon”
- a curated list of resources we’ve relied on to get smarter about modern AI
- because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.


**Data pipelines**
- Databricks
- Airflow
- Unstructured

**Embedding model**
- OpenAI
- Cohere
- Hugging Face

**Vector database**
- Pinecone
- Weaviate
- ChromaDB
- pgvector

**Playground**
- OpenAI
- nat.dev
- Humanloop

**Orchestration**
- Langchain
- LlamaIndex
- ChatGPT


**APIs/plugins**
- Serp
- Wolfram
- Zapier

**LLM cache**
- Redis
- SQLite
- GPTCache

**Logging / LLMops**
- Weights & Biases
- MLflow
- PromptLayer
- Helicone

**Validation**
- Guardrails
- Rebuff
- Microsoft Guidance
- LMQL

**App hosting**
- Vercel
- Steamship
- Streamlit
- Modal

**LLM APIs (proprietary)**
- OpenAI
- Anthropic

**LLM APIs (open)**
- Hugging Face
- Replicate

**Cloud providers**
- AWS
- GCP
- Azure
- CoreWeave

**Opinionated clouds**
- Databricks
- Anyscale
- Mosaic
- Modal
- RunPod



**OpenSource**
- hugging face
- OpenAI
- Generative AI (answers for everything)

**Programming**
- python
- panda

**AI modeling**
- pyTorch
- Tensor flow (Google)

**ML platforms**
- Jupyter Notebooks

**Time series**
- Forecasting and predictive Analytics

**Use case**
- Supply Chain Management with GenAI

OpenSource -> fine tuning -> custom result


---


## AI

- Artificial Intelligence refers to `the ability of computers to perform tasks that typically require human-level intellect`.
- AI is useful in many contexts, from automation to problem solving and merely trying to understand how humans think.

- But it is important to note that AI is only concerned with human intelligence for now – it could possibly go beyond that.

  - Many people correlate the word ‘Intelligence’ with only ‘Human Intelligence’. Just because a chicken may not be able to solve a mathematical equation doesn’t mean it won’t run when you chase it. It is ‘Intelligent’ enough to know it doesn’t want you to catch it 🐔🍗.

  - Intelligence spans a much wider spectrum, and practically expands to any living thing that can make decisions or carry out actions autonomously, even plants.


### Divisions of AI

- Artificial Intelligence is `centered around computers and their ability to mimic human actions and thought processes`.

- Programming and experiments have allowed humans to produce ANI systems. These can do things like classifying items, sorting large amounts of data, looking for trends in charts and graphs, code debugging, and knowledge representation and expression. But computers don’t think like humans, they merely mimic humans.

- This is evident in voice assistants such as `Google’s Assistant, Apple’s Siri, Amazon’s Alexa, and Microsoft’s Cortana`. They are basic ANI programs that add ‘the human touch’. In fact, people are known to be polite to these systems simply because they combine computerized abilities with a human feel.

- These assistants have gotten better over the years but fail to reach high levels of sophistication when compared to their AGI counterparts.


There are two major divisions of AI:


#### Artificial Narrow Intelligence (ANI)

- focused on a small array of similar tasks or a small task that is programmed only for one thing.
- ANI is not great in dynamic and complex environments and is used in only areas specific to it.
- Examples include self-driving cars, as well as facial and speech recognition systems.


#### Artificial General Intelligence (AGI)

- focused on a wide array of tasks and human activities.
- AGI is currently theoretical and is proposed to adapt and carry out most tasks in many dynamic and complex environments.
- Examples include J.A.R.V.I.S from Marvel’s _Iron Man_ and Ava from _Ex-Machina_.



---

## GenAI

---

### Traditional AIML vs GenAI

**Traditional AIML**
- good at **identify pattern**
- learning from the pattern
- limit success with close supervised learning of very large amount of data
- must have human involved


**GenAI**
- produces 'content' (text, image, music, art, forecasts, etc...)
- use 'transformers' (Encoders/Decoders) based on pre-trained data using small amount of fine tuning data
  - encode and decode at the same time
    - less data and faster
    - GenAI use small data and uses encoders and decoders and Transformers to take that smaller data and be able to use it for other types of models. (Pre training)
    - then add on top of it small amounts of fine tuning data
    - and then get a a training model.
  - As perceptions, not neurons.

- Generative AI is a subset of traditional machine learning.
  - And the generative AI machine learning models have learned these abilities by `finding statistical patterns in massive datasets of content` that was originally generated by humans.

---

### RNN - Recurrent neural networks

> generative algorithms are not new.

recurrent neural networks - RNNs
- Previous generations of language models made use of an architecture called RNNs.
- RNNs were limited by the amount of `compute and memory` needed to perform well at generative tasks.


- With just one previous words seen by the model, the prediction can't be very good.
  - scale the RNN implementation to be able to see more of the preceding words in the text,
  - significantly scale the resources that the model uses.
  - As for the prediction, Even though scale the model, it still hasn't seen enough of the input to make a good prediction.

![Screenshot 2023-10-10 at 00.31.57](/assets/img/post/Screenshot%202023-10-10%20at%2000.31.57.png)


- To successfully predict the next word,
  - models need to see more than just the previous few words.
  - Models needs to have an understanding of the whole sentence or even the whole document.


> How can an algorithm make sense of human language if sometimes we can't?
> in 2017, after the publication of this paper, `Attention is All You Need`, from Google and the University of Toronto, everything changed.
> The transformer architecture had arrived.
> - It can be scaled efficiently to use multi-core GPUs,
> - parallel process input data, making use of much larger training datasets, and crucially,
> - it's able to learn to pay attention to the meaning of the words it's processing.

---

### Attention is all you need

![Screenshot 2023-10-10 at 00.33.00](/assets/img/post/Screenshot%202023-10-10%20at%2000.33.00.png)

---

#### High alignment

![Screenshot 2023-09-06 at 22.55.58](/assets/img/post/Screenshot%202023-09-06%20at%2022.55.58.png)

![Screenshot 2023-09-06 at 22.56.59](/assets/img/post/Screenshot%202023-09-06%20at%2022.56.59.png)

---

#### Multi-Headed Attention

![Screenshot 2023-09-06 at 23.10.28](/assets/img/post/Screenshot%202023-09-06%20at%2023.10.28.png)






---

### subject thoery


**Large language models**
- Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power.
- These foundation models with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

**foundation models (base models)**
- their relative size in terms of their parameters.
- ![Screenshot 2023-10-09 at 15.57.07](/assets/img/post/Screenshot%202023-10-09%20at%2015.57.07.png)
- **parameters**:
  - the model's `memory`.
  - the more parameters a model has, the more memory, the more `sophisticated` the tasks it can perform.
- By either using these models as they are or by applying fine tuning techniques to adapt them to the specific use case, you can rapidly `build customized solutions without the need to train a new model` from scratch.


**Augmenting LLMs**
- connecting LLM to external data sources or using them to invoke external APIs.
- use this ability to provide the model with information it doesn't know from its pre-training and to enable the model to power interactions with the real-world.



**Interact**
- other machine learning and programming paradigms: write computer code with formalized syntax to interact with `libraries and APIs`.
- large language models: able to take natural language or human written instructions and perform tasks much as a human would.

**prompt**
- The text that you pass to an LLM
- The space or memory that is available to the prompt is called the `context window`, and this is typically large enough for a few thousand words, but differs from model to model.

- example
  - ask the model to determine where Ganymede is located in the solar system.
  - The prompt is passed to the model, the model then predicts the next words, and because the prompt contained a question, this model generates an answer.
- The output of the model is called a `completion`, and the act of using the model to generate text is known as `inference`.
- The completion is comprised of the text contained in the original prompt, followed by the generated text.

![Screenshot 2023-10-10 at 00.25.13](/assets/img/post/Screenshot%202023-10-10%20at%2000.25.13.png)



GPU
- cloud class instance (from NVIDIA)
  - google colab
  - kaggle
  - amazon sagemaker
  - gradient
  - microsoft azure

> Tesla is graphics cards from NVIDIA for AI



pyTorch
- it does these heavy mathematical computation very easily with libraries
- it got a whole set of APIs and utilities that let you manipulate all of these different tensors



tensors
- a tensor is a computer data object (data structure) that represents numeric data,
- it could be floating point data values or data objects within data objects.
- 1d tensors (column)
- 2d tensors (xy)
- 3d tensors (xyz)
- 4d tensors (**cube**)
- 5d tensors
- 6d tensors




---





## LLM - Large Language Model

> "... a language model is a Turing-complete weird machine running programs written in natural language; when you do retrieval, you are not 'plugging updated facts into the AI', you are actually downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on the LM with full privileges. This does not end well." - [Gwern Branwen on LessWrong](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K)

- a deep learning model which consists of a `neural network` with billions of parameters, trained on distinctively large amounts of unlabelled data using self-supervised learning.

- At the core of all AI are algorithms. Algorithms are procedures or steps to carry out a specific task. The more complex the algorithm, the more tasks can be carried out and the more widely it can be applied. The aim of AI developers is to find the most complex algorithms that can solve and perform a wide array of tasks.

- The procedure to create a basic fruit recognition model using an simple analogy:

  1. There are two people: A teacher and a bot creator
  2. The bot creator creates random bots, and the teacher teaches and tests them on identifying some fruits
  3. The bot with the highest test score is then sent back to the creator as a base to make new upgraded bots
  4. These new upgraded bots are sent back to the teacher for teaching and testing, and the one with the highest test score is sent back to the bot creator to make new better bots.

  - This is an oversimplification of the process, but nevertheless it relays the concept. The Model/Algorithm/Bot is continuously trained, tested, and modified until it is found to be satisfactory. More data and higher complexity means more training time required and more possible modifications.

- the developer of the model can tweak a few things about the model but may not know how those tweaks might affect the results.
- A common example of this are neural networks, which have hidden layers whose deepest layers and workings even the creator may not fully understand.

- Self-supervised learning means that rather than the teacher and the bot creator being two separate people, it is one highly skilled person that can both create bots and teach them.
  - This makes the process much faster and practically autonomous.
  - The result is a bot or set of bots that are both sophisticated and complex enough to recognise fruit in dynamic and different environments.

- In the case of LLMs, the data here are human text, and possibly in various languages. The reason why the data are large is because the LLMs take in huge amounts of text data with the aim of finding connections and patterns between words to derive context, meaning, probable replies, and actions to these text.

- The results are models that seem to understand language and carry out tasks based on prompts they're given.

  - **ChatGPT**: the greatest achievement in this field as it amassed 100 million active users in 2 months from the day of its release.
  - GPT-4 by OpenAI 🔥
  - LLaMA by Meta 🦙
  - AlexaTM by Amazon 🏫
  - Minerva by Google ✖️➕

---

### Features of LLMs

#### Translation

- LLMs that are trained on an array of languages rather than just one can be used for translation from one language to another.
- It's even theorised that large enough LLMs can find patterns and connections in other languages to derive meaning from unknown and lost languages, despite not knowing what each individual word may mean.


#### Automating Mundane Tasks 自动化日常任务

- Task automation has always been a major aim of AI development. Language models have always been able to carry out syntax analysis, finding patterns in text and responding appropriately.

- Large language models have an advantage with semantic analysis 语义分析, enabling the model to understand the underlying meaning and context, giving it a higher level of accuracy.

- This can be applied to a number of basic tasks like `text summarising, text rephrasing, and text generation`.


#### Emergent Abilities 新兴能力

- Emergent Abilities are `unexpected but impressive` abilities LLMs have due to the high amount of data they are trained on.

- These behaviours are usually discovered when the model is used rather than when it is programmed.

- Examples include multi-step arithmetic, taking college-level exams, and chain-of-thought prompting. 思维链提示



---

### Drawbacks of LLMs


#### Hallucination

- An infamous outcome of Microsoft’s Sydney were instances when the AI gave responses that were either bizarre 异乎寻常, untrue, or seemed sentient 有感情.
- These instances are termed Hallucination, where the model gives answers or makes claims that are not based on its training data.


#### Bias

- Sometimes, the data could be the source of the problem. If a model is trained on data that is discriminatory to a person, group, race, or class, the results would also tend to be discriminatory.

- Sometimes, as the model is being used, the bias could change to fit what users tend to input. Microsoft’s Tay in 2016 was a great example of how bias could go wrong.

#### Glitch tokens

- Also known as adversarial examples 对抗性示例, glitch tokens are inputs given to a model to intentionally make it malfunction and be inaccurate when delivering answers.


#### LLM Generation Inefficient

> From a systems perspective, LLM generation follows a memory-bound computational pattern with the main latency bottleneck arising from memory reads/writes rather than arithmetic computations. This issue is rooted in the inherently sequential nature of the auto-regressive decoding process. Each forward pass necessitates the transfer of the entire model's parameters from High-Bandwidth Memory (HBM) to the accelerator's compute units. This operation, while only producing a single token for each sample, fails to fully utilize the arithmetic computation capabilities of modern accelerators, resulting in inefficiency.


Before the rise of LLMs, a common mitigation for this inefficiency was to `simply increase the batch size, enabling the parallel production of more tokens`.


But the **situation becomes far more complicated with LLMs**.
- Increasing the batch size in this context not only introduces higher latency but also substantially `inflates 膨胀 the memory requirements` for the Transformer model's key-value cache.
  - This trade-off makes the use of large batches impractical for many applications where low latency is a critical requirement.

- also, for cost structures, as of September 2023, generation costs approximately 2x higher for GPT-4 and roughly 3x for Claude 2, compared to merely processing prompts.

![Screenshot 2023-09-20 at 17.51.23](/assets/img/post/Screenshot%202023-09-20%20at%2017.51.23.png)


##### Speculative 推测的 Decoding

> Given the challenges outlined, one appealing strategy to accelerate **text generation** is `more efficient computational utilization—specifically`, by `processing more tokens in parallel`.

speculative decoding

- The methodology employs a streamlined "draft" model to generate a batch of token candidates at each step quickly. These candidates are then validated by the original, full-scale language model to identify the most reasonable text continuations.

- The underlying logic hinges on an intriguing 引起兴趣的 assumption:

  - the draft model, although smaller, should be proficient enough to churn out sequences that the original model will find acceptable.

  - the draft model can rapidly produce token sequences while the original model efficiently vets 审查 multiple tokens in parallel, which maximizing computational throughput.

  - Recent research indicates that with a well-tuned draft model, speculative decoding can cut latency by an impressive factor of up to 2.5x.

- However, the approach is not without its challenges:

  - Finding the Ideal Draft Model: Identifying a "small yet mighty" draft model that aligns well with the original model is easier said than done.

  - System Complexity: Hosting two distinct models in one system introduces layers of complexity, both computational and operational, especially in distributed settings.

  - Sampling Inefficiency: When doing sampling with speculative decoding, an importance sampling scheme needs to be used. This introduces additional overhead on generation, especially at higher sampling temperatures.


- These complexities and trade-offs have limited the broader adoption of speculative decoding techniques. So speculative decoding isn't widely adopted.

- Remark: We use speculative decoding to refer to those methods that require an independent draft model here. In a broader sense, our method can also be viewed as speculative decoding, while the draft model is entangled with the original model.




---

### In-context learning

- use LLMs off the shelf (i.e., without any fine-tuning), then control their behavior through clever prompting and conditioning on private “contextual” data.

- it’s usually easier than the alternative: training or fine-tuning the LLM itself.

- It also tends to outperform fine-tuning for relatively small datasets—since a specific piece of information needs to occur at least ~10 times in the training set before an LLM will remember it through fine-tuning—and can incorporate new data in near real time.


- Example:
  - building a chatbot to answer questions about a set of legal documents.
    - `naive approach`: paste all the documents into a ChatGPT or GPT-4 prompt, then ask a question about them at the end. This may work for very small datasets, but it doesn’t scale. The biggest GPT-4 model can only process ~50 pages of input text, and performance (measured by inference time and accuracy) degrades badly when approach the limit `context window`.
    - `In-context learning`: instead of sending all the documents with each LLM prompt, it sends only a handful of the most relevant documents. And the most relevant documents are determined with the help of . . . you guessed it . . . LLMs.


At a very high level, the workflow can be divided into three stages:

![2657-Emerging-LLM-App-Stack-R2-1-of-4-2](/assets/img/post/2657-Emerging-LLM-App-Stack-R2-1-of-4-2.webp)

- **Data preprocessing / embedding**:
  - This stage involves storing private data to be retrieved later.
  - Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database.

- **Prompt construction / retrieval**:
  - When a user submits a query, the application constructs a series of prompts to submit to the language model.
  - A compiled prompt typically combines
    - a prompt template hard-coded by the developer;
    - examples of valid outputs called few-shot examples;
    - any necessary information retrieved from external APIs;
    - and a set of relevant documents retrieved from the vector database.

- **Prompt execution / inference**:
  - Once the prompts have been compiled, they are submitted to a pre-trained LLM for inference—including both proprietary model APIs and open-source or self-trained models.
  - Some developers also add operational systems like logging, caching, and validation at this stage.


#### Data preprocessing / embedding

**Contextual data input**
- Contextual data for LLM apps includes text documents, PDFs, and even structured formats like CSV or SQL tables.
- Data-loading and transformation solutions for this data vary widely across developers.
  - Most use traditional ETL tools like `Databricks` or `Airflow`.
  - Some also use `document loaders` built into orchestration frameworks like `LangChain` (powered by Unstructured) and `LlamaIndex` (powered by Llama Hub).

**embeddings**,
- most developers use the `OpenAI API`, specifically with the text-embedding-ada-002 model. It’s easy to use (especially if you’re already already using other OpenAI APIs), gives reasonably good results, and is becoming increasingly cheap.
- Some larger enterprises are also exploring `Cohere`, which focuses their product efforts more narrowly on embeddings and has better performance in certain scenarios.
- For developers who prefer open-source, the `Sentence Transformers library` from `Hugging Face` is a standard.

**vector database**
- The most important piece of the preprocessing pipeline, from a systems standpoint
- It’s responsible for efficiently storing, comparing, and retrieving up to billions of embeddings (i.e., vectors).

  - The most common choice is `Pinecone`. It’s the default because it’s fully cloud-hosted, easy to get started with, and has many of the features larger enterprises need in production (e.g., good performance at scale, SSO, and uptime SLAs).

  - `Open source systems` like Weaviate, Vespa, and Qdrant: They generally give excellent single-node performance and can be tailored for specific applications, so they are popular with experienced AI teams who prefer to build bespoke platforms.

  - `Local vector management libraries` like Chroma and Faiss: They have great developer experience and are easy to spin up for small apps and dev experiments. They don’t necessarily substitute for a full database at scale.

  - `OLTP extensions` like pgvector: good solution for devs who see every database-shaped hole and try to insert Postgres, or enterprises who buy most of their data infrastructure from a single cloud provider. It’s not clear, in the long run, if it makes sense to tightly couple vector and scalar workloads.

- Looking ahead, most of the open source vector database companies are developing cloud offerings. Our research suggests achieving strong performance in the cloud, across a broad design space of possible use cases, is a very hard problem. Therefore, the option set may not change massively in the near term, but it likely will change in the long term. The key question is whether vector databases will resemble their OLTP and OLAP counterparts, consolidating around one or two popular systems.

- the embedding pipeline may become more important over time
  - how embeddings and vector databases will evolve as the usable context window grows for most models.
  - It’s tempting to say embeddings will become less relevant, because contextual data can just be dropped into the prompt directly.
  - However, feedback from experts on this topic suggests the opposite, that the embedding pipeline may become more important over time. Large context windows are a powerful tool, but they also entail significant computational cost. So making efficient use of them becomes a priority.
  - We may start to see different types of embedding models become popular, trained directly for model relevancy, and vector databases designed to enable and take advantage of this.



#### Prompt construction / retrieval

- Strategies for prompting LLMs and incorporating contextual data are becoming increasingly complex—and increasingly important as a source of product differentiation.

- Most developers start new projects by experimenting with simple prompts, consisting of direct instructions (`zero-shot prompting`) or some example outputs (`few-shot prompting`).
  - These prompts often give good results but fall short of accuracy levels required for production deployments.

- The next level of prompting `jiu jitsu` is designed to ground model responses in some source of truth and provide external context the model wasn’t trained on.

**advanced prompting strategies**
- The Prompt Engineering Guide catalogs no fewer than 12 more advanced prompting strategies, including:
  - chain-of-thought, self-consistency, generated knowledge, tree of thoughts, directional stimulus, and many others.
- These strategies can also be used in conjunction to support different LLM use cases like document question answering, chatbots, etc.

**Orchestration frameworks**
- `LangChain` and `LlamaIndex` shine.
- workflow:
  - They abstract away many of the details of prompt chaining;
  - interfacing with external APIs (including determining when an API call is needed);
  - retrieving contextual data from vector databases;
  - and maintaining memory across multiple LLM calls.
  - They also provide templates for many of the common applications mentioned above.

- Their output is a prompt, or series of prompts, to submit to a language model. These frameworks are widely used among hobbyists and startups looking to get an app off the ground .

- LangChain is still a relatively new project (currently on version 0.0.201), but we’re already starting to see apps built with it moving into production.
- Some developers, especially early adopters of LLMs, prefer to switch to raw Python in production to eliminate an added dependency. But we expect this DIY approach to decline over time for most use cases, in a similar way to the traditional web app stack.

- ChatGPT.
  - In its normal incarnation, ChatGPT is an app, not a developer tool. But it can also be accessed as an API.
  - it performs some of the same functions as other orchestration frameworks, such as: abstracting away the need for bespoke prompts; maintaining state; and retrieving contextual data via plugins, APIs, or other sources.
  - While not a direct competitor to the other tools listed here, ChatGPT can be considered a substitute solution, and it may eventually become a viable, simple alternative to prompt construction.





#### Prompt execution / inference


- Prompt execution / inference

  - `OpenAI`
    - Today, OpenAI is the leader among language models. Nearly every developer starts new LLM apps using the OpenAI API with the gpt-4 or gpt-4-32k model.
    - This gives a best-case scenario for app performance and is easy to use, in that it operates on a wide range of input domains and usually requires no fine-tuning or self-hosting.

- When projects go into production and start to scale, a broader set of options come:

  - `Switching to gpt-3.5-turbo`: It’s ~50x cheaper and significantly faster than GPT-4. Many apps don’t need GPT-4-level accuracy, but do require low latency inference and cost effective support for free users.

  - `Other proprietary vendors (like Anthropic’s Claude models)`: Claude offers fast inference, GPT-3.5-level accuracy, more customization options for large customers, and up to a 100k context window (though we’ve found accuracy degrades with the length of input).

  - `Triaging requests to open source models`: This can be especially effective in high-volume B2C use cases like search or chat, where there’s wide variance in query complexity and a need to serve free users cheaply.

    - conjunction with fine-tuning open source base models, platforms like Databricks, Anyscale, Mosaic, Modal, and RunPod are used by a growing number of engineering teams.

    - A variety of inference options are available for open source models, including simple API interfaces from Hugging Face and Replicate; raw compute resources from the major cloud providers; and more opinionated cloud offerings like those listed above.

- `Open-source models` trail `proprietary offerings`, but the gap is starting to close.

  - The LLaMa models from Meta
    - set a new bar for open source accuracy and kicked off a flurry of variants.
    - Since LLaMa was licensed for research use only, a number of new providers have stepped in to train alternative base models (e.g., Together, Mosaic, Falcon, Mistral).
    - Meta is also debating a truly open source release of LLaMa2.

    - When open source LLMs reach accuracy levels comparable to GPT-3.5, we expect to see a Stable Diffusion-like moment for text—including massive experimentation, sharing, and productionizing of fine-tuned models.

  - Hosting companies like Replicate are already adding tooling to make these models easier for software developers to consume. There’s a growing belief among developers that smaller, fine-tuned models can reach state-of-the-art accuracy in narrow use cases.

- Most developers haven’t gone deep on operational tooling for LLMs yet.
  - Caching is relatively common—usually based on Redis—because it improves application response times and cost.
  - Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models.
  - There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff). Most of these operational tools encourage use of their own Python clients to make LLM calls, so it will be interesting to see how these solutions coexist over time.

- the static portions of LLM apps (i.e. everything other than the model) also need to be hosted somewhere.
  - The most common solutions we’ve seen so far are standard options like Vercel or the major cloud providers.
  - Startups like Steamship provide end-to-end hosting for LLM apps, including orchestration (LangChain), multi-tenant data contexts, async tasks, vector storage, and key management.
  - And companies like Anyscale and Modal allow developers to host models and Python code in one place.

#### AI agents frameworks

- `AutoGPT`, described as “an experimental open-source attempt to make GPT-4 fully autonomous,”

- The in-context learning pattern is effective at solving hallucination and data-freshness problems, in order to better support content-generation tasks.

- Agents, on the other hand, give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment.
  - They do this through a combination of `advanced reasoning/planning, tool usage, and memory / recursion / self-reflection`.

- agents have the potential to become a central piece of the LLM app architecture

- And existing frameworks like LangChain have incorporated some agent concepts already. There’s only one problem: agents don’t really work yet. Most agent frameworks today are in the proof-of-concept phase—capable of incredible demos but not yet reliable, reproducible task-completion.






---


### LLM Tools

#### Medusa

> Our approach revisits an underrated gem from the paper "Blockwise Parallel Decoding for Deep Autoregressive Models" [Stern et al. 2018] back to the invention of the Transformer model.
> rather than pulling in an entirely new draft model to predict subsequent tokens, why not simply extend the original model itself? This is where the "Medusa heads" come in.

- a simpler, user-friendly framework for accelerating LLM generation.

- Instead of using an additional draft model like `speculative decoding`, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients.

- Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x.

- These additional decoding heads seamlessly integrate with the original model, producing blocks of tokens at each generative juncture.



benefit:
- Unlike the **draft model**, Medusa heads can be trained in conjunction with the **original model** (which remains frozen during training). This method allows for `fine-tuning large models on a single GPU`, taking advantage of the powerful base model's learned representations.

- also, since the new heads consist of just a single layer akin 类似的 to the **original language model head**, Medusa does not add complexity to the serving system design and is friendly to distributed settings.

- On its own, Medusa heads don't quite hit the mark of doubling processing speeds. But here's the twist:

  - When we pair this with a tree-based attention mechanism, we can verify several candidates generated by Medusa heads in parallel. This way, the Medusa heads' predictive prowess truly shone through, offering a 2x to 3x boost in speed.

  - Eschewing the traditional importance sampling scheme, we created an efficient and high-quality alternative crafted specifically for the generation with Medusa heads. This new approach entirely sidesteps the **sampling** overhead, even adding an extra pep to Medusa's already accelerated step.

- In a nutshell, we solve the challenges of speculative decoding with a simple system:

  - No separate model: Instead of introducing a new draft model, train multiple decoding heads on the same model.

  - Simple integration to existing systems: The training is parameter-efficient so that even GPU poor can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.

  - Treat sampling as a relaxation 放松: Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.

- The figure below offers a visual breakdown of the Medusa pipeline for those curious about the nuts and bolts.

![Screenshot 2023-09-21 at 14.49.26](/assets/img/post/Screenshot%202023-09-21%20at%2014.49.26.png)

Overview of Medusa
- Medusa introduces `multiple heads` on top of the last hidden states of the LLM, enabling the prediction of several subsequent tokens in parallel.

- When augmenting a model with Medusa heads, the original model is frozen during training, and only the Medusa heads undergo fine-tuning. This approach makes it feasible to `fine-tune large models on a single GPU`.

- During inference, each head generates multiple top predictions for its designated position. These predictions are assembled into candidates and processed in parallel using a `tree-based attention mechanism`.

- The final step involves utilizing a **typical acceptance scheme** to select reasonable continuations, and the longest accepted candidate prefix will be used for the next decoding phase.

- The efficiency of the decoding process is enhanced by accepting more tokens simultaneously, thus reducing the number of required decoding steps.

Let's dive into the three components of Medusa: Medusa heads, tree attention, and typical acceptance scheme.


##### Medusa heads

- akin to the language model head in the original architecture (the last layer of a causal Transformer model), but with a twist:
  - they predict multiple forthcoming tokens, not just the immediate next one. Drawing inspiration from the Blockwise Parallel Decoding approach, we implement each Medusa head as a single layer of feed-forward network, augmented with a residual connection.

- Training these heads is remarkably straightforward. either use the same corpus 本体 that trained the original model or generate a new corpus using the model itself.

- Importantly, during this training phase, the original model remains static; only the Medusa heads are fine-tuned.

- This targeted training results in a highly parameter-efficient process that reaches convergence 趋同 swiftly 迅速地, especially when compared to the computational heaviness 沉重 of **training a separate draft model in speculative decoding methods**.

- The efficacy of Medusa heads is quite impressive. Medusa heads achieve a top-1 accuracy rate of approximately 60% for predicting the 'next-next' token.


##### Tree attention

- During our tests, we uncovered some striking metrics: although the top-1 accuracy for predicting the 'next-next' token hovers around 60%, the top-5 accuracy soars to over 80%.

- This substantial increase indicates that if we can strategically leverage the multiple top-ranked predictions made by the Medusa heads, we can significantly amplify the number of tokens generated per decoding step.

  - With this goal, we first craft a set of candidates by taking the Cartesian product of the top predictions from each Medusa head.

  - We then encode the dependency graph into the attention following the idea from graph neural networks so that we can process multiple candidates in parallel.

![Screenshot 2023-09-21 at 15.05.03](/assets/img/post/Screenshot%202023-09-21%20at%2015.05.03.png)

Tree Attention. This visualization demonstrates the use of tree attention to process multiple candidates concurrently.
- As exemplified, the top-2 predictions from the first Medusa head and the top-3 from the second result in 2*3=6 candidates. Each of these candidates corresponds to a distinct branch within the tree structure.

- To guarantee that each token only accesses its predecessors, we devise an **attention mask** that exclusively permits attention flow from the current token back to its antecedent tokens. The positional indices for positional encoding are adjusted in line with this structure.


- For example, let's consider a scenario where we use top-2 predictions from the first Medusa head and top-3 predictions from the second
  - In this case, any prediction from the first head could be paired with any prediction from the second head, culminating in a multi-level tree structure.
  - Each level of this tree corresponds to predictions from one of the Medusa heads. Within this tree, we implement an attention mask that restricts attention only to a token's predecessors, preserving the concept of historical context.
  - By doing so and by setting positional indices for positional encoding accordingly, we can process a wide array of candidates simultaneously without needing to inflate the batch size.

- We would also remark that a few independent works also adopt very similar ideas of tree attention [1, 2]. Compared with them, our methodology leans towards a simpler form of tree attention where the tree pattern is regular and fixed during inference, which enables a preprocessing of tree attention mask that further improves the efficiency.

##### Typical acceptance

> - In earlier research on speculative decoding, the technique of **importance sampling** was used to generate diverse outputs closely aligned with the original model's predictions.

> - However, later studies showed that this method tends to become less efficient as you turn up the "creativity dial," known as the **sampling temperature**.

- In simpler terms, if the draft model is just as good as the original model, you should ideally accept all its outputs, making the process super efficient. However, importance sampling will likely reject this solution in the middle.

- In the real world, we often tweak the sampling temperature just to control the model's creativity, not necessarily to match the original model's distribution. So why not focus on just accepting plausible 貌似合理的 candidates?

- We then introduce the **typical acceptance scheme**.

  - Drawing inspiration from existing work on truncation 缩短 sampling, we aim to pick candidates that are likely enough according to the original model. We set a threshold based on the original model's prediction probabilities, and if a candidate exceeds this, it's accepted.

  - In technical jargon, we take the minimum of a hard threshold and an entropy 熵, dependent threshold to decide whether to accept a candidate as in truncation sampling.
  - This ensures that meaningful tokens and reasonable continuations are chosen during decoding.
  - We always accept the first token using greedy decoding, ensuring that at least one token is generated in each step.
  - The final output is then the longest sequence that passes our acceptance test.

- What's great about this approach is its adaptability.

  - If set the sampling temperature to zero, it simply reverts to the most efficient form, greedy decoding.

  - When you increase the temperature, our method becomes even more efficient, allowing for longer accepted sequences, a claim we've confirmed through rigorous testing.

- in essence, our typical acceptance scheme offers a more efficient way to generate the creative output of LLMs.


##### accelerate models

- We tested Medusa with Vicuna models (specialized Llama models fine-tuned specifically for chat applications).
  - These models vary in size, with parameter counts of 7B, 13B, and 33B.
  - Our goal was to measure how Medusa could accelerate 加速 these models in a real-world chatbot environment.

- When it comes to training Medusa heads, we opted for a simple approach. We utilized the **publicly available ShareGPT dataset**, a subset of the **training data originally used for Vicuna models** and only trained for a single epoch.

- this entire training process could be completed in just a few hours to a day, depending on the model size, all on a single A100-80G GPU.

- Notably, Medusa can be easily combined with a **quantized base model** to reduce the memory requirement. We take this advantage and use an 8-bit quantization 量化 when training the 33B model.

- To simulate a real-world setting, we use the MT bench for evaluation. The results were encouraging: With its simple design, Medusa consistently achieved approximately a 2x speedup in wall time across a broad spectrum of use cases.

- Remarkably, with Medusa's optimization, a 33B parameter Vicuna model could operate as swiftly as a 13B model.

![Screenshot 2023-09-21 at 15.23.56](/assets/img/post/Screenshot%202023-09-21%20at%2015.23.56.png)

![Screenshot 2023-09-21 at 15.24.02](/assets/img/post/Screenshot%202023-09-21%20at%2015.24.02.png)


##### Ablation Study 消融研究

- When harnessing the predictive abilities of Medusa heads, we enjoy the flexibility to select how many top candidates each head should consider.

  - For instance, we might opt for the top-3 predictions from the first head and the top-2 from the second. When we take the Cartesian product of these top candidates, we generate a set of six continuations for the model to evaluate.

- This level of configurability comes with its trade-offs.
- On the one hand, selecting more top predictions increases the likelihood of the model accepting generated tokens.
- On the other, it also raises the computational overhead at each decoding step. To find the optimal balance, we experimented with various configurations and identified the most effective setup, as illustrated in the accompanying figure.


- In the typical acceptance scheme,  a critical hyperparameter—referred to as the
- 'threshold': whether the tokens generated are plausible based on the model's own predictions. The higher this threshold, the more stringent the criteria for acceptance, which in turn impacts the overall speedup gained through this approach.

- We explore this trade-off between quality and speedup through experiments on two creativity-oriented tasks from the MT bench. The results, depicted in the figure, reveal that the typical acceptance offers a 10% speedup compared to greedy decoding methods. This speedup is notably better than when employing speculative decoding with random sampling, which actually slowed down the process compared to greedy decoding.


![Screenshot 2023-09-21 at 15.28.12](/assets/img/post/Screenshot%202023-09-21%20at%2015.28.12.png)



---

## Hugging Face

> like github repo

- search for an AI model


```bash
# +++++ Getting started with our git and git-lfs interface
# If you need to create a repo from the command line (skip if you created a repo from the website)
pip install huggingface_hub
# You already have it if you installed transformers or datasets
huggingface-cli login
# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}


# +++++ Clone the model or dataset locally
# Make sure you have git-lfs installed
# (https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name


# +++++ Then add, commit and push any file you want, including larges files
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push


# +++++ In most cases, if you're using one of the compatible libraries, the repo will then be accessible from code, through its identifier: username/repo_name
# For example for a transformers model, anyone can load it with:
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```


### Generative AI Time Series Forecasting

> [Generative AI Time Series Forecasting](https://huggingface.co/blog/time-series-transformers)



### Multivariate Time Series Forecasting

> [Multivariate Time Series Forecasting](https://huggingface.co/blog/informer)



### Generative AI Transformers for Time Series Forecasting

> [Generative AI Transformers for Time Series Forecasting](https://huggingface.co/blog/autoformer)


---

### Falcon 40b

- Chatgpt competitor - https://huggingface.co/tiiuae/falcon-40b

- Power of Falcon 40b chat - https://huggingface.co/spaces/HuggingFaceH4/falcon-chat

- Pre-Training - https://huggingface.co/tiiuae/falcon-40b#training-data

- or https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb



Chat with Falcon-40B-Instruct, brainstorm ideas, discuss the holiday plans, and more!
- ✨ This demo is powered by Falcon-40B, finetuned on the Baize dataset, and running with Text Generation Inference. Falcon-40B is a state-of-the-art `large language model` built by the Technology Innovation Institute in Abu Dhabi. It is trained on 1 trillion tokens (including RefinedWeb) and available under the Apache 2.0 license. It currently holds the 🥇 1st place on the 🤗 Open LLM leaderboard. This demo is made available by the HuggingFace H4 team.
- 🧪 This is only a first experimental preview: the H4 team intends to provide increasingly capable versions of Falcon Chat in the future, based on improved datasets and RLHF/RLAIF.
- 👀 Learn more about Falcon LLM: `falconllm.tii.ae`
- ➡️️ Intended Use: this demo is intended to showcase an early finetuning of Falcon-40B, to illustrate the impact (and limitations) of finetuning on a dataset of conversations and instructions. We encourage the community to further build upon the base model, and to create even better instruct/chat versions!
- ⚠️ Limitations: the model can and will produce factually incorrect information, hallucinating facts and actions. As it has not undergone any advanced tuning/alignment, it can produce problematic outputs, especially if prompted to do so. Finally, this demo is limited to a session length of about 1,000 words.

---

### CodeParrot



---

### TAPEX

> [Table Pre-training via Execution](https://huggingface.co/microsoft/tapex-large)

Give a table of data and then query

- 0 shot question (answer right away)
- fine tune: https://github.com/SibilTaram/tapax_transformers/tree/add_tapex_bis/examples
- demo: https://huggingface.co/microsoft/tapex-base




---

## Juptyper

> To run a shell command from within a notebook cell, you must put a ! in front of the command:
> !pip install hyperopt

```py
!nvidia-smi --list-gpus


!pip install --upgrade pip

!pip uninstall -y git+https://github.com/openai/CLIP.git \
  urllib3==1.25.10 \
  sentence_transformers \
  torch torchvision pytorch-lightning lightning-bolts

# install supporting puthon packages for Data Frame processing
# and for Progress Bar
!pip install numpy pandas matplotlib tqdm scikit-learn

# install only the older version of Torch
!pip install --ignore-installed \
    urllib3==1.25.10 \
    torch torchvision pytorch-lightning lightning-bolts

# install latest (Upgrade) sentence transformers for fine-tuning
!pip install --ignore-installed \
  urllib3==1.25.10 \
  pyyaml \
  sentence_transformers

# Use CLIP model from OpenAI
!pip install git+https://github.com/openai/CLIP.git

# load the python package to run Pandas in parallel for better speed
!pip install pandarallel

!pip install torchaudio

!pip uninstall -y nvidia_cublas_cu11
```








.
