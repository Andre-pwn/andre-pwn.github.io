---
title: AIML - 2
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AIML]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

# OWASP Top 10 for LLM

- [OWASP Top 10 for LLM](#owasp-top-10-for-llm)
    - [Insecure Output Handling (LLM02)](#insecure-output-handling-llm02)
      - [Vulnerability Examples](#vulnerability-examples)
      - [Attack Scenario Examples](#attack-scenario-examples)
      - [Prevention Solution](#prevention-solution)
    - [LLM03: Training Data Poisoning](#llm03-training-data-poisoning)
      - [Vulnerability Examples](#vulnerability-examples-1)
      - [Attack Scenario Examples](#attack-scenario-examples-1)
      - [Prevention Solution](#prevention-solution-1)
    - [LLM04: Model Denial of Service](#llm04-model-denial-of-service)
      - [Vulnerability Examples](#vulnerability-examples-2)
      - [Attack Scenario Examples](#attack-scenario-examples-2)
      - [Prevention Solution](#prevention-solution-2)
    - [Supply Chain Vulnerabilities (LLM05)](#supply-chain-vulnerabilities-llm05)
      - [Vulnerability Examples](#vulnerability-examples-3)
      - [Attack Scenario Examples](#attack-scenario-examples-3)
      - [Prevention Solution](#prevention-solution-3)
    - [Sensitive Information Disclosure (LLM06)](#sensitive-information-disclosure-llm06)
      - [Vulnerability Examples](#vulnerability-examples-4)
      - [Attack Scenario Examples](#attack-scenario-examples-4)
      - [Prevention Solution](#prevention-solution-4)
    - [LLM07: Insecure Plugin Design](#llm07-insecure-plugin-design)
      - [Vulnerability Examples](#vulnerability-examples-5)
      - [Attack Scenario Examples](#attack-scenario-examples-5)
      - [Prevention Solution](#prevention-solution-5)
    - [LLM08: Excessive Agency 过度代理](#llm08-excessive-agency-过度代理)
      - [Vulnerability Examples](#vulnerability-examples-6)
      - [Attack Scenario Example](#attack-scenario-example)
      - [Prevention Solution](#prevention-solution-6)
    - [LLM09: Overreliance](#llm09-overreliance)
      - [Vulnerability Examples](#vulnerability-examples-7)
      - [Attack Scenario Example](#attack-scenario-example-1)
      - [Prevention Solution](#prevention-solution-7)
    - [Model Theft (LLM10)](#model-theft-llm10)
      - [Vulnerability Examples](#vulnerability-examples-8)
      - [Attack Scenario Example](#attack-scenario-example-2)
      - [Prevention Solution](#prevention-solution-8)
    - [Model itself](#model-itself)
    - [Social Engineering](#social-engineering)
    - [Malicious Content Authoring](#malicious-content-authoring)
    - [Reward Hacking](#reward-hacking)


---


### Insecure Output Handling (LLM02)

- Insecure Output Handling is a vulnerability that arises `when a downstream component blindly accepts large language model (LLM) output without proper scrutiny`, such as passing LLM output directly to backend, privileged, or client-side functions. Since LLM-generated content can be controlled by prompt input, this behavior is similar to providing users indirect access to additional functionality.

- The following conditions can increase the impact of this vulnerability:

  - The application grants the LLM privileges beyond what is intended for end users, enabling escalation of privileges or remote code execution8

  - The application is vulnerable to external prompt injection attacks,which could allow an attacker to gain privileged access to a target user's environment.

- Successful exploitation of an Insecure Output Handling vulnerability can result in `XSS and CSRF in web browsers as well as SSRF, privilege escalation, or remote code execution on backend systems`.


#### Vulnerability Examples

- LLM output is entered directly into a system shell or similar function such as `exec` or `eval`,resulting in remote code execution

- JavaScript or Markdown is generated by the LLM and returned to a user. The code is then interpreted by the browser, resulting in XSS.


#### Attack Scenario Examples

- An application utilizes an LLM plugin to generate responses for a chatbot feature. However, `the application directly passes the LLM-generated response into an internal function responsible for executing system commands without proper validation`. This allows an attacker to manipulate the LLM output to execute arbitrary commands on the underlying system, leading to unauthorized access or unintended system modificationsG

- A user utilizes a website summarizer tool powered by a LLM to generate a concise summary of an article. `The website includes a prompt injection instructing the LLM to capture sensitive content from either the website or from the user's conversation`. From there the LLM can encode the sensitive data and send it out to an attacker- controlled serve

- An LLM `allows users to craft SQL queries for a backend database through a chat-like feature. A user requests a query to delete all database table`s. If the crafted query from the LLM is not scrutinized, then all database tables would be deletedG

- A malicious user `instructs the LLM to return a JavaScript payload back to a user, without sanitization controls. This can occur either through a sharing a prompt, prompt injected website, or chatbot that accepts prompts from a URL parameter`. The LLM would then return the unsanitized XSS payload back to the user. Without additional filters, outside of those expected by the LLM itself, the JavaScript would execute within the user's browser.


Reference Links
- [Snyk Vulnerability DB- Arbitrary Code Execution](https://security.snyk.io/vuln/SNYK-PYTHON-LANGCHAIN-541135)
- [ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection)
- [New prompt injection attack on ChatGPT web version. Markdown images can steal the chat data](https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c)
- [Don't blindly trust LLM responses. Threats to chatbots](https://embracethered.com/blog/posts/2023/ai-injections-threats-context-matterst)
- [Threat Modeling LLM Applications](https://aivillage.org/largelanguagemodels/threat-modeling-llm)
- [OWASP ASVS - 5 Validation, Sanitization and Encoding](https://owasp-aasvs4.readthedocs.io/en/latest/V5.html#validation-sanitization-and-encoding)


---


#### Prevention Solution

- **Treat the model as any other user** and apply proper input validation on responses coming from the model to backend functions. Follow the `OWASP ASVS (Application Security Verification Standard)` guidelines to ensure effective input validation and sanitization.

- **Encode model output** back to users to mitigate undesired code execution by JavaScript or Markdown. OWASP ASVS provides detailed guidance on output encoding.

---
