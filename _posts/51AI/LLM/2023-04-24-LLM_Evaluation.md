---
title: LLM - Evaluation
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AI, LLM]
# img: /assets/img/sample/rabbit.png
tags: [AI, ML]
---

- [Evaluation](#evaluation)
  - [overall](#overall)
    - [computation-based evaluation pipeline](#computation-based-evaluation-pipeline)
      - [*Recall vs. Precision*](#recall-vs-precision)
      - [Implementation](#implementation)
    - [Sample](#sample)
    - [LlamaIndex](#llamaindex)

---

# Evaluation

---

## overall

---

### computation-based evaluation pipeline

evaluate the performance of foundation models and tuned models

- evaluated using a set of metrics against the evaluation dataset

evaluation dataset
- create an evaluation dataset that contains prompt and ground truth pairs.
  - For each pair, the prompt is the input that you want to evaluate, and the ground truth is the ideal response for that prompt.
  - During evaluation, the prompt in each pair of the evaluation dataset is passed to the model to produce an output.
  - `The output generated by the model` and `the ground truth from the evaluation dataset` are used to compute the evaluation metrics.

- dataset must include a minimum of 1 prompt and ground truth pair and at least 10 pairs for meaningful metrics.

- The more examples you give, the more meaningful the results.


The type of metrics used for evaluation depends on the task that you are evaluating.

| Task               | Metric                           |
| ------------------ | -------------------------------- |
| Classification     | Micro-F1, Macro-F1, Per class F1 |
| Summarization      | ROUGE-L                          |
| Question answering | Exact Match                      |
| Text generation    | BLEU, ROUGE-L                    |


**Recall-Oriented Understudy for Gisting Evaluation (ROUGE)**:
- A metric used to evaluate the quality of automatic summaries of text. It works by comparing a generated summary to a set of reference summaries created by humans.

- take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:
  - `rouge-1`, which measures unigram overlap 单字重叠
  - `rouge-2`, which measures bigram overlap 二元组重叠
  - `rouge-l`, which measures the longest common subsequence 最长公共子序列

---

#### *Recall vs. Precision*

**Recall**: prioritizes `how much of the information in the reference summaries` is captured in the generated summary.

**Precision**: `how much of the generated summary` is relevant to the original text.


#### Implementation

```py
from google.auth import default
import vertexai
from vertexai.preview.language_models import (
    EvaluationTextClassificationSpec,
    TextGenerationModel,
)

# Set credentials for the pipeline components used in the evaluation task
credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])

def evaluate_model(project_id: str, location: str) -> object:
    """Evaluate the performance of a generative AI model."""

    vertexai.init(project=project_id, location=location, credentials=credentials)

    # Create a reference to a generative AI model
    model = TextGenerationModel.from_pretrained("text-bison@002")

    # Define the evaluation specification for a text classification task
    task_spec = EvaluationTextClassificationSpec(
        ground_truth_data=[
            "gs://cloud-samples-data/ai-platform/generative_ai/llm_classification_bp_input_prompts_with_ground_truth.jsonl"
        ],
        class_names=["nature", "news", "sports", "health", "startups"],
        target_column_name="ground_truth",
    )

    # Evaluate the model
    eval_metrics = model.evaluate(task_spec=task_spec)
    print(eval_metrics)
    return eval_metrics
```





---

### Sample

### LlamaIndex

```py
from deepeval.integrations.llama_index import (
    DeepEvalAnswerRelevancyEvaluator,
    DeepEvalFaithfulnessEvaluator,
    DeepEvalContextualRelevancyEvaluator,
    DeepEvalSummarizationEvaluator,
    DeepEvalBiasEvaluator,
    DeepEvalToxicityEvaluator,
)
```

![Screenshot 2024-04-29 at 12.15.14](/assets/img/Screenshot%202024-04-29%20at%2012.15.14_xjzmg6dsi.png)

![Screenshot 2024-04-29 at 12.16.25](/assets/img/Screenshot%202024-04-29%20at%2012.16.25_1t3jx1uox.png)

Evaluating Response Faithfulness (i.e. Hallucination)

- The `FaithfulnessEvaluator` evaluates if the answer is <font color=LightSlateBlue> faithful </font> to the retrieved contexts (in other words, whether if there's hallucination).

![Screenshot 2024-04-29 at 12.33.12](/assets/img/Screenshot%202024-04-29%20at%2012.33.12_frl8djwa0.png)

Evaluating Query + Response Relevancy

- The `RelevancyEvaluator` evaluates if the retrieved context and the answer is <font color=LightSlateBlue> relevant and consistent </font> for the given query.

![Screenshot 2024-04-29 at 12.39.39](/assets/img/Screenshot%202024-04-29%20at%2012.39.39.png)

![Screenshot 2024-04-29 at 12.39.24](/assets/img/Screenshot%202024-04-29%20at%2012.39.24.png)
